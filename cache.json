{"2024-07-19T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2407.14507v1","updated":"2024-07-19T17:59:03Z","published":"2024-07-19T17:59:03Z","title":"Internal Consistency and Self-Feedback in Large Language Models: A\n  Survey","summary":"  Large language models (LLMs) are expected to respond accurately but often\nexhibit deficient reasoning or generate hallucinatory content. To address\nthese, studies prefixed with ``Self-'' such as Self-Consistency, Self-Improve,\nand Self-Refine have been initiated. They share a commonality: involving LLMs\nevaluating and updating itself to mitigate the issues. Nonetheless, these\nefforts lack a unified perspective on summarization, as existing surveys\npredominantly focus on categorization without examining the motivations behind\nthese works.\n  In this paper, we summarize a theoretical framework, termed Internal\nConsistency, which offers unified explanations for phenomena such as the lack\nof reasoning and the presence of hallucinations. Internal Consistency assesses\nthe coherence among LLMs' latent layer, decoding layer, and response layer\nbased on sampling methodologies. Expanding upon the Internal Consistency\nframework, we introduce a streamlined yet effective theoretical framework\ncapable of mining Internal Consistency, named Self-Feedback. The Self-Feedback\nframework consists of two modules: Self-Evaluation and Self-Update. This\nframework has been employed in numerous studies.\n  We systematically classify these studies by tasks and lines of work;\nsummarize relevant evaluation methods and benchmarks; and delve into the\nconcern, ``Does Self-Feedback Really Work?'' We propose several critical\nviewpoints, including the ``Hourglass Evolution of Internal Consistency'',\n``Consistency Is (Almost) Correctness'' hypothesis, and ``The Paradox of Latent\nand Explicit Reasoning''. Furthermore, we outline promising directions for\nfuture research. We have open-sourced the experimental code, reference list,\nand statistical data, available at\n\\url{https://github.com/IAAR-Shanghai/ICSFSurvey}.\n","authors":["Xun Liang","Shichao Song","Zifan Zheng","Hanyu Wang","Qingchen Yu","Xunkai Li","Rong-Hua Li","Feiyu Xiong","Zhiyu Li"],"pdf_url":"https://arxiv.org/pdf/2407.14507v1.pdf","comment":"27 pages, 9 figures, 10 tables, 14 equations"},{"id":"http://arxiv.org/abs/2407.14506v1","updated":"2024-07-19T17:58:36Z","published":"2024-07-19T17:58:36Z","title":"On Pre-training of Multimodal Language Models Customized for Chart\n  Understanding","summary":"  Recent studies customizing Multimodal Large Language Models (MLLMs) for\ndomain-specific tasks have yielded promising results, especially in the field\nof scientific chart comprehension. These studies generally utilize visual\ninstruction tuning with specialized datasets to enhance question and answer\n(QA) accuracy within the chart domain. However, they often neglect the\nfundamental discrepancy between natural image-caption pre-training data and\ndigital chart image-QA data, particularly in the models' capacity to extract\nunderlying numeric values from charts. This paper tackles this oversight by\nexploring the training processes necessary to improve MLLMs' comprehension of\ncharts. We present three key findings: (1) Incorporating raw data values in\nalignment pre-training markedly improves comprehension of chart data. (2)\nReplacing images with their textual representation randomly during end-to-end\nfine-tuning transfer the language reasoning capability to chart interpretation\nskills. (3) Requiring the model to first extract the underlying chart data and\nthen answer the question in the fine-tuning can further improve the accuracy.\nConsequently, we introduce CHOPINLLM, an MLLM tailored for in-depth chart\ncomprehension. CHOPINLLM effectively interprets various types of charts,\nincluding unannotated ones, while maintaining robust reasoning abilities.\nFurthermore, we establish a new benchmark to evaluate MLLMs' understanding of\ndifferent chart types across various comprehension levels. Experimental results\nshow that CHOPINLLM exhibits strong performance in understanding both annotated\nand unannotated charts across a wide range of types.\n","authors":["Wan-Cyuan Fan","Yen-Chun Chen","Mengchen Liu","Lu Yuan","Leonid Sigal"],"pdf_url":"https://arxiv.org/pdf/2407.14506v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.06910v2","updated":"2024-07-19T17:47:42Z","published":"2024-04-10T11:03:17Z","title":"Superposition Prompting: Improving and Accelerating Retrieval-Augmented\n  Generation","summary":"  Despite the successes of large language models (LLMs), they exhibit\nsignificant drawbacks, particularly when processing long contexts. Their\ninference cost scales quadratically with respect to sequence length, making it\nexpensive for deployment in some real-world text processing applications, such\nas retrieval-augmented generation (RAG). Additionally, LLMs also exhibit the\n\"distraction phenomenon\", where irrelevant context in the prompt degrades\noutput quality. To address these drawbacks, we propose a novel RAG prompting\nmethodology, *superposition prompting*, which can be directly applied to\npre-trained transformer-based LLMs *without the need for fine-tuning*. At a\nhigh level, superposition prompting allows the LLM to process input documents\nin parallel *prompt paths*, discarding paths once they are deemed irrelevant.\nWe demonstrate the capability of our method to simultaneously enhance time\nefficiency across a variety of question-answering benchmarks using multiple\npre-trained LLMs. Furthermore, our technique significantly improves accuracy\nwhen the retrieved context is large relative the context the model was trained\non. For example, our approach facilitates a 93x reduction in compute time while\n*improving* accuracy by 43% on the NaturalQuestions-Open dataset with the\nMPT-7B instruction-tuned model over naive RAG.\n","authors":["Thomas Merth","Qichen Fu","Mohammad Rastegari","Mahyar Najibi"],"pdf_url":"https://arxiv.org/pdf/2404.06910v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14487v1","updated":"2024-07-19T17:41:08Z","published":"2024-07-19T17:41:08Z","title":"Evaluating the Reliability of Self-Explanations in Large Language Models","summary":"  This paper investigates the reliability of explanations generated by large\nlanguage models (LLMs) when prompted to explain their previous output. We\nevaluate two kinds of such self-explanations - extractive and counterfactual -\nusing three state-of-the-art LLMs (2B to 8B parameters) on two different\nclassification tasks (objective and subjective). Our findings reveal, that,\nwhile these self-explanations can correlate with human judgement, they do not\nfully and accurately follow the model's decision process, indicating a gap\nbetween perceived and actual model reasoning. We show that this gap can be\nbridged because prompting LLMs for counterfactual explanations can produce\nfaithful, informative, and easy-to-verify results. These counterfactuals offer\na promising alternative to traditional explainability methods (e.g. SHAP,\nLIME), provided that prompts are tailored to specific tasks and checked for\nvalidity.\n","authors":["Korbinian Randl","John Pavlopoulos","Aron Henriksson","Tony Lindgren"],"pdf_url":"https://arxiv.org/pdf/2407.14487v1.pdf","comment":"Not peer-reviewed. Under review at Discovery Science 2024"},{"id":"http://arxiv.org/abs/2407.14482v1","updated":"2024-07-19T17:35:47Z","published":"2024-07-19T17:35:47Z","title":"ChatQA 2: Bridging the Gap to Proprietary LLMs in Long Context and RAG\n  Capabilities","summary":"  In this work, we introduce ChatQA 2, a Llama3-based model designed to bridge\nthe gap between open-access LLMs and leading proprietary models (e.g.,\nGPT-4-Turbo) in long-context understanding and retrieval-augmented generation\n(RAG) capabilities. These two capabilities are essential for LLMs to process\nlarge volumes of information that cannot fit into a single prompt and are\ncomplementary to each other, depending on the downstream tasks and\ncomputational budgets. We present a detailed continued training recipe to\nextend the context window of Llama3-70B-base from 8K to 128K tokens, along with\na three-stage instruction tuning process to enhance the model's\ninstruction-following, RAG performance, and long-context understanding\ncapabilities. Our results demonstrate that the Llama3-ChatQA-2-70B model\nachieves accuracy comparable to GPT-4-Turbo-2024-0409 on many long-context\nunderstanding tasks and surpasses it on the RAG benchmark. Interestingly, we\nfind that the state-of-the-art long-context retriever can alleviate the top-k\ncontext fragmentation issue in RAG, further improving RAG-based results for\nlong-context understanding tasks. We also provide extensive comparisons between\nRAG and long-context solutions using state-of-the-art long-context LLMs.\n","authors":["Peng Xu","Wei Ping","Xianchao Wu","Zihan Liu","Mohammad Shoeybi","Bryan Catanzaro"],"pdf_url":"https://arxiv.org/pdf/2407.14482v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.11372v2","updated":"2024-07-19T17:31:58Z","published":"2023-06-20T08:27:47Z","title":"Democratizing LLMs for Low-Resource Languages by Leveraging their\n  English Dominant Abilities with Linguistically-Diverse Prompts","summary":"  Large language models (LLMs) are known to effectively perform tasks by simply\nobserving few exemplars. However, in low-resource languages, obtaining such\nhand-picked exemplars can still be challenging, where unsupervised techniques\nmay be necessary. Moreover, competent generative capabilities of LLMs are\nobserved only in high-resource languages, while their performances among\nunder-represented languages fall behind due to pre-training data imbalance. To\nelicit LLMs' ability onto low-resource languages without any supervised data,\nwe propose to assemble synthetic exemplars from a diverse set of high-resource\nlanguages to prompt the LLMs to translate from any language into English. These\nprompts are then used to create intra-lingual exemplars to perform tasks in the\ntarget languages. Our unsupervised prompting method performs on par with\nsupervised few-shot learning in LLMs of different sizes for translations\nbetween English and 13 Indic and 21 African low-resource languages. We also\nshow that fine-tuning a 7B model on data generated from our method helps it\nperform competitively with a 175B model. In non-English translation tasks, our\nmethod even outperforms supervised prompting by up to 3 chrF++ in many\nlow-resource languages. When evaluated on zero-shot multilingual summarization,\nour method surpasses other English-pivoting baselines by up to 4 ROUGE-L and is\nalso favored by GPT-4.\n","authors":["Xuan-Phi Nguyen","Sharifah Mahani Aljunied","Shafiq Joty","Lidong Bing"],"pdf_url":"https://arxiv.org/pdf/2306.11372v2.pdf","comment":"ACL 2024 Main Conference"},{"id":"http://arxiv.org/abs/2407.14467v1","updated":"2024-07-19T17:14:16Z","published":"2024-07-19T17:14:16Z","title":"Check-Eval: A Checklist-based Approach for Evaluating Text Quality","summary":"  Evaluating the quality of text generated by large language models (LLMs)\nremains a significant challenge. Traditional metrics often fail to align well\nwith human judgments, particularly in tasks requiring creativity and nuance. In\nthis paper, we propose Check-Eval, a novel evaluation framework leveraging LLMs\nto assess the quality of generated text through a checklist-based approach.\nCheck-Eval can be employed as both a reference-free and reference-dependent\nevaluation method, providing a structured and interpretable assessment of text\nquality. The framework consists of two main stages: checklist generation and\nchecklist evaluation. We validate Check-Eval on two benchmark datasets:\nPortuguese Legal Semantic Textual Similarity and SummEval. Our results\ndemonstrate that Check-Eval achieves higher correlations with human judgments\ncompared to existing metrics, such as G-Eval and GPTScore, underscoring its\npotential as a more reliable and effective evaluation framework for natural\nlanguage generation tasks. The code for our experiments is available at\nhttps://anonymous.4open.science/r/check-eval-0DB4.\n","authors":["Jayr Pereira","Roberto Lotufo"],"pdf_url":"https://arxiv.org/pdf/2407.14467v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14458v1","updated":"2024-07-19T17:01:12Z","published":"2024-07-19T17:01:12Z","title":"AudioInsight: Detecting Social Contexts Relevant to Social Anxiety from\n  Speech","summary":"  During social interactions, understanding the intricacies of the context can\nbe vital, particularly for socially anxious individuals. While previous\nresearch has found that the presence of a social interaction can be detected\nfrom ambient audio, the nuances within social contexts, which influence how\nanxiety provoking interactions are, remain largely unexplored. As an\nalternative to traditional, burdensome methods like self-report, this study\npresents a novel approach that harnesses ambient audio segments to detect\nsocial threat contexts. We focus on two key dimensions: number of interaction\npartners (dyadic vs. group) and degree of evaluative threat (explicitly\nevaluative vs. not explicitly evaluative). Building on data from a Zoom-based\nsocial interaction study (N=52 college students, of whom the majority N=45 are\nsocially anxious), we employ deep learning methods to achieve strong detection\nperformance. Under sample-wide 5-fold Cross Validation (CV), our model\ndistinguished dyadic from group interactions with 90\\% accuracy and detected\nevaluative threat at 83\\%. Using a leave-one-group-out CV, accuracies were 82\\%\nand 77\\%, respectively. While our data are based on virtual interactions due to\npandemic constraints, our method has the potential to extend to diverse\nreal-world settings. This research underscores the potential of passive sensing\nand AI to differentiate intricate social contexts, and may ultimately advance\nthe ability of context-aware digital interventions to offer personalized mental\nhealth support.\n","authors":["Varun Reddy","Zhiyuan Wang","Emma Toner","Max Larrazabal","Mehdi Boukhechba","Bethany A. Teachman","Laura E. Barnes"],"pdf_url":"https://arxiv.org/pdf/2407.14458v1.pdf","comment":"8 pages, 4 figures, 3 tables. Accepted by ACII 2024, Glasgow, UK. To\n  appear in the Proceedings of ACII 2024"},{"id":"http://arxiv.org/abs/2407.14414v1","updated":"2024-07-19T15:40:59Z","published":"2024-07-19T15:40:59Z","title":"System-1.x: Learning to Balance Fast and Slow Planning with Language\n  Models","summary":"  Language models can be used to solve long-horizon planning problems in two\ndistinct modes: a fast 'System-1' mode, directly generating plans without any\nexplicit search or backtracking, and a slow 'System-2' mode, planning\nstep-by-step by explicitly searching over possible actions. While System-2 is\ntypically more effective, it is also more computationally expensive, making it\ninfeasible for long plans or large action spaces. Moreover, isolated System-1\nor 2 ignores the user's end goals, failing to provide ways to control the\nmodel's behavior. To this end, we propose the System-1.x Planner, a\ncontrollable planning framework with LLMs that is capable of generating hybrid\nplans and balancing between the two planning modes based on the difficulty of\nthe problem at hand. System-1.x consists of (i) a controller, (ii) a System-1\nPlanner, and (iii) a System-2 Planner. Based on a user-specified hybridization\nfactor (x) governing the mixture between System-1 and 2, the controller\ndecomposes a problem into sub-goals, and classifies them as easy or hard to be\nsolved by either System-1 or 2, respectively. We fine-tune all three components\non top of a single base LLM, requiring only search traces as supervision.\nExperiments with two diverse planning tasks -- Maze Navigation and Blocksworld\n-- show that our System-1.x Planner outperforms a System-1 Planner, a System-2\nPlanner trained to approximate A* search, and also a symbolic planner (A*). We\ndemonstrate the following key properties of our planner: (1) controllability:\nincreasing the hybridization factor (e.g., System-1.75 vs 1.5) performs more\nsearch, improving performance, (2) flexibility: by building a neuro-symbolic\nvariant with a neural System-1 and a symbolic System-2, we can use existing\nsymbolic methods, and (3) generalizability: by being able to learn from\ndifferent search algorithms, our method is robust to the choice of search\nalgorithm.\n","authors":["Swarnadeep Saha","Archiki Prasad","Justin Chih-Yao Chen","Peter Hase","Elias Stengel-Eskin","Mohit Bansal"],"pdf_url":"https://arxiv.org/pdf/2407.14414v1.pdf","comment":"29 pages (10 tables)"},{"id":"http://arxiv.org/abs/2407.14402v1","updated":"2024-07-19T15:30:32Z","published":"2024-07-19T15:30:32Z","title":"The Vision of Autonomic Computing: Can LLMs Make It a Reality?","summary":"  The Vision of Autonomic Computing (ACV), proposed over two decades ago,\nenvisions computing systems that self-manage akin to biological organisms,\nadapting seamlessly to changing environments. Despite decades of research,\nachieving ACV remains challenging due to the dynamic and complex nature of\nmodern computing systems. Recent advancements in Large Language Models (LLMs)\noffer promising solutions to these challenges by leveraging their extensive\nknowledge, language understanding, and task automation capabilities. This paper\nexplores the feasibility of realizing ACV through an LLM-based multi-agent\nframework for microservice management. We introduce a five-level taxonomy for\nautonomous service maintenance and present an online evaluation benchmark based\non the Sock Shop microservice demo project to assess our framework's\nperformance. Our findings demonstrate significant progress towards achieving\nLevel 3 autonomy, highlighting the effectiveness of LLMs in detecting and\nresolving issues within microservice architectures. This study contributes to\nadvancing autonomic computing by pioneering the integration of LLMs into\nmicroservice management frameworks, paving the way for more adaptive and\nself-managing computing systems. The code will be made available at\nhttps://aka.ms/ACV-LLM.\n","authors":["Zhiyang Zhang","Fangkai Yang","Xiaoting Qin","Jue Zhang","Qingwei Lin","Gong Cheng","Dongmei Zhang","Saravan Rajmohan","Qi Zhang"],"pdf_url":"https://arxiv.org/pdf/2407.14402v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07678v2","updated":"2024-07-19T15:27:35Z","published":"2024-03-12T14:12:59Z","title":"MoralBERT: A Fine-Tuned Language Model for Capturing Moral Values in\n  Social Discussions","summary":"  Moral values play a fundamental role in how we evaluate information, make\ndecisions, and form judgements around important social issues. Controversial\ntopics, including vaccination, abortion, racism, and sexual orientation, often\nelicit opinions and attitudes that are not solely based on evidence but rather\nreflect moral worldviews. Recent advances in Natural Language Processing (NLP)\nshow that moral values can be gauged in human-generated textual content.\nBuilding on the Moral Foundations Theory (MFT), this paper introduces\nMoralBERT, a range of language representation models fine-tuned to capture\nmoral sentiment in social discourse. We describe a framework for both\naggregated and domain-adversarial training on multiple heterogeneous MFT\nhuman-annotated datasets sourced from Twitter (now X), Reddit, and Facebook\nthat broaden textual content diversity in terms of social media audience\ninterests, content presentation and style, and spreading patterns. We show that\nthe proposed framework achieves an average F1 score that is between 11% and 32%\nhigher than lexicon-based approaches, Word2Vec embeddings, and zero-shot\nclassification with large language models such as GPT-4 for in-domain\ninference. Domain-adversarial training yields better out-of domain predictions\nthan aggregate training while achieving comparable performance to zero-shot\nlearning. Our approach contributes to annotation-free and effective morality\nlearning, and provides useful insights towards a more comprehensive\nunderstanding of moral narratives in controversial social debates using NLP.\n","authors":["Vjosa Preniqi","Iacopo Ghinassi","Julia Ive","Charalampos Saitis","Kyriaki Kalimeri"],"pdf_url":"https://arxiv.org/pdf/2403.07678v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14371v1","updated":"2024-07-19T15:01:24Z","published":"2024-07-19T15:01:24Z","title":"Open Artificial Knowledge","summary":"  The tremendous success of chat-based AI systems like ChatGPT, Claude, and\nGemini stems from Large Language Models (LLMs) trained on vast amount of\ndatasets. However, acquiring high-quality, diverse, and ethically sourced\ntraining data remains a significant challenge. We introduce the Open Artificial\nKnowledge (OAK) dataset, a large-scale resource of over 500 million tokens (at\nthe moment of writing) designed to address this issue. OAK leverages an\nensemble of state-of-the-art LLMs, including GPT4o, LLaMa3-70B, LLaMa3-8B,\nMixtral-8x7B, Gemma-7B, and Gemma-2-9B , to generate high-quality text across\ndiverse domains, guided by Wikipedia's main categories. Our methodology ensures\nbroad knowledge coverage while maintaining coherence and factual accuracy. The\nOAK dataset aims to foster the development of more capable and aligned language\nmodels while addressing critical issues of data scarcity and privacy in LLM\ntraining, and it is freely available on www.oakdataset.org.\n","authors":["Vadim Borisov","Richard H. Schreiber"],"pdf_url":"https://arxiv.org/pdf/2407.14371v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14346v1","updated":"2024-07-19T14:28:53Z","published":"2024-07-19T14:28:53Z","title":"Improving Retrieval in Sponsored Search by Leveraging Query Context\n  Signals","summary":"  Accurately retrieving relevant bid keywords for user queries is critical in\nSponsored Search but remains challenging, particularly for short, ambiguous\nqueries. Existing dense and generative retrieval models often fail to capture\nnuanced user intent in these cases. To address this, we propose an approach to\nenhance query understanding by augmenting queries with rich contextual signals\nderived from web search results and large language models, stored in an online\ncache. Specifically, we use web search titles and snippets to ground queries in\nreal-world information and utilize GPT-4 to generate query rewrites and\nexplanations that clarify user intent. These signals are efficiently integrated\nthrough a Fusion-in-Decoder based Unity architecture, enabling both dense and\ngenerative retrieval with serving costs on par with traditional context-free\nmodels. To address scenarios where context is unavailable in the cache, we\nintroduce context glancing, a curriculum learning strategy that improves model\nrobustness and performance even without contextual signals during inference.\nExtensive offline experiments demonstrate that our context-aware approach\nsubstantially outperforms context-free models. Furthermore, online A/B testing\non a prominent search engine across 160+ countries shows significant\nimprovements in user engagement and revenue.\n","authors":["Akash Kumar Mohankumar","Gururaj K","Gagan Madan","Amit Singh"],"pdf_url":"https://arxiv.org/pdf/2407.14346v1.pdf","comment":"8 pages, 8 tables, 1 figure"},{"id":"http://arxiv.org/abs/2407.10086v2","updated":"2024-07-19T14:28:26Z","published":"2024-07-14T05:22:53Z","title":"Rapid Biomedical Research Classification: The Pandemic PACT Advanced\n  Categorisation Engine","summary":"  This paper introduces the Pandemic PACT Advanced Categorisation Engine\n(PPACE) along with its associated dataset. PPACE is a fine-tuned model\ndeveloped to automatically classify research abstracts from funded biomedical\nprojects according to WHO-aligned research priorities. This task is crucial for\nmonitoring research trends and identifying gaps in global health preparedness\nand response. Our approach builds on human-annotated projects, which are\nallocated one or more categories from a predefined list. A large language model\nis then used to generate `rationales' explaining the reasoning behind these\nannotations. This augmented data, comprising expert annotations and rationales,\nis subsequently used to fine-tune a smaller, more efficient model. Developed as\npart of the Pandemic PACT project, which aims to track and analyse research\nfunding and clinical evidence for a wide range of diseases with outbreak\npotential, PPACE supports informed decision-making by research funders,\npolicymakers, and independent researchers. We introduce and release both the\ntrained model and the instruction-based dataset used for its training. Our\nevaluation shows that PPACE significantly outperforms its baselines. The\nrelease of PPACE and its associated dataset offers valuable resources for\nresearchers in multilabel biomedical document classification and supports\nadvancements in aligning biomedical research with key global health priorities.\n","authors":["Omid Rohanian","Mohammadmahdi Nouriborji","Olena Seminog","Rodrigo Furst","Thomas Mendy","Shanthi Levanita","Zaharat Kadri-Alabi","Nusrat Jabin","Daniela Toale","Georgina Humphreys","Emilia Antonio","Adrian Bucher","Alice Norton","David A. Clifton"],"pdf_url":"https://arxiv.org/pdf/2407.10086v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14344v1","updated":"2024-07-19T14:28:07Z","published":"2024-07-19T14:28:07Z","title":"LLMs left, right, and center: Assessing GPT's capabilities to label\n  political bias from web domains","summary":"  This research investigates whether OpenAI's GPT-4, a state-of-the-art large\nlanguage model, can accurately classify the political bias of news sources\nbased solely on their URLs. Given the subjective nature of political labels,\nthird-party bias ratings like those from Ad Fontes Media, AllSides, and Media\nBias/Fact Check (MBFC) are often used in research to analyze news source\ndiversity. This study aims to determine if GPT-4 can replicate these human\nratings on a seven-degree scale (\"far-left\" to \"far-right\"). The analysis\ncompares GPT-4's classifications against MBFC's, and controls for website\npopularity using Open PageRank scores. Findings reveal a high correlation\n($\\text{Spearman's } \\rho = .89$, $n = 5,877$, $p < 0.001$) between GPT-4's and\nMBFC's ratings, indicating the model's potential reliability. However, GPT-4\nabstained from classifying approximately $\\frac{2}{3}$ of the dataset,\nparticularly less popular and less biased sources. The study also identifies a\nslight leftward skew in GPT-4's classifications compared to MBFC's. The\nanalysis suggests that while GPT-4 can be a scalable, cost-effective tool for\npolitical bias classification of news websites, but its use should complement\nhuman judgment to mitigate biases. Further research is recommended to explore\nthe model's performance across different settings, languages, and additional\ndatasets.\n","authors":["Raphael Hernandes"],"pdf_url":"https://arxiv.org/pdf/2407.14344v1.pdf","comment":"12 pages, 4 figures"},{"id":"http://arxiv.org/abs/2407.12878v2","updated":"2024-07-19T14:24:47Z","published":"2024-07-16T08:58:00Z","title":"Do LLMs have Consistent Values?","summary":"  Values are a basic driving force underlying human behavior. Large Language\nModels (LLM) technology is constantly improving towards human-like dialogue.\nHowever, little research has been done to study the values exhibited in text\ngenerated by LLMs. Here we study this question by turning to the rich\nliterature on value structure in psychology. We ask whether LLMs exhibit the\nsame value structure that has been demonstrated in humans, including the\nranking of values, and correlation between values. We show that the results of\nthis analysis strongly depend on how the LLM is prompted, and that under a\nparticular prompting strategy (referred to as 'Value Anchoring') the agreement\nwith human data is quite compelling. Our results serve both to improve our\nunderstanding of values in LLMs, as well as introduce novel methods for\nassessing consistency in LLM responses.\n","authors":["Naama Rozen","Gal Elidan","Amir Globerson","Ella Daniel"],"pdf_url":"https://arxiv.org/pdf/2407.12878v2.pdf","comment":"10 pages, 5 figures, and there are more in the appendix"},{"id":"http://arxiv.org/abs/2407.11282v3","updated":"2024-07-19T14:16:35Z","published":"2024-07-15T23:41:11Z","title":"Uncertainty is Fragile: Manipulating Uncertainty in Large Language\n  Models","summary":"  Large Language Models (LLMs) are employed across various high-stakes domains,\nwhere the reliability of their outputs is crucial. One commonly used method to\nassess the reliability of LLMs' responses is uncertainty estimation, which\ngauges the likelihood of their answers being correct. While many studies focus\non improving the accuracy of uncertainty estimations for LLMs, our research\ninvestigates the fragility of uncertainty estimation and explores potential\nattacks. We demonstrate that an attacker can embed a backdoor in LLMs, which,\nwhen activated by a specific trigger in the input, manipulates the model's\nuncertainty without affecting the final output. Specifically, the proposed\nbackdoor attack method can alter an LLM's output probability distribution,\ncausing the probability distribution to converge towards an attacker-predefined\ndistribution while ensuring that the top-1 prediction remains unchanged. Our\nexperimental results demonstrate that this attack effectively undermines the\nmodel's self-evaluation reliability in multiple-choice questions. For instance,\nwe achieved a 100 attack success rate (ASR) across three different triggering\nstrategies in four models. Further, we investigate whether this manipulation\ngeneralizes across different prompts and domains. This work highlights a\nsignificant threat to the reliability of LLMs and underscores the need for\nfuture defenses against such attacks. The code is available at\nhttps://github.com/qcznlp/uncertainty_attack.\n","authors":["Qingcheng Zeng","Mingyu Jin","Qinkai Yu","Zhenting Wang","Wenyue Hua","Zihao Zhou","Guangyan Sun","Yanda Meng","Shiqing Ma","Qifan Wang","Felix Juefei-Xu","Kaize Ding","Fan Yang","Ruixiang Tang","Yongfeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2407.11282v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14270v2","updated":"2024-07-19T14:07:25Z","published":"2024-03-21T10:15:57Z","title":"Scene-Graph ViT: End-to-End Open-Vocabulary Visual Relationship\n  Detection","summary":"  Visual relationship detection aims to identify objects and their\nrelationships in images. Prior methods approach this task by adding separate\nrelationship modules or decoders to existing object detection architectures.\nThis separation increases complexity and hinders end-to-end training, which\nlimits performance. We propose a simple and highly efficient decoder-free\narchitecture for open-vocabulary visual relationship detection. Our model\nconsists of a Transformer-based image encoder that represents objects as tokens\nand models their relationships implicitly. To extract relationship information,\nwe introduce an attention mechanism that selects object pairs likely to form a\nrelationship. We provide a single-stage recipe to train this model on a mixture\nof object and relationship detection data. Our approach achieves\nstate-of-the-art relationship detection performance on Visual Genome and on the\nlarge-vocabulary GQA benchmark at real-time inference speeds. We provide\nablations, real-world qualitative examples, and analyses of zero-shot\nperformance.\n","authors":["Tim Salzmann","Markus Ryll","Alex Bewley","Matthias Minderer"],"pdf_url":"https://arxiv.org/pdf/2403.14270v2.pdf","comment":"ECCV camera-ready; changed to graph-constrained results"},{"id":"http://arxiv.org/abs/2311.04131v5","updated":"2024-07-19T13:57:52Z","published":"2023-11-07T16:58:51Z","title":"Towards Interpretable Sequence Continuation: Analyzing Shared Circuits\n  in Large Language Models","summary":"  While transformer models exhibit strong capabilities on linguistic tasks,\ntheir complex architectures make them difficult to interpret. Recent work has\naimed to reverse engineer transformer models into human-readable\nrepresentations called circuits that implement algorithmic functions. We extend\nthis research by analyzing and comparing circuits for similar sequence\ncontinuation tasks, which include increasing sequences of Arabic numerals,\nnumber words, and months. By applying circuit interpretability analysis, we\nidentify a key sub-circuit in both GPT-2 Small and Llama-2-7B responsible for\ndetecting sequence members and for predicting the next member in a sequence.\nOur analysis reveals that semantically related sequences rely on shared circuit\nsubgraphs with analogous roles. Additionally, we show that this sub-circuit has\neffects on various math-related prompts, such as on intervaled circuits,\nSpanish number word and months continuation, and natural language word\nproblems. Overall, documenting shared computational structures enables better\nmodel behavior predictions, identification of errors, and safer editing\nprocedures. This mechanistic understanding of transformers is a critical step\ntowards building more robust, aligned, and interpretable language models.\n","authors":["Michael Lan","Philip Torr","Fazl Barez"],"pdf_url":"https://arxiv.org/pdf/2311.04131v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14321v1","updated":"2024-07-19T13:57:11Z","published":"2024-07-19T13:57:11Z","title":"Multimodal Misinformation Detection using Large Vision-Language Models","summary":"  The increasing proliferation of misinformation and its alarming impact have\nmotivated both industry and academia to develop approaches for misinformation\ndetection and fact checking. Recent advances on large language models (LLMs)\nhave shown remarkable performance in various tasks, but whether and how LLMs\ncould help with misinformation detection remains relatively underexplored. Most\nof existing state-of-the-art approaches either do not consider evidence and\nsolely focus on claim related features or assume the evidence to be provided.\nFew approaches consider evidence retrieval as part of the misinformation\ndetection but rely on fine-tuning models. In this paper, we investigate the\npotential of LLMs for misinformation detection in a zero-shot setting. We\nincorporate an evidence retrieval component into the process as it is crucial\nto gather pertinent information from various sources to detect the veracity of\nclaims. To this end, we propose a novel re-ranking approach for multimodal\nevidence retrieval using both LLMs and large vision-language models (LVLM). The\nretrieved evidence samples (images and texts) serve as the input for an\nLVLM-based approach for multimodal fact verification (LVLM4FV). To enable a\nfair evaluation, we address the issue of incomplete ground truth for evidence\nsamples in an existing evidence retrieval dataset by annotating a more complete\nset of evidence samples for both image and text retrieval. Our experimental\nresults on two datasets demonstrate the superiority of the proposed approach in\nboth evidence retrieval and fact verification tasks and also better\ngeneralization capability across dataset compared to the supervised baseline.\n","authors":["Sahar Tahmasebi","Eric Müller-Budack","Ralph Ewerth"],"pdf_url":"https://arxiv.org/pdf/2407.14321v1.pdf","comment":"Accepted for publication in: Conference on Information and Knowledge\n  Management (CIKM) 2024"},{"id":"http://arxiv.org/abs/2403.00811v2","updated":"2024-07-19T13:47:15Z","published":"2024-02-25T02:35:56Z","title":"Cognitive Bias in High-Stakes Decision-Making with LLMs","summary":"  Large language models (LLMs) offer significant potential as tools to support\nan expanding range of decision-making tasks. Given their training on human\n(created) data, LLMs have been shown to inherit societal biases against\nprotected groups, as well as be subject to bias functionally resembling\ncognitive bias. Human-like bias can impede fair and explainable decisions made\nwith LLM assistance. Our work introduces BiasBuster, a framework designed to\nuncover, evaluate, and mitigate cognitive bias in LLMs, particularly in\nhigh-stakes decision-making tasks. Inspired by prior research in psychology and\ncognitive science, we develop a dataset containing 16,800 prompts to evaluate\ndifferent cognitive biases (e.g., prompt-induced, sequential, inherent). We\ntest various bias mitigation strategies, amidst proposing a novel method\nutilising LLMs to debias their own prompts. Our analysis provides a\ncomprehensive picture of the presence and effects of cognitive bias across\ncommercial and open-source models. We demonstrate that our self-help debiasing\neffectively mitigates model answers that display patterns akin to human\ncognitive bias without having to manually craft examples for each bias.\n","authors":["Jessica Echterhoff","Yao Liu","Abeer Alessa","Julian McAuley","Zexue He"],"pdf_url":"https://arxiv.org/pdf/2403.00811v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14309v1","updated":"2024-07-19T13:42:56Z","published":"2024-07-19T13:42:56Z","title":"How to Engage Your Readers? Generating Guiding Questions to Promote\n  Active Reading","summary":"  Using questions in written text is an effective strategy to enhance\nreadability. However, what makes an active reading question good, what the\nlinguistic role of these questions is, and what is their impact on human\nreading remains understudied. We introduce GuidingQ, a dataset of 10K in-text\nquestions from textbooks and scientific articles. By analyzing the dataset, we\npresent a comprehensive understanding of the use, distribution, and linguistic\ncharacteristics of these questions. Then, we explore various approaches to\ngenerate such questions using language models. Our results highlight the\nimportance of capturing inter-question relationships and the challenge of\nquestion position identification in generating these questions. Finally, we\nconduct a human study to understand the implication of such questions on\nreading comprehension. We find that the generated questions are of high quality\nand are almost as effective as human-written questions in terms of improving\nreaders' memorization and comprehension.\n","authors":["Peng Cui","Vilém Zouhar","Xiaoyu Zhang","Mrinmaya Sachan"],"pdf_url":"https://arxiv.org/pdf/2407.14309v1.pdf","comment":"arXiv admin note: text overlap with arXiv:1504.00704 by other authors"},{"id":"http://arxiv.org/abs/2407.14296v1","updated":"2024-07-19T13:26:52Z","published":"2024-07-19T13:26:52Z","title":"Foundation Models for Autonomous Robots in Unstructured Environments","summary":"  Automating activities through robots in unstructured environments, such as\nconstruction sites, has been a long-standing desire. However, the high degree\nof unpredictable events in these settings has resulted in far less adoption\ncompared to more structured settings, such as manufacturing, where robots can\nbe hard-coded or trained on narrowly defined datasets. Recently, pretrained\nfoundation models, such as Large Language Models (LLMs), have demonstrated\nsuperior generalization capabilities by providing zero-shot solutions for\nproblems do not present in the training data, proposing them as a potential\nsolution for introducing robots to unstructured environments. To this end, this\nstudy investigates potential opportunities and challenges of pretrained\nfoundation models from a multi-dimensional perspective. The study\nsystematically reviews application of foundation models in two field of robotic\nand unstructured environment and then synthesized them with deliberative acting\ntheory. Findings showed that linguistic capabilities of LLMs have been utilized\nmore than other features for improving perception in human-robot interactions.\nOn the other hand, findings showed that the use of LLMs demonstrated more\napplications in project management and safety in construction, and natural\nhazard detection in disaster management. Synthesizing these findings, we\nlocated the current state-of-the-art in this field on a five-level scale of\nautomation, placing them at conditional automation. This assessment was then\nused to envision future scenarios, challenges, and solutions toward autonomous\nsafe unstructured environments. Our study can be seen as a benchmark to track\nour progress toward that future.\n","authors":["Hossein Naderi","Alireza Shojaei"],"pdf_url":"https://arxiv.org/pdf/2407.14296v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2312.07843,\n  arXiv:2402.05741 by other authors"},{"id":"http://arxiv.org/abs/2407.14295v1","updated":"2024-07-19T13:26:35Z","published":"2024-07-19T13:26:35Z","title":"CoVoSwitch: Machine Translation of Synthetic Code-Switched Text Based on\n  Intonation Units","summary":"  Multilingual code-switching research is often hindered by the lack and\nlinguistically biased status of available datasets. To expand language\nrepresentation, we synthesize code-switching data by replacing intonation units\ndetected through PSST, a speech segmentation model fine-tuned from OpenAI's\nWhisper, using a speech-to-text translation dataset, CoVoST 2. With our\ndataset, CoVoSwitch, spanning 13 languages, we evaluate the code-switching\ntranslation performance of two multilingual translation models, M2M-100 418M\nand NLLB-200 600M. We reveal that the inclusion of code-switching units results\nin higher translation performance than monolingual settings and that models are\nbetter at code-switching translation into English than non-English. Further,\nlow-resource languages gain most from integration of code-switched units when\ntranslating into English but much less when translating into non-English.\nTranslations into low-resource languages also perform worse than even raw\ncode-switched inputs. We find that systems excel at copying English tokens but\nstruggle with non-English tokens, that the off-target problem in monolingual\nsettings is also relevant in code-switching settings, and that models\nhallucinate in code-switching translation by introducing words absent in both\nof the original source sentences. CoVoSwitch and code are available at\nhttps://github.com/sophiayk20/covoswitch.\n","authors":["Yeeun Kang"],"pdf_url":"https://arxiv.org/pdf/2407.14295v1.pdf","comment":"Accepted to ACL 2024 Student Research Workshop (ACL-SRW 2024)"},{"id":"http://arxiv.org/abs/2406.00799v4","updated":"2024-07-19T13:07:25Z","published":"2024-06-02T16:53:21Z","title":"Are you still on track!? Catching LLM Task Drift with Activations","summary":"  Large Language Models (LLMs) are routinely used in retrieval-augmented\napplications to orchestrate tasks and process inputs from users and other\nsources. These inputs, even in a single LLM interaction, can come from a\nvariety of sources, of varying trustworthiness and provenance. This opens the\ndoor to prompt injection attacks, where the LLM receives and acts upon\ninstructions from supposedly data-only sources, thus deviating from the user's\noriginal instructions. We define this as task drift, and we propose to catch it\nby scanning and analyzing the LLM's activations. We compare the LLM's\nactivations before and after processing the external input in order to detect\nwhether this input caused instruction drift. We develop two probing methods and\nfind that simply using a linear classifier can detect drift with near perfect\nROC AUC on an out-of-distribution test set. We show that this approach\ngeneralizes surprisingly well to unseen task domains, such as prompt\ninjections, jailbreaks, and malicious instructions, without being trained on\nany of these attacks. Our setup does not require any modification of the LLM\n(e.g., fine-tuning) or any text generation, thus maximizing deployability and\ncost efficiency and avoiding reliance on unreliable model output. To foster\nfuture research on activation-based task inspection, decoding, and\ninterpretability, we will release our large-scale TaskTracker toolkit,\ncomprising a dataset of over 500K instances, representations from 5 SoTA\nlanguage models, and inspection tools.\n","authors":["Sahar Abdelnabi","Aideen Fay","Giovanni Cherubin","Ahmed Salem","Mario Fritz","Andrew Paverd"],"pdf_url":"https://arxiv.org/pdf/2406.00799v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.06800v2","updated":"2024-07-19T13:07:06Z","published":"2024-07-09T12:14:48Z","title":"Learn and Don't Forget: Adding a New Language to ASR Foundation Models","summary":"  Foundation ASR models often support many languages, e.g. 100 languages in\nWhisper. However, there has been limited work on integrating an additional,\ntypically low-resource, language, while maintaining performance on the original\nlanguage set. Fine-tuning, while simple, may degrade the accuracy of the\noriginal set. We compare three approaches that exploit adaptation parameters:\nsoft language code tuning, train only the language code; soft prompt tuning,\ntrain prepended tokens; and LoRA where a small set of additional parameters are\noptimised. Elastic Weight Consolidation (EWC) offers an alternative compromise\nwith the potential to maintain performance in specific target languages.\nResults show that direct fine-tuning yields the best performance for the new\nlanguage but degrades existing language capabilities. EWC can address this\nissue for specific languages. If only adaptation parameters are used, the\nlanguage capabilities are maintained but at the cost of performance in the new\nlanguage.\n","authors":["Mengjie Qian","Siyuan Tang","Rao Ma","Kate M. Knill","Mark J. F. Gales"],"pdf_url":"https://arxiv.org/pdf/2407.06800v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.11969v2","updated":"2024-07-19T13:03:01Z","published":"2024-07-16T17:59:55Z","title":"Does Refusal Training in LLMs Generalize to the Past Tense?","summary":"  Refusal training is widely used to prevent LLMs from generating harmful,\nundesirable, or illegal outputs. We reveal a curious generalization gap in the\ncurrent refusal training approaches: simply reformulating a harmful request in\nthe past tense (e.g., \"How to make a Molotov cocktail?\" to \"How did people make\na Molotov cocktail?\") is often sufficient to jailbreak many state-of-the-art\nLLMs. We systematically evaluate this method on Llama-3 8B, Claude-3.5 Sonnet,\nGPT-3.5 Turbo, Gemma-2 9B, Phi-3-Mini, GPT-4o mini, GPT-4o, and R2D2 models\nusing GPT-3.5 Turbo as a reformulation model. For example, the success rate of\nthis simple attack on GPT-4o increases from 1% using direct requests to 88%\nusing 20 past tense reformulation attempts on harmful requests from\nJailbreakBench with GPT-4 as a jailbreak judge. Interestingly, we also find\nthat reformulations in the future tense are less effective, suggesting that\nrefusal guardrails tend to consider past historical questions more benign than\nhypothetical future questions. Moreover, our experiments on fine-tuning GPT-3.5\nTurbo show that defending against past reformulations is feasible when past\ntense examples are explicitly included in the fine-tuning data. Overall, our\nfindings highlight that the widely used alignment techniques -- such as SFT,\nRLHF, and adversarial training -- employed to align the studied models can be\nbrittle and do not always generalize as intended. We provide code and jailbreak\nartifacts at https://github.com/tml-epfl/llm-past-tense.\n","authors":["Maksym Andriushchenko","Nicolas Flammarion"],"pdf_url":"https://arxiv.org/pdf/2407.11969v2.pdf","comment":"Update in v2: Claude-3.5 Sonnet and GPT-4o mini. We provide code and\n  jailbreak artifacts at https://github.com/tml-epfl/llm-past-tense"},{"id":"http://arxiv.org/abs/2406.17663v2","updated":"2024-07-19T12:59:11Z","published":"2024-06-25T15:52:15Z","title":"LLM-ARC: Enhancing LLMs with an Automated Reasoning Critic","summary":"  We introduce LLM-ARC, a neuro-symbolic framework designed to enhance the\nlogical reasoning capabilities of Large Language Models (LLMs), by combining\nthem with an Automated Reasoning Critic (ARC). LLM-ARC employs an Actor-Critic\nmethod where the LLM Actor generates declarative logic programs along with\ntests for semantic correctness, while the Automated Reasoning Critic evaluates\nthe code, runs the tests and provides feedback on test failures for iterative\nrefinement. Implemented using Answer Set Programming (ASP), LLM-ARC achieves a\nnew state-of-the-art accuracy of 88.32% on the FOLIO benchmark which tests\ncomplex logical reasoning capabilities. Our experiments demonstrate significant\nimprovements over LLM-only baselines, highlighting the importance of logic test\ngeneration and iterative self-refinement. We achieve our best result using a\nfully automated self-supervised training loop where the Actor is trained on\nend-to-end dialog traces with Critic feedback. We discuss potential\nenhancements and provide a detailed error analysis, showcasing the robustness\nand efficacy of LLM-ARC for complex natural language reasoning tasks.\n","authors":["Aditya Kalyanpur","Kailash Karthik Saravanakumar","Victor Barres","Jennifer Chu-Carroll","David Melville","David Ferrucci"],"pdf_url":"https://arxiv.org/pdf/2406.17663v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.00067v2","updated":"2024-07-19T12:40:49Z","published":"2024-02-29T19:00:47Z","title":"Query-OPT: Optimizing Inference of Large Language Models via Multi-Query\n  Instructions in Meeting Summarization","summary":"  This work focuses on the task of query-based meeting summarization in which\nthe summary of a context (meeting transcript) is generated in response to a\nspecific query. When using Large Language Models (LLMs) for this task, usually\na new call to the LLM inference endpoint/API is triggered for each new query,\neven if the context stays the same. However, repeated calls to the LLM\ninference endpoints would significantly increase the costs of using them in\nproduction, making LLMs impractical for many real-world use cases. To address\nthis problem, in this paper, we investigate whether combining the queries for\nthe same input context in a single prompt to minimize repeated calls can be\nsuccessfully used in meeting summarization. In this regard, we conduct\nextensive experiments by comparing the performance of various popular LLMs:\nGPT-4, Gemini, Claude-3, LLaMA-2, Mistral, Phi-3, and Qwen-2 in single-query\nand multi-query settings. We observe that 100% reliability in generating the\nresponse in the expected format is usually limited to certain closed-source\nLLMs, with most open-source LLMs lagging behind (except a few 7B parameters\nLLMs like Mistral and Phi-3). We conclude that multi-query prompting could be\nuseful to significantly optimize the inference costs in meeting summarization.\n","authors":["Md Tahmid Rahman Laskar","Elena Khasanova","Xue-Yong Fu","Cheng Chen","Shashi Bhushan TN"],"pdf_url":"https://arxiv.org/pdf/2403.00067v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14259v1","updated":"2024-07-19T12:37:15Z","published":"2024-07-19T12:37:15Z","title":"Voices in a Crowd: Searching for Clusters of Unique Perspectives","summary":"  Language models have been shown to reproduce underlying biases existing in\ntheir training data, which is the majority perspective by default. Proposed\nsolutions aim to capture minority perspectives by either modelling annotator\ndisagreements or grouping annotators based on shared metadata, both of which\nface significant challenges. We propose a framework that trains models without\nencoding annotator metadata, extracts latent embeddings informed by annotator\nbehaviour, and creates clusters of similar opinions, that we refer to as\nvoices. Resulting clusters are validated post-hoc via internal and external\nquantitative metrics, as well a qualitative analysis to identify the type of\nvoice that each cluster represents. Our results demonstrate the strong\ngeneralisation capability of our framework, indicated by resulting clusters\nbeing adequately robust, while also capturing minority perspectives based on\ndifferent demographic factors throughout two distinct datasets.\n","authors":["Nikolas Vitsakis","Amit Parekh","Ioannis Konstas"],"pdf_url":"https://arxiv.org/pdf/2407.14259v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14246v1","updated":"2024-07-19T12:28:22Z","published":"2024-07-19T12:28:22Z","title":"Conditioning Chat-GPT for information retrieval: the Unipa-GPT case\n  study","summary":"  This paper illustrates the architecture and training of Unipa-GPT, a chatbot\nrelying on a Large Language Model, developed for assisting students in choosing\na bachelor/master degree course at the University of Palermo. Unipa-GPT relies\non gpt-3.5-turbo, it was presented in the context of the European Researchers'\nNight (SHARPER night). In our experiments we adopted both the Retrieval\nAugmented Generation (RAG) approach and fine-tuning to develop the system. The\nwhole architecture of Unipa-GPT is presented, both the RAG and the fine-tuned\nsystems are compared, and a brief discussion on their performance is reported.\nFurther comparison with other Large Language Models and the experimental\nresults during the SHARPER night are illustrated.\n","authors":["Irene Siragusa","Roberto Pirrone"],"pdf_url":"https://arxiv.org/pdf/2407.14246v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14224v1","updated":"2024-07-19T11:48:36Z","published":"2024-07-19T11:48:36Z","title":"Hierarchical Windowed Graph Attention Network and a Large Scale Dataset\n  for Isolated Indian Sign Language Recognition","summary":"  Automatic Sign Language (SL) recognition is an important task in the computer\nvision community. To build a robust SL recognition system, we need a\nconsiderable amount of data which is lacking particularly in Indian sign\nlanguage (ISL). In this paper, we propose a large-scale isolated ISL dataset\nand a novel SL recognition model based on skeleton graph structure. The dataset\ncovers 2,002 daily used common words in the deaf community recorded by 20 (10\nmale and 10 female) deaf adult signers (contains 40033 videos). We propose a SL\nrecognition model namely Hierarchical Windowed Graph Attention Network (HWGAT)\nby utilizing the human upper body skeleton graph structure. The HWGAT tries to\ncapture distinctive motions by giving attention to different body parts induced\nby the human skeleton graph structure. The utility of the proposed dataset and\nthe usefulness of our model are evaluated through extensive experiments. We\npre-trained the proposed model on the proposed dataset and fine-tuned it across\ndifferent sign language datasets further boosting the performance of 1.10,\n0.46, 0.78, and 6.84 percentage points on INCLUDE, LSA64, AUTSL and WLASL\nrespectively compared to the existing state-of-the-art skeleton-based models.\n","authors":["Suvajit Patra","Arkadip Maitra","Megha Tiwari","K. Kumaran","Swathy Prabhu","Swami Punyeshwarananda","Soumitra Samanta"],"pdf_url":"https://arxiv.org/pdf/2407.14224v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.00259v2","updated":"2024-07-19T11:48:21Z","published":"2023-09-30T05:20:02Z","title":"AutoHall: Automated Hallucination Dataset Generation for Large Language\n  Models","summary":"  While Large language models (LLMs) have garnered widespread applications\nacross various domains due to their powerful language understanding and\ngeneration capabilities, the detection of non-factual or hallucinatory content\ngenerated by LLMs remains scarce. Currently, one significant challenge in\nhallucination detection is the laborious task of time-consuming and expensive\nmanual annotation of the hallucinatory generation. To address this issue, this\npaper first introduces a method for automatically constructing model-specific\nhallucination datasets based on existing fact-checking datasets called\nAutoHall. Furthermore, we propose a zero-resource and black-box hallucination\ndetection method based on self-contradiction. We conduct experiments towards\nprevalent open-/closed-source LLMs, achieving superior hallucination detection\nperformance compared to extant baselines. Moreover, our experiments reveal\nvariations in hallucination proportions and types among different models.\n","authors":["Zouying Cao","Yifei Yang","Hai Zhao"],"pdf_url":"https://arxiv.org/pdf/2310.00259v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.05550v2","updated":"2024-07-19T11:32:32Z","published":"2023-11-09T17:49:02Z","title":"Towards End-to-End Spoken Grammatical Error Correction","summary":"  Grammatical feedback is crucial for L2 learners, teachers, and testers.\nSpoken grammatical error correction (GEC) aims to supply feedback to L2\nlearners on their use of grammar when speaking. This process usually relies on\na cascaded pipeline comprising an ASR system, disfluency removal, and GEC, with\nthe associated concern of propagating errors between these individual modules.\nIn this paper, we introduce an alternative \"end-to-end\" approach to spoken GEC,\nexploiting a speech recognition foundation model, Whisper. This foundation\nmodel can be used to replace the whole framework or part of it, e.g., ASR and\ndisfluency removal. These end-to-end approaches are compared to more standard\ncascaded approaches on the data obtained from a free-speaking spoken language\nassessment test, Linguaskill. Results demonstrate that end-to-end spoken GEC is\npossible within this architecture, but the lack of available data limits\ncurrent performance compared to a system using large quantities of text-based\nGEC data. Conversely, end-to-end disfluency detection and removal, which is\neasier for the attention-based Whisper to learn, does outperform cascaded\napproaches. Additionally, the paper discusses the challenges of providing\nfeedback to candidates when using end-to-end systems for spoken GEC.\n","authors":["Stefano Bannò","Rao Ma","Mengjie Qian","Kate M. Knill","Mark J. F. Gales"],"pdf_url":"https://arxiv.org/pdf/2311.05550v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14212v1","updated":"2024-07-19T11:18:44Z","published":"2024-07-19T11:18:44Z","title":"Braille-to-Speech Generator: Audio Generation Based on Joint Fine-Tuning\n  of CLIP and Fastspeech2","summary":"  An increasing number of Chinese people are troubled by different degrees of\nvisual impairment, which has made the modal conversion between a single image\nor video frame in the visual field and the audio expressing the same\ninformation a research hotspot. Deep learning technologies such as OCR+Vocoder\nand Im2Wav enable English audio synthesis or image-to-sound matching in a\nself-supervised manner. However, the audio data used for training is limited\nand English is not universal for visually impaired people with different\neducational levels. Therefore, for the sake of solving the problems of data\nvolume and language applicability to improve the reading efficiency of visually\nimpaired people, a set of image-to-speech framework CLIP-KNN-Fastspeech2 based\non the Chinese context was constructed. The framework integrates multiple basic\nmodels and adopts the strategy of independent pre-training and joint\nfine-tuning. First, the Chinese CLIP and Fastspeech2 text-to-speech models were\npre-trained on two public datasets, MUGE and Baker, respectively, and their\nconvergence was verified. Subsequently, joint fine-tuning was performed using a\nself-built Braille image dataset. Experimental results on multiple public\ndatasets such as VGGSound, Flickr8k, ImageHear, and the self-built Braille\ndataset BIT-DP show that the model has improved objective indicators such as\nBLEU4,FAD(Fr\\'echet Audio Distance), WER(Word Error Ratio), and even inference\nspeed. This verifies that the constructed model still has the ability to\nsynthesize high-quality speech under limited data, and also proves the\neffectiveness of the joint training strategy that integrates multiple basic\nmodels.\n","authors":["Chun Xu","En-Wei Sun"],"pdf_url":"https://arxiv.org/pdf/2407.14212v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.01490v2","updated":"2024-07-19T10:45:21Z","published":"2024-07-01T17:26:21Z","title":"LLM See, LLM Do: Guiding Data Generation to Target Non-Differentiable\n  Objectives","summary":"  The widespread adoption of synthetic data raises new questions about how\nmodels generating the data can influence other large language models (LLMs) via\ndistilled data. To start, our work exhaustively characterizes the impact of\npassive inheritance of model properties by systematically studying the\nconsequences of synthetic data integration. We provide one of the most\ncomprehensive studies to-date of how the source of synthetic data shapes\nmodels' internal biases, calibration and generations' textual attributes and\npreferences. We find that models are surprisingly sensitive towards certain\nattributes even when the synthetic data prompts appear \"neutral\". which invites\nthe question whether this sensitivity can be exploited for good.\n  Our findings invite the question can we explicitly steer the models towards\nthe properties we want at test time by exploiting the data generation process?\nThis would have historically been considered infeasible due to the cost of\ncollecting data with a specific characteristic or objective in mind. However,\nimprovement in the quality of synthetic data, as well as a shift towards\ngeneral-purpose models designed to follow a diverse way of instructions, means\nthis question is timely. We propose active inheritance as a term to describe\nintentionally constraining synthetic data according to a non-differentiable\nobjective. We demonstrate how active inheritance can steer the generation\nprofiles of models towards desirable non-differentiable attributes, e.g. high\nlexical diversity or low toxicity.\n","authors":["Luísa Shimabucoro","Sebastian Ruder","Julia Kreutzer","Marzieh Fadaee","Sara Hooker"],"pdf_url":"https://arxiv.org/pdf/2407.01490v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14192v1","updated":"2024-07-19T10:40:10Z","published":"2024-07-19T10:40:10Z","title":"LeKUBE: A Legal Knowledge Update BEnchmark","summary":"  Recent advances in Large Language Models (LLMs) have significantly shaped the\napplications of AI in multiple fields, including the studies of legal\nintelligence. Trained on extensive legal texts, including statutes and legal\ndocuments, the legal LLMs can capture important legal knowledge/concepts\neffectively and provide important support for downstream legal applications\nsuch as legal consultancy. Yet, the dynamic nature of legal statutes and\ninterpretations also poses new challenges to the use of LLMs in legal\napplications. Particularly, how to update the legal knowledge of LLMs\neffectively and efficiently has become an important research problem in\npractice. Existing benchmarks for evaluating knowledge update methods are\nmostly designed for the open domain and cannot address the specific challenges\nof the legal domain, such as the nuanced application of new legal knowledge,\nthe complexity and lengthiness of legal regulations, and the intricate nature\nof legal reasoning. To address this gap, we introduce the Legal Knowledge\nUpdate BEnchmark, i.e. LeKUBE, which evaluates knowledge update methods for\nlegal LLMs across five dimensions. Specifically, we categorize the needs of\nknowledge updates in the legal domain with the help of legal professionals, and\nthen hire annotators from law schools to create synthetic updates to the\nChinese Criminal and Civil Code as well as sets of questions of which the\nanswers would change after the updates. Through a comprehensive evaluation of\nstate-of-the-art knowledge update methods, we reveal a notable gap between\nexisting knowledge update methods and the unique needs of the legal domain,\nemphasizing the need for further research and development of knowledge update\nmechanisms tailored for legal LLMs.\n","authors":["Changyue Wang","Weihang Su","Hu Yiran","Qingyao Ai","Yueyue Wu","Cheng Luo","Yiqun Liu","Min Zhang","Shaoping Ma"],"pdf_url":"https://arxiv.org/pdf/2407.14192v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.00476v2","updated":"2024-07-19T10:26:02Z","published":"2024-06-29T15:47:28Z","title":"Large Language Models for Power Scheduling: A User-Centric Approach","summary":"  While traditional optimization and scheduling schemes are designed to meet\nfixed, predefined system requirements, future systems are moving toward\nuser-driven approaches and personalized services, aiming to achieve high\nquality-of-experience (QoE) and flexibility. This challenge is particularly\npronounced in wireless and digitalized energy networks, where users'\nrequirements have largely not been taken into consideration due to the lack of\na common language between users and machines. The emergence of powerful large\nlanguage models (LLMs) marks a radical departure from traditional\nsystem-centric methods into more advanced user-centric approaches by providing\na natural communication interface between users and devices. In this paper, for\nthe first time, we introduce a novel architecture for resource scheduling\nproblems by constructing three LLM agents to convert an arbitrary user's voice\nrequest (VRQ) into a resource allocation vector. Specifically, we design an LLM\nintent recognition agent to translate the request into an optimization problem\n(OP), an LLM OP parameter identification agent, and an LLM OP solving agent. To\nevaluate system performance, we construct a database of typical VRQs in the\ncontext of electric vehicle (EV) charging. As a proof of concept, we primarily\nuse Llama 3 8B. Through testing with different prompt engineering scenarios,\nthe obtained results demonstrate the efficiency of the proposed architecture.\nThe conducted performance analysis allows key insights to be extracted. For\ninstance, having a larger set of candidate OPs to model the real-world problem\nmight degrade the final performance because of a higher recognition/OP\nclassification noise level. All results and codes are open source.\n","authors":["Thomas Mongaillard","Samson Lasaulce","Othman Hicheur","Chao Zhang","Lina Bariah","Vineeth S. Varma","Hang Zou","Qiyang Zhao","Merouane Debbah"],"pdf_url":"https://arxiv.org/pdf/2407.00476v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14180v1","updated":"2024-07-19T10:15:45Z","published":"2024-07-19T10:15:45Z","title":"Automatic Classification of News Subjects in Broadcast News: Application\n  to a Gender Bias Representation Analysis","summary":"  This paper introduces a computational framework designed to delineate gender\ndistribution biases in topics covered by French TV and radio news. We\ntranscribe a dataset of 11.7k hours, broadcasted in 2023 on 21 French channels.\nA Large Language Model (LLM) is used in few-shot conversation mode to obtain a\ntopic classification on those transcriptions. Using the generated LLM\nannotations, we explore the finetuning of a specialized smaller classification\nmodel, to reduce the computational cost. To evaluate the performances of these\nmodels, we construct and annotate a dataset of 804 dialogues. This dataset is\nmade available free of charge for research purposes. We show that women are\nnotably underrepresented in subjects such as sports, politics and conflicts.\nConversely, on topics such as weather, commercials and health, women have more\nspeaking time than their overall average across all subjects. We also observe\nrepresentations differences between private and public service channels.\n","authors":["Valentin Pelloin","Lena Dodson","Émile Chapuis","Nicolas Hervé","David Doukhan"],"pdf_url":"https://arxiv.org/pdf/2407.14180v1.pdf","comment":"Accepted to Interspeech 2024"},{"id":"http://arxiv.org/abs/2407.14145v1","updated":"2024-07-19T09:23:30Z","published":"2024-07-19T09:23:30Z","title":"PassTSL: Modeling Human-Created Passwords through Two-Stage Learning","summary":"  Textual passwords are still the most widely used user authentication\nmechanism. Due to the close connections between textual passwords and natural\nlanguages, advanced technologies in natural language processing (NLP) and\nmachine learning (ML) could be used to model passwords for different purposes\nsuch as studying human password-creation behaviors and developing more advanced\npassword cracking methods for informing better defence mechanisms. In this\npaper, we propose PassTSL (modeling human-created Passwords through Two-Stage\nLearning), inspired by the popular pretraining-finetuning framework in NLP and\ndeep learning (DL). We report how different pretraining settings affected\nPassTSL and proved its effectiveness by applying it to six large leaked\npassword databases. Experimental results showed that it outperforms five\nstate-of-the-art (SOTA) password cracking methods on password guessing by a\nsignificant margin ranging from 4.11% to 64.69% at the maximum point. Based on\nPassTSL, we also implemented a password strength meter (PSM), and our\nexperiments showed that it was able to estimate password strength more\naccurately, causing fewer unsafe errors (overestimating the password strength)\nthan two other SOTA PSMs when they produce the same rate of safe errors\n(underestimating the password strength): a neural-network based method and\nzxcvbn. Furthermore, we explored multiple finetuning settings, and our\nevaluations showed that, even a small amount of additional training data, e.g.,\nonly 0.1% of the pretrained data, can lead to over 3% improvement in password\nguessing on average. We also proposed a heuristic approach to selecting\nfinetuning passwords based on JS (Jensen-Shannon) divergence and experimental\nresults validated its usefulness. In summary, our contributions demonstrate the\npotential and feasibility of applying advanced NLP and ML methods to password\nmodeling and cracking.\n","authors":["Yangde Wang","Haozhang Li","Weidong Qiu","Shujun Li","Peng Tang"],"pdf_url":"https://arxiv.org/pdf/2407.14145v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14133v1","updated":"2024-07-19T09:03:30Z","published":"2024-07-19T09:03:30Z","title":"I Know About \"Up\"! Enhancing Spatial Reasoning in Visual Language Models\n  Through 3D Reconstruction","summary":"  Visual Language Models (VLMs) are essential for various tasks, particularly\nvisual reasoning tasks, due to their robust multi-modal information\nintegration, visual reasoning capabilities, and contextual awareness. However,\nexisting \\VLMs{}' visual spatial reasoning capabilities are often inadequate,\nstruggling even with basic tasks such as distinguishing left from right. To\naddress this, we propose the \\ours{} model, designed to enhance the visual\nspatial reasoning abilities of VLMS. ZeroVLM employs Zero-1-to-3, a 3D\nreconstruction model for obtaining different views of the input images and\nincorporates a prompting mechanism to further improve visual spatial reasoning.\nExperimental results on four visual spatial reasoning datasets show that our\n\\ours{} achieves up to 19.48% accuracy improvement, which indicates the\neffectiveness of the 3D reconstruction and prompting mechanisms of our ZeroVLM.\n","authors":["Zaiqiao Meng","Hao Zhou","Yifang Chen"],"pdf_url":"https://arxiv.org/pdf/2407.14133v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06852v3","updated":"2024-07-19T08:50:24Z","published":"2024-06-10T23:54:21Z","title":"A Survey of Backdoor Attacks and Defenses on Large Language Models:\n  Implications for Security Measures","summary":"  The large language models (LLMs), which bridge the gap between human language\nunderstanding and complex problem-solving, achieve state-of-the-art performance\non several NLP tasks, particularly in few-shot and zero-shot settings. Despite\nthe demonstrable efficacy of LMMs, due to constraints on computational\nresources, users have to engage with open-source language models or outsource\nthe entire training process to third-party platforms. However, research has\ndemonstrated that language models are susceptible to potential security\nvulnerabilities, particularly in backdoor attacks. Backdoor attacks are\ndesigned to introduce targeted vulnerabilities into language models by\npoisoning training samples or model weights, allowing attackers to manipulate\nmodel responses through malicious triggers. While existing surveys on backdoor\nattacks provide a comprehensive overview, they lack an in-depth examination of\nbackdoor attacks specifically targeting LLMs. To bridge this gap and grasp the\nlatest trends in the field, this paper presents a novel perspective on backdoor\nattacks for LLMs by focusing on fine-tuning methods. Specifically, we\nsystematically classify backdoor attacks into three categories: full-parameter\nfine-tuning, parameter-efficient fine-tuning, and attacks without fine-tuning.\nBased on insights from a substantial review, we also discuss crucial issues for\nfuture research on backdoor attacks, such as further exploring attack\nalgorithms that do not require fine-tuning, or developing more covert attack\nalgorithms.\n","authors":["Shuai Zhao","Meihuizi Jia","Zhongliang Guo","Leilei Gan","Xiaoyu Xu","Jie Fu","Yichao Feng","Fengjun Pan","Luu Anh Tuan"],"pdf_url":"https://arxiv.org/pdf/2406.06852v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08754v2","updated":"2024-07-19T08:23:38Z","published":"2024-06-13T02:24:08Z","title":"Exploiting Uncommon Text-Encoded Structures for Automated Jailbreaks in\n  LLMs","summary":"  Large Language Models (LLMs) are widely used in natural language processing\nbut face the risk of jailbreak attacks that maliciously induce them to generate\nharmful content. Existing jailbreak attacks, including character-level and\ncontext-level attacks, mainly focus on the prompt of the plain text without\nspecifically exploring the significant influence of its structure. In this\npaper, we focus on studying how prompt structure contributes to the jailbreak\nattack. We introduce a novel structure-level attack method based on tail\nstructures that are rarely used during LLM training, which we refer to as\nUncommon Text-Encoded Structure (UTES). We extensively study 12 UTESs templates\nand 6 obfuscation methods to build an effective automated jailbreak tool named\nStructuralSleight that contains three escalating attack strategies: Structural\nAttack, Structural and Character/Context Obfuscation Attack, and Fully\nObfuscated Structural Attack. Extensive experiments on existing LLMs show that\nStructuralSleight significantly outperforms baseline methods. In particular,\nthe attack success rate reaches 94.62\\% on GPT-4o, which has not been addressed\nby state-of-the-art techniques.\n","authors":["Bangxin Li","Hengrui Xing","Chao Huang","Jin Qian","Huangqing Xiao","Linfeng Feng","Cong Tian"],"pdf_url":"https://arxiv.org/pdf/2406.08754v2.pdf","comment":"12 pages, 4 figures"},{"id":"http://arxiv.org/abs/2303.07865v3","updated":"2024-07-19T08:06:30Z","published":"2023-03-14T12:56:47Z","title":"Geolocation Predicting of Tweets Using BERT-Based Models","summary":"  This research is aimed to solve the tweet/user geolocation prediction task\nand provide a flexible methodology for the geotagging of textual big data. The\nsuggested approach implements neural networks for natural language processing\n(NLP) to estimate the location as coordinate pairs (longitude, latitude) and\ntwo-dimensional Gaussian Mixture Models (GMMs). The scope of proposed models\nhas been finetuned on a Twitter dataset using pretrained Bidirectional Encoder\nRepresentations from Transformers (BERT) as base models. Performance metrics\nshow a median error of fewer than 30 km on a worldwide-level, and fewer than 15\nkm on the US-level datasets for the models trained and evaluated on text\nfeatures of tweets' content and metadata context. Our source code and data are\navailable at https://github.com/K4TEL/geo-twitter.git\n","authors":["Kateryna Lutsai","Christoph H. Lampert"],"pdf_url":"https://arxiv.org/pdf/2303.07865v3.pdf","comment":"32 pages, 5 tables, 9 figures"},{"id":"http://arxiv.org/abs/2407.14088v1","updated":"2024-07-19T07:54:30Z","published":"2024-07-19T07:54:30Z","title":"Impact of Model Size on Fine-tuned LLM Performance in Data-to-Text\n  Generation: A State-of-the-Art Investigation","summary":"  Data-to-text (D2T) generation aims to generate human-readable text from\nsemi-structured data, such as tables and graphs. The recent success of D2T is\nlargely attributed to advancements in LLMs. Despite the success of LLMs, no\nresearch has been conducted to illustrate the impact of model size on the\nperformance of fine-tuned LLMs for D2T tasks. D2T model performance is\ntypically assessed based on three key qualities: \\textit{readability}\n(indicates fluency and coherence), \\textit{informativeness} (measures content\nsimilarity), and \\textit{faithfulness} (assesses consistency of factual\ninformation). It is currently uncertain whether increasing the size of LLMs\neffectively improves performance in D2T tasks across these three qualities. The\nobjective of this study is to investigate the performance of fine-tuned LLMs in\nD2T tasks in terms of model size. Through extensive comparative analysis, we\naim to elucidate both the advantages and limitations of scaling model sizes\nacross five widely used D2T datasets (E2E, ViGGo, WikiTableText, DART, and\nWebNLG) and twelve state-of-the-art LLMs with varying sizes from five different\nLLM families (T5, BART, OPT, BLOOM, and Llama 2). To comprehensively cover all\nthe three essential qualities of D2T models, we incorporate six widely\nrecognized automatic metrics -- \\textsc{BLEU}, \\textsc{METEOR},\n\\textsc{BERTScore}, \\textsc{MoverScore}, \\textsc{Parent}, and\n\\textsc{BARTScore}. We also provide an in-depth analysis of LLM performance\nconcerning model size in the presence of source-reference divergence, a\ncritical aspect of D2T tasks. Our investigation reveals that increasing LLM\nsize enhances \\textit{readability} and \\textit{informativeness} in D2T tasks,\nbut larger (in terms of size) LLMs may sacrifice \\textit{faithfulness}.\nMoreover, small-sized LLMs show more resilience than larger ones when\nsource-reference divergence is present.\n","authors":["Joy Mahapatra","Utpal Garain"],"pdf_url":"https://arxiv.org/pdf/2407.14088v1.pdf","comment":"30 pages"},{"id":"http://arxiv.org/abs/2403.02504v2","updated":"2024-07-19T07:47:18Z","published":"2024-03-04T21:51:11Z","title":"A Tutorial on the Pretrain-Finetune Paradigm for Natural Language\n  Processing","summary":"  Given that natural language serves as the primary conduit for expressing\nthoughts and emotions, text analysis has become a key technique in\npsychological research. It enables the extraction of valuable insights from\nnatural language, facilitating endeavors like personality traits assessment,\nmental health monitoring, and sentiment analysis in interpersonal\ncommunications. In text analysis, existing studies often resort to either human\ncoding, which is time-consuming, using pre-built dictionaries, which often\nfails to cover all possible scenarios, or training models from scratch, which\nrequires large amounts of labeled data. In this tutorial, we introduce the\npretrain-finetune paradigm. The pretrain-finetune paradigm represents a\ntransformative approach in text analysis and natural language processing. This\nparadigm distinguishes itself through the use of large pretrained language\nmodels, demonstrating remarkable efficiency in finetuning tasks, even with\nlimited training data. This efficiency is especially beneficial for research in\nsocial sciences, where the number of annotated samples is often quite limited.\nOur tutorial offers a comprehensive introduction to the pretrain-finetune\nparadigm. We first delve into the fundamental concepts of pretraining and\nfinetuning, followed by practical exercises using real-world applications. We\ndemonstrate the application of the paradigm across various tasks, including\nmulti-class classification and regression. Emphasizing its efficacy and\nuser-friendliness, the tutorial aims to encourage broader adoption of this\nparadigm. To this end, we have provided open access to all our code and\ndatasets. The tutorial is highly beneficial across various psychology\ndisciplines, providing a comprehensive guide to employing text analysis in\ndiverse research settings.\n","authors":["Yu Wang","Wen Qu"],"pdf_url":"https://arxiv.org/pdf/2403.02504v2.pdf","comment":"28 pages, 6 figures, 3 tables"},{"id":"http://arxiv.org/abs/2407.14085v1","updated":"2024-07-19T07:42:48Z","published":"2024-07-19T07:42:48Z","title":"An Improved Method for Class-specific Keyword Extraction: A Case Study\n  in the German Business Registry","summary":"  The task of $\\textit{keyword extraction}$ is often an important initial step\nin unsupervised information extraction, forming the basis for tasks such as\ntopic modeling or document classification. While recent methods have proven to\nbe quite effective in the extraction of keywords, the identification of\n$\\textit{class-specific}$ keywords, or only those pertaining to a predefined\nclass, remains challenging. In this work, we propose an improved method for\nclass-specific keyword extraction, which builds upon the popular\n$\\textbf{KeyBERT}$ library to identify only keywords related to a class\ndescribed by $\\textit{seed keywords}$. We test this method using a dataset of\nGerman business registry entries, where the goal is to classify each business\naccording to an economic sector. Our results reveal that our method greatly\nimproves upon previous approaches, setting a new standard for\n$\\textit{class-specific}$ keyword extraction.\n","authors":["Stephen Meisenbacher","Tim Schopf","Weixin Yan","Patrick Holl","Florian Matthes"],"pdf_url":"https://arxiv.org/pdf/2407.14085v1.pdf","comment":"7 pages, 1 figure, 1 table. Accepted to KONVENS 2024"},{"id":"http://arxiv.org/abs/2407.12833v2","updated":"2024-07-19T07:38:35Z","published":"2024-07-03T15:41:54Z","title":"ESQA: Event Sequences Question Answering","summary":"  Event sequences (ESs) arise in many practical domains including finance,\nretail, social networks, and healthcare. In the context of machine learning,\nevent sequences can be seen as a special type of tabular data with annotated\ntimestamps. Despite the importance of ESs modeling and analysis, little effort\nwas made in adapting large language models (LLMs) to the ESs domain. In this\npaper, we highlight the common difficulties of ESs processing and propose a\nnovel solution capable of solving multiple downstream tasks with little or no\nfinetuning. In particular, we solve the problem of working with long sequences\nand improve time and numeric features processing. The resulting method, called\nESQA, effectively utilizes the power of LLMs and, according to extensive\nexperiments, achieves state-of-the-art results in the ESs domain.\n","authors":["Irina Abdullaeva","Andrei Filatov","Mikhail Orlov","Ivan Karpukhin","Viacheslav Vasilev","Denis Dimitrov","Andrey Kuznetsov","Ivan Kireev","Andrey Savchenko"],"pdf_url":"https://arxiv.org/pdf/2407.12833v2.pdf","comment":"25 pages, 3 figures"},{"id":"http://arxiv.org/abs/2407.14076v1","updated":"2024-07-19T07:12:43Z","published":"2024-07-19T07:12:43Z","title":"Domain-Specific Pretraining of Language Models: A Comparative Study in\n  the Medical Field","summary":"  There are many cases where LLMs are used for specific tasks in a single\ndomain. These usually require less general, but more domain-specific knowledge.\nHighly capable, general-purpose state-of-the-art language models like GPT-4 or\nClaude-3-opus can often be used for such tasks, but they are very large and\ncannot be run locally, even if they were not proprietary. This can be a problem\nwhen working with sensitive data. This paper focuses on domain-specific and\nmixed-domain pretraining as potentially more efficient methods than general\npretraining for specialized language models. We will take a look at work\nrelated to domain-specific pretraining, specifically in the medical area, and\ncompare benchmark results of specialized language models to general-purpose\nlanguage models.\n","authors":["Tobias Kerner"],"pdf_url":"https://arxiv.org/pdf/2407.14076v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14057v1","updated":"2024-07-19T06:34:45Z","published":"2024-07-19T06:34:45Z","title":"LazyLLM: Dynamic Token Pruning for Efficient Long Context LLM Inference","summary":"  The inference of transformer-based large language models consists of two\nsequential stages: 1) a prefilling stage to compute the KV cache of prompts and\ngenerate the first token, and 2) a decoding stage to generate subsequent\ntokens. For long prompts, the KV cache must be computed for all tokens during\nthe prefilling stage, which can significantly increase the time needed to\ngenerate the first token. Consequently, the prefilling stage may become a\nbottleneck in the generation process. An open question remains whether all\nprompt tokens are essential for generating the first token. To answer this, we\nintroduce a novel method, LazyLLM, that selectively computes the KV for tokens\nimportant for the next token prediction in both the prefilling and decoding\nstages. Contrary to static pruning approaches that prune the prompt at once,\nLazyLLM allows language models to dynamically select different subsets of\ntokens from the context in different generation steps, even though they might\nbe pruned in previous steps. Extensive experiments on standard datasets across\nvarious tasks demonstrate that LazyLLM is a generic method that can be\nseamlessly integrated with existing language models to significantly accelerate\nthe generation without fine-tuning. For instance, in the multi-document\nquestion-answering task, LazyLLM accelerates the prefilling stage of the LLama\n2 7B model by 2.34x while maintaining accuracy.\n","authors":["Qichen Fu","Minsik Cho","Thomas Merth","Sachin Mehta","Mohammad Rastegari","Mahyar Najibi"],"pdf_url":"https://arxiv.org/pdf/2407.14057v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14056v1","updated":"2024-07-19T06:33:10Z","published":"2024-07-19T06:33:10Z","title":"Rasa: Building Expressive Speech Synthesis Systems for Indian Languages\n  in Low-resource Settings","summary":"  We release Rasa, the first multilingual expressive TTS dataset for any Indian\nlanguage, which contains 10 hours of neutral speech and 1-3 hours of expressive\nspeech for each of the 6 Ekman emotions covering 3 languages: Assamese,\nBengali, & Tamil. Our ablation studies reveal that just 1 hour of neutral and\n30 minutes of expressive data can yield a Fair system as indicated by MUSHRA\nscores. Increasing neutral data to 10 hours, with minimal expressive data,\nsignificantly enhances expressiveness. This offers a practical recipe for\nresource-constrained languages, prioritizing easily obtainable neutral data\nalongside smaller amounts of expressive data. We show the importance of\nsyllabically balanced data and pooling emotions to enhance expressiveness. We\nalso highlight challenges in generating specific emotions, e.g., fear and\nsurprise.\n","authors":["Praveen Srinivasa Varadhan","Ashwin Sankar","Giri Raju","Mitesh M. Khapra"],"pdf_url":"https://arxiv.org/pdf/2407.14056v1.pdf","comment":"Accepted at INTERSPEECH 2024. First two authors listed contributed\n  equally"},{"id":"http://arxiv.org/abs/2402.15131v2","updated":"2024-07-19T06:14:20Z","published":"2024-02-23T06:32:18Z","title":"Interactive-KBQA: Multi-Turn Interactions for Knowledge Base Question\n  Answering with Large Language Models","summary":"  This study explores the realm of knowledge base question answering (KBQA).\nKBQA is considered a challenging task, particularly in parsing intricate\nquestions into executable logical forms. Traditional semantic parsing\n(SP)-based methods require extensive data annotations, which result in\nsignificant costs. Recently, the advent of few-shot in-context learning,\npowered by large language models (LLMs), has showcased promising capabilities.\nHowever, fully leveraging LLMs to parse questions into logical forms in\nlow-resource scenarios poses a substantial challenge. To tackle these hurdles,\nwe introduce Interactive-KBQA, a framework designed to generate logical forms\nthrough direct interaction with knowledge bases (KBs). Within this framework,\nwe have developed three generic APIs for KB interaction. For each category of\ncomplex question, we devised exemplars to guide LLMs through the reasoning\nprocesses. Our method achieves competitive results on the WebQuestionsSP,\nComplexWebQuestions, KQA Pro, and MetaQA datasets with a minimal number of\nexamples (shots). Importantly, our approach supports manual intervention,\nallowing for the iterative refinement of LLM outputs. By annotating a dataset\nwith step-wise reasoning processes, we showcase our model's adaptability and\nhighlight its potential for contributing significant enhancements to the field.\n","authors":["Guanming Xiong","Junwei Bao","Wen Zhao"],"pdf_url":"https://arxiv.org/pdf/2402.15131v2.pdf","comment":"This work has been accepted by the ACL 2024 main conference. Code and\n  data are available at: https://github.com/JimXiongGM/Interactive-KBQA"},{"id":"http://arxiv.org/abs/2407.14049v1","updated":"2024-07-19T06:07:32Z","published":"2024-07-19T06:07:32Z","title":"Prompted Aspect Key Point Analysis for Quantitative Review Summarization","summary":"  Key Point Analysis (KPA) aims for quantitative summarization that provides\nkey points (KPs) as succinct textual summaries and quantities measuring their\nprevalence. KPA studies for arguments and reviews have been reported in the\nliterature. A majority of KPA studies for reviews adopt supervised learning to\nextract short sentences as KPs before matching KPs to review comments for\nquantification of KP prevalence. Recent abstractive approaches still generate\nKPs based on sentences, often leading to KPs with overlapping and hallucinated\nopinions, and inaccurate quantification. In this paper, we propose Prompted\nAspect Key Point Analysis (PAKPA) for quantitative review summarization. PAKPA\nemploys aspect sentiment analysis and prompted in-context learning with Large\nLanguage Models (LLMs) to generate and quantify KPs grounded in aspects for\nbusiness entities, which achieves faithful KPs with accurate quantification,\nand removes the need for large amounts of annotated data for supervised\ntraining. Experiments on the popular review dataset Yelp and the\naspect-oriented review summarization dataset SPACE show that our framework\nachieves state-of-the-art performance. Source code and data are available at:\nhttps://github.com/antangrocket1312/PAKPA\n","authors":["An Quang Tang","Xiuzhen Zhang","Minh Ngoc Dinh","Erik Cambria"],"pdf_url":"https://arxiv.org/pdf/2407.14049v1.pdf","comment":"Accepted by ACL 2024 Main Conference"},{"id":"http://arxiv.org/abs/2407.14044v1","updated":"2024-07-19T05:47:40Z","published":"2024-07-19T05:47:40Z","title":"ECCO: Can We Improve Model-Generated Code Efficiency Without Sacrificing\n  Functional Correctness?","summary":"  Although large language models (LLMs) have been largely successful in\ngenerating functionally correct programs, conditioning models to produce\nefficient solutions while ensuring correctness remains a challenge. Further,\nunreliability in benchmarking code efficiency is a hurdle across varying\nhardware specifications for popular interpreted languages such as Python. In\nthis paper, we present ECCO, a reproducible benchmark for evaluating program\nefficiency via two paradigms: natural language (NL) based code generation and\nhistory-based code editing. On ECCO, we adapt and thoroughly investigate the\nthree most promising existing LLM-based approaches: in-context learning,\niterative refinement with execution or NL feedback, and fine-tuning conditioned\non execution and editing history. While most methods degrade functional\ncorrectness and moderately increase program efficiency, we find that adding\nexecution information often helps maintain functional correctness, and NL\nfeedback enhances more on efficiency. We release our benchmark to support\nfuture work on LLM-based generation of efficient code.\n","authors":["Siddhant Waghjale","Vishruth Veerendranath","Zora Zhiruo Wang","Daniel Fried"],"pdf_url":"https://arxiv.org/pdf/2407.14044v1.pdf","comment":"Code: https://github.com/CodeEff/ECCO, 14 pages, 11 figures,\n  Pre-print"},{"id":"http://arxiv.org/abs/2407.14039v1","updated":"2024-07-19T05:33:09Z","published":"2024-07-19T05:33:09Z","title":"BERTer: The Efficient One","summary":"  We explore advanced fine-tuning techniques to boost BERT's performance in\nsentiment analysis, paraphrase detection, and semantic textual similarity. Our\napproach leverages SMART regularization to combat overfitting, improves\nhyperparameter choices, employs a cross-embedding Siamese architecture for\nimproved sentence embeddings, and introduces innovative early exiting methods.\nOur fine-tuning findings currently reveal substantial improvements in model\nefficiency and effectiveness when combining multiple fine-tuning architectures,\nachieving a state-of-the-art performance score of on the test set, surpassing\ncurrent benchmarks and highlighting BERT's adaptability in multifaceted\nlinguistic tasks.\n","authors":["Pradyumna Saligram","Andrew Lanpouthakoun"],"pdf_url":"https://arxiv.org/pdf/2407.14039v1.pdf","comment":"10 pages, 4 figures"},{"id":"http://arxiv.org/abs/2402.00742v2","updated":"2024-07-19T05:12:15Z","published":"2024-02-01T16:39:28Z","title":"Transforming and Combining Rewards for Aligning Large Language Models","summary":"  A common approach for aligning language models to human preferences is to\nfirst learn a reward model from preference data, and then use this reward model\nto update the language model. We study two closely related problems that arise\nin this approach. First, any monotone transformation of the reward model\npreserves preference ranking; is there a choice that is ``better'' than others?\nSecond, we often wish to align language models to multiple properties: how\nshould we combine multiple reward models? Using a probabilistic interpretation\nof the alignment procedure, we identify a natural choice for transformation for\n(the common case of) rewards learned from Bradley-Terry preference models. The\nderived transformation is straightforward: we apply a log-sigmoid function to\nthe centered rewards, a method we term ``LSC-transformation''\n(log-sigmoid-centered transformation). This transformation has two important\nproperties. First, it emphasizes improving poorly-performing outputs, rather\nthan outputs that already score well. This mitigates both underfitting (where\nsome prompts are not improved) and reward hacking (where the model learns to\nexploit misspecification of the reward model). Second, it enables principled\naggregation of rewards by linking summation to logical conjunction: the sum of\ntransformed rewards corresponds to the probability that the output is ``good''\nin all measured properties, in a sense we make precise. Experiments aligning\nlanguage models to be both helpful and harmless using RLHF show substantial\nimprovements over the baseline (non-transformed) approach.\n","authors":["Zihao Wang","Chirag Nagpal","Jonathan Berant","Jacob Eisenstein","Alex D'Amour","Sanmi Koyejo","Victor Veitch"],"pdf_url":"https://arxiv.org/pdf/2402.00742v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14030v1","updated":"2024-07-19T05:04:24Z","published":"2024-07-19T05:04:24Z","title":"HeCiX: Integrating Knowledge Graphs and Large Language Models for\n  Biomedical Research","summary":"  Despite advancements in drug development strategies, 90% of clinical trials\nfail. This suggests overlooked aspects in target validation and drug\noptimization. In order to address this, we introduce HeCiX-KG,\nHetionet-Clinicaltrials neXus Knowledge Graph, a novel fusion of data from\nClinicalTrials.gov and Hetionet in a single knowledge graph. HeCiX-KG combines\ndata on previously conducted clinical trials from ClinicalTrials.gov, and\ndomain expertise on diseases and genes from Hetionet. This offers a thorough\nresource for clinical researchers. Further, we introduce HeCiX, a system that\nuses LangChain to integrate HeCiX-KG with GPT-4, and increase its usability.\nHeCiX shows high performance during evaluation against a range of clinically\nrelevant issues, proving this model to be promising for enhancing the\neffectiveness of clinical research. Thus, this approach provides a more\nholistic view of clinical trials and existing biological data.\n","authors":["Prerana Sanjay Kulkarni","Muskaan Jain","Disha Sheshanarayana","Srinivasan Parthiban"],"pdf_url":"https://arxiv.org/pdf/2407.14030v1.pdf","comment":"8 pages, 3 figures, under review"},{"id":"http://arxiv.org/abs/2404.14294v3","updated":"2024-07-19T04:47:36Z","published":"2024-04-22T15:53:08Z","title":"A Survey on Efficient Inference for Large Language Models","summary":"  Large Language Models (LLMs) have attracted extensive attention due to their\nremarkable performance across various tasks. However, the substantial\ncomputational and memory requirements of LLM inference pose challenges for\ndeployment in resource-constrained scenarios. Efforts within the field have\nbeen directed towards developing techniques aimed at enhancing the efficiency\nof LLM inference. This paper presents a comprehensive survey of the existing\nliterature on efficient LLM inference. We start by analyzing the primary causes\nof the inefficient LLM inference, i.e., the large model size, the\nquadratic-complexity attention operation, and the auto-regressive decoding\napproach. Then, we introduce a comprehensive taxonomy that organizes the\ncurrent literature into data-level, model-level, and system-level optimization.\nMoreover, the paper includes comparative experiments on representative methods\nwithin critical sub-fields to provide quantitative insights. Last but not\nleast, we provide some knowledge summary and discuss future research\ndirections.\n","authors":["Zixuan Zhou","Xuefei Ning","Ke Hong","Tianyu Fu","Jiaming Xu","Shiyao Li","Yuming Lou","Luning Wang","Zhihang Yuan","Xiuhong Li","Shengen Yan","Guohao Dai","Xiao-Ping Zhang","Yuhan Dong","Yu Wang"],"pdf_url":"https://arxiv.org/pdf/2404.14294v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.02146v2","updated":"2024-07-19T04:38:57Z","published":"2023-07-05T09:42:47Z","title":"LOAF-M2L: Joint Learning of Wording and Formatting for Singable\n  Melody-to-Lyric Generation","summary":"  Despite previous efforts in melody-to-lyric generation research, there is\nstill a significant compatibility gap between generated lyrics and melodies,\nnegatively impacting the singability of the outputs. This paper bridges the\nsingability gap with a novel approach to generating singable lyrics by jointly\nLearning wOrding And Formatting during Melody-to-Lyric training. After\ngeneral-domain pretraining, our proposed model acquires length awareness first\nfrom a large text-only lyric corpus. Then, we introduce a new objective\ninformed by musicological research on the relationship between melody and\nlyrics during melody-to-lyric training, which enables the model to learn the\nfine-grained format requirements of the melody. Our model achieves 3.75% and\n21.44% absolute accuracy gains in the outputs' number-of-line and\nsyllable-per-line requirements compared to naive fine-tuning, without\nsacrificing text fluency. Furthermore, our model demonstrates a 63.92% and\n74.18% relative improvement of music-lyric compatibility and overall quality in\nthe subjective evaluation, compared to the state-of-the-art melody-to-lyric\ngeneration model, highlighting the significance of formatting learning.\n","authors":["Longshen Ou","Xichu Ma","Ye Wang"],"pdf_url":"https://arxiv.org/pdf/2307.02146v2.pdf","comment":"An extension of our previous work arXiv:2305.16816 [cs.CL]"},{"id":"http://arxiv.org/abs/2407.06314v2","updated":"2024-07-19T04:14:59Z","published":"2024-07-08T18:27:54Z","title":"Personality Analysis for Social Media Users using Arabic language and\n  its Effect on Sentiment Analysis","summary":"  Social media is heading toward personalization more and more, where\nindividuals reveal their beliefs, interests, habits, and activities, simply\noffering glimpses into their personality traits. This study, explores the\ncorrelation between the use of Arabic language on twitter, personality traits\nand its impact on sentiment analysis. We indicated the personality traits of\nusers based on the information extracted from their profile activities, and the\ncontent of their tweets. Our analysis incorporated linguistic features, profile\nstatistics (including gender, age, bio, etc.), as well as additional features\nlike emoticons. To obtain personality data, we crawled the timelines and\nprofiles of users who took the 16personalities test in Arabic on\n16personalities.com. Our dataset \"AraPers\" comprised 3,250 users who shared\ntheir personality results on twitter. We implemented various machine learning\ntechniques, to reveal personality traits and developed a dedicated model for\nthis purpose, achieving a 74.86% accuracy rate with BERT, analysis of this\ndataset proved that linguistic features, profile features and derived model can\nbe used to differentiate between different personality traits. Furthermore, our\nfindings demonstrated that personality affect sentiment in social media. This\nresearch contributes to the ongoing efforts in developing robust understanding\nof the relation between human behaviour on social media and personality\nfeatures for real-world applications, such as political discourse analysis, and\npublic opinion tracking.\n","authors":["Mokhaiber Dandash","Masoud Asadpour"],"pdf_url":"https://arxiv.org/pdf/2407.06314v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.09025v5","updated":"2024-07-19T04:13:59Z","published":"2024-02-14T09:01:13Z","title":"SLEB: Streamlining LLMs through Redundancy Verification and Elimination\n  of Transformer Blocks","summary":"  Large language models (LLMs) have proven to be highly effective across\nvarious natural language processing tasks. However, their large number of\nparameters poses significant challenges for practical deployment. Pruning, a\ntechnique aimed at reducing the size and complexity of LLMs, offers a potential\nsolution by removing redundant components from the network. Despite the promise\nof pruning, existing methods often struggle to achieve substantial end-to-end\nLLM inference speedup. In this paper, we introduce SLEB, a novel approach\ndesigned to streamline LLMs by eliminating redundant transformer blocks. We\nchoose the transformer block as the fundamental unit for pruning, because LLMs\nexhibit block-level redundancy with high similarity between the outputs of\nneighboring blocks. This choice allows us to effectively enhance the processing\nspeed of LLMs. Our experimental results demonstrate that SLEB outperforms\nprevious LLM pruning methods in accelerating LLM inference while also\nmaintaining superior perplexity and accuracy, making SLEB as a promising\ntechnique for enhancing the efficiency of LLMs. The code is available at:\nhttps://github.com/jiwonsong-dev/SLEB.\n","authors":["Jiwon Song","Kyungseok Oh","Taesu Kim","Hyungjun Kim","Yulhwa Kim","Jae-Joon Kim"],"pdf_url":"https://arxiv.org/pdf/2402.09025v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14000v1","updated":"2024-07-19T03:12:10Z","published":"2024-07-19T03:12:10Z","title":"Clinical Reading Comprehension with Encoder-Decoder Models Enhanced by\n  Direct Preference Optimization","summary":"  Extractive question answering over clinical text is a crucial need to help\ndeal with the deluge of clinical text generated in hospitals. While encoder\nmodels (e.g., BERT) have been popular for this reading comprehension task,\nrecently encoder-decoder models (e.g., T5) are on the rise. There is also the\nemergence of preference optimization techniques to align decoder-only LLMs with\nhuman preferences. In this paper, we combine encoder-decoder models with the\ndirect preference optimization (DPO) method to improve over prior state of the\nart for the RadQA radiology question answering task by 12-15 F1 points. To the\nbest of our knowledge, this effort is the first to show that DPO method also\nworks for reading comprehension via novel heuristics to generate preference\ndata without human inputs.\n","authors":["Md Sultan Al Nahian","Ramakanth Kavuluru"],"pdf_url":"https://arxiv.org/pdf/2407.14000v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13999v1","updated":"2024-07-19T03:03:21Z","published":"2024-07-19T03:03:21Z","title":"NeLLCom-X: A Comprehensive Neural-Agent Framework to Simulate Language\n  Learning and Group Communication","summary":"  Recent advances in computational linguistics include simulating the emergence\nof human-like languages with interacting neural network agents, starting from\nsets of random symbols. The recently introduced NeLLCom framework (Lian et al.,\n2023) allows agents to first learn an artificial language and then use it to\ncommunicate, with the aim of studying the emergence of specific linguistics\nproperties. We extend this framework (NeLLCom-X) by introducing more realistic\nrole-alternating agents and group communication in order to investigate the\ninterplay between language learnability, communication pressures, and group\nsize effects. We validate NeLLCom-X by replicating key findings from prior\nresearch simulating the emergence of a word-order/case-marking trade-off. Next,\nwe investigate how interaction affects linguistic convergence and emergence of\nthe trade-off. The novel framework facilitates future simulations of diverse\nlinguistic aspects, emphasizing the importance of interaction and group\ndynamics in language evolution.\n","authors":["Yuchen Lian","Tessa Verhoef","Arianna Bisazza"],"pdf_url":"https://arxiv.org/pdf/2407.13999v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13998v1","updated":"2024-07-19T03:02:51Z","published":"2024-07-19T03:02:51Z","title":"RAG-QA Arena: Evaluating Domain Robustness for Long-form Retrieval\n  Augmented Question Answering","summary":"  Question answering based on retrieval augmented generation (RAG-QA) is an\nimportant research topic in NLP and has a wide range of real-world\napplications. However, most existing datasets for this task are either\nconstructed using a single source corpus or consist of short extractive\nanswers, which fall short of evaluating large language model (LLM) based RAG-QA\nsystems on cross-domain generalization. To address these limitations, we create\nLong-form RobustQA (LFRQA), a new dataset comprising human-written long-form\nanswers that integrate short extractive answers from multiple documents into a\nsingle, coherent narrative, covering 26K queries and large corpora across seven\ndifferent domains. We further propose RAG-QA Arena by directly comparing\nmodel-generated answers against LFRQA's answers using LLMs as evaluators. We\nshow via extensive experiments that RAG-QA Arena and human judgments on answer\nquality are highly correlated. Moreover, only 41.3% of the most competitive\nLLM's answers are preferred to LFRQA's answers, demonstrating RAG-QA Arena as a\nchallenging evaluation platform for future research.\n","authors":["Rujun Han","Yuhao Zhang","Peng Qi","Yumo Xu","Jenyuan Wang","Lan Liu","William Yang Wang","Bonan Min","Vittorio Castelli"],"pdf_url":"https://arxiv.org/pdf/2407.13998v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18312v2","updated":"2024-07-19T02:37:42Z","published":"2024-06-26T12:51:37Z","title":"AI-native Memory: A Pathway from LLMs Towards AGI","summary":"  Large language models (LLMs) have demonstrated the world with the sparks of\nartificial general intelligence (AGI). One opinion, especially from some\nstartups working on LLMs, argues that an LLM with nearly unlimited context\nlength can realize AGI. However, they might be too optimistic about the\nlong-context capability of (existing) LLMs -- (1) Recent literature has shown\nthat their effective context length is significantly smaller than their claimed\ncontext length; and (2) Our reasoning-in-a-haystack experiments further\ndemonstrate that simultaneously finding the relevant information from a long\ncontext and conducting (simple) reasoning is nearly impossible. In this paper,\nwe envision a pathway from LLMs to AGI through the integration of\n\\emph{memory}. We believe that AGI should be a system where LLMs serve as core\nprocessors. In addition to raw data, the memory in this system would store a\nlarge number of important conclusions derived from reasoning processes.\nCompared with retrieval-augmented generation (RAG) that merely processing raw\ndata, this approach not only connects semantically related information closer,\nbut also simplifies complex inferences at the time of querying. As an\nintermediate stage, the memory will likely be in the form of natural language\ndescriptions, which can be directly consumed by users too. Ultimately, every\nagent/person should have its own large personal model, a deep neural network\nmodel (thus \\emph{AI-native}) that parameterizes and compresses all types of\nmemory, even the ones cannot be described by natural languages. Finally, we\ndiscuss the significant potential of AI-native memory as the transformative\ninfrastructure for (proactive) engagement, personalization, distribution, and\nsocial in the AGI era, as well as the incurred privacy and security challenges\nwith preliminary solutions.\n","authors":["Jingbo Shang","Zai Zheng","Xiang Ying","Felix Tao","Mindverse Team"],"pdf_url":"https://arxiv.org/pdf/2406.18312v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13982v1","updated":"2024-07-19T02:14:17Z","published":"2024-07-19T02:14:17Z","title":"Reexamining Racial Disparities in Automatic Speech Recognition\n  Performance: The Role of Confounding by Provenance","summary":"  Automatic speech recognition (ASR) models trained on large amounts of audio\ndata are now widely used to convert speech to written text in a variety of\napplications from video captioning to automated assistants used in healthcare\nand other domains. As such, it is important that ASR models and their use is\nfair and equitable. Prior work examining the performance of commercial ASR\nsystems on the Corpus of Regional African American Language (CORAAL)\ndemonstrated significantly worse ASR performance on African American English\n(AAE). The current study seeks to understand the factors underlying this\ndisparity by examining the performance of the current state-of-the-art neural\nnetwork based ASR system (Whisper, OpenAI) on the CORAAL dataset. Two key\nfindings have been identified as a result of the current study. The first\nconfirms prior findings of significant dialectal variation even across\nneighboring communities, and worse ASR performance on AAE that can be improved\nto some extent with fine-tuning of ASR models. The second is a novel finding\nnot discussed in prior work on CORAAL: differences in audio recording practices\nwithin the dataset have a significant impact on ASR accuracy resulting in a\n``confounding by provenance'' effect in which both language use and recording\nquality differ by study location. These findings highlight the need for further\nsystematic investigation to disentangle the effects of recording quality and\ninherent linguistic diversity when examining the fairness and bias present in\nneural ASR models, as any bias in ASR accuracy may have negative downstream\neffects on disparities in various domains of life in which ASR technology is\nused.\n","authors":["Changye Li","Trevor Cohen","Serguei Pakhomov"],"pdf_url":"https://arxiv.org/pdf/2407.13982v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13193v2","updated":"2024-07-19T02:00:56Z","published":"2024-07-18T06:06:53Z","title":"Retrieval-Augmented Generation for Natural Language Processing: A Survey","summary":"  Large language models (LLMs) have demonstrated great success in various\nfields, benefiting from their huge amount of parameters that store knowledge.\nHowever, LLMs still suffer from several key issues, such as hallucination\nproblems, knowledge update issues, and lacking domain-specific expertise. The\nappearance of retrieval-augmented generation (RAG), which leverages an external\nknowledge database to augment LLMs, makes up those drawbacks of LLMs. This\npaper reviews all significant techniques of RAG, especially in the retriever\nand the retrieval fusions. Besides, tutorial codes are provided for\nimplementing the representative techniques in RAG. This paper further discusses\nthe RAG training, including RAG with/without datastore update. Then, we\nintroduce the application of RAG in representative natural language processing\ntasks and industrial scenarios. Finally, this paper discusses the future\ndirections and challenges of RAG for promoting its development.\n","authors":["Shangyu Wu","Ying Xiong","Yufei Cui","Haolun Wu","Can Chen","Ye Yuan","Lianming Huang","Xue Liu","Tei-Wei Kuo","Nan Guan","Chun Jason Xue"],"pdf_url":"https://arxiv.org/pdf/2407.13193v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.12852v2","updated":"2024-07-19T01:54:26Z","published":"2024-07-08T16:49:34Z","title":"Historical Ink: Semantic Shift Detection for 19th Century Spanish","summary":"  This paper explores the evolution of word meanings in 19th-century Spanish\ntexts, with an emphasis on Latin American Spanish, using computational\nlinguistics techniques. It addresses the Semantic Shift Detection (SSD) task,\nwhich is crucial for understanding linguistic evolution, particularly in\nhistorical contexts. The study focuses on analyzing a set of Spanish target\nwords. To achieve this, a 19th-century Spanish corpus is constructed, and a\ncustomizable pipeline for SSD tasks is developed. This pipeline helps find the\nsenses of a word and measure their semantic change between two corpora using\nfine-tuned BERT-like models with old Spanish texts for both Latin American and\ngeneral Spanish cases. The results provide valuable insights into the cultural\nand societal shifts reflected in language changes over time.\n","authors":["Tony Montes","Laura Manrique-Gómez","Rubén Manrique"],"pdf_url":"https://arxiv.org/pdf/2407.12852v2.pdf","comment":"13 pages; added a preprint-reference URL"},{"id":"http://arxiv.org/abs/2407.12828v2","updated":"2024-07-19T01:33:56Z","published":"2024-07-02T14:33:44Z","title":"Why Does New Knowledge Create Messy Ripple Effects in LLMs?","summary":"  Extensive previous research has focused on post-training knowledge editing\n(KE) for language models (LMs) to ensure that knowledge remains accurate and\nup-to-date. One desired property and open question in KE is to let edited LMs\ncorrectly handle ripple effects, where LM is expected to answer its logically\nrelated knowledge accurately. In this paper, we answer the question of why most\nKE methods still create messy ripple effects. We conduct extensive analysis and\nidentify a salient indicator, GradSim, that effectively reveals when and why\nupdated knowledge ripples in LMs. GradSim is computed by the cosine similarity\nbetween gradients of the original fact and its related knowledge. We observe a\nstrong positive correlation between ripple effect performance and GradSim\nacross different LMs, KE methods, and evaluation metrics. Further\ninvestigations into three counter-intuitive failure cases (Negation,\nOver-Ripple, Multi-Lingual) of ripple effects demonstrate that these failures\nare often associated with very low GradSim. This finding validates that GradSim\nis an effective indicator of when knowledge ripples in LMs.\n","authors":["Jiaxin Qin","Zixuan Zhang","Chi Han","Manling Li","Pengfei Yu","Heng Ji"],"pdf_url":"https://arxiv.org/pdf/2407.12828v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.12703v2","updated":"2024-07-19T00:34:13Z","published":"2024-07-17T16:25:37Z","title":"Subgraph-Aware Training of Text-based Methods for Knowledge Graph\n  Completion","summary":"  Fine-tuning pre-trained language models (PLMs) has recently shown a potential\nto improve knowledge graph completion (KGC). However, most PLM-based methods\nencode only textual information, neglecting various topological structures of\nknowledge graphs (KGs). In this paper, we empirically validate the significant\nrelations between the structural properties of KGs and the performance of the\nPLM-based methods. To leverage the structural knowledge, we propose a\nSubgraph-Aware Training framework for KGC (SATKGC) that combines (i)\nsubgraph-aware mini-batching to encourage hard negative sampling, and (ii) a\nnew contrastive learning method to focus more on harder entities and harder\nnegative triples in terms of the structural properties. To the best of our\nknowledge, this is the first study to comprehensively incorporate the\nstructural inductive bias of the subgraphs into fine-tuning PLMs. Extensive\nexperiments on four KGC benchmarks demonstrate the superiority of SATKGC. Our\ncode is available.\n","authors":["Youmin Ko","Hyemin Yang","Taeuk Kim","Hyunjoon Kim"],"pdf_url":"https://arxiv.org/pdf/2407.12703v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.10276v2","updated":"2024-07-19T00:29:05Z","published":"2024-05-16T17:33:50Z","title":"Revisiting OPRO: The Limitations of Small-Scale LLMs as Optimizers","summary":"  Numerous recent works aim to enhance the efficacy of Large Language Models\n(LLMs) through strategic prompting. In particular, the Optimization by\nPROmpting (OPRO) approach provides state-of-the-art performance by leveraging\nLLMs as optimizers where the optimization task is to find instructions that\nmaximize the task accuracy. In this paper, we revisit OPRO for automated\nprompting with relatively small-scale LLMs, such as LLaMa-2 family and Mistral\n7B. Our investigation reveals that OPRO shows limited effectiveness in\nsmall-scale LLMs, with limited inference capabilities constraining optimization\nability. We suggest future automatic prompting engineering to consider both\nmodel capabilities and computational costs. Additionally, for small-scale LLMs,\nwe recommend direct instructions that clearly outline objectives and\nmethodologies as robust prompt baselines, ensuring efficient and effective\nprompt engineering in ongoing research.\n","authors":["Tuo Zhang","Jinyue Yuan","Salman Avestimehr"],"pdf_url":"https://arxiv.org/pdf/2405.10276v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10995v2","updated":"2024-07-19T00:27:42Z","published":"2024-06-24T14:05:56Z","title":"LionGuard: Building a Contextualized Moderation Classifier to Tackle\n  Localized Unsafe Content","summary":"  As large language models (LLMs) become increasingly prevalent in a wide\nvariety of applications, concerns about the safety of their outputs have become\nmore significant. Most efforts at safety-tuning or moderation today take on a\npredominantly Western-centric view of safety, especially for toxic, hateful, or\nviolent speech. In this paper, we describe LionGuard, a\nSingapore-contextualized moderation classifier that can serve as guardrails\nagainst unsafe LLM outputs. When assessed on Singlish data, LionGuard\noutperforms existing widely-used moderation APIs, which are not finetuned for\nthe Singapore context, by 14% (binary) and up to 51% (multi-label). Our work\nhighlights the benefits of localization for moderation classifiers and presents\na practical and scalable approach for low-resource languages.\n","authors":["Jessica Foo","Shaun Khoo"],"pdf_url":"https://arxiv.org/pdf/2407.10995v2.pdf","comment":"Preprint"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2407.14509v1","updated":"2024-07-19T17:59:38Z","published":"2024-07-19T17:59:38Z","title":"DEPICT: Diffusion-Enabled Permutation Importance for Image\n  Classification Tasks","summary":"  We propose a permutation-based explanation method for image classifiers.\nCurrent image-model explanations like activation maps are limited to\ninstance-based explanations in the pixel space, making it difficult to\nunderstand global model behavior. In contrast, permutation based explanations\nfor tabular data classifiers measure feature importance by comparing model\nperformance on data before and after permuting a feature. We propose an\nexplanation method for image-based models that permutes interpretable concepts\nacross dataset images. Given a dataset of images labeled with specific concepts\nlike captions, we permute a concept across examples in the text space and then\ngenerate images via a text-conditioned diffusion model. Feature importance is\nthen reflected by the change in model performance relative to unpermuted data.\nWhen applied to a set of concepts, the method generates a ranking of feature\nimportance. We show this approach recovers underlying model feature importance\non synthetic and real-world image classification tasks.\n","authors":["Sarah Jabbour","Gregory Kondas","Ella Kazerooni","Michael Sjoding","David Fouhey","Jenna Wiens"],"pdf_url":"https://arxiv.org/pdf/2407.14509v1.pdf","comment":"36 pages, 18 figures, 9 tables, to be published in ECCV 2024"},{"id":"http://arxiv.org/abs/2407.14505v1","updated":"2024-07-19T17:58:36Z","published":"2024-07-19T17:58:36Z","title":"T2V-CompBench: A Comprehensive Benchmark for Compositional Text-to-video\n  Generation","summary":"  Text-to-video (T2V) generation models have advanced significantly, yet their\nability to compose different objects, attributes, actions, and motions into a\nvideo remains unexplored. Previous text-to-video benchmarks also neglect this\nimportant ability for evaluation. In this work, we conduct the first systematic\nstudy on compositional text-to-video generation. We propose T2V-CompBench, the\nfirst benchmark tailored for compositional text-to-video generation.\nT2V-CompBench encompasses diverse aspects of compositionality, including\nconsistent attribute binding, dynamic attribute binding, spatial relationships,\nmotion binding, action binding, object interactions, and generative numeracy.\nWe further carefully design evaluation metrics of MLLM-based metrics,\ndetection-based metrics, and tracking-based metrics, which can better reflect\nthe compositional text-to-video generation quality of seven proposed categories\nwith 700 text prompts. The effectiveness of the proposed metrics is verified by\ncorrelation with human evaluations. We also benchmark various text-to-video\ngenerative models and conduct in-depth analysis across different models and\ndifferent compositional categories. We find that compositional text-to-video\ngeneration is highly challenging for current models, and we hope that our\nattempt will shed light on future research in this direction.\n","authors":["Kaiyue Sun","Kaiyi Huang","Xian Liu","Yue Wu","Zihan Xu","Zhenguo Li","Xihui Liu"],"pdf_url":"https://arxiv.org/pdf/2407.14505v1.pdf","comment":"13 pages (30 in total), project page:\n  https://t2v-compbench.github.io/"},{"id":"http://arxiv.org/abs/2407.14506v1","updated":"2024-07-19T17:58:36Z","published":"2024-07-19T17:58:36Z","title":"On Pre-training of Multimodal Language Models Customized for Chart\n  Understanding","summary":"  Recent studies customizing Multimodal Large Language Models (MLLMs) for\ndomain-specific tasks have yielded promising results, especially in the field\nof scientific chart comprehension. These studies generally utilize visual\ninstruction tuning with specialized datasets to enhance question and answer\n(QA) accuracy within the chart domain. However, they often neglect the\nfundamental discrepancy between natural image-caption pre-training data and\ndigital chart image-QA data, particularly in the models' capacity to extract\nunderlying numeric values from charts. This paper tackles this oversight by\nexploring the training processes necessary to improve MLLMs' comprehension of\ncharts. We present three key findings: (1) Incorporating raw data values in\nalignment pre-training markedly improves comprehension of chart data. (2)\nReplacing images with their textual representation randomly during end-to-end\nfine-tuning transfer the language reasoning capability to chart interpretation\nskills. (3) Requiring the model to first extract the underlying chart data and\nthen answer the question in the fine-tuning can further improve the accuracy.\nConsequently, we introduce CHOPINLLM, an MLLM tailored for in-depth chart\ncomprehension. CHOPINLLM effectively interprets various types of charts,\nincluding unannotated ones, while maintaining robust reasoning abilities.\nFurthermore, we establish a new benchmark to evaluate MLLMs' understanding of\ndifferent chart types across various comprehension levels. Experimental results\nshow that CHOPINLLM exhibits strong performance in understanding both annotated\nand unannotated charts across a wide range of types.\n","authors":["Wan-Cyuan Fan","Yen-Chun Chen","Mengchen Liu","Lu Yuan","Leonid Sigal"],"pdf_url":"https://arxiv.org/pdf/2407.14506v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14502v1","updated":"2024-07-19T17:57:33Z","published":"2024-07-19T17:57:33Z","title":"M2D2M: Multi-Motion Generation from Text with Discrete Diffusion Models","summary":"  We introduce the Multi-Motion Discrete Diffusion Models (M2D2M), a novel\napproach for human motion generation from textual descriptions of multiple\nactions, utilizing the strengths of discrete diffusion models. This approach\nadeptly addresses the challenge of generating multi-motion sequences, ensuring\nseamless transitions of motions and coherence across a series of actions. The\nstrength of M2D2M lies in its dynamic transition probability within the\ndiscrete diffusion model, which adapts transition probabilities based on the\nproximity between motion tokens, encouraging mixing between different modes.\nComplemented by a two-phase sampling strategy that includes independent and\njoint denoising steps, M2D2M effectively generates long-term, smooth, and\ncontextually coherent human motion sequences, utilizing a model trained for\nsingle-motion generation. Extensive experiments demonstrate that M2D2M\nsurpasses current state-of-the-art benchmarks for motion generation from text\ndescriptions, showcasing its efficacy in interpreting language semantics and\ngenerating dynamic, realistic motions.\n","authors":["Seunggeun Chi","Hyung-gun Chi","Hengbo Ma","Nakul Agarwal","Faizan Siddiqui","Karthik Ramani","Kwonjoon Lee"],"pdf_url":"https://arxiv.org/pdf/2407.14502v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14499v1","updated":"2024-07-19T17:50:11Z","published":"2024-07-19T17:50:11Z","title":"Discover-then-Name: Task-Agnostic Concept Bottlenecks via Automated\n  Concept Discovery","summary":"  Concept Bottleneck Models (CBMs) have recently been proposed to address the\n'black-box' problem of deep neural networks, by first mapping images to a\nhuman-understandable concept space and then linearly combining concepts for\nclassification. Such models typically require first coming up with a set of\nconcepts relevant to the task and then aligning the representations of a\nfeature extractor to map to these concepts. However, even with powerful\nfoundational feature extractors like CLIP, there are no guarantees that the\nspecified concepts are detectable. In this work, we leverage recent advances in\nmechanistic interpretability and propose a novel CBM approach -- called\nDiscover-then-Name-CBM (DN-CBM) -- that inverts the typical paradigm: instead\nof pre-selecting concepts based on the downstream classification task, we use\nsparse autoencoders to first discover concepts learnt by the model, and then\nname them and train linear probes for classification. Our concept extraction\nstrategy is efficient, since it is agnostic to the downstream task, and uses\nconcepts already known to the model. We perform a comprehensive evaluation\nacross multiple datasets and CLIP architectures and show that our method yields\nsemantically meaningful concepts, assigns appropriate names to them that make\nthem easy to interpret, and yields performant and interpretable CBMs. Code\navailable at https://github.com/neuroexplicit-saar/discover-then-name.\n","authors":["Sukrut Rao","Sweta Mahajan","Moritz Böhle","Bernt Schiele"],"pdf_url":"https://arxiv.org/pdf/2407.14499v1.pdf","comment":"40 pages, 21 figures, 6 tables, European Conference on Computer\n  Vision (ECCV) 2024"},{"id":"http://arxiv.org/abs/2407.14498v1","updated":"2024-07-19T17:49:48Z","published":"2024-07-19T17:49:48Z","title":"Enhancing Layout Hotspot Detection Efficiency with YOLOv8 and PCA-Guided\n  Augmentation","summary":"  In this paper, we present a YOLO-based framework for layout hotspot\ndetection, aiming to enhance the efficiency and performance of the design rule\nchecking (DRC) process. Our approach leverages the YOLOv8 vision model to\ndetect multiple hotspots within each layout image, even when dealing with large\nlayout image sizes. Additionally, to enhance pattern-matching effectiveness, we\nintroduce a novel approach to augment the layout image using information\nextracted through Principal Component Analysis (PCA). The core of our proposed\nmethod is an algorithm that utilizes PCA to extract valuable auxiliary\ninformation from the layout image. This extracted information is then\nincorporated into the layout image as an additional color channel. This\naugmentation significantly improves the accuracy of multi-hotspot detection\nwhile reducing the false alarm rate of the object detection algorithm. We\nevaluate the effectiveness of our framework using four datasets generated from\nlayouts found in the ICCAD-2019 benchmark dataset. The results demonstrate that\nour framework achieves a precision (recall) of approximately 83% (86%) while\nmaintaining a false alarm rate of less than 7.4\\%. Also, the studies show that\nthe proposed augmentation approach could improve the detection ability of\nnever-seen-before (NSB) hotspots by about 10%.\n","authors":["Dongyang Wu","Siyang Wang","Mehdi Kamal","Massoud Pedram"],"pdf_url":"https://arxiv.org/pdf/2407.14498v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14491v1","updated":"2024-07-19T17:44:33Z","published":"2024-07-19T17:44:33Z","title":"PD-TPE: Parallel Decoder with Text-guided Position Encoding for 3D\n  Visual Grounding","summary":"  3D visual grounding aims to locate the target object mentioned by free-formed\nnatural language descriptions in 3D point cloud scenes. Most previous work\nrequires the encoder-decoder to simultaneously align the attribute information\nof the target object and its relational information with the surrounding\nenvironment across modalities. This causes the queries' attention to be\ndispersed, potentially leading to an excessive focus on points irrelevant to\nthe input language descriptions. To alleviate these issues, we propose PD-TPE,\na visual-language model with a double-branch decoder. The two branches perform\nproposal feature decoding and surrounding layout awareness in parallel. Since\ntheir attention maps are not influenced by each other, the queries focus on\ntokens relevant to each branch's specific objective. In particular, we design a\nnovel Text-guided Position Encoding method, which differs between the two\nbranches. In the main branch, the priori relies on the relative positions\nbetween tokens and predicted 3D boxes, which direct the model to pay more\nattention to tokens near the object; in the surrounding branch, it is guided by\nthe similarity between visual and text features, so that the queries attend to\ntokens that can provide effective layout information. Extensive experiments\ndemonstrate that we surpass the state-of-the-art on two widely adopted 3D\nvisual grounding datasets, ScanRefer and NR3D, by 1.8% and 2.2%, respectively.\nCodes will be made publicly available.\n","authors":["Chenshu Hou","Liang Peng","Xiaopei Wu","Wenxiao Wang","Xiaofei He"],"pdf_url":"https://arxiv.org/pdf/2407.14491v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.06660v2","updated":"2024-07-19T17:33:49Z","published":"2023-12-11T18:59:52Z","title":"EdgeSAM: Prompt-In-the-Loop Distillation for On-Device Deployment of SAM","summary":"  This paper presents EdgeSAM, an accelerated variant of the Segment Anything\nModel (SAM), optimized for efficient execution on edge devices with minimal\ncompromise in performance. Our approach involves distilling the original\nViT-based SAM image encoder into a purely CNN-based architecture, better suited\nfor edge devices. We carefully benchmark various distillation strategies and\ndemonstrate that taskagnostic encoder distillation fails to capture the full\nknowledge embodied in SAM. To overcome this bottleneck, we include both the\nprompt encoder and mask decoder in the distillation process, with box and point\nprompts in the loop, so that the distilled model can accurately capture the\nintricate dynamics between user input and mask generation. To mitigate dataset\nbias issues stemming from point prompt distillation, we incorporate a\nlightweight module within the encoder. As a result, EdgeSAM achieves a 37-fold\nspeed increase compared to the original SAM, and it also outperforms\nMobileSAM/EfficientSAM, being over 7 times as fast when deployed on edge\ndevices while enhancing the mIoUs on COCO and LVIS by 2.3/1.5 and 3.1/1.6,\nrespectively. It is also the first SAM variant that can run at over 30 FPS on\nan iPhone 14. Code and demo are available at\nhttps://www.mmlab-ntu.com/project/edgesam.\n","authors":["Chong Zhou","Xiangtai Li","Chen Change Loy","Bo Dai"],"pdf_url":"https://arxiv.org/pdf/2312.06660v2.pdf","comment":"Project page: https://mmlab-ntu.github.io/project/edgesam/"},{"id":"http://arxiv.org/abs/2407.14478v1","updated":"2024-07-19T17:28:49Z","published":"2024-07-19T17:28:49Z","title":"A review on vision-based motion estimation","summary":"  Compared to contact sensors-based motion measurement, vision-based motion\nmeasurement has advantages of low cost and high efficiency and have been under\nactive development in the past decades. This paper provides a review on\nexisting motion measurement methods. In addition to the development of each\nbranch of vision-based motion measurement methods, this paper also discussed\nthe advantages and disadvantages of existing methods. Based on this discussion,\nit was identified that existing methods have a common limitation in optimally\nbalancing accuracy and robustness. To address issue, we developed the Gaussian\nkernel-based motion measurement method. Preliminary study shows that the\ndeveloped method can achieve high accuracy on simple synthesized images.\n","authors":["Hongyi Liu","Haifeng Wang"],"pdf_url":"https://arxiv.org/pdf/2407.14478v1.pdf","comment":"2 figures"},{"id":"http://arxiv.org/abs/2407.14474v1","updated":"2024-07-19T17:24:25Z","published":"2024-07-19T17:24:25Z","title":"Contrastive Learning with Counterfactual Explanations for Radiology\n  Report Generation","summary":"  Due to the common content of anatomy, radiology images with their\ncorresponding reports exhibit high similarity. Such inherent data bias can\npredispose automatic report generation models to learn entangled and spurious\nrepresentations resulting in misdiagnostic reports. To tackle these, we propose\na novel \\textbf{Co}unter\\textbf{F}actual \\textbf{E}xplanations-based framework\n(CoFE) for radiology report generation. Counterfactual explanations serve as a\npotent tool for understanding how decisions made by algorithms can be changed\nby asking ``what if'' scenarios. By leveraging this concept, CoFE can learn\nnon-spurious visual representations by contrasting the representations between\nfactual and counterfactual images. Specifically, we derive counterfactual\nimages by swapping a patch between positive and negative samples until a\npredicted diagnosis shift occurs. Here, positive and negative samples are the\nmost semantically similar but have different diagnosis labels. Additionally,\nCoFE employs a learnable prompt to efficiently fine-tune the pre-trained large\nlanguage model, encapsulating both factual and counterfactual content to\nprovide a more generalizable prompt representation. Extensive experiments on\ntwo benchmarks demonstrate that leveraging the counterfactual explanations\nenables CoFE to generate semantically coherent and factually complete reports\nand outperform in terms of language generation and clinical efficacy metrics.\n","authors":["Mingjie Li","Haokun Lin","Liang Qiu","Xiaodan Liang","Ling Chen","Abdulmotaleb Elsaddik","Xiaojun Chang"],"pdf_url":"https://arxiv.org/pdf/2407.14474v1.pdf","comment":"ECCV 2024"},{"id":"http://arxiv.org/abs/2407.14473v1","updated":"2024-07-19T17:21:53Z","published":"2024-07-19T17:21:53Z","title":"MLMT-CNN for Object Detection and Segmentation in Multi-layer and\n  Multi-spectral Images","summary":"  Precisely localising solar Active Regions (AR) from multi-spectral images is\na challenging but important task in understanding solar activity and its\ninfluence on space weather. A main challenge comes from each modality capturing\na different location of the 3D objects, as opposed to typical multi-spectral\nimaging scenarios where all image bands observe the same scene. Thus, we refer\nto this special multi-spectral scenario as multi-layer. We present a multi-task\ndeep learning framework that exploits the dependencies between image bands to\nproduce 3D AR localisation (segmentation and detection) where different image\nbands (and physical locations) have their own set of results. Furthermore, to\naddress the difficulty of producing dense AR annotations for training\nsupervised machine learning (ML) algorithms, we adapt a training strategy based\non weak labels (i.e. bounding boxes) in a recursive manner. We compare our\ndetection and segmentation stages against baseline approaches for solar image\nanalysis (multi-channel coronal hole detection, SPOCA for ARs) and\nstate-of-the-art deep learning methods (Faster RCNN, U-Net). Additionally, both\ndetection a nd segmentation stages are quantitatively validated on artificially\ncreated data of similar spatial configurations made from annotated multi-modal\nmagnetic resonance images. Our framework achieves an average of 0.72 IoU\n(segmentation) and 0.90 F1 score (detection) across all modalities, comparing\nto the best performing baseline methods with scores of 0.53 and 0.58,\nrespectively, on the artificial dataset, and 0.84 F1 score in the AR detection\ntask comparing to baseline of 0.82 F1 score. Our segmentation results are\nqualitatively validated by an expert on real ARs.\n","authors":["Majedaldein Almahasneh","Adeline Paiement","Xianghua Xie","Jean Aboudarham"],"pdf_url":"https://arxiv.org/pdf/2407.14473v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14464v1","updated":"2024-07-19T17:06:27Z","published":"2024-07-19T17:06:27Z","title":"AttentNet: Fully Convolutional 3D Attention for Lung Nodule Detection","summary":"  Motivated by the increasing popularity of attention mechanisms, we observe\nthat popular convolutional (conv.) attention models like Squeeze-and-Excite\n(SE) and Convolutional Block Attention Module (CBAM) rely on expensive\nmulti-layer perception (MLP) layers. These MLP layers significantly increase\ncomputational complexity, making such models less applicable to 3D image\ncontexts, where data dimensionality and computational costs are higher. In 3D\nmedical imaging, such as 3D pulmonary CT scans, efficient processing is crucial\ndue to the large data volume. Traditional 2D attention generalized to 3D\nincreases the computational load, creating demand for more efficient attention\nmechanisms for 3D tasks. We investigate the possibility of incorporating fully\nconvolutional (conv.) attention in 3D context. We present two 3D fully conv.\nattention blocks, demonstrating their effectiveness in 3D context. Using\npulmonary CT scans for 3D lung nodule detection, we present AttentNet, an\nautomated lung nodule detection framework from CT images, performing detection\nas an ensemble of two stages, candidate proposal and false positive (FP)\nreduction. We compare the proposed 3D attention blocks to popular 2D conv.\nattention methods generalized to 3D modules and to self-attention units. For\nthe FP reduction stage, we also use a joint analysis approach to aggregate\nspatial information from different contextual levels. We use LUNA-16 lung\nnodule detection dataset to demonstrate the benefits of the proposed fully\nconv. attention blocks compared to baseline popular lung nodule detection\nmethods when no attention is used. Our work does not aim at achieving\nstate-of-the-art results in the lung nodule detection task, rather to\ndemonstrate the benefits of incorporating fully conv. attention within a 3D\ncontext.\n","authors":["Majedaldein Almahasneh","Xianghua Xie","Adeline Paiement"],"pdf_url":"https://arxiv.org/pdf/2407.14464v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.08070v2","updated":"2024-07-19T16:51:02Z","published":"2024-02-12T21:31:13Z","title":"Multi-Attribute Vision Transformers are Efficient and Robust Learners","summary":"  Since their inception, Vision Transformers (ViTs) have emerged as a\ncompelling alternative to Convolutional Neural Networks (CNNs) across a wide\nspectrum of tasks. ViTs exhibit notable characteristics, including global\nattention, resilience against occlusions, and adaptability to distribution\nshifts. One underexplored aspect of ViTs is their potential for multi-attribute\nlearning, referring to their ability to simultaneously grasp multiple\nattribute-related tasks. In this paper, we delve into the multi-attribute\nlearning capability of ViTs, presenting a straightforward yet effective\nstrategy for training various attributes through a single ViT network as\ndistinct tasks. We assess the resilience of multi-attribute ViTs against\nadversarial attacks and compare their performance against ViTs designed for\nsingle attributes. Moreover, we further evaluate the robustness of\nmulti-attribute ViTs against a recent transformer based attack called\nPatch-Fool. Our empirical findings on the CelebA dataset provide validation for\nour assertion. Our code is available at https://github.com/hananshafi/MTL-ViT\n","authors":["Hanan Gani","Nada Saadi","Noor Hussein","Karthik Nandakumar"],"pdf_url":"https://arxiv.org/pdf/2402.08070v2.pdf","comment":"Accepted at IEEE ICIP 2024. arXiv admin note: text overlap with\n  arXiv:2207.08677 by other authors"},{"id":"http://arxiv.org/abs/2406.00365v2","updated":"2024-07-19T16:32:05Z","published":"2024-06-01T08:58:40Z","title":"SynthBA: Reliable Brain Age Estimation Across Multiple MRI Sequences and\n  Resolutions","summary":"  Brain age is a critical measure that reflects the biological ageing process\nof the brain. The gap between brain age and chronological age, referred to as\nbrain PAD (Predicted Age Difference), has been utilized to investigate\nneurodegenerative conditions. Brain age can be predicted using MRIs and machine\nlearning techniques. However, existing methods are often sensitive to\nacquisition-related variabilities, such as differences in acquisition\nprotocols, scanners, MRI sequences, and resolutions, significantly limiting\ntheir application in highly heterogeneous clinical settings. In this study, we\nintroduce Synthetic Brain Age (SynthBA), a robust deep-learning model designed\nfor predicting brain age. SynthBA utilizes an advanced domain randomization\ntechnique, ensuring effective operation across a wide array of\nacquisition-related variabilities. To assess the effectiveness and robustness\nof SynthBA, we evaluate its predictive capabilities on internal and external\ndatasets, encompassing various MRI sequences and resolutions, and compare it\nwith state-of-the-art techniques. Additionally, we calculate the brain PAD in a\nlarge cohort of subjects with Alzheimer's Disease (AD), demonstrating a\nsignificant correlation with AD-related measures of cognitive dysfunction.\nSynthBA holds the potential to facilitate the broader adoption of brain age\nprediction in clinical settings, where re-training or fine-tuning is often\nunfeasible. The SynthBA source code and pre-trained models are publicly\navailable at https://github.com/LemuelPuglisi/SynthBA.\n","authors":["Lemuel Puglisi","Alessia Rondinella","Linda De Meo","Francesco Guarnera","Sebastiano Battiato","Daniele Ravì"],"pdf_url":"https://arxiv.org/pdf/2406.00365v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05049v2","updated":"2024-07-19T16:31:19Z","published":"2024-03-08T04:52:22Z","title":"XPSR: Cross-modal Priors for Diffusion-based Image Super-Resolution","summary":"  Diffusion-based methods, endowed with a formidable generative prior, have\nreceived increasing attention in Image Super-Resolution (ISR) recently.\nHowever, as low-resolution (LR) images often undergo severe degradation, it is\nchallenging for ISR models to perceive the semantic and degradation\ninformation, resulting in restoration images with incorrect content or\nunrealistic artifacts. To address these issues, we propose a\n\\textit{Cross-modal Priors for Super-Resolution (XPSR)} framework. Within XPSR,\nto acquire precise and comprehensive semantic conditions for the diffusion\nmodel, cutting-edge Multimodal Large Language Models (MLLMs) are utilized. To\nfacilitate better fusion of cross-modal priors, a \\textit{Semantic-Fusion\nAttention} is raised. To distill semantic-preserved information instead of\nundesired degradations, a \\textit{Degradation-Free Constraint} is attached\nbetween LR and its high-resolution (HR) counterpart. Quantitative and\nqualitative results show that XPSR is capable of generating high-fidelity and\nhigh-realism images across synthetic and real-world datasets. Codes are\nreleased at \\url{https://github.com/qyp2000/XPSR}.\n","authors":["Yunpeng Qu","Kun Yuan","Kai Zhao","Qizhi Xie","Jinhua Hao","Ming Sun","Chao Zhou"],"pdf_url":"https://arxiv.org/pdf/2403.05049v2.pdf","comment":"19 pages, 7 figures; including supplementary material"},{"id":"http://arxiv.org/abs/2407.04237v3","updated":"2024-07-19T16:30:59Z","published":"2024-07-05T03:43:08Z","title":"GSD: View-Guided Gaussian Splatting Diffusion for 3D Reconstruction","summary":"  We present GSD, a diffusion model approach based on Gaussian Splatting (GS)\nrepresentation for 3D object reconstruction from a single view. Prior works\nsuffer from inconsistent 3D geometry or mediocre rendering quality due to\nimproper representations. We take a step towards resolving these shortcomings\nby utilizing the recent state-of-the-art 3D explicit representation, Gaussian\nSplatting, and an unconditional diffusion model. This model learns to generate\n3D objects represented by sets of GS ellipsoids. With these strong generative\n3D priors, though learning unconditionally, the diffusion model is ready for\nview-guided reconstruction without further model fine-tuning. This is achieved\nby propagating fine-grained 2D features through the efficient yet flexible\nsplatting function and the guided denoising sampling process. In addition, a 2D\ndiffusion model is further employed to enhance rendering fidelity, and improve\nreconstructed GS quality by polishing and re-using the rendered images. The\nfinal reconstructed objects explicitly come with high-quality 3D structure and\ntexture, and can be efficiently rendered in arbitrary views. Experiments on the\nchallenging real-world CO3D dataset demonstrate the superiority of our\napproach. Project page: $\\href{https://yxmu.foo/GSD/}{\\text{this https URL}}$\n","authors":["Yuxuan Mu","Xinxin Zuo","Chuan Guo","Yilin Wang","Juwei Lu","Xiaofeng Wu","Songcen Xu","Peng Dai","Youliang Yan","Li Cheng"],"pdf_url":"https://arxiv.org/pdf/2407.04237v3.pdf","comment":"Accepted for ECCV 2024"},{"id":"http://arxiv.org/abs/2312.15130v3","updated":"2024-07-19T16:28:09Z","published":"2023-12-23T01:38:41Z","title":"PACE: A Large-Scale Dataset with Pose Annotations in Cluttered\n  Environments","summary":"  We introduce PACE (Pose Annotations in Cluttered Environments), a large-scale\nbenchmark designed to advance the development and evaluation of pose estimation\nmethods in cluttered scenarios. PACE provides a large-scale real-world\nbenchmark for both instance-level and category-level settings. The benchmark\nconsists of 55K frames with 258K annotations across 300 videos, covering 238\nobjects from 43 categories and featuring a mix of rigid and articulated items\nin cluttered scenes. To annotate the real-world data efficiently, we develop an\ninnovative annotation system with a calibrated 3-camera setup. Additionally, we\noffer PACE-Sim, which contains 100K photo-realistic simulated frames with 2.4M\nannotations across 931 objects. We test state-of-the-art algorithms in PACE\nalong two tracks: pose estimation, and object pose tracking, revealing the\nbenchmark's challenges and research opportunities. Our benchmark code and data\nis available on https://github.com/qq456cvb/PACE.\n","authors":["Yang You","Kai Xiong","Zhening Yang","Zhengxiang Huang","Junwei Zhou","Ruoxi Shi","Zhou Fang","Adam W. Harley","Leonidas Guibas","Cewu Lu"],"pdf_url":"https://arxiv.org/pdf/2312.15130v3.pdf","comment":"14 pages; Accepted to ECCV 2024"},{"id":"http://arxiv.org/abs/2407.01519v2","updated":"2024-07-19T16:25:53Z","published":"2024-07-01T17:59:12Z","title":"DiffIR2VR-Zero: Zero-Shot Video Restoration with Diffusion-based Image\n  Restoration Models","summary":"  This paper introduces a method for zero-shot video restoration using\npre-trained image restoration diffusion models. Traditional video restoration\nmethods often need retraining for different settings and struggle with limited\ngeneralization across various degradation types and datasets. Our approach uses\na hierarchical token merging strategy for keyframes and local frames, combined\nwith a hybrid correspondence mechanism that blends optical flow and\nfeature-based nearest neighbor matching (latent merging). We show that our\nmethod not only achieves top performance in zero-shot video restoration but\nalso significantly surpasses trained models in generalization across diverse\ndatasets and extreme degradations (8$\\times$ super-resolution and high-standard\ndeviation video denoising). We present evidence through quantitative metrics\nand visual comparisons on various challenging datasets. Additionally, our\ntechnique works with any 2D restoration diffusion model, offering a versatile\nand powerful tool for video enhancement tasks without extensive retraining.\nThis research leads to more efficient and widely applicable video restoration\ntechnologies, supporting advancements in fields that require high-quality video\noutput. See our project page for video results at\nhttps://jimmycv07.github.io/DiffIR2VR_web/.\n","authors":["Chang-Han Yeh","Chin-Yang Lin","Zhixiang Wang","Chi-Wei Hsiao","Ting-Hsuan Chen","Yu-Lun Liu"],"pdf_url":"https://arxiv.org/pdf/2407.01519v2.pdf","comment":"Project page: https://jimmycv07.github.io/DiffIR2VR_web/"},{"id":"http://arxiv.org/abs/2406.05800v2","updated":"2024-07-19T16:16:50Z","published":"2024-06-09T14:30:18Z","title":"SlowPerception: Physical-World Latency Attack against Visual Perception\n  in Autonomous Driving","summary":"  Autonomous Driving (AD) systems critically depend on visual perception for\nreal-time object detection and multiple object tracking (MOT) to ensure safe\ndriving. However, high latency in these visual perception components can lead\nto significant safety risks, such as vehicle collisions. While previous\nresearch has extensively explored latency attacks within the digital realm,\ntranslating these methods effectively to the physical world presents\nchallenges. For instance, existing attacks rely on perturbations that are\nunrealistic or impractical for AD, such as adversarial perturbations affecting\nareas like the sky, or requiring large patches that obscure most of a camera's\nview, thus making them impossible to be conducted effectively in the real\nworld.\n  In this paper, we introduce SlowPerception, the first physical-world latency\nattack against AD perception, via generating projector-based universal\nperturbations. SlowPerception strategically creates numerous phantom objects on\nvarious surfaces in the environment, significantly increasing the computational\nload of Non-Maximum Suppression (NMS) and MOT, thereby inducing substantial\nlatency. Our SlowPerception achieves second-level latency in physical-world\nsettings, with an average latency of 2.5 seconds across different AD perception\nsystems, scenarios, and hardware configurations. This performance significantly\noutperforms existing state-of-the-art latency attacks. Additionally, we conduct\nAD system-level impact assessments, such as vehicle collisions, using\nindustry-grade AD systems with production-grade AD simulators with a 97%\naverage rate. We hope that our analyses can inspire further research in this\ncritical domain, enhancing the robustness of AD systems against emerging\nvulnerabilities.\n","authors":["Chen Ma","Ningfei Wang","Zhengyu Zhao","Qi Alfred Chen","Chao Shen"],"pdf_url":"https://arxiv.org/pdf/2406.05800v2.pdf","comment":"This submission was made without all contributors' consent"},{"id":"http://arxiv.org/abs/2407.14439v1","updated":"2024-07-19T16:11:15Z","published":"2024-07-19T16:11:15Z","title":"Token-level Correlation-guided Compression for Efficient Multimodal\n  Document Understanding","summary":"  Cropping high-resolution document images into multiple sub-images is the most\nwidely used approach for current Multimodal Large Language Models (MLLMs) to do\ndocument understanding. Most of current document understanding methods preserve\nall tokens within sub-images and treat them equally. This neglects their\ndifferent informativeness and leads to a significant increase in the number of\nimage tokens. To perform a more adaptive and efficient document understanding,\nwe propose Token-level Correlation-guided Compression, a parameter-free and\nplug-and-play methodology to optimize token processing. Firstly, we propose an\ninnovative approach for assessing the pattern repetitiveness based on the\ncorrelation between each patch tokens. This method identifies redundant tokens,\nallowing for the determination of the sub-image's information density.\nSecondly, we present a token-level sampling method that efficiently captures\nthe most informative tokens by delving into the correlation between the [CLS]\ntoken and patch tokens. By integrating these strategies, we develop a\nplug-and-play adaptive compressor module that can be seamlessly incorporated\ninto MLLMs utilizing cropping techniques. This module not only enhances the\nprocessing speed during training and inference but also maintains comparable\nperformance. We conduct experiments with the SOTA document understanding model\nmPLUG-DocOwl1.5 and the effectiveness is demonstrated through extensive\ncomparisons with other compression methods.\n","authors":["Renshan Zhang","Yibo Lyu","Rui Shao","Gongwei Chen","Weili Guan","Liqiang Nie"],"pdf_url":"https://arxiv.org/pdf/2407.14439v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.17316v5","updated":"2024-07-19T16:10:14Z","published":"2023-10-26T11:23:24Z","title":"Defect Spectrum: A Granular Look of Large-Scale Defect Datasets with\n  Rich Semantics","summary":"  Defect inspection is paramount within the closed-loop manufacturing system.\nHowever, existing datasets for defect inspection often lack precision and\nsemantic granularity required for practical applications. In this paper, we\nintroduce the Defect Spectrum, a comprehensive benchmark that offers precise,\nsemantic-abundant, and large-scale annotations for a wide range of industrial\ndefects. Building on four key industrial benchmarks, our dataset refines\nexisting annotations and introduces rich semantic details, distinguishing\nmultiple defect types within a single image. Furthermore, we introduce\nDefect-Gen, a two-stage diffusion-based generator designed to create\nhigh-quality and diverse defective images, even when working with limited\ndatasets. The synthetic images generated by Defect-Gen significantly enhance\nthe efficacy of defect inspection models. Overall, The Defect Spectrum dataset\ndemonstrates its potential in defect inspection research, offering a solid\nplatform for testing and refining advanced models.\n","authors":["Shuai Yang","Zhifei Chen","Pengguang Chen","Xi Fang","Yixun Liang","Shu Liu","Yingcong Chen"],"pdf_url":"https://arxiv.org/pdf/2310.17316v5.pdf","comment":"Accepted by ECCV2024. Please see our project page at\n  https://envision-research.github.io/Defect_Spectrum/"},{"id":"http://arxiv.org/abs/2407.14434v1","updated":"2024-07-19T16:06:11Z","published":"2024-07-19T16:06:11Z","title":"Co-synthesis of Histopathology Nuclei Image-Label Pairs using a\n  Context-Conditioned Joint Diffusion Model","summary":"  In multi-class histopathology nuclei analysis tasks, the lack of training\ndata becomes a main bottleneck for the performance of learning-based methods.\nTo tackle this challenge, previous methods have utilized generative models to\nincrease data by generating synthetic samples. However, existing methods often\noverlook the importance of considering the context of biological tissues (e.g.,\nshape, spatial layout, and tissue type) in the synthetic data. Moreover, while\ngenerative models have shown superior performance in synthesizing realistic\nhistopathology images, none of the existing methods are capable of producing\nimage-label pairs at the same time. In this paper, we introduce a novel\nframework for co-synthesizing histopathology nuclei images and paired semantic\nlabels using a context-conditioned joint diffusion model. We propose\nconditioning of a diffusion model using nucleus centroid layouts with\nstructure-related text prompts to incorporate spatial and structural context\ninformation into the generation targets. Moreover, we enhance the granularity\nof our synthesized semantic labels by generating instance-wise nuclei labels\nusing distance maps synthesized concurrently in conjunction with the images and\nsemantic labels. We demonstrate the effectiveness of our framework in\ngenerating high-quality samples on multi-institutional, multi-organ, and\nmulti-modality datasets. Our synthetic data consistently outperforms existing\naugmentation methods in the downstream tasks of nuclei segmentation and\nclassification.\n","authors":["Seonghui Min","Hyun-Jic Oh","Won-Ki Jeong"],"pdf_url":"https://arxiv.org/pdf/2407.14434v1.pdf","comment":"ECCV 2024 accepted"},{"id":"http://arxiv.org/abs/2407.14429v1","updated":"2024-07-19T15:59:04Z","published":"2024-07-19T15:59:04Z","title":"Dataset Distillation in Medical Imaging: A Feasibility Study","summary":"  Data sharing in the medical image analysis field has potential yet remains\nunderappreciated. The aim is often to share datasets efficiently with other\nsites to train models effectively. One possible solution is to avoid\ntransferring the entire dataset while still achieving similar model\nperformance. Recent progress in data distillation within computer science\noffers promising prospects for sharing medical data efficiently without\nsignificantly compromising model effectiveness. However, it remains uncertain\nwhether these methods would be applicable to medical imaging, since medical and\nnatural images are distinct fields. Moreover, it is intriguing to consider what\nlevel of performance could be achieved with these methods. To answer these\nquestions, we conduct investigations on a variety of leading data distillation\nmethods, in different contexts of medical imaging. We evaluate the feasibility\nof these methods with extensive experiments in two aspects: 1) Assess the\nimpact of data distillation across multiple datasets characterized by minor or\ngreat variations. 2) Explore the indicator to predict the distillation\nperformance. Our extensive experiments across multiple medical datasets reveal\nthat data distillation can significantly reduce dataset size while maintaining\ncomparable model performance to that achieved with the full dataset, suggesting\nthat a small, representative sample of images can serve as a reliable indicator\nof distillation success. This study demonstrates that data distillation is a\nviable method for efficient and secure medical data sharing, with the potential\nto facilitate enhanced collaborative research and clinical applications.\n","authors":["Muyang Li","Can Cui","Quan Liu","Ruining Deng","Tianyuan Yao","Marilyn Lionts","Yuankai Huo"],"pdf_url":"https://arxiv.org/pdf/2407.14429v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14426v1","updated":"2024-07-19T15:53:44Z","published":"2024-07-19T15:53:44Z","title":"Controllable and Efficient Multi-Class Pathology Nuclei Data\n  Augmentation using Text-Conditioned Diffusion Models","summary":"  In the field of computational pathology, deep learning algorithms have made\nsignificant progress in tasks such as nuclei segmentation and classification.\nHowever, the potential of these advanced methods is limited by the lack of\navailable labeled data. Although image synthesis via recent generative models\nhas been actively explored to address this challenge, existing works have\nbarely addressed label augmentation and are mostly limited to single-class and\nunconditional label generation. In this paper, we introduce a novel two-stage\nframework for multi-class nuclei data augmentation using text-conditional\ndiffusion models. In the first stage, we innovate nuclei label synthesis by\ngenerating multi-class semantic labels and corresponding instance maps through\na joint diffusion model conditioned by text prompts that specify the label\nstructure information. In the second stage, we utilize a semantic and\ntext-conditional latent diffusion model to efficiently generate high-quality\npathology images that align with the generated nuclei label images. We\ndemonstrate the effectiveness of our method on large and diverse pathology\nnuclei datasets, with evaluations including qualitative and quantitative\nanalyses, as well as assessments of downstream tasks.\n","authors":["Hyun-Jic Oh","Won-Ki Jeong"],"pdf_url":"https://arxiv.org/pdf/2407.14426v1.pdf","comment":"MICCAI 2024 accepted"},{"id":"http://arxiv.org/abs/2407.14419v1","updated":"2024-07-19T15:43:24Z","published":"2024-07-19T15:43:24Z","title":"HOTS3D: Hyper-Spherical Optimal Transport for Semantic Alignment of\n  Text-to-3D Generation","summary":"  Recent CLIP-guided 3D generation methods have achieved promising results but\nstruggle with generating faithful 3D shapes that conform with input text due to\nthe gap between text and image embeddings. To this end, this paper proposes\nHOTS3D which makes the first attempt to effectively bridge this gap by aligning\ntext features to the image features with spherical optimal transport (SOT).\nHowever, in high-dimensional situations, solving the SOT remains a challenge.\nTo obtain the SOT map for high-dimensional features obtained from CLIP encoding\nof two modalities, we mathematically formulate and derive the solution based on\nVillani's theorem, which can directly align two hyper-sphere distributions\nwithout manifold exponential maps. Furthermore, we implement it by leveraging\ninput convex neural networks (ICNNs) for the optimal Kantorovich potential.\nWith the optimally mapped features, a diffusion-based generator and a\nNerf-based decoder are subsequently utilized to transform them into 3D shapes.\nExtensive qualitative and qualitative comparisons with state-of-the-arts\ndemonstrate the superiority of the proposed HOTS3D for 3D shape generation,\nespecially on the consistency with text semantics.\n","authors":["Zezeng Li","Weimin Wang","WenHai Li","Na Lei","Xianfeng Gu"],"pdf_url":"https://arxiv.org/pdf/2407.14419v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14418v1","updated":"2024-07-19T15:43:16Z","published":"2024-07-19T15:43:16Z","title":"Improving classification of road surface conditions via road area\n  extraction and contrastive learning","summary":"  Maintaining roads is crucial to economic growth and citizen well-being\nbecause roads are a vital means of transportation. In various countries, the\ninspection of road surfaces is still done manually, however, to automate it,\nresearch interest is now focused on detecting the road surface defects via the\nvisual data. While, previous research has been focused on deep learning methods\nwhich tend to process the entire image and leads to heavy computational cost.\nIn this study, we focus our attention on improving the classification\nperformance while keeping the computational cost of our solution low. Instead\nof processing the whole image, we introduce a segmentation model to only focus\nthe downstream classification model to the road surface in the image.\nFurthermore, we employ contrastive learning during model training to improve\nthe road surface condition classification. Our experiments on the public RTK\ndataset demonstrate a significant improvement in our proposed method when\ncompared to previous works.\n","authors":["Linh Trinh","Ali Anwar","Siegfried Mercelis"],"pdf_url":"https://arxiv.org/pdf/2407.14418v1.pdf","comment":"7 pages"},{"id":"http://arxiv.org/abs/2407.14412v1","updated":"2024-07-19T15:39:19Z","published":"2024-07-19T15:39:19Z","title":"DEAL: Disentangle and Localize Concept-level Explanations for VLMs","summary":"  Large pre-trained Vision-Language Models (VLMs) have become ubiquitous\nfoundational components of other models and downstream tasks. Although\npowerful, our empirical results reveal that such models might not be able to\nidentify fine-grained concepts. Specifically, the explanations of VLMs with\nrespect to fine-grained concepts are entangled and mislocalized. To address\nthis issue, we propose to DisEntAngle and Localize (DEAL) the concept-level\nexplanations for VLMs without human annotations. The key idea is encouraging\nthe concept-level explanations to be distinct while maintaining consistency\nwith category-level explanations. We conduct extensive experiments and ablation\nstudies on a wide range of benchmark datasets and vision-language models. Our\nempirical results demonstrate that the proposed method significantly improves\nthe concept-level explanations of the model in terms of disentanglability and\nlocalizability. Surprisingly, the improved explainability alleviates the\nmodel's reliance on spurious correlations, which further benefits the\nprediction accuracy.\n","authors":["Tang Li","Mengmeng Ma","Xi Peng"],"pdf_url":"https://arxiv.org/pdf/2407.14412v1.pdf","comment":"In Proceedings of the European Conference on Computer Vision (ECCV),\n  2024"},{"id":"http://arxiv.org/abs/2405.01607v2","updated":"2024-07-19T15:25:15Z","published":"2024-05-02T04:53:42Z","title":"Wildfire Risk Prediction: A Review","summary":"  Wildfires have significant impacts on global vegetation, wildlife, and\nhumans. They destroy plant communities and wildlife habitats and contribute to\nincreased emissions of carbon dioxide, nitrogen oxides, methane, and other\npollutants. The prediction of wildfires relies on various independent variables\ncombined with regression or machine learning methods. In this technical review,\nwe describe the options for independent variables, data processing techniques,\nmodels, independent variables collinearity and importance estimation methods,\nand model performance evaluation metrics. First, we divide the independent\nvariables into 4 aspects, including climate and meteorology conditions,\nsocio-economical factors, terrain and hydrological features, and wildfire\nhistorical records. Second, preprocessing methods are described for different\nmagnitudes, different spatial-temporal resolutions, and different formats of\ndata. Third, the collinearity and importance evaluation methods of independent\nvariables are also considered. Fourth, we discuss the application of\nstatistical models, traditional machine learning models, and deep learning\nmodels in wildfire risk prediction. In this subsection, compared with other\nreviews, this manuscript particularly discusses the evaluation metrics and\nrecent advancements in deep learning methods. Lastly, addressing the\nlimitations of current research, this paper emphasizes the need for more\neffective deep learning time series forecasting algorithms, the utilization of\nthree-dimensional data including ground and trunk fuel, extraction of more\naccurate historical fire point data, and improved model evaluation metrics.\n","authors":["Zhengsen Xu","Jonathan Li","Linlin Xu"],"pdf_url":"https://arxiv.org/pdf/2405.01607v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2208.12527v3","updated":"2024-07-19T15:05:09Z","published":"2022-08-26T09:35:20Z","title":"Unsupervised Spike Depth Estimation via Cross-modality Cross-domain\n  Knowledge Transfer","summary":"  Neuromorphic spike data, an upcoming modality with high temporal resolution,\nhas shown promising potential in autonomous driving by mitigating the\nchallenges posed by high-velocity motion blur. However, training the spike\ndepth estimation network holds significant challenges in two aspects: sparse\nspatial information for pixel-wise tasks and difficulties in achieving paired\ndepth labels for temporally intensive spike streams. Therefore, we introduce\nopen-source RGB data to support spike depth estimation, leveraging its\nannotations and spatial information. The inherent differences in modalities and\ndata distribution make it challenging to directly apply transfer learning from\nopen-source RGB to target spike data. To this end, we propose a cross-modality\ncross-domain (BiCross) framework to realize unsupervised spike depth estimation\nby introducing simulated mediate source spike data. Specifically, we design a\nCoarse-to-Fine Knowledge Distillation (CFKD) approach to facilitate\ncomprehensive cross-modality knowledge transfer while preserving the unique\nstrengths of both modalities, utilizing a spike-oriented uncertainty scheme.\nThen, we propose a Self-Correcting Teacher-Student (SCTS) mechanism to screen\nout reliable pixel-wise pseudo labels and ease the domain shift of the student\nmodel, which avoids error accumulation in target spike data. To verify the\neffectiveness of BiCross, we conduct extensive experiments on four scenarios,\nincluding Synthetic to Real, Extreme Weather, Scene Changing, and Real Spike.\nOur method achieves state-of-the-art (SOTA) performances, compared with\nRGB-oriented unsupervised depth estimation methods. Code and dataset:\nhttps://github.com/Theia-4869/BiCross\n","authors":["Jiaming Liu","Qizhe Zhang","Xiaoqi Li","Jianing Li","Guanqun Wang","Ming Lu","Tiejun Huang","Shanghang Zhang"],"pdf_url":"https://arxiv.org/pdf/2208.12527v3.pdf","comment":"Accepted by ICRA2024"},{"id":"http://arxiv.org/abs/2407.14367v1","updated":"2024-07-19T14:53:18Z","published":"2024-07-19T14:53:18Z","title":"Thinking Racial Bias in Fair Forgery Detection: Models, Datasets and\n  Evaluations","summary":"  Due to the successful development of deep image generation technology,\nforgery detection plays a more important role in social and economic security.\nRacial bias has not been explored thoroughly in the deep forgery detection\nfield. In the paper, we first contribute a dedicated dataset called the Fair\nForgery Detection (FairFD) dataset, where we prove the racial bias of public\nstate-of-the-art (SOTA) methods. Different from existing forgery detection\ndatasets, the self-construct FairFD dataset contains a balanced racial ratio\nand diverse forgery generation images with the largest-scale subjects.\nAdditionally, we identify the problems with naive fairness metrics when\nbenchmarking forgery detection models. To comprehensively evaluate fairness, we\ndesign novel metrics including Approach Averaged Metric and Utility Regularized\nMetric, which can avoid deceptive results. Extensive experiments conducted with\nnine representative forgery detection models demonstrate the value of the\nproposed dataset and the reasonability of the designed fairness metrics. We\nalso conduct more in-depth analyses to offer more insights to inspire\nresearchers in the community.\n","authors":["Decheng Liu","Zongqi Wang","Chunlei Peng","Nannan Wang","Ruimin Hu","Xinbo Gao"],"pdf_url":"https://arxiv.org/pdf/2407.14367v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14357v1","updated":"2024-07-19T14:38:47Z","published":"2024-07-19T14:38:47Z","title":"Interior Object Geometry via Fitted Frames","summary":"  We describe a representation targeted for anatomic objects which is designed\nto enable strong locational correspondence within object populations and thus\nto provide powerful object statistics. The method generates fitted frames on\nthe boundary and in the interior of objects and produces alignment-free\ngeometric features from them. It accomplishes this by understanding an object\nas the diffeomorphic deformation of an ellipsoid and using a skeletal\nrepresentation fitted throughout the deformation to produce a model of the\ntarget object, where the object is provided initially in the form of a boundary\nmesh. Via classification performance on hippocampi shape between individuals\nwith a disorder vs. others, we compare our method to two state-of-the-art\nmethods for producing object representations that are intended to capture\ngeometric correspondence across a population of objects and to yield geometric\nfeatures useful for statistics, and we show improved classification performance\nby this new representation, which we call the evolutionary s-rep. The geometric\nfeatures that are derived from each of the representations, especially via\nfitted frames, is discussed.\n","authors":["Stephen M. Pizer","Zhiyuan Liu","Junjie Zhao","Nicholas Tapp-Hughes","James Damon","Miaomiao Zhang","JS Marron","Jared Vicory"],"pdf_url":"https://arxiv.org/pdf/2407.14357v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14352v1","updated":"2024-07-19T14:34:25Z","published":"2024-07-19T14:34:25Z","title":"Vision-Based Power Line Cables and Pylons Detection for Low Flying\n  Aircrafts","summary":"  Power lines are dangerous for low-flying aircrafts, especially in\nlow-visibility conditions. Thus, a vision-based system able to analyze the\naircraft's surroundings and to provide the pilots with a \"second pair of eyes\"\ncan contribute to enhancing their safety. To this end, we have developed a deep\nlearning approach to jointly detect power line cables and pylons from images\ncaptured at distances of several hundred meters by aircraft-mounted cameras. In\ndoing so, we have combined a modern convolutional architecture with transfer\nlearning and a loss function adapted to curvilinear structure delineation. We\nuse a single network for both detection tasks and demonstrated its performance\non two benchmarking datasets. We have integrated it within an onboard system\nand run it in flight, and have demonstrated with our experiments that it\noutperforms the prior distant cable detection method on both datasets, while\nalso successfully detecting pylons, given their annotations are available for\nthe data.\n","authors":["Jakub Gwizdała","Doruk Oner","Soumava Kumar Roy","Mian Akbar Shah","Ad Eberhard","Ivan Egorov","Philipp Krüsi","Grigory Yakushev"],"pdf_url":"https://arxiv.org/pdf/2407.14352v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.20260v3","updated":"2024-07-19T14:28:52Z","published":"2024-03-29T16:08:59Z","title":"Prototype-based Interpretable Breast Cancer Prediction Models: Analysis\n  and Challenges","summary":"  Deep learning models have achieved high performance in medical applications,\nhowever, their adoption in clinical practice is hindered due to their black-box\nnature. Self-explainable models, like prototype-based models, can be especially\nbeneficial as they are interpretable by design. However, if the learnt\nprototypes are of low quality then the prototype-based models are as good as\nblack-box. Having high quality prototypes is a pre-requisite for a truly\ninterpretable model. In this work, we propose a prototype evaluation framework\nfor coherence (PEF-C) for quantitatively evaluating the quality of the\nprototypes based on domain knowledge. We show the use of PEF-C in the context\nof breast cancer prediction using mammography. Existing works on\nprototype-based models on breast cancer prediction using mammography have\nfocused on improving the classification performance of prototype-based models\ncompared to black-box models and have evaluated prototype quality through\nanecdotal evidence. We are the first to go beyond anecdotal evidence and\nevaluate the quality of the mammography prototypes systematically using our\nPEF-C. Specifically, we apply three state-of-the-art prototype-based models,\nProtoPNet, BRAIxProtoPNet++ and PIP-Net on mammography images for breast cancer\nprediction and evaluate these models w.r.t. i) classification performance, and\nii) quality of the prototypes, on three public datasets. Our results show that\nprototype-based models are competitive with black-box models in terms of\nclassification performance, and achieve a higher score in detecting ROIs.\nHowever, the quality of the prototypes are not yet sufficient and can be\nimproved in aspects of relevance, purity and learning a variety of prototypes.\nWe call the XAI community to systematically evaluate the quality of the\nprototypes to check their true usability in high stake decisions and improve\nsuch models further.\n","authors":["Shreyasi Pathak","Jörg Schlötterer","Jeroen Veltman","Jeroen Geerdink","Maurice van Keulen","Christin Seifert"],"pdf_url":"https://arxiv.org/pdf/2403.20260v3.pdf","comment":"Accepted at World Conference on Explainable Artificial Intelligence.\n  Cham: Springer Nature Switzerland, 2024; 21 pages, 5 figures, 3 tables"},{"id":"http://arxiv.org/abs/2407.14340v1","updated":"2024-07-19T14:21:56Z","published":"2024-07-19T14:21:56Z","title":"Large Kernel Distillation Network for Efficient Single Image\n  Super-Resolution","summary":"  Efficient and lightweight single-image super-resolution (SISR) has achieved\nremarkable performance in recent years. One effective approach is the use of\nlarge kernel designs, which have been shown to improve the performance of SISR\nmodels while reducing their computational requirements. However, current\nstate-of-the-art (SOTA) models still face problems such as high computational\ncosts. To address these issues, we propose the Large Kernel Distillation\nNetwork (LKDN) in this paper. Our approach simplifies the model structure and\nintroduces more efficient attention modules to reduce computational costs while\nalso improving performance. Specifically, we employ the reparameterization\ntechnique to enhance model performance without adding extra cost. We also\nintroduce a new optimizer from other tasks to SISR, which improves training\nspeed and performance. Our experimental results demonstrate that LKDN\noutperforms existing lightweight SR methods and achieves SOTA performance.\n","authors":["Chengxing Xie","Xiaoming Zhang","Linze Li","Haiteng Meng","Tianlin Zhang","Tianrui Li","Xiaole Zhao"],"pdf_url":"https://arxiv.org/pdf/2407.14340v1.pdf","comment":"Accepted to CVPR workshop 2023"},{"id":"http://arxiv.org/abs/2406.02411v2","updated":"2024-07-19T14:21:27Z","published":"2024-06-04T15:21:37Z","title":"Decoupling of neural network calibration measures","summary":"  A lot of effort is currently invested in safeguarding autonomous driving\nsystems, which heavily rely on deep neural networks for computer vision. We\ninvestigate the coupling of different neural network calibration measures with\na special focus on the Area Under the Sparsification Error curve (AUSE) metric.\nWe elaborate on the well-known inconsistency in determining optimal calibration\nusing the Expected Calibration Error (ECE) and we demonstrate similar issues\nfor the AUSE, the Uncertainty Calibration Score (UCS), as well as the\nUncertainty Calibration Error (UCE). We conclude that the current methodologies\nleave a degree of freedom, which prevents a unique model calibration for the\nhomologation of safety-critical functionalities. Furthermore, we propose the\nAUSE as an indirect measure for the residual uncertainty, which is irreducible\nfor a fixed network architecture and is driven by the stochasticity in the\nunderlying data generation process (aleatoric contribution) as well as the\nlimitation in the hypothesis space (epistemic contribution).\n","authors":["Dominik Werner Wolf","Prasannavenkatesh Balaji","Alexander Braun","Markus Ulrich"],"pdf_url":"https://arxiv.org/pdf/2406.02411v2.pdf","comment":"Accepted at the German Conference on Pattern Recognition (GCPR) 2024"},{"id":"http://arxiv.org/abs/2403.16528v2","updated":"2024-07-19T14:16:31Z","published":"2024-03-25T08:14:22Z","title":"Open-Set Recognition in the Age of Vision-Language Models","summary":"  Are vision-language models (VLMs) for open-vocabulary perception inherently\nopen-set models because they are trained on internet-scale datasets? We answer\nthis question with a clear no - VLMs introduce closed-set assumptions via their\nfinite query set, making them vulnerable to open-set conditions. We\nsystematically evaluate VLMs for open-set recognition and find they frequently\nmisclassify objects not contained in their query set, leading to alarmingly low\nprecision when tuned for high recall and vice versa. We show that naively\nincreasing the size of the query set to contain more and more classes does not\nmitigate this problem, but instead causes diminishing task performance and\nopen-set performance. We establish a revised definition of the open-set problem\nfor the age of VLMs, define a new benchmark and evaluation protocol to\nfacilitate standardised evaluation and research in this important area, and\nevaluate promising baseline approaches based on predictive uncertainty and\ndedicated negative embeddings on a range of open-vocabulary VLM classifiers and\nobject detectors.\n","authors":["Dimity Miller","Niko Sünderhauf","Alex Kenna","Keita Mason"],"pdf_url":"https://arxiv.org/pdf/2403.16528v2.pdf","comment":"29 pages, Accepted for publication in ECCV 2024"},{"id":"http://arxiv.org/abs/2407.14330v1","updated":"2024-07-19T14:10:35Z","published":"2024-07-19T14:10:35Z","title":"Straightforward Layer-wise Pruning for More Efficient Visual Adaptation","summary":"  Parameter-efficient transfer learning (PETL) aims to adapt large pre-trained\nmodels using limited parameters. While most PETL approaches update the added\nparameters and freeze pre-trained weights during training, the minimal impact\nof task-specific deep layers on cross-domain data poses a challenge as PETL\ncannot modify them, resulting in redundant model structures. Structural pruning\neffectively reduces model redundancy; however, common pruning methods often\nlead to an excessive increase in stored parameters due to varying pruning\nstructures based on pruning rates and data. Recognizing the storage parameter\nvolume issue, we propose a Straightforward layer-wise pruning method, called\nSLS, for pruning PETL-transferred models. By evaluating parameters from a\nfeature perspective of each layer and utilizing clustering metrics to assess\ncurrent parameters based on clustering phenomena in low-dimensional space\nobtained through t-SNE, SLS facilitates informed pruning decisions. Our study\nreveals that layer-wise pruning, with a focus on storing pruning indices,\naddresses storage volume concerns. Notably, mainstream Layer-wise pruning\nmethods may not be suitable for assessing layer importance in PETL-transferred\nmodels, where the majority of parameters are pre-trained and have limited\nrelevance to downstream datasets. Comparative analysis against state-of-the-art\nPETL methods demonstrates that the pruned model achieved a notable balance\nbetween model throughput and accuracy. Moreover, SLS effectively reduces\nstorage overhead arising from varying pruned structures while enhancing the\naccuracy and speed of pruned models compared to conventional pruning methods.\n","authors":["Ruizi Han","Jinglei Tang"],"pdf_url":"https://arxiv.org/pdf/2407.14330v1.pdf","comment":"published to ECCV2024"},{"id":"http://arxiv.org/abs/2403.14270v2","updated":"2024-07-19T14:07:25Z","published":"2024-03-21T10:15:57Z","title":"Scene-Graph ViT: End-to-End Open-Vocabulary Visual Relationship\n  Detection","summary":"  Visual relationship detection aims to identify objects and their\nrelationships in images. Prior methods approach this task by adding separate\nrelationship modules or decoders to existing object detection architectures.\nThis separation increases complexity and hinders end-to-end training, which\nlimits performance. We propose a simple and highly efficient decoder-free\narchitecture for open-vocabulary visual relationship detection. Our model\nconsists of a Transformer-based image encoder that represents objects as tokens\nand models their relationships implicitly. To extract relationship information,\nwe introduce an attention mechanism that selects object pairs likely to form a\nrelationship. We provide a single-stage recipe to train this model on a mixture\nof object and relationship detection data. Our approach achieves\nstate-of-the-art relationship detection performance on Visual Genome and on the\nlarge-vocabulary GQA benchmark at real-time inference speeds. We provide\nablations, real-world qualitative examples, and analyses of zero-shot\nperformance.\n","authors":["Tim Salzmann","Markus Ryll","Alex Bewley","Matthias Minderer"],"pdf_url":"https://arxiv.org/pdf/2403.14270v2.pdf","comment":"ECCV camera-ready; changed to graph-constrained results"},{"id":"http://arxiv.org/abs/2407.14326v1","updated":"2024-07-19T14:04:05Z","published":"2024-07-19T14:04:05Z","title":"Panoptic Segmentation of Mammograms with Text-To-Image Diffusion Model","summary":"  Mammography is crucial for breast cancer surveillance and early diagnosis.\nHowever, analyzing mammography images is a demanding task for radiologists, who\noften review hundreds of mammograms daily, leading to overdiagnosis and\novertreatment. Computer-Aided Diagnosis (CAD) systems have been developed to\nassist in this process, but their capabilities, particularly in lesion\nsegmentation, remained limited. With the contemporary advances in deep learning\ntheir performance may be improved. Recently, vision-language diffusion models\nemerged, demonstrating outstanding performance in image generation and\ntransferability to various downstream tasks. We aim to harness their\ncapabilities for breast lesion segmentation in a panoptic setting, which\nencompasses both semantic and instance-level predictions. Specifically, we\npropose leveraging pretrained features from a Stable Diffusion model as inputs\nto a state-of-the-art panoptic segmentation architecture, resulting in accurate\ndelineation of individual breast lesions. To bridge the gap between natural and\nmedical imaging domains, we incorporated a mammography-specific MAM-E diffusion\nmodel and BiomedCLIP image and text encoders into this framework. We evaluated\nour approach on two recently published mammography datasets, CDD-CESM and\nVinDr-Mammo. For the instance segmentation task, we noted 40.25 AP0.1 and 46.82\nAP0.05, as well as 25.44 PQ0.1 and 26.92 PQ0.05. For the semantic segmentation\ntask, we achieved Dice scores of 38.86 and 40.92, respectively.\n","authors":["Kun Zhao","Jakub Prokop","Javier Montalt Tordera","Sadegh Mohammadi"],"pdf_url":"https://arxiv.org/pdf/2407.14326v1.pdf","comment":"13 pages, 4 figures. Submitted to Deep Generative Models workshop @\n  MICCAI 2024"},{"id":"http://arxiv.org/abs/2106.11653v5","updated":"2024-07-19T13:51:12Z","published":"2021-06-22T10:21:39Z","title":"A Curriculum-style Self-training Approach for Source-Free Semantic\n  Segmentation","summary":"  Source-free domain adaptation has developed rapidly in recent years, where\nthe well-trained source model is adapted to the target domain instead of the\nsource data, offering the potential for privacy concerns and intellectual\nproperty protection. However, a number of feature alignment techniques in prior\ndomain adaptation methods are not feasible in this challenging problem setting.\nThereby, we resort to probing inherent domain-invariant feature learning and\npropose a curriculum-style self-training approach for source-free domain\nadaptive semantic segmentation. In particular, we introduce a curriculum-style\nentropy minimization method to explore the implicit knowledge from the source\nmodel, which fits the trained source model to the target data using certain\ninformation from easy-to-hard predictions. We then train the segmentation\nnetwork by the proposed complementary curriculum-style self-training, which\nutilizes the negative and positive pseudo labels following the\ncurriculum-learning manner. Although negative pseudo-labels with high\nuncertainty cannot be identified with the correct labels, they can definitely\nindicate absent classes. Moreover, we employ an information propagation scheme\nto further reduce the intra-domain discrepancy within the target domain, which\ncould act as a standard post-processing method for the domain adaptation field.\nFurthermore, we extend the proposed method to a more challenging black-box\nsource model scenario where only the source model's predictions are available.\nExtensive experiments validate that our method yields state-of-the-art\nperformance on source-free semantic segmentation tasks for both\nsynthetic-to-real and adverse conditions datasets. The code and corresponding\ntrained models are released at \\url{https://github.com/yxiwang/ATP}.\n","authors":["Yuxi Wang","Jian Liang","Zhaoxiang Zhang"],"pdf_url":"https://arxiv.org/pdf/2106.11653v5.pdf","comment":"This paper is accepted by TPAMI2024"},{"id":"http://arxiv.org/abs/2407.14314v1","updated":"2024-07-19T13:47:02Z","published":"2024-07-19T13:47:02Z","title":"EmoCAM: Toward Understanding What Drives CNN-based Emotion Recognition","summary":"  Convolutional Neural Networks are particularly suited for image analysis\ntasks, such as Image Classification, Object Recognition or Image Segmentation.\nLike all Artificial Neural Networks, however, they are \"black box\" models, and\nsuffer from poor explainability. This work is concerned with the specific\ndownstream task of Emotion Recognition from images, and proposes a framework\nthat combines CAM-based techniques with Object Detection on a corpus level to\nbetter understand on which image cues a particular model, in our case EmoNet,\nrelies to assign a specific emotion to an image. We demonstrate that the model\nmostly focuses on human characteristics, but also explore the pronounced effect\nof specific image modifications.\n","authors":["Youssef Doulfoukar","Laurent Mertens","Joost Vennekens"],"pdf_url":"https://arxiv.org/pdf/2407.14314v1.pdf","comment":"10 pages, 7 figures"},{"id":"http://arxiv.org/abs/2309.05300v3","updated":"2024-07-19T13:43:13Z","published":"2023-09-11T08:35:23Z","title":"Decoupling Common and Unique Representations for Multimodal\n  Self-supervised Learning","summary":"  The increasing availability of multi-sensor data sparks wide interest in\nmultimodal self-supervised learning. However, most existing approaches learn\nonly common representations across modalities while ignoring intra-modal\ntraining and modality-unique representations. We propose Decoupling Common and\nUnique Representations (DeCUR), a simple yet effective method for multimodal\nself-supervised learning. By distinguishing inter- and intra-modal embeddings\nthrough multimodal redundancy reduction, DeCUR can integrate complementary\ninformation across different modalities. We evaluate DeCUR in three common\nmultimodal scenarios (radar-optical, RGB-elevation, and RGB-depth), and\ndemonstrate its consistent improvement regardless of architectures and for both\nmultimodal and modality-missing settings. With thorough experiments and\ncomprehensive analysis, we hope this work can provide valuable insights and\nraise more interest in researching the hidden relationships of multimodal\nrepresentations.\n","authors":["Yi Wang","Conrad M Albrecht","Nassim Ait Ali Braham","Chenying Liu","Zhitong Xiong","Xiao Xiang Zhu"],"pdf_url":"https://arxiv.org/pdf/2309.05300v3.pdf","comment":"Accepted to ECCV 2024. 27 pages, 8 figures"},{"id":"http://arxiv.org/abs/2407.14302v1","updated":"2024-07-19T13:33:38Z","published":"2024-07-19T13:33:38Z","title":"Dyn-Adapter: Towards Disentangled Representation for Efficient Visual\n  Recognition","summary":"  Parameter-efficient transfer learning (PETL) is a promising task, aiming to\nadapt the large-scale pre-trained model to downstream tasks with a relatively\nmodest cost. However, current PETL methods struggle in compressing\ncomputational complexity and bear a heavy inference burden due to the complete\nforward process. This paper presents an efficient visual recognition paradigm,\ncalled Dynamic Adapter (Dyn-Adapter), that boosts PETL efficiency by subtly\ndisentangling features in multiple levels. Our approach is simple: first, we\ndevise a dynamic architecture with balanced early heads for multi-level feature\nextraction, along with adaptive training strategy. Second, we introduce a\nbidirectional sparsity strategy driven by the pursuit of powerful\ngeneralization ability. These qualities enable us to fine-tune efficiently and\neffectively: we reduce FLOPs during inference by 50%, while maintaining or even\nyielding higher recognition accuracy. Extensive experiments on diverse datasets\nand pretrained backbones demonstrate the potential of Dyn-Adapter serving as a\ngeneral efficiency booster for PETL in vision recognition tasks.\n","authors":["Yurong Zhang","Honghao Chen","Xinyu Zhang","Xiangxiang Chu","Li Song"],"pdf_url":"https://arxiv.org/pdf/2407.14302v1.pdf","comment":"ECCV 2024"},{"id":"http://arxiv.org/abs/2407.13188v2","updated":"2024-07-19T13:30:52Z","published":"2024-07-18T05:53:17Z","title":"Safe-SD: Safe and Traceable Stable Diffusion with Text Prompt Trigger\n  for Invisible Generative Watermarking","summary":"  Recently, stable diffusion (SD) models have typically flourished in the field\nof image synthesis and personalized editing, with a range of photorealistic and\nunprecedented images being successfully generated. As a result, widespread\ninterest has been ignited to develop and use various SD-based tools for visual\ncontent creation. However, the exposure of AI-created content on public\nplatforms could raise both legal and ethical risks. In this regard, the\ntraditional methods of adding watermarks to the already generated images (i.e.\npost-processing) may face a dilemma (e.g., being erased or modified) in terms\nof copyright protection and content monitoring, since the powerful image\ninversion and text-to-image editing techniques have been widely explored in\nSD-based methods. In this work, we propose a Safe and high-traceable Stable\nDiffusion framework (namely Safe-SD) to adaptively implant the graphical\nwatermarks (e.g., QR code) into the imperceptible structure-related pixels\nduring the generative diffusion process for supporting text-driven invisible\nwatermarking and detection. Different from the previous high-cost\ninjection-then-detection training framework, we design a simple and unified\narchitecture, which makes it possible to simultaneously train watermark\ninjection and detection in a single network, greatly improving the efficiency\nand convenience of use. Moreover, to further support text-driven generative\nwatermarking and deeply explore its robustness and high-traceability, we\nelaborately design lambda sampling and encryption algorithm to fine-tune a\nlatent diffuser wrapped by a VAE for balancing high-fidelity image synthesis\nand high-traceable watermark detection. We present our quantitative and\nqualitative results on two representative datasets LSUN, COCO and FFHQ,\ndemonstrating state-of-the-art performance of Safe-SD and showing it\nsignificantly outperforms the previous approaches.\n","authors":["Zhiyuan Ma","Guoli Jia","Biqing Qi","Bowen Zhou"],"pdf_url":"https://arxiv.org/pdf/2407.13188v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14292v1","updated":"2024-07-19T13:24:05Z","published":"2024-07-19T13:24:05Z","title":"Adaptive Frequency Enhancement Network for Single Image Deraining","summary":"  Image deraining aims to improve the visibility of images damaged by rainy\nconditions, targeting the removal of degradation elements such as rain streaks,\nraindrops, and rain accumulation. While numerous single image deraining methods\nhave shown promising results in image enhancement within the spatial domain,\nreal-world rain degradation often causes uneven damage across an image's entire\nfrequency spectrum, posing challenges for these methods in enhancing different\nfrequency components. In this paper, we introduce a novel end-to-end Adaptive\nFrequency Enhancement Network (AFENet) specifically for single image deraining\nthat adaptively enhances images across various frequencies. We employ\nconvolutions of different scales to adaptively decompose image frequency bands,\nintroduce a feature enhancement module to boost the features of different\nfrequency components and present a novel interaction module for interchanging\nand merging information from various frequency branches. Simultaneously, we\npropose a feature aggregation module that efficiently and adaptively fuses\nfeatures from different frequency bands, facilitating enhancements across the\nentire frequency spectrum. This approach empowers the deraining network to\neliminate diverse and complex rainy patterns and to reconstruct image details\naccurately. Extensive experiments on both real and synthetic scenes demonstrate\nthat our method not only achieves visually appealing enhancement results but\nalso surpasses existing methods in performance.\n","authors":["Fei Yan","Yuhong He","Keyu Chen","En Cheng","Jikang Ma"],"pdf_url":"https://arxiv.org/pdf/2407.14292v1.pdf","comment":"8pages"},{"id":"http://arxiv.org/abs/2405.17398v3","updated":"2024-07-19T13:20:05Z","published":"2024-05-27T17:49:15Z","title":"Vista: A Generalizable Driving World Model with High Fidelity and\n  Versatile Controllability","summary":"  World models can foresee the outcomes of different actions, which is of\nparamount importance for autonomous driving. Nevertheless, existing driving\nworld models still have limitations in generalization to unseen environments,\nprediction fidelity of critical details, and action controllability for\nflexible application. In this paper, we present Vista, a generalizable driving\nworld model with high fidelity and versatile controllability. Based on a\nsystematic diagnosis of existing methods, we introduce several key ingredients\nto address these limitations. To accurately predict real-world dynamics at high\nresolution, we propose two novel losses to promote the learning of moving\ninstances and structural information. We also devise an effective latent\nreplacement approach to inject historical frames as priors for coherent\nlong-horizon rollouts. For action controllability, we incorporate a versatile\nset of controls from high-level intentions (command, goal point) to low-level\nmaneuvers (trajectory, angle, and speed) through an efficient learning\nstrategy. After large-scale training, the capabilities of Vista can seamlessly\ngeneralize to different scenarios. Extensive experiments on multiple datasets\nshow that Vista outperforms the most advanced general-purpose video generator\nin over 70% of comparisons and surpasses the best-performing driving world\nmodel by 55% in FID and 27% in FVD. Moreover, for the first time, we utilize\nthe capacity of Vista itself to establish a generalizable reward for real-world\naction evaluation without accessing the ground truth actions.\n","authors":["Shenyuan Gao","Jiazhi Yang","Li Chen","Kashyap Chitta","Yihang Qiu","Andreas Geiger","Jun Zhang","Hongyang Li"],"pdf_url":"https://arxiv.org/pdf/2405.17398v3.pdf","comment":"Code and model: https://github.com/OpenDriveLab/Vista, video demos:\n  https://vista-demo.github.io"},{"id":"http://arxiv.org/abs/2407.14280v1","updated":"2024-07-19T13:05:57Z","published":"2024-07-19T13:05:57Z","title":"How to Blend Concepts in Diffusion Models","summary":"  For the last decade, there has been a push to use multi-dimensional (latent)\nspaces to represent concepts; and yet how to manipulate these concepts or\nreason with them remains largely unclear. Some recent methods exploit multiple\nlatent representations and their connection, making this research question even\nmore entangled. Our goal is to understand how operations in the latent space\naffect the underlying concepts. To that end, we explore the task of concept\nblending through diffusion models. Diffusion models are based on a connection\nbetween a latent representation of textual prompts and a latent space that\nenables image reconstruction and generation. This task allows us to try\ndifferent text-based combination strategies, and evaluate easily through a\nvisual analysis. Our conclusion is that concept blending through space\nmanipulation is possible, although the best strategy depends on the context of\nthe blend.\n","authors":["Giorgio Longari","Lorenzo Olearo","Simone Melzi","Rafael Peñaloza","Alessandro Raganato"],"pdf_url":"https://arxiv.org/pdf/2407.14280v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14279v1","updated":"2024-07-19T13:01:12Z","published":"2024-07-19T13:01:12Z","title":"OpenSU3D: Open World 3D Scene Understanding using Foundation Models","summary":"  In this paper, we present a novel, scalable approach for constructing open\nset, instance-level 3D scene representations, advancing open world\nunderstanding of 3D environments. Existing methods require pre-constructed 3D\nscenes and face scalability issues due to per-point feature vector learning,\nlimiting their efficacy with complex queries. Our method overcomes these\nlimitations by incrementally building instance-level 3D scene representations\nusing 2D foundation models, efficiently aggregating instance-level details such\nas masks, feature vectors, names, and captions. We introduce fusion schemes for\nfeature vectors to enhance their contextual knowledge and performance on\ncomplex queries. Additionally, we explore large language models for robust\nautomatic annotation and spatial reasoning tasks. We evaluate our proposed\napproach on multiple scenes from ScanNet and Replica datasets demonstrating\nzero-shot generalization capabilities, exceeding current state-of-the-art\nmethods in open world 3D scene understanding.\n","authors":["Rafay Mohiuddin","Sai Manoj Prakhya","Fiona Collins","Ziyuan Liu","André Borrmann"],"pdf_url":"https://arxiv.org/pdf/2407.14279v1.pdf","comment":"Project Page: https://opensu3d.github.io/"},{"id":"http://arxiv.org/abs/2407.14277v1","updated":"2024-07-19T12:58:18Z","published":"2024-07-19T12:58:18Z","title":"Patch-based Intuitive Multimodal Prototypes Network (PIMPNet) for\n  Alzheimer's Disease classification","summary":"  Volumetric neuroimaging examinations like structural Magnetic Resonance\nImaging (sMRI) are routinely applied to support the clinical diagnosis of\ndementia like Alzheimer's Disease (AD). Neuroradiologists examine 3D sMRI to\ndetect and monitor abnormalities in brain morphology due to AD, like global\nand/or local brain atrophy and shape alteration of characteristic structures.\nThere is a strong research interest in developing diagnostic systems based on\nDeep Learning (DL) models to analyse sMRI for AD. However, anatomical\ninformation extracted from an sMRI examination needs to be interpreted together\nwith patient's age to distinguish AD patterns from the regular alteration due\nto a normal ageing process. In this context, part-prototype neural networks\nintegrate the computational advantages of DL in an interpretable-by-design\narchitecture and showed promising results in medical imaging applications. We\npresent PIMPNet, the first interpretable multimodal model for 3D images and\ndemographics applied to the binary classification of AD from 3D sMRI and\npatient's age. Despite age prototypes do not improve predictive performance\ncompared to the single modality model, this lays the foundation for future work\nin the direction of the model's design and multimodal prototype training\nprocess\n","authors":["Lisa Anita De Santi","Jörg Schlötterer","Meike Nauta","Vincenzo Positano","Christin Seifert"],"pdf_url":"https://arxiv.org/pdf/2407.14277v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.04899v2","updated":"2024-07-19T12:40:28Z","published":"2024-03-07T21:08:51Z","title":"Towards Scene Graph Anticipation","summary":"  Spatio-temporal scene graphs represent interactions in a video by decomposing\nscenes into individual objects and their pair-wise temporal relationships.\nLong-term anticipation of the fine-grained pair-wise relationships between\nobjects is a challenging problem. To this end, we introduce the task of Scene\nGraph Anticipation (SGA). We adapt state-of-the-art scene graph generation\nmethods as baselines to anticipate future pair-wise relationships between\nobjects and propose a novel approach SceneSayer. In SceneSayer, we leverage\nobject-centric representations of relationships to reason about the observed\nvideo frames and model the evolution of relationships between objects. We take\na continuous time perspective and model the latent dynamics of the evolution of\nobject interactions using concepts of NeuralODE and NeuralSDE, respectively. We\ninfer representations of future relationships by solving an Ordinary\nDifferential Equation and a Stochastic Differential Equation, respectively.\nExtensive experimentation on the Action Genome dataset validates the efficacy\nof the proposed methods.\n","authors":["Rohith Peddi","Saksham Singh"," Saurabh","Parag Singla","Vibhav Gogate"],"pdf_url":"https://arxiv.org/pdf/2403.04899v2.pdf","comment":"ECCV 2024, Code: https://github.com/rohithpeddi/SceneSayer"},{"id":"http://arxiv.org/abs/2407.14257v1","updated":"2024-07-19T12:36:36Z","published":"2024-07-19T12:36:36Z","title":"SparseCraft: Few-Shot Neural Reconstruction through Stereopsis Guided\n  Geometric Linearization","summary":"  We present a novel approach for recovering 3D shape and view dependent\nappearance from a few colored images, enabling efficient 3D reconstruction and\nnovel view synthesis. Our method learns an implicit neural representation in\nthe form of a Signed Distance Function (SDF) and a radiance field. The model is\ntrained progressively through ray marching enabled volumetric rendering, and\nregularized with learning-free multi-view stereo (MVS) cues. Key to our\ncontribution is a novel implicit neural shape function learning strategy that\nencourages our SDF field to be as linear as possible near the level-set, hence\nrobustifying the training against noise emanating from the supervision and\nregularization signals. Without using any pretrained priors, our method, called\nSparseCraft, achieves state-of-the-art performances both in novel-view\nsynthesis and reconstruction from sparse views in standard benchmarks, while\nrequiring less than 10 minutes for training.\n","authors":["Mae Younes","Amine Ouasfi","Adnane Boukhayma"],"pdf_url":"https://arxiv.org/pdf/2407.14257v1.pdf","comment":"ECCV 2024. Project page: https://sparsecraft.github.io"},{"id":"http://arxiv.org/abs/2407.14249v1","updated":"2024-07-19T12:30:03Z","published":"2024-07-19T12:30:03Z","title":"An Attention-based Representation Distillation Baseline for Multi-Label\n  Continual Learning","summary":"  The field of Continual Learning (CL) has inspired numerous researchers over\nthe years, leading to increasingly advanced countermeasures to the issue of\ncatastrophic forgetting. Most studies have focused on the single-class\nscenario, where each example comes with a single label. The recent literature\nhas successfully tackled such a setting, with impressive results. Differently,\nwe shift our attention to the multi-label scenario, as we feel it to be more\nrepresentative of real-world open problems. In our work, we show that existing\nstate-of-the-art CL methods fail to achieve satisfactory performance, thus\nquestioning the real advance claimed in recent years. Therefore, we assess both\nold-style and novel strategies and propose, on top of them, an approach called\nSelective Class Attention Distillation (SCAD). It relies on a knowledge\ntransfer technique that seeks to align the representations of the student\nnetwork -- which trains continuously and is subject to forgetting -- with the\nteacher ones, which is pretrained and kept frozen. Importantly, our method is\nable to selectively transfer the relevant information from the teacher to the\nstudent, thereby preventing irrelevant information from harming the student's\nperformance during online training. To demonstrate the merits of our approach,\nwe conduct experiments on two different multi-label datasets, showing that our\nmethod outperforms the current state-of-the-art Continual Learning methods. Our\nfindings highlight the importance of addressing the unique challenges posed by\nmulti-label environments in the field of Continual Learning. The code of SCAD\nis available at https://github.com/aimagelab/SCAD-LOD-2024.\n","authors":["Martin Menabue","Emanuele Frascaroli","Matteo Boschini","Lorenzo Bonicelli","Angelo Porrello","Simone Calderara"],"pdf_url":"https://arxiv.org/pdf/2407.14249v1.pdf","comment":"Accepted at LOD 2024"},{"id":"http://arxiv.org/abs/2407.14245v1","updated":"2024-07-19T12:27:11Z","published":"2024-07-19T12:27:11Z","title":"Dataset Distillation by Automatic Training Trajectories","summary":"  Dataset Distillation is used to create a concise, yet informative, synthetic\ndataset that can replace the original dataset for training purposes. Some\nleading methods in this domain prioritize long-range matching, involving the\nunrolling of training trajectories with a fixed number of steps (NS) on the\nsynthetic dataset to align with various expert training trajectories. However,\ntraditional long-range matching methods possess an overfitting-like problem,\nthe fixed step size NS forces synthetic dataset to distortedly conform seen\nexpert training trajectories, resulting in a loss of generality-especially to\nthose from unencountered architecture. We refer to this as the Accumulated\nMismatching Problem (AMP), and propose a new approach, Automatic Training\nTrajectories (ATT), which dynamically and adaptively adjusts trajectory length\nNS to address the AMP. Our method outperforms existing methods particularly in\ntests involving cross-architectures. Moreover, owing to its adaptive nature, it\nexhibits enhanced stability in the face of parameter variations.\n","authors":["Dai Liu","Jindong Gu","Hu Cao","Carsten Trinitis","Martin Schulz"],"pdf_url":"https://arxiv.org/pdf/2407.14245v1.pdf","comment":"The paper is accepted at ECCV 2024"},{"id":"http://arxiv.org/abs/2407.14242v1","updated":"2024-07-19T12:22:32Z","published":"2024-07-19T12:22:32Z","title":"Continual Panoptic Perception: Towards Multi-modal Incremental\n  Interpretation of Remote Sensing Images","summary":"  Continual learning (CL) breaks off the one-way training manner and enables a\nmodel to adapt to new data, semantics and tasks continuously. However, current\nCL methods mainly focus on single tasks. Besides, CL models are plagued by\ncatastrophic forgetting and semantic drift since the lack of old data, which\noften occurs in remote-sensing interpretation due to the intricate fine-grained\nsemantics. In this paper, we propose Continual Panoptic Perception (CPP), a\nunified continual learning model that leverages multi-task joint learning\ncovering pixel-level classification, instance-level segmentation and\nimage-level perception for universal interpretation in remote sensing images.\nConcretely, we propose a collaborative cross-modal encoder (CCE) to extract the\ninput image features, which supports pixel classification and caption\ngeneration synchronously. To inherit the knowledge from the old model without\nexemplar memory, we propose a task-interactive knowledge distillation (TKD)\nmethod, which leverages cross-modal optimization and task-asymmetric\npseudo-labeling (TPL) to alleviate catastrophic forgetting. Furthermore, we\nalso propose a joint optimization mechanism to achieve end-to-end multi-modal\npanoptic perception. Experimental results on the fine-grained panoptic\nperception dataset validate the effectiveness of the proposed model, and also\nprove that joint optimization can boost sub-task CL efficiency with over 13\\%\nrelative improvement on panoptic quality.\n","authors":["Bo Yuan","Danpei Zhao","Zhuoran Liu","Wentao Li","Tian Li"],"pdf_url":"https://arxiv.org/pdf/2407.14242v1.pdf","comment":"Accepted in ACMMM 2024"},{"id":"http://arxiv.org/abs/2306.13990v2","updated":"2024-07-19T12:08:27Z","published":"2023-06-24T14:50:20Z","title":"Cross-Validation Is All You Need: A Statistical Approach To Label Noise\n  Estimation","summary":"  Machine learning models experience deteriorated performance when trained in\nthe presence of noisy labels. This is particularly problematic for medical\ntasks, such as survival prediction, which typically face high label noise\ncomplexity with few clear-cut solutions. Inspired by the large fluctuations\nacross folds in the cross-validation performance of survival analyses, we\ndesign Monte-Carlo experiments to show that such fluctuation could be caused by\nlabel noise. We propose two novel and straightforward label noise detection\nalgorithms that effectively identify noisy examples by pinpointing the samples\nthat more frequently contribute to inferior cross-validation results. We first\nintroduce Repeated Cross-Validation (ReCoV), a parameter-free label noise\ndetection algorithm that is robust to model choice. We further develop\nfastReCoV, a less robust but more tractable and efficient variant of ReCoV\nsuitable for deep learning applications. Through extensive experiments, we show\nthat ReCoV and fastReCoV achieve state-of-the-art label noise detection\nperformance in a wide range of modalities, models and tasks, including survival\nanalysis, which has yet to be addressed in the literature. Our code and data\nare publicly available at https://github.com/GJiananChen/ReCoV.\n","authors":["Jianan Chen","Vishwesh Ramanathan","Tony Xu","Anne L. Martel"],"pdf_url":"https://arxiv.org/pdf/2306.13990v2.pdf","comment":"Accepted by MICCAI 2024"},{"id":"http://arxiv.org/abs/2407.14231v1","updated":"2024-07-19T11:58:30Z","published":"2024-07-19T11:58:30Z","title":"Realistic Evaluation of Test-Time Adaptation Algorithms: Unsupervised\n  Hyperparameter Selection","summary":"  Test-Time Adaptation (TTA) has recently emerged as a promising strategy for\ntackling the problem of machine learning model robustness under distribution\nshifts by adapting the model during inference without access to any labels.\nBecause of task difficulty, hyperparameters strongly influence the\neffectiveness of adaptation. However, the literature has provided little\nexploration into optimal hyperparameter selection. In this work, we tackle this\nproblem by evaluating existing TTA methods using surrogate-based hp-selection\nstrategies (which do not assume access to the test labels) to obtain a more\nrealistic evaluation of their performance. We show that some of the recent\nstate-of-the-art methods exhibit inferior performance compared to the previous\nalgorithms when using our more realistic evaluation setup. Further, we show\nthat forgetting is still a problem in TTA as the only method that is robust to\nhp-selection resets the model to the initial state at every step. We analyze\ndifferent types of unsupervised selection strategies, and while they work\nreasonably well in most scenarios, the only strategies that work consistently\nwell use some kind of supervision (either by a limited number of annotated test\nsamples or by using pretraining data). Our findings underscore the need for\nfurther research with more rigorous benchmarking by explicitly stating model\nselection strategies, to facilitate which we open-source our code.\n","authors":["Sebastian Cygert","Damian Sójka","Tomasz Trzciński","Bartłomiej Twardowski"],"pdf_url":"https://arxiv.org/pdf/2407.14231v1.pdf","comment":"preprint"},{"id":"http://arxiv.org/abs/2407.14230v1","updated":"2024-07-19T11:57:56Z","published":"2024-07-19T11:57:56Z","title":"ETSCL: An Evidence Theory-Based Supervised Contrastive Learning\n  Framework for Multi-modal Glaucoma Grading","summary":"  Glaucoma is one of the leading causes of vision impairment. Digital imaging\ntechniques, such as color fundus photography (CFP) and optical coherence\ntomography (OCT), provide quantitative and noninvasive methods for glaucoma\ndiagnosis. Recently, in the field of computer-aided glaucoma diagnosis,\nmulti-modality methods that integrate the CFP and OCT modalities have achieved\ngreater diagnostic accuracy compared to single-modality methods. However, it\nremains challenging to extract reliable features due to the high similarity of\nmedical images and the unbalanced multi-modal data distribution. Moreover,\nexisting methods overlook the uncertainty estimation of different modalities,\nleading to unreliable predictions. To address these challenges, we propose a\nnovel framework, namely ETSCL, which consists of a contrastive feature\nextraction stage and a decision-level fusion stage. Specifically, the\nsupervised contrastive loss is employed to enhance the discriminative power in\nthe feature extraction process, resulting in more effective features. In\naddition, we utilize the Frangi vesselness algorithm as a preprocessing step to\nincorporate vessel information to assist in the prediction. In the\ndecision-level fusion stage, an evidence theory-based multi-modality classifier\nis employed to combine multi-source information with uncertainty estimation.\nExtensive experiments demonstrate that our method achieves state-of-the-art\nperformance. The code is available at\n\\url{https://github.com/master-Shix/ETSCL}.\n","authors":["Zhiyuan Yang","Bo Zhang","Yufei Shi","Ningze Zhong","Johnathan Loh","Huihui Fang","Yanwu Xu","Si Yong Yeo"],"pdf_url":"https://arxiv.org/pdf/2407.14230v1.pdf","comment":"Accepted by Ophthalmic Medical Image Analysis Workshop at MICCAI'24"},{"id":"http://arxiv.org/abs/2407.06546v2","updated":"2024-07-19T11:52:15Z","published":"2024-07-09T04:56:11Z","title":"Exploring the Causality of End-to-End Autonomous Driving","summary":"  Deep learning-based models are widely deployed in autonomous driving areas,\nespecially the increasingly noticed end-to-end solutions. However, the\nblack-box property of these models raises concerns about their trustworthiness\nand safety for autonomous driving, and how to debug the causality has become a\npressing concern. Despite some existing research on the explainability of\nautonomous driving, there is currently no systematic solution to help\nresearchers debug and identify the key factors that lead to the final predicted\naction of end-to-end autonomous driving. In this work, we propose a\ncomprehensive approach to explore and analyze the causality of end-to-end\nautonomous driving. First, we validate the essential information that the final\nplanning depends on by using controlled variables and counterfactual\ninterventions for qualitative analysis. Then, we quantitatively assess the\nfactors influencing model decisions by visualizing and statistically analyzing\nthe response of key model inputs. Finally, based on the comprehensive study of\nthe multi-factorial end-to-end autonomous driving system, we have developed a\nstrong baseline and a tool for exploring causality in the close-loop simulator\nCARLA. It leverages the essential input sources to obtain a well-designed\nmodel, resulting in highly competitive capabilities. As far as we know, our\nwork is the first to unveil the mystery of end-to-end autonomous driving and\nturn the black box into a white one. Thorough close-loop experiments\ndemonstrate that our method can be applied to end-to-end autonomous driving\nsolutions for causality debugging. Code will be available at\nhttps://github.com/bdvisl/DriveInsight.\n","authors":["Jiankun Li","Hao Li","Jiangjiang Liu","Zhikang Zou","Xiaoqing Ye","Fan Wang","Jizhou Huang","Hua Wu","Haifeng Wang"],"pdf_url":"https://arxiv.org/pdf/2407.06546v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.12347v3","updated":"2024-07-19T11:50:33Z","published":"2023-12-19T17:26:44Z","title":"SMC-NCA: Semantic-guided Multi-level Contrast for Semi-supervised\n  Temporal Action Segmentation","summary":"  Semi-supervised temporal action segmentation (SS-TA) aims to perform\nframe-wise classification in long untrimmed videos, where only a fraction of\nvideos in the training set have labels. Recent studies have shown the potential\nof contrastive learning in unsupervised representation learning using\nunlabelled data. However, learning the representation of each frame by\nunsupervised contrastive learning for action segmentation remains an open and\nchallenging problem. In this paper, we propose a novel Semantic-guided\nMulti-level Contrast scheme with a Neighbourhood-Consistency-Aware unit\n(SMC-NCA) to extract strong frame-wise representations for SS-TAS.\nSpecifically, for representation learning, SMC is first used to explore intra-\nand inter-information variations in a unified and contrastive way, based on\naction-specific semantic information and temporal information highlighting\nrelations between actions. Then, the NCA module, which is responsible for\nenforcing spatial consistency between neighbourhoods centered at different\nframes to alleviate over-segmentation issues, works alongside SMC for\nsemi-supervised learning (SSL). Our SMC outperforms the other state-of-the-art\nmethods on three benchmarks, offering improvements of up to 17.8% and 12.6% in\nterms of Edit distance and accuracy, respectively. Additionally, the NCA unit\nresults in significantly better segmentation performance in the presence of\nonly 5% labelled videos. We also demonstrate the generalizability and\neffectiveness of the proposed method on our Parkinson Disease's Mouse Behaviour\n(PDMB) dataset. Code is available at https://github.com/FeixiangZhou/SMC-NCA.\n","authors":["Feixiang Zhou","Zheheng Jiang","Huiyu Zhou","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2312.12347v3.pdf","comment":"Accepted to IEEE Transactions on Multimedia"},{"id":"http://arxiv.org/abs/2407.14224v1","updated":"2024-07-19T11:48:36Z","published":"2024-07-19T11:48:36Z","title":"Hierarchical Windowed Graph Attention Network and a Large Scale Dataset\n  for Isolated Indian Sign Language Recognition","summary":"  Automatic Sign Language (SL) recognition is an important task in the computer\nvision community. To build a robust SL recognition system, we need a\nconsiderable amount of data which is lacking particularly in Indian sign\nlanguage (ISL). In this paper, we propose a large-scale isolated ISL dataset\nand a novel SL recognition model based on skeleton graph structure. The dataset\ncovers 2,002 daily used common words in the deaf community recorded by 20 (10\nmale and 10 female) deaf adult signers (contains 40033 videos). We propose a SL\nrecognition model namely Hierarchical Windowed Graph Attention Network (HWGAT)\nby utilizing the human upper body skeleton graph structure. The HWGAT tries to\ncapture distinctive motions by giving attention to different body parts induced\nby the human skeleton graph structure. The utility of the proposed dataset and\nthe usefulness of our model are evaluated through extensive experiments. We\npre-trained the proposed model on the proposed dataset and fine-tuned it across\ndifferent sign language datasets further boosting the performance of 1.10,\n0.46, 0.78, and 6.84 percentage points on INCLUDE, LSA64, AUTSL and WLASL\nrespectively compared to the existing state-of-the-art skeleton-based models.\n","authors":["Suvajit Patra","Arkadip Maitra","Megha Tiwari","K. Kumaran","Swathy Prabhu","Swami Punyeshwarananda","Soumitra Samanta"],"pdf_url":"https://arxiv.org/pdf/2407.14224v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.18583v2","updated":"2024-07-19T11:46:17Z","published":"2024-04-29T10:47:37Z","title":"Context Matters: Leveraging Spatiotemporal Metadata for Semi-Supervised\n  Learning on Remote Sensing Images","summary":"  Remote sensing projects typically generate large amounts of imagery that can\nbe used to train powerful deep neural networks. However, the amount of labeled\nimages is often small, as remote sensing applications generally require expert\nlabelers. Thus, semi-supervised learning (SSL), i.e., learning with a small\npool of labeled and a larger pool of unlabeled data, is particularly useful in\nthis domain. Current SSL approaches generate pseudo-labels from model\npredictions for unlabeled samples. As the quality of these pseudo-labels is\ncrucial for performance, utilizing additional information to improve\npseudo-label quality yields a promising direction. For remote sensing images,\ngeolocation and recording time are generally available and provide a valuable\nsource of information as semantic concepts, such as land cover, are highly\ndependent on spatiotemporal context, e.g., due to seasonal effects and\nvegetation zones. In this paper, we propose to exploit spatiotemporal\nmetainformation in SSL to improve the quality of pseudo-labels and, therefore,\nthe final model performance. We show that directly adding the available\nmetadata to the input of the predictor at test time degenerates the prediction\nquality for metadata outside the spatiotemporal distribution of the training\nset. Thus, we propose a teacher-student SSL framework where only the teacher\nnetwork uses metainformation to improve the quality of pseudo-labels on the\ntraining set. Correspondingly, our student network benefits from the improved\npseudo-labels but does not receive metadata as input, making it invariant to\nspatiotemporal shifts at test time. Furthermore, we propose methods for\nencoding and injecting spatiotemporal information into the model and introduce\na novel distillation mechanism to enhance the knowledge transfer between\nteacher and student. Our framework dubbed Spatiotemporal SSL can be easily\ncombined with several stat...\n","authors":["Maximilian Bernhard","Tanveer Hannan","Niklas Strauß","Matthias Schubert"],"pdf_url":"https://arxiv.org/pdf/2404.18583v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14209v1","updated":"2024-07-19T11:15:02Z","published":"2024-07-19T11:15:02Z","title":"Unlearning Concepts from Text-to-Video Diffusion Models","summary":"  With the advancement of computer vision and natural language processing,\ntext-to-video generation, enabled by text-to-video diffusion models, has become\nmore prevalent. These models are trained using a large amount of data from the\ninternet. However, the training data often contain copyrighted content,\nincluding cartoon character icons and artist styles, private portraits, and\nunsafe videos. Since filtering the data and retraining the model is\nchallenging, methods for unlearning specific concepts from text-to-video\ndiffusion models have been investigated. However, due to the high computational\ncomplexity and relative large optimization scale, there is little work on\nunlearning methods for text-to-video diffusion models. We propose a novel\nconcept-unlearning method by transferring the unlearning capability of the text\nencoder of text-to-image diffusion models to text-to-video diffusion models.\nSpecifically, the method optimizes the text encoder using few-shot unlearning,\nwhere several generated images are used. We then use the optimized text encoder\nin text-to-video diffusion models to generate videos. Our method costs low\ncomputation resources and has small optimization scale. We discuss the\ngenerated videos after unlearning a concept. The experiments demonstrates that\nour method can unlearn copyrighted cartoon characters, artist styles, objects\nand people's facial characteristics. Our method can unlearn a concept within\nabout 100 seconds on an RTX 3070. Since there was no concept unlearning method\nfor text-to-video diffusion models before, we make concept unlearning feasible\nand more accessible in the text-to-video domain.\n","authors":["Shiqi Liu","Yihua Tan"],"pdf_url":"https://arxiv.org/pdf/2407.14209v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14208v1","updated":"2024-07-19T11:13:31Z","published":"2024-07-19T11:13:31Z","title":"Memory-Efficient Pseudo-Labeling for Online Source-Free Universal Domain\n  Adaptation using a Gaussian Mixture Model","summary":"  In practice, domain shifts are likely to occur between training and test\ndata, necessitating domain adaptation (DA) to adjust the pre-trained source\nmodel to the target domain. Recently, universal domain adaptation (UniDA) has\ngained attention for addressing the possibility of an additional category\n(label) shift between the source and target domain. This means new classes can\nappear in the target data, some source classes may no longer be present, or\nboth at the same time. For practical applicability, UniDA methods must handle\nboth source-free and online scenarios, enabling adaptation without access to\nthe source data and performing batch-wise updates in parallel with prediction.\nIn an online setting, preserving knowledge across batches is crucial. However,\nexisting methods often require substantial memory, e.g. by using memory queues,\nwhich is impractical because memory is limited and valuable, in particular on\nembedded systems. Therefore, we consider memory-efficiency as an additional\nconstraint in this paper. To achieve memory-efficient online source-free\nuniversal domain adaptation (SF-UniDA), we propose a novel method that\ncontinuously captures the distribution of known classes in the feature space\nusing a Gaussian mixture model (GMM). This approach, combined with\nentropy-based out-of-distribution detection, allows for the generation of\nreliable pseudo-labels. Finally, we combine a contrastive loss with a KL\ndivergence loss to perform the adaptation. Our approach not only achieves\nstate-of-the-art results in all experiments on the DomainNet dataset but also\nsignificantly outperforms the existing methods on the challenging VisDA-C\ndataset, setting a new benchmark for online SF-UniDA. Our code is available at\nhttps://github.com/pascalschlachter/GMM.\n","authors":["Pascal Schlachter","Simon Wagner","Bin Yang"],"pdf_url":"https://arxiv.org/pdf/2407.14208v1.pdf","comment":"Submitted at IEEE/CVF Winter Conference on Applications of Computer\n  Vision (WACV) 2025"},{"id":"http://arxiv.org/abs/2407.12239v2","updated":"2024-07-19T11:13:16Z","published":"2024-07-17T01:11:20Z","title":"Motion and Structure from Event-based Normal Flow","summary":"  Recovering the camera motion and scene geometry from visual data is a\nfundamental problem in the field of computer vision. Its success in standard\nvision is attributed to the maturity of feature extraction, data association\nand multi-view geometry. The recent emergence of neuromorphic event-based\ncameras places great demands on approaches that use raw event data as input to\nsolve this fundamental problem.Existing state-of-the-art solutions typically\ninfer implicitly data association by iteratively reversing the event data\ngeneration process. However, the nonlinear nature of these methods limits their\napplicability in real-time tasks, and the constant-motion assumption leads to\nunstable results under agile motion. To this end, we rethink the problem\nformulation in a way that aligns better with the differential working principle\nof event cameras.We show that the event-based normal flow can be used, via the\nproposed geometric error term, as an alternative to the full flow in solving a\nfamily of geometric problems that involve instantaneous first-order kinematics\nand scene geometry. Furthermore, we develop a fast linear solver and a\ncontinuous-time nonlinear solver on top of the proposed geometric error\nterm.Experiments on both synthetic and real data show the superiority of our\nlinear solver in terms of accuracy and efficiency, and indicate its\ncomplementary feature as an initialization method for existing nonlinear\nsolvers. Besides, our continuous-time non-linear solver exhibits exceptional\ncapability in accommodating sudden variations in motion since it does not rely\non the constant-motion assumption.\n","authors":["Zhongyang Ren","Bangyan Liao","Delei Kong","Jinghang Li","Peidong Liu","Laurent Kneip","Guillermo Gallego","Yi Zhou"],"pdf_url":"https://arxiv.org/pdf/2407.12239v2.pdf","comment":"This paper has been accepted by ECCV 2024"},{"id":"http://arxiv.org/abs/2407.14204v1","updated":"2024-07-19T11:01:30Z","published":"2024-07-19T11:01:30Z","title":"Bucketed Ranking-based Losses for Efficient Training of Object Detectors","summary":"  Ranking-based loss functions, such as Average Precision Loss and Rank&Sort\nLoss, outperform widely used score-based losses in object detection. These loss\nfunctions better align with the evaluation criteria, have fewer\nhyperparameters, and offer robustness against the imbalance between positive\nand negative classes. However, they require pairwise comparisons among $P$\npositive and $N$ negative predictions, introducing a time complexity of\n$\\mathcal{O}(PN)$, which is prohibitive since $N$ is often large (e.g., $10^8$\nin ATSS). Despite their advantages, the widespread adoption of ranking-based\nlosses has been hindered by their high time and space complexities.\n  In this paper, we focus on improving the efficiency of ranking-based loss\nfunctions. To this end, we propose Bucketed Ranking-based Losses which group\nnegative predictions into $B$ buckets ($B \\ll N$) in order to reduce the number\nof pairwise comparisons so that time complexity can be reduced. Our method\nenhances the time complexity, reducing it to $\\mathcal{O}(\\max (N \\log(N),\nP^2))$. To validate our method and show its generality, we conducted\nexperiments on 2 different tasks, 3 different datasets, 7 different detectors.\nWe show that Bucketed Ranking-based (BR) Losses yield the same accuracy with\nthe unbucketed versions and provide $2\\times$ faster training on average. We\nalso train, for the first time, transformer-based object detectors using\nranking-based losses, thanks to the efficiency of our BR. When we train CoDETR,\na state-of-the-art transformer-based object detector, using our BR Loss, we\nconsistently outperform its original results over several different backbones.\nCode is available at https://github.com/blisgard/BucketedRankingBasedLosses\n","authors":["Feyza Yavuz","Baris Can Cam","Adnan Harun Dogan","Kemal Oksuz","Emre Akbas","Sinan Kalkan"],"pdf_url":"https://arxiv.org/pdf/2407.14204v1.pdf","comment":"to appear in ECCV 2024"},{"id":"http://arxiv.org/abs/2101.05687v3","updated":"2024-07-19T10:57:35Z","published":"2021-01-14T16:06:08Z","title":"Towards Accurate Camouflaged Object Detection with Mixture Convolution\n  and Interactive Fusion","summary":"  Camouflaged object detection (COD), which aims to identify the objects that\nconceal themselves into the surroundings, has recently drawn increasing\nresearch efforts in the field of computer vision. In practice, the success of\ndeep learning based COD is mainly determined by two key factors, including (i)\nA significantly large receptive field, which provides rich context information,\nand (ii) An effective fusion strategy, which aggregates the rich multi-level\nfeatures for accurate COD. Motivated by these observations, in this paper, we\npropose a novel deep learning based COD approach, which integrates the large\nreceptive field and effective feature fusion into a unified framework.\nSpecifically, we first extract multi-level features from a backbone network.\nThe resulting features are then fed to the proposed dual-branch mixture\nconvolution modules, each of which utilizes multiple asymmetric convolutional\nlayers and two dilated convolutional layers to extract rich context features\nfrom a large receptive field. Finally, we fuse the features using\nspecially-designed multilevel interactive fusion modules, each of which employs\nan attention mechanism along with feature interaction for effective feature\nfusion. Our method detects camouflaged objects with an effective fusion\nstrategy, which aggregates the rich context information from a large receptive\nfield. All of these designs meet the requirements of COD well, allowing the\naccurate detection of camouflaged objects. Extensive experiments on widely-used\nbenchmark datasets demonstrate that our method is capable of accurately\ndetecting camouflaged objects and outperforms the state-of-the-art methods.\n","authors":["Geng Chen","Xinrui Chen","Bo Dong","Mingchen Zhuge","Yongxiong Wang","Hongbo Bi","Jian Chen","Peng Wang","Yanning Zhang"],"pdf_url":"https://arxiv.org/pdf/2101.05687v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14198v1","updated":"2024-07-19T10:49:26Z","published":"2024-07-19T10:49:26Z","title":"Double-Shot 3D Shape Measurement with a Dual-Branch Network","summary":"  The structured light (SL)-based 3D measurement techniques with deep learning\nhave been widely studied, among which speckle projection profilometry (SPP) and\nfringe projection profilometry (FPP) are two popular methods. However, they\ngenerally use a single projection pattern for reconstruction, resulting in\nfringe order ambiguity or poor reconstruction accuracy. To alleviate these\nproblems, we propose a parallel dual-branch Convolutional Neural Network\n(CNN)-Transformer network (PDCNet), to take advantage of convolutional\noperations and self-attention mechanisms for processing different SL\nmodalities. Within PDCNet, a Transformer branch is used to capture global\nperception in the fringe images, while a CNN branch is designed to collect\nlocal details in the speckle images. To fully integrate complementary features,\nwe design a double-stream attention aggregation module (DAAM) that consist of a\nparallel attention subnetwork for aggregating multi-scale spatial structure\ninformation. This module can dynamically retain local and global\nrepresentations to the maximum extent. Moreover, an adaptive mixture density\nhead with bimodal Gaussian distribution is proposed for learning a\nrepresentation that is precise near discontinuities. Compared to the standard\ndisparity regression strategy, this adaptive mixture head can effectively\nimproves performance at object boundaries. Extensive experiments demonstrate\nthat our method can reduce fringe order ambiguity while producing high-accuracy\nresults on a self-made dataset. We also show that the proposed architecture\nreveals the potential in infrared-visible image fusion task.\n","authors":["Mingyang Lei","Jingfan Fan","Long Shao","Hong Song","Deqiang Xiao","Danni Ai","Tianyu Fu","Ying Gu","Jian Yang"],"pdf_url":"https://arxiv.org/pdf/2407.14198v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14197v1","updated":"2024-07-19T10:49:07Z","published":"2024-07-19T10:49:07Z","title":"A Benchmark for Gaussian Splatting Compression and Quality Assessment\n  Study","summary":"  To fill the gap of traditional GS compression method, in this paper, we first\npropose a simple and effective GS data compression anchor called Graph-based GS\nCompression (GGSC). GGSC is inspired by graph signal processing theory and uses\ntwo branches to compress the primitive center and attributes. We split the\nwhole GS sample via KDTree and clip the high-frequency components after the\ngraph Fourier transform. Followed by quantization, G-PCC and adaptive\narithmetic coding are used to compress the primitive center and attribute\nresidual matrix to generate the bitrate file. GGSS is the first work to explore\ntraditional GS compression, with advantages that can reveal the GS distortion\ncharacteristics corresponding to typical compression operation, such as\nhigh-frequency clipping and quantization. Second, based on GGSC, we create a GS\nQuality Assessment dataset (GSQA) with 120 samples. A subjective experiment is\nconducted in a laboratory environment to collect subjective scores after\nrendering GS into Processed Video Sequences (PVS). We analyze the\ncharacteristics of different GS distortions based on Mean Opinion Scores (MOS),\ndemonstrating the sensitivity of different attributes distortion to visual\nquality. The GGSC code and the dataset, including GS samples, MOS, and PVS, are\nmade publicly available at https://github.com/Qi-Yangsjtu/GGSC.\n","authors":["Qi Yang","Kaifa Yang","Yuke Xing","Yiling Xu","Zhu Li"],"pdf_url":"https://arxiv.org/pdf/2407.14197v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.09372v3","updated":"2024-07-19T10:44:53Z","published":"2023-08-18T08:06:49Z","title":"Which Transformer to Favor: A Comparative Analysis of Efficiency in\n  Vision Transformers","summary":"  Self-attention in Transformers comes with a high computational cost because\nof their quadratic computational complexity, but their effectiveness in\naddressing problems in language and vision has sparked extensive research aimed\nat enhancing their efficiency. However, diverse experimental conditions,\nspanning multiple input domains, prevent a fair comparison based solely on\nreported results, posing challenges for model selection. To address this gap in\ncomparability, we perform a large-scale benchmark of more than 45 models for\nimage classification, evaluating key efficiency aspects, including accuracy,\nspeed, and memory usage. Our benchmark provides a standardized baseline for\nefficiency-oriented transformers. We analyze the results based on the Pareto\nfront -- the boundary of optimal models. Surprisingly, despite claims of other\nmodels being more efficient, ViT remains Pareto optimal across multiple\nmetrics. We observe that hybrid attention-CNN models exhibit remarkable\ninference memory- and parameter-efficiency. Moreover, our benchmark shows that\nusing a larger model in general is more efficient than using higher resolution\nimages. Thanks to our holistic evaluation, we provide a centralized resource\nfor practitioners and researchers, facilitating informed decisions when\nselecting or developing efficient transformers.\n","authors":["Tobias Christian Nauen","Sebastian Palacio","Federico Raue","Andreas Dengel"],"pdf_url":"https://arxiv.org/pdf/2308.09372v3.pdf","comment":"v3: new models, analysis of scaling behaviors"},{"id":"http://arxiv.org/abs/2407.14191v1","updated":"2024-07-19T10:39:24Z","published":"2024-07-19T10:39:24Z","title":"Normative Diffusion Autoencoders: Application to Amyotrophic Lateral\n  Sclerosis","summary":"  Predicting survival in Amyotrophic Lateral Sclerosis (ALS) is a challenging\ntask. Magnetic resonance imaging (MRI) data provide in vivo insight into brain\nhealth, but the low prevalence of the condition and resultant data scarcity\nlimit training set sizes for prediction models. Survival models are further\nhindered by the subtle and often highly localised profile of ALS-related\nneurodegeneration. Normative models present a solution as they increase\nstatistical power by leveraging large healthy cohorts. Separately, diffusion\nmodels excel in capturing the semantics embedded within images including subtle\nsigns of accelerated brain ageing, which may help predict survival in ALS.\nHere, we combine the benefits of generative and normative modelling by\nintroducing the normative diffusion autoencoder framework. To our knowledge,\nthis is the first use of normative modelling within a diffusion autoencoder, as\nwell as the first application of normative modelling to ALS. Our approach\noutperforms generative and non-generative normative modelling benchmarks in ALS\nprognostication, demonstrating enhanced predictive accuracy in the context of\nALS survival prediction and normative modelling in general.\n","authors":["Ayodeji Ijishakin","Adamos Hadjasavilou","Ahmed Abdulaal","Nina Montana-Brown","Florence Townend","Edoardo Spinelli","Massimo Fillipi","Federica Agosta","James Cole","Andrea Malaspina"],"pdf_url":"https://arxiv.org/pdf/2407.14191v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14188v1","updated":"2024-07-19T10:32:06Z","published":"2024-07-19T10:32:06Z","title":"TaGAT: Topology-Aware Graph Attention Network For Multi-modal Retinal\n  Image Fusion","summary":"  In the realm of medical image fusion, integrating information from various\nmodalities is crucial for improving diagnostics and treatment planning,\nespecially in retinal health, where the important features exhibit differently\nin different imaging modalities. Existing deep learning-based approaches\ninsufficiently focus on retinal image fusion, and thus fail to preserve enough\nanatomical structure and fine vessel details in retinal image fusion. To\naddress this, we propose the Topology-Aware Graph Attention Network (TaGAT) for\nmulti-modal retinal image fusion, leveraging a novel Topology-Aware Encoder\n(TAE) with Graph Attention Networks (GAT) to effectively enhance spatial\nfeatures with retinal vasculature's graph topology across modalities. The TAE\nencodes the base and detail features, extracted via a Long-short Range (LSR)\nencoder from retinal images, into the graph extracted from the retinal vessel.\nWithin the TAE, the GAT-based Graph Information Update (GIU) block dynamically\nrefines and aggregates the node features to generate topology-aware graph\nfeatures. The updated graph features with base and detail features are combined\nand decoded as a fused image. Our model outperforms state-of-the-art methods in\nFluorescein Fundus Angiography (FFA) with Color Fundus (CF) and Optical\nCoherence Tomography (OCT) with confocal microscopy retinal image fusion. The\nsource code can be accessed via https://github.com/xintian-99/TaGAT.\n","authors":["Xin Tian","Nantheera Anantrasirichai","Lindsay Nicholson","Alin Achim"],"pdf_url":"https://arxiv.org/pdf/2407.14188v1.pdf","comment":"11 pages, 2 figures, accepted by MICCAI 2024"},{"id":"http://arxiv.org/abs/2403.18328v2","updated":"2024-07-19T10:30:08Z","published":"2024-03-27T08:09:04Z","title":"PIPNet3D: Interpretable Detection of Alzheimer in MRI Scans","summary":"  Information from neuroimaging examinations is increasingly used to support\ndiagnoses of dementia, e.g., Alzheimer's disease. While current clinical\npractice is mainly based on visual inspection and feature engineering, Deep\nLearning approaches can be used to automate the analysis and to discover new\nimage-biomarkers. Part-prototype neural networks (PP-NN) are an alternative to\nstandard blackbox models, and have shown promising results in general computer\nvision. PP-NN's base their reasoning on prototypical image regions that are\nlearned fully unsupervised, and combined with a simple-to-understand decision\nlayer. We present PIPNet3D, a PP-NN for volumetric images. We apply PIPNet3D to\nthe clinical diagnosis of Alzheimer's Disease from structural Magnetic\nResonance Imaging (sMRI). We assess the quality of prototypes under a\nsystematic evaluation framework, propose new functionally grounded metrics to\nevaluate brain prototypes and develop an evaluation scheme to assess their\ncoherency with domain experts. Our results show that PIPNet3D is an\ninterpretable, compact model for Alzheimer's diagnosis with its reasoning well\naligned to medical domain knowledge. Notably, PIPNet3D achieves the same\naccuracy as its blackbox counterpart; and removing the remaining clinically\nirrelevant prototypes from its decision process does not decrease predictive\nperformance.\n","authors":["Lisa Anita De Santi","Jörg Schlötterer","Michael Scheschenja","Joel Wessendorf","Meike Nauta","Vincenzo Positano","Christin Seifert"],"pdf_url":"https://arxiv.org/pdf/2403.18328v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.12073v2","updated":"2024-07-19T10:25:18Z","published":"2024-07-16T14:56:13Z","title":"Relational Representation Distillation","summary":"  Knowledge distillation (KD) is an effective method for transferring knowledge\nfrom a large, well-trained teacher model to a smaller, more efficient student\nmodel. Despite its success, one of the main challenges in KD is ensuring the\nefficient transfer of complex knowledge while maintaining the student's\ncomputational efficiency. Unlike previous works that applied contrastive\nobjectives promoting explicit negative instances, we introduce Relational\nRepresentation Distillation (RRD). Our approach leverages pairwise similarities\nto explore and reinforce the relationships between the teacher and student\nmodels. Inspired by self-supervised learning principles, it uses a relaxed\ncontrastive loss that focuses on similarity rather than exact replication. This\nmethod aligns the output distributions of teacher samples in a large memory\nbuffer, improving the robustness and performance of the student model without\nthe need for strict negative instance differentiation. Our approach\ndemonstrates superior performance on CIFAR-100, outperforming traditional KD\ntechniques and surpassing 13 state-of-the-art methods. It also transfers\nsuccessfully to other datasets like Tiny ImageNet and STL-10. The code will be\nmade public soon.\n","authors":["Nikolaos Giakoumoglou","Tania Stathaki"],"pdf_url":"https://arxiv.org/pdf/2407.12073v2.pdf","comment":"8 pages, 4 figures, 3 tables. arXiv admin note: text overlap with\n  arXiv:2407.11802"},{"id":"http://arxiv.org/abs/2403.09434v3","updated":"2024-07-19T10:23:21Z","published":"2024-03-14T14:25:10Z","title":"Reconstruction and Simulation of Elastic Objects with Spring-Mass 3D\n  Gaussians","summary":"  Reconstructing and simulating elastic objects from visual observations is\ncrucial for applications in computer vision and robotics. Existing methods,\nsuch as 3D Gaussians, model 3D appearance and geometry, but lack the ability to\nestimate physical properties for objects and simulate them. The core challenge\nlies in integrating an expressive yet efficient physical dynamics model. We\npropose Spring-Gaus, a 3D physical object representation for reconstructing and\nsimulating elastic objects from videos of the object from multiple viewpoints.\nIn particular, we develop and integrate a 3D Spring-Mass model into 3D Gaussian\nkernels, enabling the reconstruction of the visual appearance, shape, and\nphysical dynamics of the object. Our approach enables future prediction and\nsimulation under various initial states and environmental properties. We\nevaluate Spring-Gaus on both synthetic and real-world datasets, demonstrating\naccurate reconstruction and simulation of elastic objects. Project page:\nhttps://zlicheng.com/spring_gaus/.\n","authors":["Licheng Zhong","Hong-Xing Yu","Jiajun Wu","Yunzhu Li"],"pdf_url":"https://arxiv.org/pdf/2403.09434v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14177v1","updated":"2024-07-19T10:09:51Z","published":"2024-07-19T10:09:51Z","title":"EVLM: An Efficient Vision-Language Model for Visual Understanding","summary":"  In the field of multi-modal language models, the majority of methods are\nbuilt on an architecture similar to LLaVA. These models use a single-layer ViT\nfeature as a visual prompt, directly feeding it into the language models\nalongside textual tokens. However, when dealing with long sequences of visual\nsignals or inputs such as videos, the self-attention mechanism of language\nmodels can lead to significant computational overhead. Additionally, using\nsingle-layer ViT features makes it challenging for large language models to\nperceive visual signals fully. This paper proposes an efficient multi-modal\nlanguage model to minimize computational costs while enabling the model to\nperceive visual signals as comprehensively as possible. Our method primarily\nincludes: (1) employing cross-attention to image-text interaction similar to\nFlamingo. (2) utilize hierarchical ViT features. (3) introduce the Mixture of\nExperts (MoE) mechanism to enhance model effectiveness. Our model achieves\ncompetitive scores on public multi-modal benchmarks and performs well in tasks\nsuch as image captioning and video captioning.\n","authors":["Kaibing Chen","Dong Shen","Hanwen Zhong","Huasong Zhong","Kui Xia","Di Xu","Wei Yuan","Yifei Hu","Bin Wen","Tianke Zhang","Changyi Liu","Dewen Fan","Huihui Xiao","Jiahong Wu","Fan Yang","Size Li","Di Zhang"],"pdf_url":"https://arxiv.org/pdf/2407.14177v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.12390v2","updated":"2024-07-19T10:05:15Z","published":"2024-07-17T08:11:37Z","title":"Enhancing Facial Expression Recognition through Dual-Direction Attention\n  Mixed Feature Networks: Application to 7th ABAW Challenge","summary":"  We present our contribution to the 7th ABAW challenge at ECCV 2024, by\nutilizing a Dual-Direction Attention Mixed Feature Network for multitask facial\nexpression recognition we achieve results far beyond the proposed baseline for\nthe Multi-Task ABAW challenge. Our proposal uses the well-known DDAMFN\narchitecture as base to effectively predict valence-arousal, emotion\nrecognition, and action units. We demonstrate the architecture ability to\nhandle these tasks simultaneously, providing insights into its architecture and\nthe rationale behind its design. Additionally, we compare our results for a\nmultitask solution with independent single-task performance.\n","authors":["Josep Cabacas-Maso","Elena Ortega-Beltrán","Ismael Benito-Altamirano","Carles Ventura"],"pdf_url":"https://arxiv.org/pdf/2407.12390v2.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2407.14170v1","updated":"2024-07-19T09:58:00Z","published":"2024-07-19T09:58:00Z","title":"Forbes: Face Obfuscation Rendering via Backpropagation Refinement Scheme","summary":"  A novel algorithm for face obfuscation, called Forbes, which aims to\nobfuscate facial appearance recognizable by humans but preserve the identity\nand attributes decipherable by machines, is proposed in this paper. Forbes\nfirst applies multiple obfuscating transformations with random parameters to an\nimage to remove the identity information distinguishable by humans. Then, it\noptimizes the parameters to make the transformed image decipherable by machines\nbased on the backpropagation refinement scheme. Finally, it renders an\nobfuscated image by applying the transformations with the optimized parameters.\nExperimental results on various datasets demonstrate that Forbes achieves both\nhuman indecipherability and machine decipherability excellently. The source\ncodes are available at https://github.com/mcljtkim/Forbes.\n","authors":["Jintae Kim","Seungwon yang","Seong-Gyun Jeong","Chang-Su Kim"],"pdf_url":"https://arxiv.org/pdf/2407.14170v1.pdf","comment":"Accepted to ECCV 2024"},{"id":"http://arxiv.org/abs/2405.05079v4","updated":"2024-07-19T09:57:50Z","published":"2024-05-08T14:22:39Z","title":"Power Variable Projection for Initialization-Free Large-Scale Bundle\n  Adjustment","summary":"  Most Bundle Adjustment (BA) solvers like the Levenberg-Marquardt algorithm\nrequire a good initialization. Instead, initialization-free BA remains a\nlargely uncharted territory. The under-explored Variable Projection algorithm\n(VarPro) exhibits a wide convergence basin even without initialization. Coupled\nwith object space error formulation, recent works have shown its ability to\nsolve small-scale initialization-free bundle adjustment problem. To make such\ninitialization-free BA approaches scalable, we introduce Power Variable\nProjection (PoVar), extending a recent inverse expansion method based on power\nseries. Importantly, we link the power series expansion to Riemannian manifold\noptimization. This projective framework is crucial to solve large-scale bundle\nadjustment problems without initialization. Using the real-world BAL dataset,\nwe experimentally demonstrate that our solver achieves state-of-the-art results\nin terms of speed and accuracy. To our knowledge, this work is the first to\naddress the scalability of BA without initialization opening new venues for\ninitialization-free structure-from-motion.\n","authors":["Simon Weber","Je Hyeong Hong","Daniel Cremers"],"pdf_url":"https://arxiv.org/pdf/2405.05079v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14200v2","updated":"2024-07-19T09:50:51Z","published":"2024-03-21T07:50:45Z","title":"Debiasing surgeon: fantastic weights and how to find them","summary":"  Nowadays an ever-growing concerning phenomenon, the emergence of algorithmic\nbiases that can lead to unfair models, emerges. Several debiasing approaches\nhave been proposed in the realm of deep learning, employing more or less\nsophisticated approaches to discourage these models from massively employing\nthese biases. However, a question emerges: is this extra complexity really\nnecessary? Is a vanilla-trained model already embodying some ``unbiased\nsub-networks'' that can be used in isolation and propose a solution without\nrelying on the algorithmic biases? In this work, we show that such a\nsub-network typically exists, and can be extracted from a vanilla-trained model\nwithout requiring additional training. We further validate that such specific\narchitecture is incapable of learning a specific bias, suggesting that there\nare possible architectural countermeasures to the problem of biases in deep\nneural networks.\n","authors":["Rémi Nahon","Ivan Luiz De Moura Matos","Van-Tam Nguyen","Enzo Tartaglione"],"pdf_url":"https://arxiv.org/pdf/2403.14200v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.19043v2","updated":"2024-07-19T09:42:22Z","published":"2024-02-29T11:11:05Z","title":"WDM: 3D Wavelet Diffusion Models for High-Resolution Medical Image\n  Synthesis","summary":"  Due to the three-dimensional nature of CT- or MR-scans, generative modeling\nof medical images is a particularly challenging task. Existing approaches\nmostly apply patch-wise, slice-wise, or cascaded generation techniques to fit\nthe high-dimensional data into the limited GPU memory. However, these\napproaches may introduce artifacts and potentially restrict the model's\napplicability for certain downstream tasks. This work presents WDM, a\nwavelet-based medical image synthesis framework that applies a diffusion model\non wavelet decomposed images. The presented approach is a simple yet effective\nway of scaling 3D diffusion models to high resolutions and can be trained on a\nsingle \\SI{40}{\\giga\\byte} GPU. Experimental results on BraTS and LIDC-IDRI\nunconditional image generation at a resolution of $128 \\times 128 \\times 128$\ndemonstrate state-of-the-art image fidelity (FID) and sample diversity\n(MS-SSIM) scores compared to recent GANs, Diffusion Models, and Latent\nDiffusion Models. Our proposed method is the only one capable of generating\nhigh-quality images at a resolution of $256 \\times 256 \\times 256$,\noutperforming all comparing methods.\n","authors":["Paul Friedrich","Julia Wolleb","Florentin Bieder","Alicia Durrer","Philippe C. Cattin"],"pdf_url":"https://arxiv.org/pdf/2402.19043v2.pdf","comment":"Accepted at DGM4MICCAI 2024. Project page:\n  https://pfriedri.github.io/wdm-3d-io Code: https://github.com/pfriedri/wdm-3d"},{"id":"http://arxiv.org/abs/2403.10107v2","updated":"2024-07-19T09:38:18Z","published":"2024-03-15T08:51:15Z","title":"Enhancing Human-Centered Dynamic Scene Understanding via Multiple LLMs\n  Collaborated Reasoning","summary":"  Human-centered dynamic scene understanding plays a pivotal role in enhancing\nthe capability of robotic and autonomous systems, in which Video-based\nHuman-Object Interaction (V-HOI) detection is a crucial task in semantic scene\nunderstanding, aimed at comprehensively understanding HOI relationships within\na video to benefit the behavioral decisions of mobile robots and autonomous\ndriving systems. Although previous V-HOI detection models have made significant\nstrides in accurate detection on specific datasets, they still lack the general\nreasoning ability like human beings to effectively induce HOI relationships. In\nthis study, we propose V-HOI Multi-LLMs Collaborated Reasoning (V-HOI MLCR), a\nnovel framework consisting of a series of plug-and-play modules that could\nfacilitate the performance of current V-HOI detection models by leveraging the\nstrong reasoning ability of different off-the-shelf pre-trained large language\nmodels (LLMs). We design a two-stage collaboration system of different LLMs for\nthe V-HOI task. Specifically, in the first stage, we design a Cross-Agents\nReasoning scheme to leverage the LLM conduct reasoning from different aspects.\nIn the second stage, we perform Multi-LLMs Debate to get the final reasoning\nanswer based on the different knowledge in different LLMs. Additionally, we\ndevise an auxiliary training strategy that utilizes CLIP, a large\nvision-language model to enhance the base V-HOI models' discriminative ability\nto better cooperate with LLMs. We validate the superiority of our design by\ndemonstrating its effectiveness in improving the prediction accuracy of the\nbase V-HOI model via reasoning from multiple perspectives.\n","authors":["Hang Zhang","Wenxiao Zhang","Haoxuan Qu","Jun Liu"],"pdf_url":"https://arxiv.org/pdf/2403.10107v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.14726v2","updated":"2024-07-19T09:35:18Z","published":"2024-01-26T09:21:46Z","title":"3D Reconstruction and New View Synthesis of Indoor Environments based on\n  a Dual Neural Radiance Field","summary":"  Simultaneously achieving 3D reconstruction and new view synthesis for indoor\nenvironments has widespread applications but is technically very challenging.\nState-of-the-art methods based on implicit neural functions can achieve\nexcellent 3D reconstruction results, but their performances on new view\nsynthesis can be unsatisfactory. The exciting development of neural radiance\nfield (NeRF) has revolutionized new view synthesis, however, NeRF-based models\ncan fail to reconstruct clean geometric surfaces. We have developed a dual\nneural radiance field (Du-NeRF) to simultaneously achieve high-quality geometry\nreconstruction and view rendering. Du-NeRF contains two geometric fields, one\nderived from the SDF field to facilitate geometric reconstruction and the other\nderived from the density field to boost new view synthesis. One of the\ninnovative features of Du-NeRF is that it decouples a view-independent\ncomponent from the density field and uses it as a label to supervise the\nlearning process of the SDF field. This reduces shape-radiance ambiguity and\nenables geometry and color to benefit from each other during the learning\nprocess. Extensive experiments demonstrate that Du-NeRF can significantly\nimprove the performance of novel view synthesis and 3D reconstruction for\nindoor environments and it is particularly effective in constructing areas\ncontaining fine geometries that do not obey multi-view color consistency.\n","authors":["Zhenyu Bao","Guibiao Liao","Zhongyuan Zhao","Kanglin Liu","Qing Li","Guoping Qiu"],"pdf_url":"https://arxiv.org/pdf/2401.14726v2.pdf","comment":"20 pages, 8 figures"},{"id":"http://arxiv.org/abs/2311.18809v2","updated":"2024-07-19T09:33:12Z","published":"2023-11-30T18:52:29Z","title":"FoundPose: Unseen Object Pose Estimation with Foundation Features","summary":"  We propose FoundPose, a model-based method for 6D pose estimation of unseen\nobjects from a single RGB image. The method can quickly onboard new objects\nusing their 3D models without requiring any object- or task-specific training.\nIn contrast, existing methods typically pre-train on large-scale, task-specific\ndatasets in order to generalize to new objects and to bridge the image-to-model\ndomain gap. We demonstrate that such generalization capabilities can be\nobserved in a recent vision foundation model trained in a self-supervised\nmanner. Specifically, our method estimates the object pose from image-to-model\n2D-3D correspondences, which are established by matching patch descriptors from\nthe recent DINOv2 model between the image and pre-rendered object templates. We\nfind that reliable correspondences can be established by kNN matching of patch\ndescriptors from an intermediate DINOv2 layer. Such descriptors carry stronger\npositional information than descriptors from the last layer, and we show their\nimportance when semantic information is ambiguous due to object symmetries or a\nlack of texture. To avoid establishing correspondences against all object\ntemplates, we develop an efficient template retrieval approach that integrates\nthe patch descriptors into the bag-of-words representation and can promptly\npropose a handful of similarly looking templates. Additionally, we apply\nfeaturemetric alignment to compensate for discrepancies in the 2D-3D\ncorrespondences caused by coarse patch sampling. The resulting method\nnoticeably outperforms existing RGB methods for refinement-free pose estimation\non the standard BOP benchmark with seven diverse datasets and can be seamlessly\ncombined with an existing render-and-compare refinement method to achieve\nRGB-only state-of-the-art results. Project page: evinpinar.github.io/foundpose.\n","authors":["Evin Pınar Örnek","Yann Labbé","Bugra Tekin","Lingni Ma","Cem Keskin","Christian Forster","Tomas Hodan"],"pdf_url":"https://arxiv.org/pdf/2311.18809v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14153v1","updated":"2024-07-19T09:32:30Z","published":"2024-07-19T09:32:30Z","title":"ESP-MedSAM: Efficient Self-Prompting SAM for Universal\n  Domain-Generalized Medical Image Segmentation","summary":"  The Segment Anything Model (SAM) has demonstrated outstanding adaptation to\nmedical image segmentation but still faces three major challenges. Firstly, the\nhuge computational costs of SAM limit its real-world applicability. Secondly,\nSAM depends on manual annotations (e.g., points, boxes) as prompts, which are\nlaborious and impractical in clinical scenarios. Thirdly, SAM handles all\nsegmentation targets equally, which is suboptimal for diverse medical\nmodalities with inherent heterogeneity. To address these issues, we propose an\nEfficient Self-Prompting SAM for universal medical image segmentation, named\nESP-MedSAM. We devise a Multi-Modal Decoupled Knowledge Distillation (MMDKD)\nstrategy to distil common image knowledge and domain-specific medical knowledge\nfrom the foundation model to train a lightweight image encoder and a modality\ncontroller. Further, they combine with the additionally introduced Self-Patch\nPrompt Generator (SPPG) and Query-Decoupled Modality Decoder (QDMD) to\nconstruct ESP-MedSAM. Specifically, SPPG aims to generate a set of patch\nprompts automatically and QDMD leverages a one-to-one strategy to provide an\nindependent decoding channel for every modality. Extensive experiments indicate\nthat ESP-MedSAM outperforms state-of-the-arts in diverse medical imaging\nsegmentation takes, displaying superior zero-shot learning and modality\ntransfer ability. Especially, our framework uses only 31.4% parameters compared\nto SAM-Base.\n","authors":["Qing Xu","Jiaxuan Li","Xiangjian He","Ziyu Liu","Zhen Chen","Wenting Duan","Chenxin Li","Maggie M. He","Fiseha B. Tesema","Wooi P. Cheah","Yi Wang","Rong Qu","Jonathan M. Garibaldi"],"pdf_url":"https://arxiv.org/pdf/2407.14153v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14143v1","updated":"2024-07-19T09:20:33Z","published":"2024-07-19T09:20:33Z","title":"Class-Incremental Learning with CLIP: Adaptive Representation Adjustment\n  and Parameter Fusion","summary":"  Class-incremental learning is a challenging problem, where the goal is to\ntrain a model that can classify data from an increasing number of classes over\ntime. With the advancement of vision-language pre-trained models such as CLIP,\nthey demonstrate good generalization ability that allows them to excel in\nclass-incremental learning with completely frozen parameters. However, further\nadaptation to downstream tasks by simply fine-tuning the model leads to severe\nforgetting. Most existing works with pre-trained models assume that the\nforgetting of old classes is uniform when the model acquires new knowledge. In\nthis paper, we propose a method named Adaptive Representation Adjustment and\nParameter Fusion (RAPF). During training for new data, we measure the influence\nof new classes on old ones and adjust the representations, using textual\nfeatures. After training, we employ a decomposed parameter fusion to further\nmitigate forgetting during adapter module fine-tuning. Experiments on several\nconventional benchmarks show that our method achieves state-of-the-art results.\nOur code is available at \\url{https://github.com/linlany/RAPF}.\n","authors":["Linlan Huang","Xusheng Cao","Haori Lu","Xialei Liu"],"pdf_url":"https://arxiv.org/pdf/2407.14143v1.pdf","comment":"Accepted at ECCV 2024"},{"id":"http://arxiv.org/abs/2407.14142v1","updated":"2024-07-19T09:19:29Z","published":"2024-07-19T09:19:29Z","title":"Early Preparation Pays Off: New Classifier Pre-tuning for Class\n  Incremental Semantic Segmentation","summary":"  Class incremental semantic segmentation aims to preserve old knowledge while\nlearning new tasks, however, it is impeded by catastrophic forgetting and\nbackground shift issues. Prior works indicate the pivotal importance of\ninitializing new classifiers and mainly focus on transferring knowledge from\nthe background classifier or preparing classifiers for future classes,\nneglecting the flexibility and variance of new classifiers. In this paper, we\npropose a new classifier pre-tuning~(NeST) method applied before the formal\ntraining process, learning a transformation from old classifiers to generate\nnew classifiers for initialization rather than directly tuning the parameters\nof new classifiers. Our method can make new classifiers align with the backbone\nand adapt to the new data, preventing drastic changes in the feature extractor\nwhen learning new classes. Besides, we design a strategy considering the\ncross-task class similarity to initialize matrices used in the transformation,\nhelping achieve the stability-plasticity trade-off. Experiments on Pascal VOC\n2012 and ADE20K datasets show that the proposed strategy can significantly\nimprove the performance of previous methods. The code is available at\n\\url{https://github.com/zhengyuan-xie/ECCV24_NeST}.\n","authors":["Zhengyuan Xie","Haiquan Lu","Jia-wen Xiao","Enguang Wang","Le Zhang","Xialei Liu"],"pdf_url":"https://arxiv.org/pdf/2407.14142v1.pdf","comment":"Accepted at ECCV 2024"},{"id":"http://arxiv.org/abs/2407.14138v1","updated":"2024-07-19T09:08:20Z","published":"2024-07-19T09:08:20Z","title":"Visual Text Generation in the Wild","summary":"  Recently, with the rapid advancements of generative models, the field of\nvisual text generation has witnessed significant progress. However, it is still\nchallenging to render high-quality text images in real-world scenarios, as\nthree critical criteria should be satisfied: (1) Fidelity: the generated text\nimages should be photo-realistic and the contents are expected to be the same\nas specified in the given conditions; (2) Reasonability: the regions and\ncontents of the generated text should cohere with the scene; (3) Utility: the\ngenerated text images can facilitate related tasks (e.g., text detection and\nrecognition). Upon investigation, we find that existing methods, either\nrendering-based or diffusion-based, can hardly meet all these aspects\nsimultaneously, limiting their application range. Therefore, we propose in this\npaper a visual text generator (termed SceneVTG), which can produce high-quality\ntext images in the wild. Following a two-stage paradigm, SceneVTG leverages a\nMultimodal Large Language Model to recommend reasonable text regions and\ncontents across multiple scales and levels, which are used by a conditional\ndiffusion model as conditions to generate text images. Extensive experiments\ndemonstrate that the proposed SceneVTG significantly outperforms traditional\nrendering-based methods and recent diffusion-based methods in terms of fidelity\nand reasonability. Besides, the generated images provide superior utility for\ntasks involving text detection and text recognition. Code and datasets are\navailable at AdvancedLiterateMachinery.\n","authors":["Yuanzhi Zhu","Jiawei Liu","Feiyu Gao","Wenyu Liu","Xinggang Wang","Peng Wang","Fei Huang","Cong Yao","Zhibo Yang"],"pdf_url":"https://arxiv.org/pdf/2407.14138v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.00086v5","updated":"2024-07-19T09:07:20Z","published":"2024-03-29T17:58:50Z","title":"DVIS-DAQ: Improving Video Segmentation via Dynamic Anchor Queries","summary":"  Modern video segmentation methods adopt object queries to perform inter-frame\nassociation and demonstrate satisfactory performance in tracking continuously\nappearing objects despite large-scale motion and transient occlusion. However,\nthey all underperform on newly emerging and disappearing objects that are\ncommon in the real world because they attempt to model object emergence and\ndisappearance through feature transitions between background and foreground\nqueries that have significant feature gaps. We introduce Dynamic Anchor Queries\n(DAQ) to shorten the transition gap between the anchor and target queries by\ndynamically generating anchor queries based on the features of potential\ncandidates. Furthermore, we introduce a query-level object Emergence and\nDisappearance Simulation (EDS) strategy, which unleashes DAQ's potential\nwithout any additional cost. Finally, we combine our proposed DAQ and EDS with\nDVIS to obtain DVIS-DAQ. Extensive experiments demonstrate that DVIS-DAQ\nachieves a new state-of-the-art (SOTA) performance on five mainstream video\nsegmentation benchmarks. Code and models are available at\n\\url{https://github.com/SkyworkAI/DAQ-VS}.\n","authors":["Yikang Zhou","Tao Zhang","Shunping Ji","Shuicheng Yan","Xiangtai Li"],"pdf_url":"https://arxiv.org/pdf/2404.00086v5.pdf","comment":"Accepted by ECCV-2024"},{"id":"http://arxiv.org/abs/2407.14136v1","updated":"2024-07-19T09:05:49Z","published":"2024-07-19T09:05:49Z","title":"6DoF Head Pose Estimation through Explicit Bidirectional Interaction\n  with Face Geometry","summary":"  This study addresses the nuanced challenge of estimating head translations\nwithin the context of six-degrees-of-freedom (6DoF) head pose estimation,\nplacing emphasis on this aspect over the more commonly studied head rotations.\nIdentifying a gap in existing methodologies, we recognized the underutilized\npotential synergy between facial geometry and head translation. To bridge this\ngap, we propose a novel approach called the head Translation, Rotation, and\nface Geometry network (TRG), which stands out for its explicit bidirectional\ninteraction structure. This structure has been carefully designed to leverage\nthe complementary relationship between face geometry and head translation,\nmarking a significant advancement in the field of head pose estimation. Our\ncontributions also include the development of a strategy for estimating\nbounding box correction parameters and a technique for aligning landmarks to\nimage. Both of these innovations demonstrate superior performance in 6DoF head\npose estimation tasks. Extensive experiments conducted on ARKitFace and BIWI\ndatasets confirm that the proposed method outperforms current state-of-the-art\ntechniques. Codes are released at https://github.com/asw91666/TRG-Release.\n","authors":["Sungho Chun","Ju Yong Chang"],"pdf_url":"https://arxiv.org/pdf/2407.14136v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.16518v7","updated":"2024-07-19T09:00:43Z","published":"2023-08-31T08:03:25Z","title":"MS23D: : A 3D Object Detection Method Using Multi-Scale Semantic Feature\n  Points to Construct 3D Feature Layer","summary":"  LiDAR point clouds can effectively depict the motion and posture of objects\nin three-dimensional space. Many studies accomplish the 3D object detection by\nvoxelizing point clouds. However, in autonomous driving scenarios, the sparsity\nand hollowness of point clouds create some difficulties for voxel-based\nmethods. The sparsity of point clouds makes it challenging to describe the\ngeometric features of objects. The hollowness of point clouds poses\ndifficulties for the aggregation of 3D features. We propose a two-stage 3D\nobject detection framework, called MS23D. (1) We propose a method using voxel\nfeature points from multi-branch to construct the 3D feature layer. Using voxel\nfeature points from different branches, we construct a relatively compact 3D\nfeature layer with rich semantic features. Additionally, we propose a\ndistance-weighted sampling method, reducing the loss of foreground points\ncaused by downsampling and allowing the 3D feature layer to retain more\nforeground points. (2) In response to the hollowness of point clouds, we\npredict the offsets between deep-level feature points and the object's\ncentroid, making them as close as possible to the object's centroid. This\nenables the aggregation of these feature points with abundant semantic\nfeatures. For feature points from shallow-level, we retain them on the object's\nsurface to describe the geometric features of the object. To validate our\napproach, we evaluated its effectiveness on both the KITTI and ONCE datasets.\n","authors":["Yongxin Shao","Aihong Tan","Binrui Wang","Tianhong Yan","Zhetao Sun","Yiyang Zhang","Jiaxin Liu"],"pdf_url":"https://arxiv.org/pdf/2308.16518v7.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14128v1","updated":"2024-07-19T08:56:12Z","published":"2024-07-19T08:56:12Z","title":"OCTolyzer: Fully automatic analysis toolkit for segmentation and feature\n  extracting in optical coherence tomography (OCT) and scanning laser\n  ophthalmoscopy (SLO) data","summary":"  Purpose: To describe OCTolyzer: an open-source toolkit for retinochoroidal\nanalysis in optical coherence tomography (OCT) and scanning laser\nophthalmoscopy (SLO) images.\n  Method: OCTolyzer has two analysis suites, for SLO and OCT images. The former\nenables anatomical segmentation and feature measurement of the en face retinal\nvessels. The latter leverages image metadata for retinal layer segmentations\nand deep learning-based choroid layer segmentation to compute retinochoroidal\nmeasurements such as thickness and volume. We introduce OCTolyzer and assess\nthe reproducibility of its OCT analysis suite for choroid analysis.\n  Results: At the population-level, choroid region metrics were highly\nreproducible (Mean absolute error/Pearson/Spearman correlation for macular\nvolume choroid thickness (CT):6.7$\\mu$m/0.9933/0.9969, macular B-scan\nCT:11.6$\\mu$m/0.9858/0.9889, peripapillary CT:5.0$\\mu$m/0.9942/0.9940). Macular\nchoroid vascular index (CVI) had good reproducibility (volume\nCVI:0.0271/0.9669/0.9655, B-scan CVI:0.0130/0.9090/0.9145). At the eye-level,\nmeasurement error in regional and vessel metrics were below 5% and 20% of the\npopulation's variability, respectively. Major outliers were from poor quality\nB-scans with thick choroids and invisible choroid-sclera boundary.\n  Conclusions: OCTolyzer is the first open-source pipeline to convert OCT/SLO\ndata into reproducible and clinically meaningful retinochoroidal measurements.\nOCT processing on a standard laptop CPU takes under 2 seconds for macular or\nperipapillary B-scans and 85 seconds for volume scans. OCTolyzer can help\nimprove standardisation in the field of OCT/SLO image analysis and is freely\navailable here: https://github.com/jaburke166/OCTolyzer.\n","authors":["Jamie Burke","Justin Engelmann","Samuel Gibbon","Charlene Hamid","Diana Moukaddem","Dan Pugh","Tariq Farrah","Niall Strang","Neeraj Dhaun","Tom MacGillivray","Stuart King","Ian J. C. MacCormick"],"pdf_url":"https://arxiv.org/pdf/2407.14128v1.pdf","comment":"Main paper: 15 pages, 8 figures, 3 tables. Supplementary material: 6\n  pages, 6 figures, 6 tables. Submitted to \"New Frontiers in Optical Coherence\n  Tomography\" Special Issue at ARVO Translational Vision Science & Technology"},{"id":"http://arxiv.org/abs/2407.14126v1","updated":"2024-07-19T08:51:51Z","published":"2024-07-19T08:51:51Z","title":"Mono-ViFI: A Unified Learning Framework for Self-supervised Single- and\n  Multi-frame Monocular Depth Estimation","summary":"  Self-supervised monocular depth estimation has gathered notable interest\nsince it can liberate training from dependency on depth annotations. In\nmonocular video training case, recent methods only conduct view synthesis\nbetween existing camera views, leading to insufficient guidance. To tackle\nthis, we try to synthesize more virtual camera views by flow-based video frame\ninterpolation (VFI), termed as temporal augmentation. For multi-frame\ninference, to sidestep the problem of dynamic objects encountered by explicit\ngeometry-based methods like ManyDepth, we return to the feature fusion paradigm\nand design a VFI-assisted multi-frame fusion module to align and aggregate\nmulti-frame features, using motion and occlusion information obtained by the\nflow-based VFI model. Finally, we construct a unified self-supervised learning\nframework, named Mono-ViFI, to bilaterally connect single- and multi-frame\ndepth. In this framework, spatial data augmentation through image affine\ntransformation is incorporated for data diversity, along with a triplet depth\nconsistency loss for regularization. The single- and multi-frame models can\nshare weights, making our framework compact and memory-efficient. Extensive\nexperiments demonstrate that our method can bring significant improvements to\ncurrent advanced architectures. Source code is available at\nhttps://github.com/LiuJF1226/Mono-ViFI.\n","authors":["Jinfeng Liu","Lingtong Kong","Bo Li","Zerong Wang","Hong Gu","Jinwei Chen"],"pdf_url":"https://arxiv.org/pdf/2407.14126v1.pdf","comment":"27 pages, accepted by ECCV 2024"},{"id":"http://arxiv.org/abs/2407.14121v1","updated":"2024-07-19T08:38:48Z","published":"2024-07-19T08:38:48Z","title":"Seismic Fault SAM: Adapting SAM with Lightweight Modules and 2.5D\n  Strategy for Fault Detection","summary":"  Seismic fault detection holds significant geographical and practical\napplication value, aiding experts in subsurface structure interpretation and\nresource exploration. Despite some progress made by automated methods based on\ndeep learning, research in the seismic domain faces significant challenges,\nparticularly because it is difficult to obtain high-quality, large-scale,\nopen-source, and diverse datasets, which hinders the development of general\nfoundation models. Therefore, this paper proposes Seismic Fault SAM, which, for\nthe first time, applies the general pre-training foundation model-Segment\nAnything Model (SAM)-to seismic fault interpretation. This method aligns the\nuniversal knowledge learned from a vast amount of images with the seismic\ndomain tasks through an Adapter design. Specifically, our innovative points\ninclude designing lightweight Adapter modules, freezing most of the\npre-training weights, and only updating a small number of parameters to allow\nthe model to converge quickly and effectively learn fault features; combining\n2.5D input strategy to capture 3D spatial patterns with 2D models; integrating\ngeological constraints into the model through prior-based data augmentation\ntechniques to enhance the model's generalization capability. Experimental\nresults on the largest publicly available seismic dataset, Thebe, show that our\nmethod surpasses existing 3D models on both OIS and ODS metrics, achieving\nstate-of-the-art performance and providing an effective extension scheme for\nother seismic domain downstream tasks that lack labeled data.\n","authors":["Ran Chen","Zeren Zhang","Jinwen Ma"],"pdf_url":"https://arxiv.org/pdf/2407.14121v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14119v1","updated":"2024-07-19T08:36:25Z","published":"2024-07-19T08:36:25Z","title":"Shape and Style GAN-based Multispectral Data Augmentation for Crop/Weed\n  Segmentation in Precision Farming","summary":"  The use of deep learning methods for precision farming is gaining increasing\ninterest. However, collecting training data in this application field is\nparticularly challenging and costly due to the need of acquiring information\nduring the different growing stages of the cultivation of interest. In this\npaper, we present a method for data augmentation that uses two GANs to create\nartificial images to augment the training data. To obtain a higher image\nquality, instead of re-creating the entire scene, we take original images and\nreplace only the patches containing objects of interest with artificial ones\ncontaining new objects with different shapes and styles. In doing this, we take\ninto account both the foreground (i.e., crop samples) and the background (i.e.,\nthe soil) of the patches. Quantitative experiments, conducted on publicly\navailable datasets, demonstrate the effectiveness of the proposed approach. The\nsource code and data discussed in this work are available as open source.\n","authors":["Mulham Fawakherji","Vincenzo Suriani","Daniele Nardi","Domenico Daniele Bloisi"],"pdf_url":"https://arxiv.org/pdf/2407.14119v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14117v1","updated":"2024-07-19T08:34:23Z","published":"2024-07-19T08:34:23Z","title":"Rethinking Visual Content Refinement in Low-Shot CLIP Adaptation","summary":"  Recent adaptations can boost the low-shot capability of Contrastive\nVision-Language Pre-training (CLIP) by effectively facilitating knowledge\ntransfer. However, these adaptation methods are usually operated on the global\nview of an input image, and thus biased perception of partial local details of\nthe image. To solve this problem, we propose a Visual Content Refinement (VCR)\nbefore the adaptation calculation during the test stage. Specifically, we first\ndecompose the test image into different scales to shift the feature extractor's\nattention to the details of the image. Then, we select the image view with the\nmax prediction margin in each scale to filter out the noisy image views, where\nthe prediction margins are calculated from the pre-trained CLIP model. Finally,\nwe merge the content of the aforementioned selected image views based on their\nscales to construct a new robust representation. Thus, the merged content can\nbe directly used to help the adapter focus on both global and local parts\nwithout any extra training parameters. We apply our method to 3 popular\nlow-shot benchmark tasks with 13 datasets and achieve a significant improvement\nover state-of-the-art methods. For example, compared to the baseline\n(Tip-Adapter) on the few-shot classification task, our method achieves about\n2\\% average improvement for both training-free and training-need settings.\n","authors":["Jinda Lu","Shuo Wang","Yanbin Hao","Haifeng Liu","Xiang Wang","Meng Wang"],"pdf_url":"https://arxiv.org/pdf/2407.14117v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14110v1","updated":"2024-07-19T08:28:35Z","published":"2024-07-19T08:28:35Z","title":"MC-PanDA: Mask Confidence for Panoptic Domain Adaptation","summary":"  Domain adaptive panoptic segmentation promises to resolve the long tail of\ncorner cases in natural scene understanding. Previous state of the art\naddresses this problem with cross-task consistency, careful system-level\noptimization and heuristic improvement of teacher predictions. In contrast, we\npropose to build upon remarkable capability of mask transformers to estimate\ntheir own prediction uncertainty. Our method avoids noise amplification by\nleveraging fine-grained confidence of panoptic teacher predictions. In\nparticular, we modulate the loss with mask-wide confidence and discourage\nback-propagation in pixels with uncertain teacher or confident student.\nExperimental evaluation on standard benchmarks reveals a substantial\ncontribution of the proposed selection techniques. We report 47.4 PQ on Synthia\nto Cityscapes, which corresponds to an improvement of 6.2 percentage points\nover the state of the art. The source code is available at\nhttps://github.com/helen1c/MC-PanDA.\n","authors":["Ivan Martinović","Josip Šarić","Siniša Šegvić"],"pdf_url":"https://arxiv.org/pdf/2407.14110v1.pdf","comment":"Accepted on ECCV 2024"},{"id":"http://arxiv.org/abs/2407.14108v1","updated":"2024-07-19T08:24:36Z","published":"2024-07-19T08:24:36Z","title":"GaussianBeV: 3D Gaussian Representation meets Perception Models for BeV\n  Segmentation","summary":"  The Bird's-eye View (BeV) representation is widely used for 3D perception\nfrom multi-view camera images. It allows to merge features from different\ncameras into a common space, providing a unified representation of the 3D\nscene. The key component is the view transformer, which transforms image views\ninto the BeV. However, actual view transformer methods based on geometry or\ncross-attention do not provide a sufficiently detailed representation of the\nscene, as they use a sub-sampling of the 3D space that is non-optimal for\nmodeling the fine structures of the environment. In this paper, we propose\nGaussianBeV, a novel method for transforming image features to BeV by finely\nrepresenting the scene using a set of 3D gaussians located and oriented in 3D\nspace. This representation is then splattered to produce the BeV feature map by\nadapting recent advances in 3D representation rendering based on gaussian\nsplatting. GaussianBeV is the first approach to use this 3D gaussian modeling\nand 3D scene rendering process online, i.e. without optimizing it on a specific\nscene and directly integrated into a single stage model for BeV scene\nunderstanding. Experiments show that the proposed representation is highly\neffective and place GaussianBeV as the new state-of-the-art on the BeV semantic\nsegmentation task on the nuScenes dataset.\n","authors":["Florian Chabot","Nicolas Granger","Guillaume Lapouge"],"pdf_url":"https://arxiv.org/pdf/2407.14108v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.05906v3","updated":"2024-07-19T08:19:36Z","published":"2024-01-11T13:31:09Z","title":"PartSTAD: 2D-to-3D Part Segmentation Task Adaptation","summary":"  We introduce PartSTAD, a method designed for the task adaptation of 2D-to-3D\nsegmentation lifting. Recent studies have highlighted the advantages of\nutilizing 2D segmentation models to achieve high-quality 3D segmentation\nthrough few-shot adaptation. However, previous approaches have focused on\nadapting 2D segmentation models for domain shift to rendered images and\nsynthetic text descriptions, rather than optimizing the model specifically for\n3D segmentation. Our proposed task adaptation method finetunes a 2D bounding\nbox prediction model with an objective function for 3D segmentation. We\nintroduce weights for 2D bounding boxes for adaptive merging and learn the\nweights using a small additional neural network. Additionally, we incorporate\nSAM, a foreground segmentation model on a bounding box, to improve the\nboundaries of 2D segments and consequently those of 3D segmentation. Our\nexperiments on the PartNet-Mobility dataset show significant improvements with\nour task adaptation approach, achieving a 7.0%p increase in mIoU and a 5.2%p\nimprovement in mAP@50 for semantic and instance segmentation compared to the\nSotA few-shot 3D segmentation model.\n","authors":["Hyunjin Kim","Minhyuk Sung"],"pdf_url":"https://arxiv.org/pdf/2401.05906v3.pdf","comment":"Accepted to ECCV 2024"},{"id":"http://arxiv.org/abs/2407.14103v1","updated":"2024-07-19T08:16:46Z","published":"2024-07-19T08:16:46Z","title":"Zero-Shot Underwater Gesture Recognition","summary":"  Hand gesture recognition allows humans to interact with machines\nnon-verbally, which has a huge application in underwater exploration using\nautonomous underwater vehicles. Recently, a new gesture-based language called\nCADDIAN has been devised for divers, and supervised learning methods have been\napplied to recognize the gestures with high accuracy. However, such methods\nfail when they encounter unseen gestures in real time. In this work, we\nadvocate the need for zero-shot underwater gesture recognition (ZSUGR), where\nthe objective is to train a model with visual samples of gestures from a few\n``seen'' classes only and transfer the gained knowledge at test time to\nrecognize semantically-similar unseen gesture classes as well. After discussing\nthe problem and dataset-specific challenges, we propose new seen-unseen splits\nfor gesture classes in CADDY dataset. Then, we present a two-stage framework,\nwhere a novel transformer learns strong visual gesture cues and feeds them to a\nconditional generative adversarial network that learns to mimic feature\ndistribution. We use the trained generator as a feature synthesizer for unseen\nclasses, enabling zero-shot learning. Extensive experiments demonstrate that\nour method outperforms the existing zero-shot techniques. We conclude by\nproviding useful insights into our framework and suggesting directions for\nfuture research.\n","authors":["Sandipan Sarma","Gundameedi Sai Ram Mohan","Hariansh Sehgal","Arijit Sur"],"pdf_url":"https://arxiv.org/pdf/2407.14103v1.pdf","comment":"Accepted to ICPR 2024. 15 pages, 6 figures. Project page:\n  https://github.com/sandipan211/ZSUGR"},{"id":"http://arxiv.org/abs/2404.17890v2","updated":"2024-07-19T08:12:24Z","published":"2024-04-27T12:55:13Z","title":"DPER: Diffusion Prior Driven Neural Representation for Limited Angle and\n  Sparse View CT Reconstruction","summary":"  Limited-angle and sparse-view computed tomography (LACT and SVCT) are crucial\nfor expanding the scope of X-ray CT applications. However, they face challenges\ndue to incomplete data acquisition, resulting in diverse artifacts in the\nreconstructed CT images. Emerging implicit neural representation (INR)\ntechniques, such as NeRF, NeAT, and NeRP, have shown promise in\nunder-determined CT imaging reconstruction tasks. However, the unsupervised\nnature of INR architecture imposes limited constraints on the solution space,\nparticularly for the highly ill-posed reconstruction task posed by LACT and\nultra-SVCT. In this study, we introduce the Diffusion Prior Driven Neural\nRepresentation (DPER), an advanced unsupervised framework designed to address\nthe exceptionally ill-posed CT reconstruction inverse problems. DPER adopts the\nHalf Quadratic Splitting (HQS) algorithm to decompose the inverse problem into\ndata fidelity and distribution prior sub-problems. The two sub-problems are\nrespectively addressed by INR reconstruction scheme and pre-trained score-based\ndiffusion model. This combination first injects the implicit image local\nconsistency prior from INR. Additionally, it effectively augments the\nfeasibility of the solution space for the inverse problem through the\ngenerative diffusion model, resulting in increased stability and precision in\nthe solutions. We conduct comprehensive experiments to evaluate the performance\nof DPER on LACT and ultra-SVCT reconstruction with two public datasets (AAPM\nand LIDC), an in-house clinical COVID-19 dataset and a public raw projection\ndataset created by Mayo Clinic. The results show that our method outperforms\nthe state-of-the-art reconstruction methods on in-domain datasets, while\nachieving significant performance improvements on out-of-domain (OOD) datasets.\n","authors":["Chenhe Du","Xiyue Lin","Qing Wu","Xuanyu Tian","Ying Su","Zhe Luo","Rui Zheng","Yang Chen","Hongjiang Wei","S. Kevin Zhou","Jingyi Yu","Yuyao Zhang"],"pdf_url":"https://arxiv.org/pdf/2404.17890v2.pdf","comment":"16 pages, 11 figures"},{"id":"http://arxiv.org/abs/2407.14087v1","updated":"2024-07-19T07:51:51Z","published":"2024-07-19T07:51:51Z","title":"Score Normalization for Demographic Fairness in Face Recognition","summary":"  Fair biometric algorithms have similar verification performance across\ndifferent demographic groups given a single decision threshold. Unfortunately,\nfor state-of-the-art face recognition networks, score distributions differ\nbetween demographics. Contrary to work that tries to align those distributions\nby extra training or fine-tuning, we solely focus on score post-processing\nmethods. As proved, well-known sample-centered score normalization techniques,\nZ-norm and T-norm, do not improve fairness for high-security operating points.\nThus, we extend the standard Z/T-norm to integrate demographic information in\nnormalization. Additionally, we investigate several possibilities to\nincorporate cohort similarities for both genuine and impostor pairs per\ndemographic to improve fairness across different operating points. We run\nexperiments on two datasets with different demographics (gender and ethnicity)\nand show that our techniques generally improve the overall fairness of five\nstate-of-the-art pre-trained face recognition networks, without downgrading\nverification performance. We also indicate that an equal contribution of False\nMatch Rate (FMR) and False Non-Match Rate (FNMR) in fairness evaluation is\nrequired for the highest gains. Code and protocols are available.\n","authors":["Yu Linghu","Tiago des Freitas Pereira","Christophe Ecabert","Sébastien Marcel","Manuel Günther"],"pdf_url":"https://arxiv.org/pdf/2407.14087v1.pdf","comment":"Accepted for presentation at IJCB 2024"},{"id":"http://arxiv.org/abs/2407.14086v1","updated":"2024-07-19T07:48:45Z","published":"2024-07-19T07:48:45Z","title":"Temporal Correlation Meets Embedding: Towards a 2nd Generation of\n  JDE-based Real-Time Multi-Object Tracking","summary":"  Joint Detection and Embedding(JDE) trackers have demonstrated excellent\nperformance in Multi-Object Tracking(MOT) tasks by incorporating the extraction\nof appearance features as auxiliary tasks through embedding Re-Identification\ntask(ReID) into the detector, achieving a balance between inference speed and\ntracking performance. However, solving the competition between the detector and\nthe feature extractor has always been a challenge. Also, the issue of directly\nembedding the ReID task into MOT has remained unresolved. The lack of high\ndiscriminability in appearance features results in their limited utility. In\nthis paper, we propose a new learning approach using cross-correlation to\ncapture temporal information of objects. The feature extraction network is no\nlonger trained solely on appearance features from each frame but learns richer\nmotion features by utilizing feature heatmaps from consecutive frames,\naddressing the challenge of inter-class feature similarity. Furthermore, we\napply our learning approach to a more lightweight feature extraction network,\nand treat the feature matching scores as strong cues rather than auxiliary\ncues, employing a appropriate weight calculation to reflect the compatibility\nbetween our obtained features and the MOT task. Our tracker, named TCBTrack,\nachieves state-of-the-art performance on multiple public benchmarks, i.e.,\nMOT17, MOT20, and DanceTrack datasets. Specifically, on the DanceTrack test\nset, we achieve 56.8 HOTA, 58.1 IDF1 and 92.5 MOTA, making it the best online\ntracker that can achieve real-time performance. Comparative evaluations with\nother trackers prove that our tracker achieves the best balance between speed,\nrobustness and accuracy.\n","authors":["Yunfei Zhang","Chao Liang","Jin Gao","Zhipeng Zhang","Weiming Hu","Stephen Maybank","Xue Zhou","Liang Li"],"pdf_url":"https://arxiv.org/pdf/2407.14086v1.pdf","comment":"A submission to IJCV"},{"id":"http://arxiv.org/abs/2407.07171v2","updated":"2024-07-19T07:47:39Z","published":"2024-07-09T18:26:53Z","title":"ItTakesTwo: Leveraging Peer Representations for Semi-supervised LiDAR\n  Semantic Segmentation","summary":"  The costly and time-consuming annotation process to produce large training\nsets for modelling semantic LiDAR segmentation methods has motivated the\ndevelopment of semi-supervised learning (SSL) methods. However, such SSL\napproaches often concentrate on employing consistency learning only for\nindividual LiDAR representations. This narrow focus results in limited\nperturbations that generally fail to enable effective consistency learning.\nAdditionally, these SSL approaches employ contrastive learning based on the\nsampling from a limited set of positive and negative embedding samples. This\npaper introduces a novel semi-supervised LiDAR semantic segmentation framework\ncalled ItTakesTwo (IT2). IT2 is designed to ensure consistent predictions from\npeer LiDAR representations, thereby improving the perturbation effectiveness in\nconsistency learning. Furthermore, our contrastive learning employs informative\nsamples drawn from a distribution of positive and negative embeddings learned\nfrom the entire training set. Results on public benchmarks show that our\napproach achieves remarkable improvements over the previous state-of-the-art\n(SOTA) methods in the field. The code is available at:\nhttps://github.com/yyliu01/IT2.\n","authors":["Yuyuan Liu","Yuanhong Chen","Hu Wang","Vasileios Belagiannis","Ian Reid","Gustavo Carneiro"],"pdf_url":"https://arxiv.org/pdf/2407.07171v2.pdf","comment":"27 pages (15 pages main paper and 12 pages supplementary with\n  references), ECCV 2024 accepted"},{"id":"http://arxiv.org/abs/2404.04833v2","updated":"2024-07-19T07:39:37Z","published":"2024-04-07T06:56:51Z","title":"ShoeModel: Learning to Wear on the User-specified Shoes via Diffusion\n  Model","summary":"  With the development of the large-scale diffusion model, Artificial\nIntelligence Generated Content (AIGC) techniques are popular recently. However,\nhow to truly make it serve our daily lives remains an open question. To this\nend, in this paper, we focus on employing AIGC techniques in one filed of\nE-commerce marketing, i.e., generating hyper-realistic advertising images for\ndisplaying user-specified shoes by human. Specifically, we propose a\nshoe-wearing system, called Shoe-Model, to generate plausible images of human\nlegs interacting with the given shoes. It consists of three modules: (1) shoe\nwearable-area detection module (WD), (2) leg-pose synthesis module (LpS) and\nthe final (3) shoe-wearing image generation module (SW). Them three are\nperformed in ordered stages. Compared to baselines, our ShoeModel is shown to\ngeneralize better to different type of shoes and has ability of keeping the\nID-consistency of the given shoes, as well as automatically producing\nreasonable interactions with human. Extensive experiments show the\neffectiveness of our proposed shoe-wearing system. Figure 1 shows the input and\noutput examples of our ShoeModel.\n","authors":["Binghui Chen","Wenyu Li","Yifeng Geng","Xuansong Xie","Wangmeng Zuo"],"pdf_url":"https://arxiv.org/pdf/2404.04833v2.pdf","comment":"ECCV2024; 16 pages"},{"id":"http://arxiv.org/abs/2407.13567v2","updated":"2024-07-19T07:29:19Z","published":"2024-07-18T14:40:33Z","title":"Hyp2Nav: Hyperbolic Planning and Curiosity for Crowd Navigation","summary":"  Autonomous robots are increasingly becoming a strong fixture in social\nenvironments. Effective crowd navigation requires not only safe yet fast\nplanning, but should also enable interpretability and computational efficiency\nfor working in real-time on embedded devices. In this work, we advocate for\nhyperbolic learning to enable crowd navigation and we introduce Hyp2Nav.\nDifferent from conventional reinforcement learning-based crowd navigation\nmethods, Hyp2Nav leverages the intrinsic properties of hyperbolic geometry to\nbetter encode the hierarchical nature of decision-making processes in\nnavigation tasks. We propose a hyperbolic policy model and a hyperbolic\ncuriosity module that results in effective social navigation, best success\nrates, and returns across multiple simulation settings, using up to 6 times\nfewer parameters than competitor state-of-the-art models. With our approach, it\nbecomes even possible to obtain policies that work in 2-dimensional embedding\nspaces, opening up new possibilities for low-resource crowd navigation and\nmodel interpretability. Insightfully, the internal hyperbolic representation of\nHyp2Nav correlates with how much attention the robot pays to the surrounding\ncrowds, e.g. due to multiple people occluding its pathway or to a few of them\nshowing colliding plans, rather than to its own planned route.\n","authors":["Guido Maria D'Amely di Melendugno","Alessandro Flaborea","Pascal Mettes","Fabio Galasso"],"pdf_url":"https://arxiv.org/pdf/2407.13567v2.pdf","comment":"Accepted at IROS 2024"},{"id":"http://arxiv.org/abs/2404.03575v2","updated":"2024-07-19T07:28:03Z","published":"2024-04-04T16:38:57Z","title":"DreamScene: 3D Gaussian-based Text-to-3D Scene Generation via Formation\n  Pattern Sampling","summary":"  Text-to-3D scene generation holds immense potential for the gaming, film, and\narchitecture sectors. Despite significant progress, existing methods struggle\nwith maintaining high quality, consistency, and editing flexibility. In this\npaper, we propose DreamScene, a 3D Gaussian-based novel text-to-3D scene\ngeneration framework, to tackle the aforementioned three challenges mainly via\ntwo strategies. First, DreamScene employs Formation Pattern Sampling (FPS), a\nmulti-timestep sampling strategy guided by the formation patterns of 3D\nobjects, to form fast, semantically rich, and high-quality representations. FPS\nuses 3D Gaussian filtering for optimization stability, and leverages\nreconstruction techniques to generate plausible textures. Second, DreamScene\nemploys a progressive three-stage camera sampling strategy, specifically\ndesigned for both indoor and outdoor settings, to effectively ensure\nobject-environment integration and scene-wide 3D consistency. Last, DreamScene\nenhances scene editing flexibility by integrating objects and environments,\nenabling targeted adjustments. Extensive experiments validate DreamScene's\nsuperiority over current state-of-the-art techniques, heralding its\nwide-ranging potential for diverse applications. Code and demos will be\nreleased at https://dreamscene-project.github.io .\n","authors":["Haoran Li","Haolin Shi","Wenli Zhang","Wenjun Wu","Yong Liao","Lin Wang","Lik-hang Lee","Pengyuan Zhou"],"pdf_url":"https://arxiv.org/pdf/2404.03575v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.11448v2","updated":"2024-07-19T07:25:29Z","published":"2024-07-16T07:28:39Z","title":"cDP-MIL: Robust Multiple Instance Learning via Cascaded Dirichlet\n  Process","summary":"  Multiple instance learning (MIL) has been extensively applied to whole slide\nhistopathology image (WSI) analysis. The existing aggregation strategy in MIL,\nwhich primarily relies on the first-order distance (e.g., mean difference)\nbetween instances, fails to accurately approximate the true feature\ndistribution of each instance, leading to biased slide-level representations.\nMoreover, the scarcity of WSI observations easily leads to model overfitting,\nresulting in unstable testing performance and limited generalizability. To\ntackle these challenges, we propose a new Bayesian nonparametric framework for\nmultiple instance learning, which adopts a cascade of Dirichlet processes (cDP)\nto incorporate the instance-to-bag characteristic of the WSIs. We perform\nfeature aggregation based on the latent clusters formed by the Dirichlet\nprocess, which incorporates the covariances of the patch features and forms\nmore representative clusters. We then perform bag-level prediction with another\nDirichlet process model on the bags, which imposes a natural regularization on\nlearning to prevent overfitting and enhance generalizability. Moreover, as a\nBayesian nonparametric method, the cDP model can accurately generate posterior\nuncertainty, which allows for the detection of outlier samples and tumor\nlocalization. Extensive experiments on five WSI benchmarks validate the\nsuperior performance of our method, as well as its generalizability and ability\nto estimate uncertainties. Codes are available at\nhttps://github.com/HKU-MedAI/cDPMIL.\n","authors":["Yihang Chen","Tsai Hor Chan","Guosheng Yin","Yuming Jiang","Lequan Yu"],"pdf_url":"https://arxiv.org/pdf/2407.11448v2.pdf","comment":"Accepted by ECCV 2024"},{"id":"http://arxiv.org/abs/2311.12050v3","updated":"2024-07-19T07:18:00Z","published":"2023-11-18T09:55:56Z","title":"3D-GOI: 3D GAN Omni-Inversion for Multifaceted and Multi-object Editing","summary":"  The current GAN inversion methods typically can only edit the appearance and\nshape of a single object and background while overlooking spatial information.\nIn this work, we propose a 3D editing framework, 3D-GOI, to enable multifaceted\nediting of affine information (scale, translation, and rotation) on multiple\nobjects. 3D-GOI realizes the complex editing function by inverting the\nabundance of attribute codes (object\nshape/appearance/scale/rotation/translation, background shape/appearance, and\ncamera pose) controlled by GIRAFFE, a renowned 3D GAN. Accurately inverting all\nthe codes is challenging, 3D-GOI solves this challenge following three main\nsteps. First, we segment the objects and the background in a multi-object\nimage. Second, we use a custom Neural Inversion Encoder to obtain coarse codes\nof each object. Finally, we use a round-robin optimization algorithm to get\nprecise codes to reconstruct the image. To the best of our knowledge, 3D-GOI is\nthe first framework to enable multifaceted editing on multiple objects. Both\nqualitative and quantitative experiments demonstrate that 3D-GOI holds immense\npotential for flexible, multifaceted editing in complex multi-object scenes.Our\nproject and code are released at https://3d-goi.github.io .\n","authors":["Haoran Li","Long Ma","Yong Liao","Lechao Cheng","Yanbin Hao","Pengyuan Zhou"],"pdf_url":"https://arxiv.org/pdf/2311.12050v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19756v2","updated":"2024-07-19T07:15:07Z","published":"2024-06-28T08:54:44Z","title":"Structure-aware World Model for Probe Guidance via Large-scale\n  Self-supervised Pre-train","summary":"  The complex structure of the heart leads to significant challenges in\nechocardiography, especially in acquisition cardiac ultrasound images.\nSuccessful echocardiography requires a thorough understanding of the structures\non the two-dimensional plane and the spatial relationships between planes in\nthree-dimensional space. In this paper, we innovatively propose a large-scale\nself-supervised pre-training method to acquire a cardiac structure-aware world\nmodel. The core innovation lies in constructing a self-supervised task that\nrequires structural inference by predicting masked structures on a 2D plane and\nimagining another plane based on pose transformation in 3D space. To support\nlarge-scale pre-training, we collected over 1.36 million echocardiograms from\nten standard views, along with their 3D spatial poses. In the downstream probe\nguidance task, we demonstrate that our pre-trained model consistently reduces\nguidance errors across the ten most common standard views on the test set with\n0.29 million samples from 74 routine clinical scans, indicating that\nstructure-aware pre-training benefits the scanning.\n","authors":["Haojun Jiang","Meng Li","Zhenguo Sun","Ning Jia","Yu Sun","Shaqi Luo","Shiji Song","Gao Huang"],"pdf_url":"https://arxiv.org/pdf/2406.19756v2.pdf","comment":"Accepted by MICCAI 2024 ASMUS Workshop"},{"id":"http://arxiv.org/abs/2407.14078v1","updated":"2024-07-19T07:14:23Z","published":"2024-07-19T07:14:23Z","title":"Stable-Hair: Real-World Hair Transfer via Diffusion Model","summary":"  Current hair transfer methods struggle to handle diverse and intricate\nhairstyles, thus limiting their applicability in real-world scenarios. In this\npaper, we propose a novel diffusion-based hair transfer framework, named\n\\textit{Stable-Hair}, which robustly transfers a wide range of real-world\nhairstyles onto user-provided faces for virtual hair try-on. To achieve this\ngoal, our Stable-Hair framework is designed as a two-stage pipeline. In the\nfirst stage, we train a Bald Converter alongside stable diffusion to remove\nhair from the user-provided face images, resulting in bald images. In the\nsecond stage, we specifically designed three modules: a Hair Extractor, a\nLatent IdentityNet, and Hair Cross-Attention Layers to transfer the target\nhairstyle with highly detailed and high-fidelity to the bald image.\nSpecifically, the Hair Extractor is trained to encode reference images with the\ndesired hairstyles. To preserve the consistency of identity content and\nbackground between the source images and the transfer results, we employ a\nLatent IdentityNet to encode the source images. With the assistance of our Hair\nCross-Attention Layers in the U-Net, we can accurately and precisely transfer\nthe highly detailed and high-fidelity hairstyle to the bald image. Extensive\nexperiments have demonstrated that our approach delivers state-of-the-art\n(SOTA) results among existing hair transfer methods. Project page:\n\\textcolor{red}{\\url{https://xiaojiu-z.github.io/Stable-Hair.github.io/}}\n","authors":["Yuxuan Zhang","Qing Zhang","Yiren Song","Jiaming Liu"],"pdf_url":"https://arxiv.org/pdf/2407.14078v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13517v2","updated":"2024-07-19T06:58:32Z","published":"2024-07-18T13:48:52Z","title":"Mask2Map: Vectorized HD Map Construction Using Bird's Eye View\n  Segmentation Masks","summary":"  In this paper, we introduce Mask2Map, a novel end-to-end online HD map\nconstruction method designed for autonomous driving applications. Our approach\nfocuses on predicting the class and ordered point set of map instances within a\nscene, represented in the bird's eye view (BEV). Mask2Map consists of two\nprimary components: the Instance-Level Mask Prediction Network (IMPNet) and the\nMask-Driven Map Prediction Network (MMPNet). IMPNet generates Mask-Aware\nQueries and BEV Segmentation Masks to capture comprehensive semantic\ninformation globally. Subsequently, MMPNet enhances these query features using\nlocal contextual information through two submodules: the Positional Query\nGenerator (PQG) and the Geometric Feature Extractor (GFE). PQG extracts\ninstance-level positional queries by embedding BEV positional information into\nMask-Aware Queries, while GFE utilizes BEV Segmentation Masks to generate\npoint-level geometric features. However, we observed limited performance in\nMask2Map due to inter-network inconsistency stemming from different predictions\nto Ground Truth (GT) matching between IMPNet and MMPNet. To tackle this\nchallenge, we propose the Inter-network Denoising Training method, which guides\nthe model to denoise the output affected by both noisy GT queries and perturbed\nGT Segmentation Masks. Our evaluation conducted on nuScenes and Argoverse2\nbenchmarks demonstrates that Mask2Map achieves remarkable performance\nimprovements over previous state-of-the-art methods, with gains of 10.1% mAP\nand 4.1 mAP, respectively. Our code can be found at\nhttps://github.com/SehwanChoi0307/Mask2Map.\n","authors":["Sehwan Choi","Jungho Kim","Hongjae Shin","Jun Won Choi"],"pdf_url":"https://arxiv.org/pdf/2407.13517v2.pdf","comment":"20 pages, 9 figures"},{"id":"http://arxiv.org/abs/2407.14069v1","updated":"2024-07-19T06:53:54Z","published":"2024-07-19T06:53:54Z","title":"Self-Supervised Video Representation Learning in a Heuristic Decoupled\n  Perspective","summary":"  Video contrastive learning (v-CL) has gained prominence as a leading\nframework for unsupervised video representation learning, showcasing impressive\nperformance across various tasks such as action classification and detection.\nIn the field of video representation learning, a feature extractor should\nideally capture both static and dynamic semantics. However, our series of\nexperiments reveals that existing v-CL methods predominantly capture static\nsemantics, with limited capturing of dynamic semantics. Through causal\nanalysis, we identify the root cause: the v-CL objective lacks explicit\nmodeling of dynamic features and the measurement of dynamic similarity is\nconfounded by static semantics, while the measurement of static similarity is\nconfounded by dynamic semantics. In response, we propose \"Bi-level Optimization\nof Learning Dynamic with Decoupling and Intervention\" (BOLD-DI) to capture both\nstatic and dynamic semantics in a decoupled manner. Our method can be\nseamlessly integrated into the existing v-CL methods and experimental results\nhighlight the significant improvements.\n","authors":["Zeen Song","Jingyao Wang","Jianqi Zhang","Changwen Zheng","Wenwen Qiang"],"pdf_url":"https://arxiv.org/pdf/2407.14069v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14066v1","updated":"2024-07-19T06:50:24Z","published":"2024-07-19T06:50:24Z","title":"360VFI: A Dataset and Benchmark for Omnidirectional Video Frame\n  Interpolation","summary":"  With the development of VR-related techniques, viewers can enjoy a realistic\nand immersive experience through a head-mounted display, while omnidirectional\nvideo with a low frame rate can lead to user dizziness. However, the prevailing\nplane frame interpolation methodologies are unsuitable for Omnidirectional\nVideo Interpolation, chiefly due to the lack of models tailored to such videos\nwith strong distortion, compounded by the scarcity of valuable datasets for\nOmnidirectional Video Frame Interpolation. In this paper, we introduce the\nbenchmark dataset, 360VFI, for Omnidirectional Video Frame Interpolation. We\npresent a practical implementation that introduces a distortion prior from\nomnidirectional video into the network to modulate distortions. We especially\npropose a pyramid distortion-sensitive feature extractor that uses the unique\ncharacteristics of equirectangular projection (ERP) format as prior\ninformation. Moreover, we devise a decoder that uses an affine transformation\nto facilitate the synthesis of intermediate frames further. 360VFI is the first\ndataset and benchmark that explores the challenge of Omnidirectional Video\nFrame Interpolation. Through our benchmark analysis, we presented four\ndifferent distortion conditions scenes in the proposed 360VFI dataset to\nevaluate the challenge triggered by distortion during interpolation. Besides,\nexperimental results demonstrate that Omnidirectional Video Interpolation can\nbe effectively improved by modeling for omnidirectional distortion.\n","authors":["Wenxuan Lu","Mengshun Hu","Yansheng Qiu","Liang Liao","Zheng Wang"],"pdf_url":"https://arxiv.org/pdf/2407.14066v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2302.03453,\n  arXiv:1804.02815 by other authors"},{"id":"http://arxiv.org/abs/2407.14064v1","updated":"2024-07-19T06:41:31Z","published":"2024-07-19T06:41:31Z","title":"Refining Tuberculosis Detection in CXR Imaging: Addressing Bias in Deep\n  Neural Networks via Interpretability","summary":"  Automatic classification of active tuberculosis from chest X-ray images has\nthe potential to save lives, especially in low- and mid-income countries where\nskilled human experts can be scarce. Given the lack of available labeled data\nto train such systems and the unbalanced nature of publicly available datasets,\nwe argue that the reliability of deep learning models is limited, even if they\ncan be shown to obtain perfect classification accuracy on the test data. One\nway of evaluating the reliability of such systems is to ensure that models use\nthe same regions of input images for predictions as medical experts would. In\nthis paper, we show that pre-training a deep neural network on a large-scale\nproxy task, as well as using mixed objective optimization network (MOON), a\ntechnique to balance different classes during pre-training and fine-tuning, can\nimprove the alignment of decision foundations between models and experts, as\ncompared to a model directly trained on the target dataset. At the same time,\nthese approaches keep perfect classification accuracy according to the area\nunder the receiver operating characteristic curve (AUROC) on the test set, and\nimprove generalization on an independent, unseen dataset. For the purpose of\nreproducibility, our source code is made available online.\n","authors":["Özgür Acar Güler","Manuel Günther","André Anjos"],"pdf_url":"https://arxiv.org/pdf/2407.14064v1.pdf","comment":"Preprint of paper to be presented at EUVIP 2024"},{"id":"http://arxiv.org/abs/2407.14062v1","updated":"2024-07-19T06:41:16Z","published":"2024-07-19T06:41:16Z","title":"Decomposed Vector-Quantized Variational Autoencoder for Human Grasp\n  Generation","summary":"  Generating realistic human grasps is a crucial yet challenging task for\napplications involving object manipulation in computer graphics and robotics.\nExisting methods often struggle with generating fine-grained realistic human\ngrasps that ensure all fingers effectively interact with objects, as they focus\non encoding hand with the whole representation and then estimating both hand\nposture and position in a single step. In this paper, we propose a novel\nDecomposed Vector-Quantized Variational Autoencoder (DVQ-VAE) to address this\nlimitation by decomposing hand into several distinct parts and encoding them\nseparately. This part-aware decomposed architecture facilitates more precise\nmanagement of the interaction between each component of hand and object,\nenhancing the overall reality of generated human grasps. Furthermore, we design\na newly dual-stage decoding strategy, by first determining the type of grasping\nunder skeletal physical constraints, and then identifying the location of the\ngrasp, which can greatly improve the verisimilitude as well as adaptability of\nthe model to unseen hand-object interaction. In experiments, our model achieved\nabout 14.1% relative improvement in the quality index compared to the\nstate-of-the-art methods in four widely-adopted benchmarks. Our source code is\navailable at https://github.com/florasion/D-VQVAE.\n","authors":["Zhe Zhao","Mengshi Qi","Huadong Ma"],"pdf_url":"https://arxiv.org/pdf/2407.14062v1.pdf","comment":"To be published in The 18th European Conference on Computer Vision\n  ECCV 2024"},{"id":"http://arxiv.org/abs/2407.14059v1","updated":"2024-07-19T06:37:55Z","published":"2024-07-19T06:37:55Z","title":"Regularizing Dynamic Radiance Fields with Kinematic Fields","summary":"  This paper presents a novel approach for reconstructing dynamic radiance\nfields from monocular videos. We integrate kinematics with dynamic radiance\nfields, bridging the gap between the sparse nature of monocular videos and the\nreal-world physics. Our method introduces the kinematic field, capturing motion\nthrough kinematic quantities: velocity, acceleration, and jerk. The kinematic\nfield is jointly learned with the dynamic radiance field by minimizing the\nphotometric loss without motion ground truth. We further augment our method\nwith physics-driven regularizers grounded in kinematics. We propose\nphysics-driven regularizers that ensure the physical validity of predicted\nkinematic quantities, including advective acceleration and jerk. Additionally,\nwe control the motion trajectory based on rigidity equations formed with the\npredicted kinematic quantities. In experiments, our method outperforms the\nstate-of-the-arts by capturing physical motion patterns within challenging\nreal-world monocular videos.\n","authors":["Woobin Im","Geonho Cha","Sebin Lee","Jumin Lee","Juhyeong Seon","Dongyoon Wee","Sung-Eui Yoon"],"pdf_url":"https://arxiv.org/pdf/2407.14059v1.pdf","comment":"ECCV 2024"},{"id":"http://arxiv.org/abs/2407.14055v1","updated":"2024-07-19T06:31:22Z","published":"2024-07-19T06:31:22Z","title":"Quantum Hamiltonian Embedding of Images for Data Reuploading Classifiers","summary":"  When applying quantum computing to machine learning tasks, one of the first\nconsiderations is the design of the quantum machine learning model itself.\nConventionally, the design of quantum machine learning algorithms relies on the\n``quantisation\" of classical learning algorithms, such as using quantum linear\nalgebra to implement important subroutines of classical algorithms, if not the\nentire algorithm, seeking to achieve quantum advantage through possible\nrun-time accelerations brought by quantum computing. However, recent research\nhas started questioning whether quantum advantage via speedup is the right goal\nfor quantum machine learning [1]. Research also has been undertaken to exploit\nproperties that are unique to quantum systems, such as quantum contextuality,\nto better design quantum machine learning models [2]. In this paper, we take an\nalternative approach by incorporating the heuristics and empirical evidences\nfrom the design of classical deep learning algorithms to the design of quantum\nneural networks. We first construct a model based on the data reuploading\ncircuit [3] with the quantum Hamiltonian data embedding unitary [4]. Through\nnumerical experiments on images datasets, including the famous MNIST and\nFashionMNIST datasets, we demonstrate that our model outperforms the quantum\nconvolutional neural network (QCNN)[5] by a large margin (up to over 40% on\nMNIST test set). Based on the model design process and numerical results, we\nthen laid out six principles for designing quantum machine learning models,\nespecially quantum neural networks.\n","authors":["Peiyong Wang","Casey R. Myers","Lloyd C. L. Hollenberg","Udaya Parampalli"],"pdf_url":"https://arxiv.org/pdf/2407.14055v1.pdf","comment":"11 figures, 31 pages. Code available on\n  https://github.com/peiyong-addwater/HamEmbedding"},{"id":"http://arxiv.org/abs/2407.14054v1","updated":"2024-07-19T06:29:57Z","published":"2024-07-19T06:29:57Z","title":"PointRegGPT: Boosting 3D Point Cloud Registration using Generative\n  Point-Cloud Pairs for Training","summary":"  Data plays a crucial role in training learning-based methods for 3D point\ncloud registration. However, the real-world dataset is expensive to build,\nwhile rendering-based synthetic data suffers from domain gaps. In this work, we\npresent PointRegGPT, boosting 3D point cloud registration using generative\npoint-cloud pairs for training. Given a single depth map, we first apply a\nrandom camera motion to re-project it into a target depth map. Converting them\nto point clouds gives a training pair. To enhance the data realism, we\nformulate a generative model as a depth inpainting diffusion to process the\ntarget depth map with the re-projected source depth map as the condition. Also,\nwe design a depth correction module to alleviate artifacts caused by point\npenetration during the re-projection. To our knowledge, this is the first\ngenerative approach that explores realistic data generation for indoor point\ncloud registration. When equipped with our approach, several recent algorithms\ncan improve their performance significantly and achieve SOTA consistently on\ntwo common benchmarks. The code and dataset will be released on\nhttps://github.com/Chen-Suyi/PointRegGPT.\n","authors":["Suyi Chen","Hao Xu","Haipeng Li","Kunming Luo","Guanghui Liu","Chi-Wing Fu","Ping Tan","Shuaicheng Liu"],"pdf_url":"https://arxiv.org/pdf/2407.14054v1.pdf","comment":"To appear at the European Conference on Computer Vision (ECCV) 2024"},{"id":"http://arxiv.org/abs/2407.14053v1","updated":"2024-07-19T06:29:37Z","published":"2024-07-19T06:29:37Z","title":"DirectL: Efficient Radiance Fields Rendering for 3D Light Field Displays","summary":"  Autostereoscopic display, despite decades of development, has not achieved\nextensive application, primarily due to the daunting challenge of 3D content\ncreation for non-specialists. The emergence of Radiance Field as an innovative\n3D representation has markedly revolutionized the domains of 3D reconstruction\nand generation. This technology greatly simplifies 3D content creation for\ncommon users, broadening the applicability of Light Field Displays (LFDs).\nHowever, the combination of these two fields remains largely unexplored. The\nstandard paradigm to create optimal content for parallax-based light field\ndisplays demands rendering at least 45 slightly shifted views preferably at\nhigh resolution per frame, a substantial hurdle for real-time rendering. We\nintroduce DirectL, a novel rendering paradigm for Radiance Fields on 3D\ndisplays. We thoroughly analyze the interweaved mapping of spatial rays to\nscreen subpixels, precisely determine the light rays entering the human eye,\nand propose subpixel repurposing to significantly reduce the pixel count\nrequired for rendering. Tailored for the two predominant radiance\nfields--Neural Radiance Fields (NeRFs) and 3D Gaussian Splatting (3DGS), we\npropose corresponding optimized rendering pipelines that directly render the\nlight field images instead of multi-view images. Extensive experiments across\nvarious displays and user study demonstrate that DirectL accelerates rendering\nby up to 40 times compared to the standard paradigm without sacrificing visual\nquality. Its rendering process-only modification allows seamless integration\ninto subsequent radiance field tasks. Finally, we integrate DirectL into\ndiverse applications, showcasing the stunning visual experiences and the\nsynergy between LFDs and Radiance Fields, which unveils tremendous potential\nfor commercialization applications. \\href{direct-l.github.io}{\\textbf{Project\nHomepage}\n","authors":["Zongyuan Yang","Baolin Liu","Yingde Song","Yongping Xiong","Lan Yi","Zhaohe Zhang","Xunbo Yu"],"pdf_url":"https://arxiv.org/pdf/2407.14053v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.05499v5","updated":"2024-07-19T06:00:41Z","published":"2023-03-09T18:52:16Z","title":"Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set\n  Object Detection","summary":"  In this paper, we present an open-set object detector, called Grounding DINO,\nby marrying Transformer-based detector DINO with grounded pre-training, which\ncan detect arbitrary objects with human inputs such as category names or\nreferring expressions. The key solution of open-set object detection is\nintroducing language to a closed-set detector for open-set concept\ngeneralization. To effectively fuse language and vision modalities, we\nconceptually divide a closed-set detector into three phases and propose a tight\nfusion solution, which includes a feature enhancer, a language-guided query\nselection, and a cross-modality decoder for cross-modality fusion. While\nprevious works mainly evaluate open-set object detection on novel categories,\nwe propose to also perform evaluations on referring expression comprehension\nfor objects specified with attributes. Grounding DINO performs remarkably well\non all three settings, including benchmarks on COCO, LVIS, ODinW, and\nRefCOCO/+/g. Grounding DINO achieves a $52.5$ AP on the COCO detection\nzero-shot transfer benchmark, i.e., without any training data from COCO. It\nsets a new record on the ODinW zero-shot benchmark with a mean $26.1$ AP. Code\nwill be available at \\url{https://github.com/IDEA-Research/GroundingDINO}.\n","authors":["Shilong Liu","Zhaoyang Zeng","Tianhe Ren","Feng Li","Hao Zhang","Jie Yang","Qing Jiang","Chunyuan Li","Jianwei Yang","Hang Su","Jun Zhu","Lei Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.05499v5.pdf","comment":"Code will be available at\n  https://github.com/IDEA-Research/GroundingDINO"},{"id":"http://arxiv.org/abs/2404.03613v3","updated":"2024-07-19T05:59:21Z","published":"2024-04-04T17:34:41Z","title":"Per-Gaussian Embedding-Based Deformation for Deformable 3D Gaussian\n  Splatting","summary":"  As 3D Gaussian Splatting (3DGS) provides fast and high-quality novel view\nsynthesis, it is a natural extension to deform a canonical 3DGS to multiple\nframes for representing a dynamic scene. However, previous works fail to\naccurately reconstruct complex dynamic scenes. We attribute the failure to the\ndesign of the deformation field, which is built as a coordinate-based function.\nThis approach is problematic because 3DGS is a mixture of multiple fields\ncentered at the Gaussians, not just a single coordinate-based framework. To\nresolve this problem, we define the deformation as a function of per-Gaussian\nembeddings and temporal embeddings. Moreover, we decompose deformations as\ncoarse and fine deformations to model slow and fast movements, respectively.\nAlso, we introduce a local smoothness regularization for per-Gaussian embedding\nto improve the details in dynamic regions. Project page:\nhttps://jeongminb.github.io/e-d3dgs/\n","authors":["Jeongmin Bae","Seoha Kim","Youngsik Yun","Hahyun Lee","Gun Bang","Youngjung Uh"],"pdf_url":"https://arxiv.org/pdf/2404.03613v3.pdf","comment":"ECCV 2024. Project page: https://jeongminb.github.io/e-d3dgs/"},{"id":"http://arxiv.org/abs/2407.14047v1","updated":"2024-07-19T05:58:01Z","published":"2024-07-19T05:58:01Z","title":"OCTrack: Benchmarking the Open-Corpus Multi-Object Tracking","summary":"  We study a novel yet practical problem of open-corpus multi-object tracking\n(OCMOT), which extends the MOT into localizing, associating, and recognizing\ngeneric-category objects of both seen (base) and unseen (novel) classes, but\nwithout the category text list as prompt. To study this problem, the top\npriority is to build a benchmark. In this work, we build OCTrackB, a\nlarge-scale and comprehensive benchmark, to provide a standard evaluation\nplatform for the OCMOT problem. Compared to previous datasets, OCTrackB has\nmore abundant and balanced base/novel classes and the corresponding samples for\nevaluation with less bias. We also propose a new multi-granularity recognition\nmetric to better evaluate the generative object recognition in OCMOT. By\nconducting the extensive benchmark evaluation, we report and analyze the\nresults of various state-of-the-art methods, which demonstrate the rationale of\nOCMOT, as well as the usefulness and advantages of OCTrackB.\n","authors":["Zekun Qian","Ruize Han","Wei Feng","Junhui Hou","Linqi Song","Song Wang"],"pdf_url":"https://arxiv.org/pdf/2407.14047v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14043v1","updated":"2024-07-19T05:44:35Z","published":"2024-07-19T05:44:35Z","title":"Kinematics-based 3D Human-Object Interaction Reconstruction from Single\n  View","summary":"  Reconstructing 3D human-object interaction (HOI) from single-view RGB images\nis challenging due to the absence of depth information and potential\nocclusions. Existing methods simply predict the body poses merely rely on\nnetwork training on some indoor datasets, which cannot guarantee the\nrationality of the results if some body parts are invisible due to occlusions\nthat appear easily. Inspired by the end-effector localization task in robotics,\nwe propose a kinematics-based method that can drive the joints of human body to\nthe human-object contact regions accurately. After an improved forward\nkinematics algorithm is proposed, the Multi-Layer Perceptron is introduced into\nthe solution of inverse kinematics process to determine the poses of joints,\nwhich achieves precise results than the commonly-used numerical methods in\nrobotics. Besides, a Contact Region Recognition Network (CRRNet) is also\nproposed to robustly determine the contact regions using a single-view video.\nExperimental results demonstrate that our method outperforms the\nstate-of-the-art on benchmark BEHAVE. Additionally, our approach shows good\nportability and can be seamlessly integrated into other methods for\noptimizations.\n","authors":["Yuhang Chen","Chenxing Wang"],"pdf_url":"https://arxiv.org/pdf/2407.14043v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14041v1","updated":"2024-07-19T05:36:22Z","published":"2024-07-19T05:36:22Z","title":"Not All Noises Are Created Equally:Diffusion Noise Selection and\n  Optimization","summary":"  Diffusion models that can generate high-quality data from randomly sampled\nGaussian noises have become the mainstream generative method in both academia\nand industry. Are randomly sampled Gaussian noises equally good for diffusion\nmodels? While a large body of works tried to understand and improve diffusion\nmodels, previous works overlooked the possibility to select or optimize the\nsampled noise the possibility of selecting or optimizing sampled noises for\nimproving diffusion models. In this paper, we mainly made three contributions.\nFirst, we report that not all noises are created equally for diffusion models.\nWe are the first to hypothesize and empirically observe that the generation\nquality of diffusion models significantly depend on the noise inversion\nstability. This naturally provides us a noise selection method according to the\ninversion stability. Second, we further propose a novel noise optimization\nmethod that actively enhances the inversion stability of arbitrary given\nnoises. Our method is the first one that works on noise space to generally\nimprove generated results without fine-tuning diffusion models. Third, our\nextensive experiments demonstrate that the proposed noise selection and noise\noptimization methods both significantly improve representative diffusion\nmodels, such as SDXL and SDXL-turbo, in terms of human preference and other\nobjective evaluation metrics. For example, the human preference winning rates\nof noise selection and noise optimization over the baselines can be up to 57%\nand 72.5%, respectively, on DrawBench.\n","authors":["Zipeng Qi","Lichen Bai","Haoyi Xiong","and Zeke Xie"],"pdf_url":"https://arxiv.org/pdf/2407.14041v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.11084v2","updated":"2024-07-19T05:27:50Z","published":"2024-07-13T17:02:44Z","title":"A Survey of Distance-Based Vessel Trajectory Clustering: Data\n  Pre-processing, Methodologies, Applications, and Experimental Evaluation","summary":"  Vessel trajectory clustering, a crucial component of the maritime intelligent\ntransportation systems, provides valuable insights for applications such as\nanomaly detection and trajectory prediction. This paper presents a\ncomprehensive survey of the most prevalent distance-based vessel trajectory\nclustering methods, which encompass two main steps: trajectory similarity\nmeasurement and clustering. Initially, we conducted a thorough literature\nreview using relevant keywords to gather and summarize pertinent research\npapers and datasets. Then, this paper discussed the principal methods of data\npre-processing that prepare data for further analysis. The survey progresses to\ndetail the leading algorithms for measuring vessel trajectory similarity and\nthe main clustering techniques used in the field today. Furthermore, the\nvarious applications of trajectory clustering within the maritime context are\nexplored. Finally, the paper evaluates the effectiveness of different algorithm\ncombinations and pre-processing methods through experimental analysis, focusing\non their impact on the performance of distance-based trajectory clustering\nalgorithms. The experimental results demonstrate the effectiveness of various\ntrajectory clustering algorithms and notably highlight the significant\nimprovements that trajectory compression techniques contribute to the\nefficiency and accuracy of trajectory clustering. This comprehensive approach\nensures a deep understanding of current capabilities and future directions in\nvessel trajectory clustering.\n","authors":["Maohan Liang","Ryan Wen Liu","Ruobin Gao","Zhe Xiao","Xiaocai Zhang","Hua Wang"],"pdf_url":"https://arxiv.org/pdf/2407.11084v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05990v2","updated":"2024-07-19T05:23:28Z","published":"2023-10-08T04:54:12Z","title":"Cross-Task Data Augmentation by Pseudo-label Generation for Region Based\n  Coronary Artery Instance Segmentation","summary":"  Coronary Artery Diseases (CADs) although preventable, are one of the leading\ncauses of death and disability. Diagnosis of these diseases is often difficult\nand resource intensive. Angiographic imaging segmentation of the arteries has\nevolved as a tool of assistance that helps clinicians make an accurate\ndiagnosis. However, due to the limited amount of data and the difficulty in\ncurating a dataset, the task of segmentation has proven challenging. In this\nstudy, we introduce the use of pseudo-labels to address the issue of limited\ndata in the angiographic dataset to enhance the performance of the baseline\nYOLO model. Unlike existing data augmentation techniques that improve the model\nconstrained to a fixed dataset, we introduce the use of pseudo-labels generated\non a dataset of separate related task to diversify and improve model\nperformance. This method increases the baseline F1 score by 9% in the\nvalidation data set and by 3% in the test data set.\n","authors":["Sandesh Pokhrel","Sanjay Bhandari","Eduard Vazquez","Yash Raj Shrestha","Binod Bhattarai"],"pdf_url":"https://arxiv.org/pdf/2310.05990v2.pdf","comment":"arXiv admin note: text overlap with arXiv:2310.04749"},{"id":"http://arxiv.org/abs/2310.08116v3","updated":"2024-07-19T05:16:52Z","published":"2023-10-12T08:17:57Z","title":"Multimodal Active Measurement for Human Mesh Recovery in Close Proximity","summary":"  For physical human-robot interactions (pHRI), a robot needs to estimate the\naccurate body pose of a target person. However, in these pHRI scenarios, the\nrobot cannot fully observe the target person's body with equipped cameras\nbecause the target person must be close to the robot for physical interaction.\nThis close distance leads to severe truncation and occlusions and thus results\nin poor accuracy of human pose estimation. For better accuracy in this\nchallenging environment, we propose an active measurement and sensor fusion\nframework of the equipped cameras with touch and ranging sensors such as 2D\nLiDAR. Touch and ranging sensor measurements are sparse but reliable and\ninformative cues for localizing human body parts. In our active measurement\nprocess, camera viewpoints and sensor placements are dynamically optimized to\nmeasure body parts with higher estimation uncertainty, which is closely related\nto truncation or occlusion. In our sensor fusion process, assuming that the\nmeasurements of touch and ranging sensors are more reliable than the\ncamera-based estimations, we fuse the sensor measurements to the camera-based\nestimated pose by aligning the estimated pose towards the measured points. Our\nproposed method outperformed previous methods on the standard occlusion\nbenchmark with simulated active measurement. Furthermore, our method reliably\nestimated human poses using a real robot, even with practical constraints such\nas occlusion by blankets.\n","authors":["Takahiro Maeda","Keisuke Takeshita","Norimichi Ukita","Kazuhito Tanaka"],"pdf_url":"https://arxiv.org/pdf/2310.08116v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14032v1","updated":"2024-07-19T05:07:41Z","published":"2024-07-19T05:07:41Z","title":"Semantic-CC: Boosting Remote Sensing Image Change Captioning via\n  Foundational Knowledge and Semantic Guidance","summary":"  Remote sensing image change captioning (RSICC) aims to articulate the changes\nin objects of interest within bi-temporal remote sensing images using natural\nlanguage. Given the limitations of current RSICC methods in expressing general\nfeatures across multi-temporal and spatial scenarios, and their deficiency in\nproviding granular, robust, and precise change descriptions, we introduce a\nnovel change captioning (CC) method based on the foundational knowledge and\nsemantic guidance, which we term Semantic-CC. Semantic-CC alleviates the\ndependency of high-generalization algorithms on extensive annotations by\nharnessing the latent knowledge of foundation models, and it generates more\ncomprehensive and accurate change descriptions guided by pixel-level semantics\nfrom change detection (CD). Specifically, we propose a bi-temporal SAM-based\nencoder for dual-image feature extraction; a multi-task semantic aggregation\nneck for facilitating information interaction between heterogeneous tasks; a\nstraightforward multi-scale change detection decoder to provide pixel-level\nsemantic guidance; and a change caption decoder based on the large language\nmodel (LLM) to generate change description sentences. Moreover, to ensure the\nstability of the joint training of CD and CC, we propose a three-stage training\nstrategy that supervises different tasks at various stages. We validate the\nproposed method on the LEVIR-CC and LEVIR-CD datasets. The experimental results\ncorroborate the complementarity of CD and CC, demonstrating that Semantic-CC\ncan generate more accurate change descriptions and achieve optimal performance\nacross both tasks.\n","authors":["Yongshuo Zhu","Lu Li","Keyan Chen","Chenyang Liu","Fugen Zhou","Zhenwei Shi"],"pdf_url":"https://arxiv.org/pdf/2407.14032v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14029v1","updated":"2024-07-19T05:03:16Z","published":"2024-07-19T05:03:16Z","title":"PASS++: A Dual Bias Reduction Framework for Non-Exemplar\n  Class-Incremental Learning","summary":"  Class-incremental learning (CIL) aims to recognize new classes incrementally\nwhile maintaining the discriminability of old classes. Most existing CIL\nmethods are exemplar-based, i.e., storing a part of old data for retraining.\nWithout relearning old data, those methods suffer from catastrophic forgetting.\nIn this paper, we figure out two inherent problems in CIL, i.e., representation\nbias and classifier bias, that cause catastrophic forgetting of old knowledge.\nTo address these two biases, we present a simple and novel dual bias reduction\nframework that employs self-supervised transformation (SST) in input space and\nprototype augmentation (protoAug) in deep feature space. On the one hand, SST\nalleviates the representation bias by learning generic and diverse\nrepresentations that can transfer across different tasks. On the other hand,\nprotoAug overcomes the classifier bias by explicitly or implicitly augmenting\nprototypes of old classes in the deep feature space, which poses tighter\nconstraints to maintain previously learned decision boundaries. We further\npropose hardness-aware prototype augmentation and multi-view ensemble\nstrategies, leading to significant improvements. The proposed framework can be\neasily integrated with pre-trained models. Without storing any samples of old\nclasses, our method can perform comparably with state-of-the-art exemplar-based\napproaches which store plenty of old data. We hope to draw the attention of\nresearchers back to non-exemplar CIL by rethinking the necessity of storing old\nsamples in CIL.\n","authors":["Fei Zhu","Xu-Yao Zhang","Zhen Cheng","Cheng-Lin Liu"],"pdf_url":"https://arxiv.org/pdf/2407.14029v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07471v4","updated":"2024-07-19T05:01:03Z","published":"2024-06-11T17:18:11Z","title":"OphNet: A Large-Scale Video Benchmark for Ophthalmic Surgical Workflow\n  Understanding","summary":"  Surgical scene perception via videos is critical for advancing robotic\nsurgery, telesurgery, and AI-assisted surgery, particularly in ophthalmology.\nHowever, the scarcity of diverse and richly annotated video datasets has\nhindered the development of intelligent systems for surgical workflow analysis.\nExisting datasets face challenges such as small scale, lack of diversity in\nsurgery and phase categories, and absence of time-localized annotations. These\nlimitations impede action understanding and model generalization validation in\ncomplex and diverse real-world surgical scenarios. To address this gap, we\nintroduce OphNet, a large-scale, expert-annotated video benchmark for\nophthalmic surgical workflow understanding. OphNet features: 1) A diverse\ncollection of 2,278 surgical videos spanning 66 types of cataract, glaucoma,\nand corneal surgeries, with detailed annotations for 102 unique surgical phases\nand 150 fine-grained operations. 2) Sequential and hierarchical annotations for\neach surgery, phase, and operation, enabling comprehensive understanding and\nimproved interpretability. 3) Time-localized annotations, facilitating temporal\nlocalization and prediction tasks within surgical workflows. With approximately\n285 hours of surgical videos, OphNet is about 20 times larger than the largest\nexisting surgical workflow analysis benchmark. Code and dataset are available\nat: https://minghu0830.github.io/OphNet-benchmark/.\n","authors":["Ming Hu","Peng Xia","Lin Wang","Siyuan Yan","Feilong Tang","Zhongxing Xu","Yimin Luo","Kaimin Song","Jurgen Leitner","Xuelian Cheng","Jun Cheng","Chi Liu","Kaijing Zhou","Zongyuan Ge"],"pdf_url":"https://arxiv.org/pdf/2406.07471v4.pdf","comment":"Accepted by ECCV 2024"},{"id":"http://arxiv.org/abs/2407.12405v2","updated":"2024-07-19T04:59:04Z","published":"2024-07-17T08:32:14Z","title":"Fisheye-Calib-Adapter: An Easy Tool for Fisheye Camera Model Conversion","summary":"  The increasing necessity for fisheye cameras in fields such as robotics and\nautonomous driving has led to the proposal of various fisheye camera models.\nWhile the evolution of camera models has facilitated the development of diverse\nsystems in the field, the lack of adaptation between different fisheye camera\nmodels means that recalibration is always necessary, which is cumbersome. This\npaper introduces a conversion tool for various previously proposed fisheye\ncamera models. It is user-friendly, simple, yet extremely fast and accurate,\noffering conversion capabilities for a broader range of models compared to\nexisting tools. We have verified that models converted using our system perform\ncorrectly in applications such as SLAM. By utilizing our system, researchers\ncan obtain output parameters directly from input parameters without the need\nfor an image set and any recalibration processes, thus serving as a bridge\nacross different fisheye camera models in various research fields. We provide\nour system as an open source tool available at:\nhttps://github.com/eowjd0512/fisheye-calib-adapter\n","authors":["Sangjun Lee"],"pdf_url":"https://arxiv.org/pdf/2407.12405v2.pdf","comment":"8 pages, 4 figures"},{"id":"http://arxiv.org/abs/2312.17431v3","updated":"2024-07-19T04:55:57Z","published":"2023-12-29T01:52:22Z","title":"MVPatch: More Vivid Patch for Adversarial Camouflaged Attacks on Object\n  Detectors in the Physical World","summary":"  Recent studies have shown that Adversarial Patches (APs) can effectively\nmanipulate object detection models. However, the conspicuous patterns often\nassociated with these patches tend to attract human attention, posing a\nsignificant challenge. Existing research has primarily focused on enhancing\nattack efficacy in the physical domain while often neglecting the optimization\nof stealthiness and transferability. Furthermore, applying APs in real-world\nscenarios faces major challenges related to transferability, stealthiness, and\npracticality. To address these challenges, we introduce generalization theory\ninto the context of APs, enabling our iterative process to simultaneously\nenhance transferability and refine visual correlation with realistic images. We\npropose a Dual-Perception-Based Framework (DPBF) to generate the More Vivid\nPatch (MVPatch), which enhances transferability, stealthiness, and\npracticality. The DPBF integrates two key components: the\nModel-Perception-Based Module (MPBM) and the Human-Perception-Based Module\n(HPBM), along with regularization terms. The MPBM employs ensemble strategy to\nreduce object confidence scores across multiple detectors, thereby improving AP\ntransferability with robust theoretical support. Concurrently, the HPBM\nintroduces a lightweight method for achieving visual similarity, creating\nnatural and inconspicuous adversarial patches without relying on additional\ngenerative models. The regularization terms further enhance the practicality of\nthe generated APs in the physical domain. Additionally, we introduce\nnaturalness and transferability scores to provide an unbiased assessment of\nAPs. Extensive experimental validation demonstrates that MVPatch achieves\nsuperior transferability and a natural appearance in both digital and physical\ndomains, underscoring its effectiveness and stealthiness.\n","authors":["Zheng Zhou","Hongbo Zhao","Ju Liu","Qiaosheng Zhang","Liwei Geng","Shuchang Lyu","Wenquan Feng"],"pdf_url":"https://arxiv.org/pdf/2312.17431v3.pdf","comment":"16 pages, 8 figures. This work has been submitted to the IEEE for\n  possible publication. Copyright may be transferred without notice, after\n  which this version may no longer be accessible"},{"id":"http://arxiv.org/abs/2303.10428v4","updated":"2024-07-19T04:52:07Z","published":"2023-03-18T14:46:44Z","title":"RCA: Region Conditioned Adaptation for Visual Abductive Reasoning","summary":"  Visual abductive reasoning aims to make likely explanations for visual\nobservations. We propose a simple yet effective Region Conditioned Adaptation,\na hybrid parameter-efficient fine-tuning method that equips the frozen CLIP\nwith the ability to infer explanations from local visual cues. We encode\n``local hints'' and ``global contexts'' into visual prompts of the CLIP model\nseparately at fine and coarse-grained levels. Adapters are used for fine-tuning\nCLIP models for downstream tasks and we design a new attention adapter, that\ndirectly steers the focus of the attention map with trainable query and key\nprojections of a frozen CLIP model. Finally, we train our new model with a\nmodified contrastive loss to regress the visual feature simultaneously toward\nfeatures of literal description and plausible explanations. The loss enables\nCLIP to maintain both perception and reasoning abilities. Experiments on the\nSherlock visual abductive reasoning benchmark show that the RCA significantly\noutstands previous SOTAs, ranking the \\nth{1} on the leaderboards (e.g., Human\nAcc: RCA 31.74 \\textit{vs} CPT-CLIP 29.58, higher =better). We also validate\nthe RCA is generalizable to local perception benchmarks like RefCOCO. We\nopen-source our project at\n\\textit{\\color{magenta}{\\url{https://github.com/LUNAProject22/RPA}}}.\n","authors":["Hao Zhang","Yeo Keat Ee","Basura Fernando"],"pdf_url":"https://arxiv.org/pdf/2303.10428v4.pdf","comment":"13 pages, 11 figures, ACM Multimedia 2024"},{"id":"http://arxiv.org/abs/2407.14026v1","updated":"2024-07-19T04:51:34Z","published":"2024-07-19T04:51:34Z","title":"Semi-supervised reference-based sketch extraction using a contrastive\n  learning framework","summary":"  Sketches reflect the drawing style of individual artists; therefore, it is\nimportant to consider their unique styles when extracting sketches from color\nimages for various applications. Unfortunately, most existing sketch extraction\nmethods are designed to extract sketches of a single style. Although there have\nbeen some attempts to generate various style sketches, the methods generally\nsuffer from two limitations: low quality results and difficulty in training the\nmodel due to the requirement of a paired dataset. In this paper, we propose a\nnovel multi-modal sketch extraction method that can imitate the style of a\ngiven reference sketch with unpaired data training in a semi-supervised manner.\nOur method outperforms state-of-the-art sketch extraction methods and unpaired\nimage translation methods in both quantitative and qualitative evaluations.\n","authors":["Chang Wook Seo","Amirsaman Ashtari","Junyong Noh"],"pdf_url":"https://arxiv.org/pdf/2407.14026v1.pdf","comment":"Main paper 1-12 page, Supplementary 13-34 page"},{"id":"http://arxiv.org/abs/2407.14024v1","updated":"2024-07-19T04:50:54Z","published":"2024-07-19T04:50:54Z","title":"TTA-OOD: Test-time Augmentation for Improving Out-of-Distribution\n  Detection in Gastrointestinal Vision","summary":"  Deep learning has significantly advanced the field of gastrointestinal\nvision, enhancing disease diagnosis capabilities. One major challenge in\nautomating diagnosis within gastrointestinal settings is the detection of\nabnormal cases in endoscopic images. Due to the sparsity of data, this process\nof distinguishing normal from abnormal cases has faced significant challenges,\nparticularly with rare and unseen conditions. To address this issue, we frame\nabnormality detection as an out-of-distribution (OOD) detection problem. In\nthis setup, a model trained on In-Distribution (ID) data, which represents a\nhealthy GI tract, can accurately identify healthy cases, while abnormalities\nare detected as OOD, regardless of their class. We introduce a test-time\naugmentation segment into the OOD detection pipeline, which enhances the\ndistinction between ID and OOD examples, thereby improving the effectiveness of\nexisting OOD methods with the same model. This augmentation shifts the pixel\nspace, which translates into a more distinct semantic representation for OOD\nexamples compared to ID examples. We evaluated our method against existing\nstate-of-the-art OOD scores, showing improvements with test-time augmentation\nover the baseline approach.\n","authors":["Sandesh Pokhrel","Sanjay Bhandari","Eduard Vazquez","Tryphon Lambrou","Prashnna Gyawali","Binod Bhattarai"],"pdf_url":"https://arxiv.org/pdf/2407.14024v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13589v3","updated":"2024-07-19T04:46:24Z","published":"2024-03-20T13:37:29Z","title":"ReGround: Improving Textual and Spatial Grounding at No Cost","summary":"  When an image generation process is guided by both a text prompt and spatial\ncues, such as a set of bounding boxes, do these elements work in harmony, or\ndoes one dominate the other? Our analysis of a pretrained image diffusion model\nthat integrates gated self-attention into the U-Net reveals that spatial\ngrounding often outweighs textual grounding due to the sequential flow from\ngated self-attention to cross-attention. We demonstrate that such bias can be\nsignificantly mitigated without sacrificing accuracy in either grounding by\nsimply rewiring the network architecture, changing from sequential to parallel\nfor gated self-attention and cross-attention. This surprisingly simple yet\neffective solution does not require any fine-tuning of the network but\nsignificantly reduces the trade-off between the two groundings. Our experiments\ndemonstrate significant improvements from the original GLIGEN to the rewired\nversion in the trade-off between textual grounding and spatial grounding.\n","authors":["Phillip Y. Lee","Minhyuk Sung"],"pdf_url":"https://arxiv.org/pdf/2403.13589v3.pdf","comment":"Accepted to ECCV 2024. Project page: https://re-ground.github.io/"},{"id":"http://arxiv.org/abs/2407.13537v2","updated":"2024-07-19T04:40:31Z","published":"2024-07-18T14:09:03Z","title":"GlobalPointer: Large-Scale Plane Adjustment with Bi-Convex Relaxation","summary":"  Plane adjustment (PA) is crucial for many 3D applications, involving\nsimultaneous pose estimation and plane recovery. Despite recent advancements,\nit remains a challenging problem in the realm of multi-view point cloud\nregistration. Current state-of-the-art methods can achieve globally optimal\nconvergence only with good initialization. Furthermore, their high time\ncomplexity renders them impractical for large-scale problems. To address these\nchallenges, we first exploit a novel optimization strategy termed\n\\textit{Bi-Convex Relaxation}, which decouples the original problem into two\nsimpler sub-problems, reformulates each sub-problem using a convex relaxation\ntechnique, and alternately solves each one until the original problem\nconverges. Building on this strategy, we propose two algorithmic variants for\nsolving the plane adjustment problem, namely \\textit{GlobalPointer} and\n\\textit{GlobalPointer++}, based on point-to-plane and plane-to-plane errors,\nrespectively. Extensive experiments on both synthetic and real datasets\ndemonstrate that our method can perform large-scale plane adjustment with\nlinear time complexity, larger convergence region, and robustness to poor\ninitialization, while achieving similar accuracy as prior methods. The code is\navailable at https://github.com/wu-cvgl/GlobalPointer.\n","authors":["Bangyan Liao","Zhenjun Zhao","Lu Chen","Haoang Li","Daniel Cremers","Peidong Liu"],"pdf_url":"https://arxiv.org/pdf/2407.13537v2.pdf","comment":"Accepted to ECCV 2024. The first two authors contributed equally to\n  this work. Code: https://github.com/wu-cvgl/GlobalPointer"},{"id":"http://arxiv.org/abs/2407.08968v2","updated":"2024-07-19T04:23:11Z","published":"2024-07-12T03:32:13Z","title":"SlideGCD: Slide-based Graph Collaborative Training with Knowledge\n  Distillation for Whole Slide Image Classification","summary":"  Existing WSI analysis methods lie on the consensus that histopathological\ncharacteristics of tumors are significant guidance for cancer diagnostics.\nParticularly, as the evolution of cancers is a continuous process, the\ncorrelations and differences across various stages, anatomical locations and\npatients should be taken into account. However, recent research mainly focuses\non the inner-contextual information in a single WSI, ignoring the correlations\nbetween slides. To verify whether introducing the slide inter-correlations can\nbring improvements to WSI representation learning, we propose a generic WSI\nanalysis pipeline SlideGCD that considers the existing multi-instance learning\n(MIL) methods as the backbone and forge the WSI classification task as a node\nclassification problem. More specifically, SlideGCD declares a node buffer that\nstores previous slide embeddings for subsequent extensive slide-based graph\nconstruction and conducts graph learning to explore the inter-correlations\nimplied in the slide-based graph. Moreover, we frame the MIL classifier and\ngraph learning into two parallel workflows and deploy the knowledge\ndistillation to transfer the differentiable information to the graph neural\nnetwork. The consistent performance boosting, brought by SlideGCD, of four\nprevious state-of-the-art MIL methods is observed on two TCGA benchmark\ndatasets. The code is available at https://github.com/HFUT-miaLab/SlideGCD.\n","authors":["Tong Shu","Jun Shi","Dongdong Sun","Zhiguo Jiang","Yushan Zheng"],"pdf_url":"https://arxiv.org/pdf/2407.08968v2.pdf","comment":"Accepted for MICCAI 2024"},{"id":"http://arxiv.org/abs/2407.06113v2","updated":"2024-07-19T04:20:32Z","published":"2024-07-08T16:49:01Z","title":"C2C: Component-to-Composition Learning for Zero-Shot Compositional\n  Action Recognition","summary":"  Compositional actions consist of dynamic (verbs) and static (objects)\nconcepts. Humans can easily recognize unseen compositions using the learned\nconcepts. For machines, solving such a problem requires a model to recognize\nunseen actions composed of previously observed verbs and objects, thus\nrequiring so-called compositional generalization ability. To facilitate this\nresearch, we propose a novel Zero-Shot Compositional Action Recognition\n(ZS-CAR) task. For evaluating the task, we construct a new benchmark,\nSomething-composition (Sth-com), based on the widely used Something-Something\nV2 dataset. We also propose a novel Component-to-Composition (C2C) learning\nmethod to solve the new ZS-CAR task. C2C includes an independent component\nlearning module and a composition inference module. Last, we devise an enhanced\ntraining strategy to address the challenges of component variations between\nseen and unseen compositions and to handle the subtle balance between learning\nseen and unseen actions. The experimental results demonstrate that the proposed\nframework significantly surpasses the existing compositional generalization\nmethods and sets a new state-of-the-art. The new Sth-com benchmark and code are\navailable at https://github.com/RongchangLi/ZSCAR_C2C.\n","authors":["Rongchang Li","Zhenhua Feng","Tianyang Xu","Linze Li","Xiao-Jun Wu","Muhammad Awais","Sara Atito","Josef Kittler"],"pdf_url":"https://arxiv.org/pdf/2407.06113v2.pdf","comment":"Accepted by ECCV2024"},{"id":"http://arxiv.org/abs/2311.01473v2","updated":"2024-07-19T04:06:19Z","published":"2023-11-01T06:55:09Z","title":"Adversarial Examples in the Physical World: A Survey","summary":"  Deep neural networks (DNNs) have demonstrated high vulnerability to\nadversarial examples, raising broad security concerns about their applications.\nBesides the attacks in the digital world, the practical implications of\nadversarial examples in the physical world present significant challenges and\nsafety concerns. However, current research on physical adversarial examples\n(PAEs) lacks a comprehensive understanding of their unique characteristics,\nleading to limited significance and understanding. In this paper, we address\nthis gap by thoroughly examining the characteristics of PAEs within a practical\nworkflow encompassing training, manufacturing, and re-sampling processes. By\nanalyzing the links between physical adversarial attacks, we identify\nmanufacturing and re-sampling as the primary sources of distinct attributes and\nparticularities in PAEs. Leveraging this knowledge, we develop a comprehensive\nanalysis and classification framework for PAEs based on their specific\ncharacteristics, covering over 100 studies on physical-world adversarial\nexamples. Furthermore, we investigate defense strategies against PAEs and\nidentify open challenges and opportunities for future research. We aim to\nprovide a fresh, thorough, and systematic understanding of PAEs, thereby\npromoting the development of robust adversarial learning and its application in\nopen-world scenarios to provide the community with a continuously updated list\nof physical world adversarial sample resources, including papers, code, \\etc,\nwithin the proposed framework\n","authors":["Jiakai Wang","Xianglong Liu","Jin Hu","Donghua Wang","Siyang Wu","Tingsong Jiang","Yuanfang Guo","Aishan Liu","Aishan Liu","Jiantao Zhou"],"pdf_url":"https://arxiv.org/pdf/2311.01473v2.pdf","comment":"Adversarial examples, physical-world scenarios, attacks and defenses"},{"id":"http://arxiv.org/abs/2312.09988v3","updated":"2024-07-19T03:54:44Z","published":"2023-12-15T18:01:47Z","title":"Towards Architecture-Agnostic Untrained Network Priors for Image\n  Reconstruction with Frequency Regularization","summary":"  Untrained networks inspired by deep image priors have shown promising\ncapabilities in recovering high-quality images from noisy or partial\nmeasurements without requiring training sets. Their success is widely\nattributed to implicit regularization due to the spectral bias of suitable\nnetwork architectures. However, the application of such network-based priors\noften entails superfluous architectural decisions, risks of overfitting, and\nlengthy optimization processes, all of which hinder their practicality. To\naddress these challenges, we propose efficient architecture-agnostic techniques\nto directly modulate the spectral bias of network priors: 1)\nbandwidth-constrained input, 2) bandwidth-controllable upsamplers, and 3)\nLipschitz-regularized convolutional layers. We show that, with just a few lines\nof code, we can reduce overfitting in underperforming architectures and close\nperformance gaps with high-performing counterparts, minimizing the need for\nextensive architecture tuning. This makes it possible to employ a more compact\nmodel to achieve performance similar or superior to larger models while\nreducing runtime. Demonstrated on inpainting-like MRI reconstruction task, our\nresults signify for the first time that architectural biases, overfitting, and\nruntime issues of untrained network priors can be simultaneously addressed\nwithout architectural modifications. Our code is publicly available.\n","authors":["Yilin Liu","Yunkui Pang","Jiang Li","Yong Chen","Pew-Thian Yap"],"pdf_url":"https://arxiv.org/pdf/2312.09988v3.pdf","comment":"Accepted at ECCV 2024"},{"id":"http://arxiv.org/abs/2407.14009v1","updated":"2024-07-19T03:45:48Z","published":"2024-07-19T03:45:48Z","title":"Scale Disparity of Instances in Interactive Point Cloud Segmentation","summary":"  Interactive point cloud segmentation has become a pivotal task for\nunderstanding 3D scenes, enabling users to guide segmentation models with\nsimple interactions such as clicks, therefore significantly reducing the effort\nrequired to tailor models to diverse scenarios and new categories. However, in\nthe realm of interactive segmentation, the meaning of instance diverges from\nthat in instance segmentation, because users might desire to segment instances\nof both thing and stuff categories that vary greatly in scale. Existing methods\nhave focused on thing categories, neglecting the segmentation of stuff\ncategories and the difficulties arising from scale disparity. To bridge this\ngap, we propose ClickFormer, an innovative interactive point cloud segmentation\nmodel that accurately segments instances of both thing and stuff categories. We\npropose a query augmentation module to augment click queries by a global query\nsampling strategy, thus maintaining consistent performance across different\ninstance scales. Additionally, we employ global attention in the query-voxel\ntransformer to mitigate the risk of generating false positives, along with\nseveral other network structure improvements to further enhance the model's\nsegmentation performance. Experiments demonstrate that ClickFormer outperforms\nexisting interactive point cloud segmentation methods across both indoor and\noutdoor datasets, providing more accurate segmentation results with fewer user\nclicks in an open-world setting.\n","authors":["Chenrui Han","Xuan Yu","Yuxuan Xie","Yili Liu","Sitong Mao","Shunbo Zhou","Rong Xiong","Yue Wang"],"pdf_url":"https://arxiv.org/pdf/2407.14009v1.pdf","comment":"Accepted by 2024 IEEE/RSJ International Conference on Intelligent\n  Robots and Systems"},{"id":"http://arxiv.org/abs/2407.14007v1","updated":"2024-07-19T03:43:48Z","published":"2024-07-19T03:43:48Z","title":"Multi-modal Relation Distillation for Unified 3D Representation Learning","summary":"  Recent advancements in multi-modal pre-training for 3D point clouds have\ndemonstrated promising results by aligning heterogeneous features across 3D\nshapes and their corresponding 2D images and language descriptions. However,\ncurrent straightforward solutions often overlook intricate structural relations\namong samples, potentially limiting the full capabilities of multi-modal\nlearning. To address this issue, we introduce Multi-modal Relation Distillation\n(MRD), a tri-modal pre-training framework, which is designed to effectively\ndistill reputable large Vision-Language Models (VLM) into 3D backbones. MRD\naims to capture both intra-relations within each modality as well as\ncross-relations between different modalities and produce more discriminative 3D\nshape representations. Notably, MRD achieves significant improvements in\ndownstream zero-shot classification tasks and cross-modality retrieval tasks,\ndelivering new state-of-the-art performance.\n","authors":["Huiqun Wang","Yiping Bao","Panwang Pan","Zeming Li","Xiao Liu","Ruijie Yang","Di Huang"],"pdf_url":"https://arxiv.org/pdf/2407.14007v1.pdf","comment":"Accepted by ECCV2024"},{"id":"http://arxiv.org/abs/2404.16828v2","updated":"2024-07-19T03:31:34Z","published":"2024-04-25T17:59:56Z","title":"Made to Order: Discovering monotonic temporal changes via\n  self-supervised video ordering","summary":"  Our objective is to discover and localize monotonic temporal changes in a\nsequence of images. To achieve this, we exploit a simple proxy task of ordering\na shuffled image sequence, with `time' serving as a supervisory signal, since\nonly changes that are monotonic with time can give rise to the correct\nordering. We also introduce a transformer-based model for ordering of image\nsequences of arbitrary length with built-in attribution maps. After training,\nthe model successfully discovers and localizes monotonic changes while ignoring\ncyclic and stochastic ones. We demonstrate applications of the model in\nmultiple domains covering different scene and object types, discovering both\nobject-level and environmental changes in unseen sequences. We also demonstrate\nthat the attention-based attribution maps function as effective prompts for\nsegmenting the changing regions, and that the learned representations can be\nused for downstream applications. Finally, we show that the model achieves the\nstate-of-the-art on standard benchmarks for image ordering.\n","authors":["Charig Yang","Weidi Xie","Andrew Zisserman"],"pdf_url":"https://arxiv.org/pdf/2404.16828v2.pdf","comment":"ECCV 2024. Project page: https://charigyang.github.io/order/"},{"id":"http://arxiv.org/abs/2407.14001v1","updated":"2024-07-19T03:22:04Z","published":"2024-07-19T03:22:04Z","title":"Component Selection for Craft Assembly Tasks","summary":"  Inspired by traditional handmade crafts, where a person improvises assemblies\nbased on the available objects, we formally introduce the Craft Assembly Task.\nIt is a robotic assembly task that involves building an accurate representation\nof a given target object using the available objects, which do not directly\ncorrespond to its parts. In this work, we focus on selecting the subset of\navailable objects for the final craft, when the given input is an RGB image of\nthe target in the wild. We use a mask segmentation neural network to identify\nvisible parts, followed by retrieving labelled template meshes. These meshes\nundergo pose optimization to determine the most suitable template. Then, we\npropose to simplify the parts of the transformed template mesh to primitive\nshapes like cuboids or cylinders. Finally, we design a search algorithm to find\ncorrespondences in the scene based on local and global proportions. We develop\nbaselines for comparison that consider all possible combinations, and choose\nthe highest scoring combination for common metrics used in foreground maps and\nmask accuracy. Our approach achieves comparable results to the baselines for\ntwo different scenes, and we show qualitative results for an implementation in\na real-world scenario.\n","authors":["Vitor Hideyo Isume","Takuya Kiyokawa","Natsuki Yamanobe","Yukiyasu Domae","Weiwei Wan","Kensuke Harada"],"pdf_url":"https://arxiv.org/pdf/2407.14001v1.pdf","comment":"Submitted to IEEE RA-L"},{"id":"http://arxiv.org/abs/2407.11292v2","updated":"2024-07-19T03:02:47Z","published":"2024-07-16T00:40:57Z","title":"LoRA-PT: Low-Rank Adapting UNETR for Hippocampus Segmentation Using\n  Principal Tensor Singular Values and Vectors","summary":"  The hippocampus is a crucial brain structure associated with various\npsychiatric disorders, and its automatic and precise segmentation is essential\nfor studying these diseases. In recent years, deep learning-based methods have\nmade significant progress in hippocampus segmentation. However, training deep\nneural network models requires substantial computational resources and time, as\nwell as a large amount of labeled training data, which is often difficult to\nobtain in medical image segmentation. To address this issue, we propose a new\nparameter-efficient fine-tuning method called LoRA-PT. This method transfers\nthe pre-trained UNETR model on the BraTS2021 dataset to the hippocampus\nsegmentation task. Specifically, the LoRA-PT method categorizes the parameter\nmatrix of the transformer structure into three sizes, forming three 3D tensors.\nThrough tensor singular value decomposition, these tensors are decomposed to\ngenerate low-rank tensors with the principal singular values and singular\nvectors, while the remaining singular values and vectors form the residual\ntensor. During the fine-tuning, we only update the low-rank tensors, i.e. the\nprincipal tensor singular values and vectors, while keeping the residual tensor\nunchanged. We validated the proposed method on three public hippocampus\ndatasets. Experimental results show that LoRA-PT outperforms existing\nparameter-efficient fine-tuning methods in segmentation accuracy while\nsignificantly reducing the number of parameter updates. Our code is available\nat https://github.com/WangangCheng/LoRA-PT/tree/LoRA-PT.\n","authors":["Guanghua He","Wangang Cheng","Hancan Zhu","Gaohang Yu"],"pdf_url":"https://arxiv.org/pdf/2407.11292v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.17118v5","updated":"2024-07-19T03:01:09Z","published":"2023-12-28T16:54:53Z","title":"Fully Sparse 3D Occupancy Prediction","summary":"  Occupancy prediction plays a pivotal role in autonomous driving. Previous\nmethods typically construct dense 3D volumes, neglecting the inherent sparsity\nof the scene and suffering from high computational costs. To bridge the gap, we\nintroduce a novel fully sparse occupancy network, termed SparseOcc. SparseOcc\ninitially reconstructs a sparse 3D representation from camera-only inputs and\nsubsequently predicts semantic/instance occupancy from the 3D sparse\nrepresentation by sparse queries. A mask-guided sparse sampling is designed to\nenable sparse queries to interact with 2D features in a fully sparse manner,\nthereby circumventing costly dense features or global attention. Additionally,\nwe design a thoughtful ray-based evaluation metric, namely RayIoU, to solve the\ninconsistency penalty along the depth axis raised in traditional voxel-level\nmIoU criteria. SparseOcc demonstrates its effectiveness by achieving a RayIoU\nof 34.0, while maintaining a real-time inference speed of 17.3 FPS, with 7\nhistory frames inputs. By incorporating more preceding frames to 15, SparseOcc\ncontinuously improves its performance to 35.1 RayIoU without bells and\nwhistles.\n","authors":["Haisong Liu","Yang Chen","Haiguang Wang","Zetong Yang","Tianyu Li","Jia Zeng","Li Chen","Hongyang Li","Limin Wang"],"pdf_url":"https://arxiv.org/pdf/2312.17118v5.pdf","comment":"Accepted to ECCV 2024. Code: https://github.com/MCG-NJU/SparseOcc"},{"id":"http://arxiv.org/abs/2407.10419v2","updated":"2024-07-19T03:00:16Z","published":"2024-07-15T03:48:16Z","title":"Omni-Dimensional Frequency Learner for General Time Series Analysis","summary":"  Frequency domain representation of time series feature offers a concise\nrepresentation for handling real-world time series data with inherent\ncomplexity and dynamic nature. However, current frequency-based methods with\ncomplex operations still fall short of state-of-the-art time domain methods for\ngeneral time series analysis. In this work, we present Omni-Dimensional\nFrequency Learner (ODFL) model based on a in depth analysis among all the three\naspects of the spectrum feature: channel redundancy property among the\nfrequency dimension, the sparse and un-salient frequency energy distribution\namong the frequency dimension, and the semantic diversity among the variable\ndimension. Technically, our method is composed of a semantic-adaptive global\nfilter with attention to the un-salient frequency bands and partial operation\namong the channel dimension. Empirical results show that ODFL achieves\nconsistent state-of-the-art in five mainstream time series analysis tasks,\nincluding short- and long-term forecasting, imputation, classification, and\nanomaly detection, offering a promising foundation for time series analysis.\n","authors":["Xianing Chen","Hanting Chen","Hailin Hu"],"pdf_url":"https://arxiv.org/pdf/2407.10419v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.19401v3","updated":"2024-07-19T02:45:35Z","published":"2024-04-30T09:47:44Z","title":"UniFS: Universal Few-shot Instance Perception with Point Representations","summary":"  Instance perception tasks (object detection, instance segmentation, pose\nestimation, counting) play a key role in industrial applications of visual\nmodels. As supervised learning methods suffer from high labeling cost, few-shot\nlearning methods which effectively learn from a limited number of labeled\nexamples are desired. Existing few-shot learning methods primarily focus on a\nrestricted set of tasks, presumably due to the challenges involved in designing\na generic model capable of representing diverse tasks in a unified manner. In\nthis paper, we propose UniFS, a universal few-shot instance perception model\nthat unifies a wide range of instance perception tasks by reformulating them\ninto a dynamic point representation learning framework. Additionally, we\npropose Structure-Aware Point Learning (SAPL) to exploit the higher-order\nstructural relationship among points to further enhance representation\nlearning. Our approach makes minimal assumptions about the tasks, yet it\nachieves competitive results compared to highly specialized and well optimized\nspecialist models. Codes and data are available at\nhttps://github.com/jin-s13/UniFS.\n","authors":["Sheng Jin","Ruijie Yao","Lumin Xu","Wentao Liu","Chen Qian","Ji Wu","Ping Luo"],"pdf_url":"https://arxiv.org/pdf/2404.19401v3.pdf","comment":"Accepted by ECCV 2024"},{"id":"http://arxiv.org/abs/2308.14378v3","updated":"2024-07-19T02:41:49Z","published":"2023-08-28T07:50:04Z","title":"GKGNet: Group K-Nearest Neighbor based Graph Convolutional Network for\n  Multi-Label Image Recognition","summary":"  Multi-Label Image Recognition (MLIR) is a challenging task that aims to\npredict multiple object labels in a single image while modeling the complex\nrelationships between labels and image regions. Although convolutional neural\nnetworks and vision transformers have succeeded in processing images as regular\ngrids of pixels or patches, these representations are sub-optimal for capturing\nirregular and discontinuous regions of interest. In this work, we present the\nfirst fully graph convolutional model, Group K-nearest neighbor based Graph\nconvolutional Network (GKGNet), which models the connections between semantic\nlabel embeddings and image patches in a flexible and unified graph structure.\nTo address the scale variance of different objects and to capture information\nfrom multiple perspectives, we propose the Group KGCN module for dynamic graph\nconstruction and message passing. Our experiments demonstrate that GKGNet\nachieves state-of-the-art performance with significantly lower computational\ncosts on the challenging multi-label datasets, i.e., MS-COCO and VOC2007\ndatasets. Codes are available at https://github.com/jin-s13/GKGNet.\n","authors":["Ruijie Yao","Sheng Jin","Lumin Xu","Wang Zeng","Wentao Liu","Chen Qian","Ping Luo","Ji Wu"],"pdf_url":"https://arxiv.org/pdf/2308.14378v3.pdf","comment":"Accepted by ECCV 2024"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2407.14482v1","updated":"2024-07-19T17:35:47Z","published":"2024-07-19T17:35:47Z","title":"ChatQA 2: Bridging the Gap to Proprietary LLMs in Long Context and RAG\n  Capabilities","summary":"  In this work, we introduce ChatQA 2, a Llama3-based model designed to bridge\nthe gap between open-access LLMs and leading proprietary models (e.g.,\nGPT-4-Turbo) in long-context understanding and retrieval-augmented generation\n(RAG) capabilities. These two capabilities are essential for LLMs to process\nlarge volumes of information that cannot fit into a single prompt and are\ncomplementary to each other, depending on the downstream tasks and\ncomputational budgets. We present a detailed continued training recipe to\nextend the context window of Llama3-70B-base from 8K to 128K tokens, along with\na three-stage instruction tuning process to enhance the model's\ninstruction-following, RAG performance, and long-context understanding\ncapabilities. Our results demonstrate that the Llama3-ChatQA-2-70B model\nachieves accuracy comparable to GPT-4-Turbo-2024-0409 on many long-context\nunderstanding tasks and surpasses it on the RAG benchmark. Interestingly, we\nfind that the state-of-the-art long-context retriever can alleviate the top-k\ncontext fragmentation issue in RAG, further improving RAG-based results for\nlong-context understanding tasks. We also provide extensive comparisons between\nRAG and long-context solutions using state-of-the-art long-context LLMs.\n","authors":["Peng Xu","Wei Ping","Xianchao Wu","Zihan Liu","Mohammad Shoeybi","Bryan Catanzaro"],"pdf_url":"https://arxiv.org/pdf/2407.14482v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14399v1","updated":"2024-07-19T15:21:14Z","published":"2024-07-19T15:21:14Z","title":"PolySinger: Singing-Voice to Singing-Voice Translation from English to\n  Japanese","summary":"  The speech domain prevails in the spotlight for several natural language\nprocessing (NLP) tasks while the singing domain remains less explored. The\nculmination of NLP is the speech-to-speech translation (S2ST) task, referring\nto translation and synthesis of human speech. A disparity between S2ST and the\npossible adaptation to the singing domain, which we describe as singing-voice\nto singing-voice translation (SV2SVT), is becoming prominent as the former is\nprogressing ever faster, while the latter is at a standstill. Singing-voice\nsynthesis systems are overcoming the barrier of multi-lingual synthesis,\ndespite limited attention has been paid to multi-lingual songwriting and song\ntranslation. This paper endeavors to determine what is required for successful\nSV2SVT and proposes PolySinger (\\textbf{Poly}glot \\textbf{Singer}): the first\nsystem for SV2SVT, performing lyrics translation from English to Japanese. A\ncascaded approach is proposed to establish a framework with a high degree of\ncontrol which can potentially diminish the disparity between SV2SVT and S2ST.\nThe performance of PolySinger is evaluated by a mean opinion score test with\nnative Japanese speakers. Results and in-depth discussions with test subjects\nsuggest a solid foundation for SV2SVT, but several shortcomings must be\novercome, which are discussed for the future of SV2SVT.\n","authors":["Silas Antonisen","Iván López-Espejo"],"pdf_url":"https://arxiv.org/pdf/2407.14399v1.pdf","comment":"This paper was accepted at ISMIR 2024"},{"id":"http://arxiv.org/abs/2407.14346v1","updated":"2024-07-19T14:28:53Z","published":"2024-07-19T14:28:53Z","title":"Improving Retrieval in Sponsored Search by Leveraging Query Context\n  Signals","summary":"  Accurately retrieving relevant bid keywords for user queries is critical in\nSponsored Search but remains challenging, particularly for short, ambiguous\nqueries. Existing dense and generative retrieval models often fail to capture\nnuanced user intent in these cases. To address this, we propose an approach to\nenhance query understanding by augmenting queries with rich contextual signals\nderived from web search results and large language models, stored in an online\ncache. Specifically, we use web search titles and snippets to ground queries in\nreal-world information and utilize GPT-4 to generate query rewrites and\nexplanations that clarify user intent. These signals are efficiently integrated\nthrough a Fusion-in-Decoder based Unity architecture, enabling both dense and\ngenerative retrieval with serving costs on par with traditional context-free\nmodels. To address scenarios where context is unavailable in the cache, we\nintroduce context glancing, a curriculum learning strategy that improves model\nrobustness and performance even without contextual signals during inference.\nExtensive offline experiments demonstrate that our context-aware approach\nsubstantially outperforms context-free models. Furthermore, online A/B testing\non a prominent search engine across 160+ countries shows significant\nimprovements in user engagement and revenue.\n","authors":["Akash Kumar Mohankumar","Gururaj K","Gagan Madan","Amit Singh"],"pdf_url":"https://arxiv.org/pdf/2407.14346v1.pdf","comment":"8 pages, 8 tables, 1 figure"},{"id":"http://arxiv.org/abs/2407.14321v1","updated":"2024-07-19T13:57:11Z","published":"2024-07-19T13:57:11Z","title":"Multimodal Misinformation Detection using Large Vision-Language Models","summary":"  The increasing proliferation of misinformation and its alarming impact have\nmotivated both industry and academia to develop approaches for misinformation\ndetection and fact checking. Recent advances on large language models (LLMs)\nhave shown remarkable performance in various tasks, but whether and how LLMs\ncould help with misinformation detection remains relatively underexplored. Most\nof existing state-of-the-art approaches either do not consider evidence and\nsolely focus on claim related features or assume the evidence to be provided.\nFew approaches consider evidence retrieval as part of the misinformation\ndetection but rely on fine-tuning models. In this paper, we investigate the\npotential of LLMs for misinformation detection in a zero-shot setting. We\nincorporate an evidence retrieval component into the process as it is crucial\nto gather pertinent information from various sources to detect the veracity of\nclaims. To this end, we propose a novel re-ranking approach for multimodal\nevidence retrieval using both LLMs and large vision-language models (LVLM). The\nretrieved evidence samples (images and texts) serve as the input for an\nLVLM-based approach for multimodal fact verification (LVLM4FV). To enable a\nfair evaluation, we address the issue of incomplete ground truth for evidence\nsamples in an existing evidence retrieval dataset by annotating a more complete\nset of evidence samples for both image and text retrieval. Our experimental\nresults on two datasets demonstrate the superiority of the proposed approach in\nboth evidence retrieval and fact verification tasks and also better\ngeneralization capability across dataset compared to the supervised baseline.\n","authors":["Sahar Tahmasebi","Eric Müller-Budack","Ralph Ewerth"],"pdf_url":"https://arxiv.org/pdf/2407.14321v1.pdf","comment":"Accepted for publication in: Conference on Information and Knowledge\n  Management (CIKM) 2024"},{"id":"http://arxiv.org/abs/2407.14266v1","updated":"2024-07-19T12:45:21Z","published":"2024-07-19T12:45:21Z","title":"L^2CL: Embarrassingly Simple Layer-to-Layer Contrastive Learning for\n  Graph Collaborative Filtering","summary":"  Graph neural networks (GNNs) have recently emerged as an effective approach\nto model neighborhood signals in collaborative filtering. Towards this research\nline, graph contrastive learning (GCL) demonstrates robust capabilities to\naddress the supervision label shortage issue through generating massive\nself-supervised signals. Despite its effectiveness, GCL for recommendation\nsuffers seriously from two main challenges: i) GCL relies on graph augmentation\nto generate semantically different views for contrasting, which could\npotentially disrupt key information and introduce unwanted noise; ii) current\nworks for GCL primarily focus on contrasting representations using\nsophisticated networks architecture (usually deep) to capture high-order\ninteractions, which leads to increased computational complexity and suboptimal\ntraining efficiency. To this end, we propose L2CL, a principled Layer-to-Layer\nContrastive Learning framework that contrasts representations from different\nlayers. By aligning the semantic similarities between different layers, L2CL\nenables the learning of complex structural relationships and gets rid of the\nnoise perturbation in stochastic data augmentation. Surprisingly, we find that\nL2CL, using only one-hop contrastive learning paradigm, is able to capture\nintrinsic semantic structures and improve the quality of node representation,\nleading to a simple yet effective architecture. We also provide theoretical\nguarantees for L2CL in minimizing task-irrelevant information. Extensive\nexperiments on five real-world datasets demonstrate the superiority of our\nmodel over various state-of-the-art collaborative filtering methods. Our code\nis available at https://github.com/downeykking/L2CL.\n","authors":["Xinzhou Jin","Jintang Li","Liang Chen","Chenyun Yu","Yuanzhen Xie","Tao Xie","Chengxiang Zhuo","Zang Li","Zibin Zheng"],"pdf_url":"https://arxiv.org/pdf/2407.14266v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14094v1","updated":"2024-07-19T07:58:26Z","published":"2024-07-19T07:58:26Z","title":"User-Creator Feature Dynamics in Recommender Systems with Dual Influence","summary":"  Recommender systems present relevant contents to users and help content\ncreators reach their target audience. The dual nature of these systems\ninfluences both users and creators: users' preferences are affected by the\nitems they are recommended, while creators are incentivized to alter their\ncontents such that it is recommended more frequently. We define a model, called\nuser-creator feature dynamics, to capture the dual influences of recommender\nsystems. We prove that a recommender system with dual influence is guaranteed\nto polarize, causing diversity loss in the system. We then investigate, both\ntheoretically and empirically, approaches for mitigating polarization and\npromoting diversity in recommender systems. Unexpectedly, we find that common\ndiversity-promoting approaches do not work in the presence of dual influence,\nwhile relevancy-optimizing methods like top-$k$ recommendation can prevent\npolarization and improve diversity of the system.\n","authors":["Tao Lin","Kun Jin","Andrew Estornell","Xiaoying Zhang","Yiling Chen","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2407.14094v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14081v1","updated":"2024-07-19T07:31:32Z","published":"2024-07-19T07:31:32Z","title":"DisenSemi: Semi-supervised Graph Classification via Disentangled\n  Representation Learning","summary":"  Graph classification is a critical task in numerous multimedia applications,\nwhere graphs are employed to represent diverse types of multimedia data,\nincluding images, videos, and social networks. Nevertheless, in real-world\nscenarios, labeled graph data can be limited or scarce. To address this issue,\nwe focus on the problem of semi-supervised graph classification, which involves\nboth supervised and unsupervised models learning from labeled and unlabeled\ndata. In contrast to recent approaches that transfer the entire knowledge from\nthe unsupervised model to the supervised one, we argue that an effective\ntransfer should only retain the relevant semantics that align well with the\nsupervised task. In this paper, we propose a novel framework named DisenSemi,\nwhich learns disentangled representation for semi-supervised graph\nclassification. Specifically, a disentangled graph encoder is proposed to\ngenerate factor-wise graph representations for both supervised and unsupervised\nmodels. Then we train two models via supervised objective and mutual\ninformation (MI)-based constraints respectively. To ensure the meaningful\ntransfer of knowledge from the unsupervised encoder to the supervised one, we\nfurther define an MI-based disentangled consistency regularization between two\nmodels and identify the corresponding rationale that aligns well with the\ncurrent graph classification task. Experimental results on a range of publicly\naccessible datasets reveal the effectiveness of our DisenSemi.\n","authors":["Yifan Wang","Xiao Luo","Chong Chen","Xian-Sheng Hua","Ming Zhang","Wei Ju"],"pdf_url":"https://arxiv.org/pdf/2407.14081v1.pdf","comment":"Accepted by IEEE Transactions on Neural Networks and Learning Systems\n  (TNNLS 2024)"},{"id":"http://arxiv.org/abs/2407.14030v1","updated":"2024-07-19T05:04:24Z","published":"2024-07-19T05:04:24Z","title":"HeCiX: Integrating Knowledge Graphs and Large Language Models for\n  Biomedical Research","summary":"  Despite advancements in drug development strategies, 90% of clinical trials\nfail. This suggests overlooked aspects in target validation and drug\noptimization. In order to address this, we introduce HeCiX-KG,\nHetionet-Clinicaltrials neXus Knowledge Graph, a novel fusion of data from\nClinicalTrials.gov and Hetionet in a single knowledge graph. HeCiX-KG combines\ndata on previously conducted clinical trials from ClinicalTrials.gov, and\ndomain expertise on diseases and genes from Hetionet. This offers a thorough\nresource for clinical researchers. Further, we introduce HeCiX, a system that\nuses LangChain to integrate HeCiX-KG with GPT-4, and increase its usability.\nHeCiX shows high performance during evaluation against a range of clinically\nrelevant issues, proving this model to be promising for enhancing the\neffectiveness of clinical research. Thus, this approach provides a more\nholistic view of clinical trials and existing biological data.\n","authors":["Prerana Sanjay Kulkarni","Muskaan Jain","Disha Sheshanarayana","Srinivasan Parthiban"],"pdf_url":"https://arxiv.org/pdf/2407.14030v1.pdf","comment":"8 pages, 3 figures, under review"},{"id":"http://arxiv.org/abs/2407.01712v2","updated":"2024-07-19T04:16:03Z","published":"2024-06-21T02:31:03Z","title":"A Survey of Retrieval Algorithms in Ad and Content Recommendation\n  Systems","summary":"  This survey examines the most effective retrieval algorithms utilized in ad\nrecommendation and content recommendation systems. Ad targeting algorithms rely\non detailed user profiles and behavioral data to deliver personalized\nadvertisements, thereby driving revenue through targeted placements.\nConversely, organic retrieval systems aim to improve user experience by\nrecommending content that matches user preferences. This paper compares these\ntwo applications and explains the most effective methods employed in each.\n","authors":["Yu Zhao","Fang Liu"],"pdf_url":"https://arxiv.org/pdf/2407.01712v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13349v2","updated":"2024-07-19T03:23:01Z","published":"2024-07-18T09:49:13Z","title":"DCNv3: Towards Next Generation Deep Cross Network for CTR Prediction","summary":"  Deep & Cross Network and its derivative models have become an important\nparadigm in click-through rate (CTR) prediction due to their effective balance\nbetween computational cost and performance. However, these models face four\nmajor limitations: (1) while most models claim to capture high-order feature\ninteractions, they often do so implicitly and non-interpretably through deep\nneural networks (DNN), which limits the trustworthiness of the model's\npredictions; (2) the performance of existing explicit feature interaction\nmethods is often weaker than that of implicit DNN, undermining their necessity;\n(3) many models fail to adaptively filter noise while enhancing the order of\nfeature interactions; (4) the fusion methods of most models cannot provide\nsuitable supervision signals for their different interaction methods.\n  To address the identified limitations, this paper proposes the next\ngeneration Deep Cross Network (DCNv3) and Shallow & Deep Cross Network\n(SDCNv3). These models ensure interpretability in feature interaction modeling\nwhile exponentially increasing the order of feature interactions to achieve\ngenuine Deep Crossing rather than just Deep & Cross. Additionally, we employ a\nSelf-Mask operation to filter noise and reduce the number of parameters in the\ncross network by half. In the fusion layer, we use a simple yet effective loss\nweight calculation method called Tri-BCE to provide appropriate supervision\nsignals. Comprehensive experiments on six datasets demonstrate the\neffectiveness, efficiency, and interpretability of DCNv3 and SDCNv3. The code,\nrunning logs, and detailed hyperparameter configurations are available at:\nhttps://anonymous.4open.science/r/DCNv3-E352.\n","authors":["Honghao Li","Yiwen Zhang","Yi Zhang","Hanwei Li","Lei Sang"],"pdf_url":"https://arxiv.org/pdf/2407.13349v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14000v1","updated":"2024-07-19T03:12:10Z","published":"2024-07-19T03:12:10Z","title":"Clinical Reading Comprehension with Encoder-Decoder Models Enhanced by\n  Direct Preference Optimization","summary":"  Extractive question answering over clinical text is a crucial need to help\ndeal with the deluge of clinical text generated in hospitals. While encoder\nmodels (e.g., BERT) have been popular for this reading comprehension task,\nrecently encoder-decoder models (e.g., T5) are on the rise. There is also the\nemergence of preference optimization techniques to align decoder-only LLMs with\nhuman preferences. In this paper, we combine encoder-decoder models with the\ndirect preference optimization (DPO) method to improve over prior state of the\nart for the RadQA radiology question answering task by 12-15 F1 points. To the\nbest of our knowledge, this effort is the first to show that DPO method also\nworks for reading comprehension via novel heuristics to generate preference\ndata without human inputs.\n","authors":["Md Sultan Al Nahian","Ramakanth Kavuluru"],"pdf_url":"https://arxiv.org/pdf/2407.14000v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.11986v2","updated":"2024-07-19T00:03:11Z","published":"2023-06-21T02:42:37Z","title":"Sequential Recommendation with Controllable Diversification:\n  Representation Degeneration and Diversity","summary":"  Sequential recommendation (SR) models the dynamic user preferences and\ngenerates the next-item prediction as the affinity between the sequence and\nitems, in a joint latent space with low dimensions (i.e., the sequence and item\nembedding space). Both sequence and item representations suffer from the\nrepresentation degeneration issue due to the user/item long-tail distributions,\nwhere tail users/ items are indistinguishably distributed as a narrow cone in\nthe latent space. We argue that the representation degeneration issue is the\nroot cause of insufficient recommendation diversity in existing SR methods,\nimpairing the user potential exploration and further worsening the echo chamber\nissue.\n  In this work, we first disclose the connection between the representation\ndegeneration and recommendation diversity, in which severer representation\ndegeneration indicates lower recommendation diversity. We then propose a novel\nSingular sPectrum sMoothing regularization for Recommendation (SPMRec), which\nacts as a controllable surrogate to alleviate the degeneration and achieve the\nbalance between recommendation diversity and performance. The proposed\nsmoothing regularization alleviates the degeneration by maximizing the area\nunder the singular value curve, which is also the diversity surrogate. We\nconduct experiments on four benchmark datasets to demonstrate the superiority\nof SPMRec, and show that the proposed singular spectrum smoothing can control\nthe balance of recommendation performance and diversity simultaneously.\n","authors":["Ziwei Fan","Zhiwei Liu","Hao Peng","Philip S. Yu"],"pdf_url":"https://arxiv.org/pdf/2306.11986v2.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2407.13952v1","updated":"2024-07-19T00:01:18Z","published":"2024-07-19T00:01:18Z","title":"Knowledge Distillation Approaches for Accurate and Efficient Recommender\n  System","summary":"  Despite its breakthrough in classification problems, Knowledge distillation\n(KD) to recommendation models and ranking problems has not been studied well in\nthe previous literature. This dissertation is devoted to developing knowledge\ndistillation methods for recommender systems to fully improve the performance\nof a compact model. We propose novel distillation methods designed for\nrecommender systems. The proposed methods are categorized according to their\nknowledge sources as follows: (1) Latent knowledge: we propose two methods that\ntransfer latent knowledge of user/item representation. They effectively\ntransfer knowledge of niche tastes with a balanced distillation strategy that\nprevents the KD process from being biased towards a small number of large\npreference groups. Also, we propose a new method that transfers user/item\nrelations in the representation space. The proposed method selectively\ntransfers essential relations considering the limited capacity of the compact\nmodel. (2) Ranking knowledge: we propose three methods that transfer ranking\nknowledge from the recommendation results. They formulate the KD process as a\nranking matching problem and transfer the knowledge via a listwise learning\nstrategy. Further, we present a new learning framework that compresses the\nranking knowledge of heterogeneous recommendation models. The proposed\nframework is developed to ease the computational burdens of model ensemble\nwhich is a dominant solution for many recommendation applications. We validate\nthe benefit of our proposed methods and frameworks through extensive\nexperiments. To summarize, this dissertation sheds light on knowledge\ndistillation approaches for a better accuracy-efficiency trade-off of the\nrecommendation models.\n","authors":["SeongKu Kang"],"pdf_url":"https://arxiv.org/pdf/2407.13952v1.pdf","comment":"Doctoral Dissertation (2022)"},{"id":"http://arxiv.org/abs/2407.14650v1","updated":"2024-07-19T20:01:30Z","published":"2024-07-19T20:01:30Z","title":"Auditing the Grid-Based Placement of Private Label Products on\n  E-commerce Search Result Pages","summary":"  E-commerce platforms support the needs and livelihoods of their two most\nimportant stakeholders -- customers and producers/sellers. Multiple algorithmic\nsystems, like ``search'' systems mediate the interactions between these\nstakeholders by connecting customers to producers with relevant items. Search\nresults include (i) private label (PL) products that are manufactured/sold by\nthe platform itself, as well as (ii) third-party products on advertised /\nsponsored and organic positions. In this paper, we systematically quantify the\nextent of PL product promotion on e-commerce search results for the two largest\ne-commerce platforms operating in India -- Amazon.in and Flipkart. By analyzing\nsnapshots of search results across the two platforms, we discover high PL\npromotion on the initial result pages (~ 15% PLs are advertised on the first\nSERP of Amazon). Both platforms use different strategies to promote their PL\nproducts, such as placing more PLs on the advertised positions -- while Amazon\nplaces them on the first, middle, and last rows of the search results, Flipkart\nplaces them on the first two positions and the (entire) last column of the\nsearch results. We discover that these product placement strategies of both\nplatforms conform with existing user attention strategies proposed in the\nliterature. Finally, to supplement the findings from the collected data, we\nconduct a survey among 68 participants on Amazon Mechanical Turk. The click\npattern from our survey shows that users strongly prefer to click on products\nplaced at positions that correspond to the PL products on the search results of\nAmazon, but not so strongly on Flipkart. The click-through rate follows\npreviously proposed theoretically grounded user attention distribution patterns\nin a two-dimensional layout.\n","authors":["Siddharth D Jaiswal","Abhisek Dash","Nitika Shroff","Yashwanth Babu Vunnam","Saptarshi Ghosh","Animesh Mukherjee"],"pdf_url":"https://arxiv.org/pdf/2407.14650v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.15592v2","updated":"2024-07-19T19:36:18Z","published":"2024-04-24T01:54:40Z","title":"ImplicitAVE: An Open-Source Dataset and Multimodal LLMs Benchmark for\n  Implicit Attribute Value Extraction","summary":"  Existing datasets for attribute value extraction (AVE) predominantly focus on\nexplicit attribute values while neglecting the implicit ones, lack product\nimages, are often not publicly available, and lack an in-depth human inspection\nacross diverse domains. To address these limitations, we present ImplicitAVE,\nthe first, publicly available multimodal dataset for implicit attribute value\nextraction. ImplicitAVE, sourced from the MAVE dataset, is carefully curated\nand expanded to include implicit AVE and multimodality, resulting in a refined\ndataset of 68k training and 1.6k testing data across five domains. We also\nexplore the application of multimodal large language models (MLLMs) to implicit\nAVE, establishing a comprehensive benchmark for MLLMs on the ImplicitAVE\ndataset. Six recent MLLMs with eleven variants are evaluated across diverse\nsettings, revealing that implicit value extraction remains a challenging task\nfor MLLMs. The contributions of this work include the development and release\nof ImplicitAVE, and the exploration and benchmarking of various MLLMs for\nimplicit AVE, providing valuable insights and potential future research\ndirections. Dataset and code are available at\nhttps://github.com/HenryPengZou/ImplicitAVE\n","authors":["Henry Peng Zou","Vinay Samuel","Yue Zhou","Weizhi Zhang","Liancheng Fang","Zihe Song","Philip S. Yu","Cornelia Caragea"],"pdf_url":"https://arxiv.org/pdf/2404.15592v2.pdf","comment":"Accepted by ACL 2024 (Findings) - Scores: Soundness - 4/4/4, Dataset\n  - 4/4/4, Overall Assessment - 4/3.5/3.5, Meta - 4"},{"id":"http://arxiv.org/abs/2407.14399v1","updated":"2024-07-19T15:21:14Z","published":"2024-07-19T15:21:14Z","title":"PolySinger: Singing-Voice to Singing-Voice Translation from English to\n  Japanese","summary":"  The speech domain prevails in the spotlight for several natural language\nprocessing (NLP) tasks while the singing domain remains less explored. The\nculmination of NLP is the speech-to-speech translation (S2ST) task, referring\nto translation and synthesis of human speech. A disparity between S2ST and the\npossible adaptation to the singing domain, which we describe as singing-voice\nto singing-voice translation (SV2SVT), is becoming prominent as the former is\nprogressing ever faster, while the latter is at a standstill. Singing-voice\nsynthesis systems are overcoming the barrier of multi-lingual synthesis,\ndespite limited attention has been paid to multi-lingual songwriting and song\ntranslation. This paper endeavors to determine what is required for successful\nSV2SVT and proposes PolySinger (Polyglot Singer): the first system for SV2SVT,\nperforming lyrics translation from English to Japanese. A cascaded approach is\nproposed to establish a framework with a high degree of control which can\npotentially diminish the disparity between SV2SVT and S2ST. The performance of\nPolySinger is evaluated by a mean opinion score test with native Japanese\nspeakers. Results and in-depth discussions with test subjects suggest a solid\nfoundation for SV2SVT, but several shortcomings must be overcome, which are\ndiscussed for the future of SV2SVT.\n","authors":["Silas Antonisen","Iván López-Espejo"],"pdf_url":"https://arxiv.org/pdf/2407.14399v1.pdf","comment":"This paper was accepted at ISMIR 2024"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2407.14504v1","updated":"2024-07-19T17:58:00Z","published":"2024-07-19T17:58:00Z","title":"Nonlinear Schrödinger Network","summary":"  Deep neural networks (DNNs) have achieved exceptional performance across\nvarious fields by learning complex nonlinear mappings from large-scale\ndatasets. However, they encounter challenges such as high computational costs\nand limited interpretability. To address these issues, hybrid approaches that\nintegrate physics with AI are gaining interest. This paper introduces a novel\nphysics-based AI model called the \"Nonlinear Schr\\\"odinger Network\", which\ntreats the Nonlinear Schr\\\"odinger Equation (NLSE) as a general-purpose\ntrainable model for learning complex patterns including nonlinear mappings and\nmemory effects from data. Existing physics-informed machine learning methods\nuse neural networks to approximate the solutions of partial differential\nequations (PDEs). In contrast, our approach directly treats the PDE as a\ntrainable model to obtain general nonlinear mappings that would otherwise\nrequire neural networks. As a physics-inspired approach, it offers a more\ninterpretable and parameter-efficient alternative to traditional black-box\nneural networks, achieving comparable or better accuracy in time series\nclassification tasks while significantly reducing the number of required\nparameters. Notably, the trained Nonlinear Schr\\\"odinger Network is\ninterpretable, with all parameters having physical meanings as properties of a\nvirtual physical system that transforms the data to a more separable space.\nThis interpretability allows for insight into the underlying dynamics of the\ndata transformation process. Applications to time series forecasting have also\nbeen explored. While our current implementation utilizes the NLSE, the proposed\nmethod of using physics equations as trainable models to learn nonlinear\nmappings from data is not limited to the NLSE and may be extended to other\nmaster equations of physics.\n","authors":["Yiming Zhou","Callen MacPhee","Tingyi Zhou","Bahram Jalali"],"pdf_url":"https://arxiv.org/pdf/2407.14504v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14503v1","updated":"2024-07-19T17:57:59Z","published":"2024-07-19T17:57:59Z","title":"Catastrophic Goodhart: regularizing RLHF with KL divergence does not\n  mitigate heavy-tailed reward misspecification","summary":"  When applying reinforcement learning from human feedback (RLHF), the reward\nis learned from data and, therefore, always has some error. It is common to\nmitigate this by regularizing the policy with KL divergence from a base model,\nwith the hope that balancing reward with regularization will achieve desirable\noutcomes despite this reward misspecification. We show that when the reward\nfunction has light-tailed error, optimal policies under less restrictive KL\npenalties achieve arbitrarily high utility. However, if error is heavy-tailed,\nsome policies obtain arbitrarily high reward despite achieving no more utility\nthan the base model--a phenomenon we call catastrophic Goodhart. We adapt a\ndiscrete optimization method to measure the tails of reward models, finding\nthat they are consistent with light-tailed error. However, the pervasiveness of\nheavy-tailed distributions in many real-world applications indicates that\nfuture sources of RL reward could have heavy-tailed error, increasing the\nlikelihood of reward hacking even with KL regularization.\n","authors":["Thomas Kwa","Drake Thomas","Adrià Garriga-Alonso"],"pdf_url":"https://arxiv.org/pdf/2407.14503v1.pdf","comment":"Mechanistic Interpretability workshop at ICML 2014"},{"id":"http://arxiv.org/abs/2407.14501v1","updated":"2024-07-19T17:53:21Z","published":"2024-07-19T17:53:21Z","title":"Indoor Air Quality Dataset with Activities of Daily Living in Low to\n  Middle-income Communities","summary":"  In recent years, indoor air pollution has posed a significant threat to our\nsociety, claiming over 3.2 million lives annually. Developing nations, such as\nIndia, are most affected since lack of knowledge, inadequate regulation, and\noutdoor air pollution lead to severe daily exposure to pollutants. However,\nonly a limited number of studies have attempted to understand how indoor air\npollution affects developing countries like India. To address this gap, we\npresent spatiotemporal measurements of air quality from 30 indoor sites over\nsix months during summer and winter seasons. The sites are geographically\nlocated across four regions of type: rural, suburban, and urban, covering the\ntypical low to middle-income population in India. The dataset contains various\ntypes of indoor environments (e.g., studio apartments, classrooms, research\nlaboratories, food canteens, and residential households), and can provide the\nbasis for data-driven learning model research aimed at coping with unique\npollution patterns in developing countries. This unique dataset demands\nadvanced data cleaning and imputation techniques for handling missing data due\nto power failure or network outages during data collection. Furthermore,\nthrough a simple speech-to-text application, we provide real-time indoor\nactivity labels annotated by occupants. Therefore, environmentalists and ML\nenthusiasts can utilize this dataset to understand the complex patterns of the\npollutants under different indoor activities, identify recurring sources of\npollution, forecast exposure, improve floor plans and room structures of modern\nindoor designs, develop pollution-aware recommender systems, etc.\n","authors":["Prasenjit Karmakar","Swadhin Pradhan","Sandip Chakraborty"],"pdf_url":"https://arxiv.org/pdf/2407.14501v1.pdf","comment":"19 pages, 9 figures"},{"id":"http://arxiv.org/abs/2407.14499v1","updated":"2024-07-19T17:50:11Z","published":"2024-07-19T17:50:11Z","title":"Discover-then-Name: Task-Agnostic Concept Bottlenecks via Automated\n  Concept Discovery","summary":"  Concept Bottleneck Models (CBMs) have recently been proposed to address the\n'black-box' problem of deep neural networks, by first mapping images to a\nhuman-understandable concept space and then linearly combining concepts for\nclassification. Such models typically require first coming up with a set of\nconcepts relevant to the task and then aligning the representations of a\nfeature extractor to map to these concepts. However, even with powerful\nfoundational feature extractors like CLIP, there are no guarantees that the\nspecified concepts are detectable. In this work, we leverage recent advances in\nmechanistic interpretability and propose a novel CBM approach -- called\nDiscover-then-Name-CBM (DN-CBM) -- that inverts the typical paradigm: instead\nof pre-selecting concepts based on the downstream classification task, we use\nsparse autoencoders to first discover concepts learnt by the model, and then\nname them and train linear probes for classification. Our concept extraction\nstrategy is efficient, since it is agnostic to the downstream task, and uses\nconcepts already known to the model. We perform a comprehensive evaluation\nacross multiple datasets and CLIP architectures and show that our method yields\nsemantically meaningful concepts, assigns appropriate names to them that make\nthem easy to interpret, and yields performant and interpretable CBMs. Code\navailable at https://github.com/neuroexplicit-saar/discover-then-name.\n","authors":["Sukrut Rao","Sweta Mahajan","Moritz Böhle","Bernt Schiele"],"pdf_url":"https://arxiv.org/pdf/2407.14499v1.pdf","comment":"40 pages, 21 figures, 6 tables, European Conference on Computer\n  Vision (ECCV) 2024"},{"id":"http://arxiv.org/abs/2402.01542v4","updated":"2024-07-19T17:48:10Z","published":"2024-02-02T16:35:02Z","title":"Learning Collective Variables with Synthetic Data Augmentation through\n  Physics-Inspired Geodesic Interpolation","summary":"  In molecular dynamics simulations, rare events, such as protein folding, are\ntypically studied using enhanced sampling techniques, most of which are based\non the definition of a collective variable (CV) along which acceleration\noccurs. Obtaining an expressive CV is crucial, but often hindered by the lack\nof information about the particular event, e.g., the transition from unfolded\nto folded conformation. We propose a simulation-free data augmentation strategy\nusing physics-inspired metrics to generate geodesic interpolations resembling\nprotein folding transitions, thereby improving sampling efficiency without true\ntransition state samples. This new data can be used to improve the accuracy of\nclassifier-based methods. Alternatively, a regression-based learning scheme for\nCV models can be adopted by leveraging the interpolation progress parameter.\n","authors":["Soojung Yang","Juno Nam","Johannes C. B. Dietschreit","Rafael Gómez-Bombarelli"],"pdf_url":"https://arxiv.org/pdf/2402.01542v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.06910v2","updated":"2024-07-19T17:47:42Z","published":"2024-04-10T11:03:17Z","title":"Superposition Prompting: Improving and Accelerating Retrieval-Augmented\n  Generation","summary":"  Despite the successes of large language models (LLMs), they exhibit\nsignificant drawbacks, particularly when processing long contexts. Their\ninference cost scales quadratically with respect to sequence length, making it\nexpensive for deployment in some real-world text processing applications, such\nas retrieval-augmented generation (RAG). Additionally, LLMs also exhibit the\n\"distraction phenomenon\", where irrelevant context in the prompt degrades\noutput quality. To address these drawbacks, we propose a novel RAG prompting\nmethodology, *superposition prompting*, which can be directly applied to\npre-trained transformer-based LLMs *without the need for fine-tuning*. At a\nhigh level, superposition prompting allows the LLM to process input documents\nin parallel *prompt paths*, discarding paths once they are deemed irrelevant.\nWe demonstrate the capability of our method to simultaneously enhance time\nefficiency across a variety of question-answering benchmarks using multiple\npre-trained LLMs. Furthermore, our technique significantly improves accuracy\nwhen the retrieved context is large relative the context the model was trained\non. For example, our approach facilitates a 93x reduction in compute time while\n*improving* accuracy by 43% on the NaturalQuestions-Open dataset with the\nMPT-7B instruction-tuned model over naive RAG.\n","authors":["Thomas Merth","Qichen Fu","Mohammad Rastegari","Mahyar Najibi"],"pdf_url":"https://arxiv.org/pdf/2404.06910v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14495v1","updated":"2024-07-19T17:47:08Z","published":"2024-07-19T17:47:08Z","title":"Conformal Thresholded Intervals for Efficient Regression","summary":"  This paper introduces Conformal Thresholded Intervals (CTI), a novel\nconformal regression method that aims to produce the smallest possible\nprediction set with guaranteed coverage. Unlike existing methods that rely on\nnested conformal framework and full conditional distribution estimation, CTI\nestimates the conditional probability density for a new response to fall into\neach interquantile interval using off-the-shelf multi-output quantile\nregression. CTI constructs prediction sets by thresholding the estimated\nconditional interquantile intervals based on their length, which is inversely\nproportional to the estimated probability density. The threshold is determined\nusing a calibration set to ensure marginal coverage. Experimental results\ndemonstrate that CTI achieves optimal performance across various datasets.\n","authors":["Rui Luo","Zhixin Zhou"],"pdf_url":"https://arxiv.org/pdf/2407.14495v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14494v1","updated":"2024-07-19T17:46:51Z","published":"2024-07-19T17:46:51Z","title":"InterpBench: Semi-Synthetic Transformers for Evaluating Mechanistic\n  Interpretability Techniques","summary":"  Mechanistic interpretability methods aim to identify the algorithm a neural\nnetwork implements, but it is difficult to validate such methods when the true\nalgorithm is unknown. This work presents InterpBench, a collection of\nsemi-synthetic yet realistic transformers with known circuits for evaluating\nthese techniques. We train these neural networks using a stricter version of\nInterchange Intervention Training (IIT) which we call Strict IIT (SIIT). Like\nthe original, SIIT trains neural networks by aligning their internal\ncomputation with a desired high-level causal model, but it also prevents\nnon-circuit nodes from affecting the model's output. We evaluate SIIT on sparse\ntransformers produced by the Tracr tool and find that SIIT models maintain\nTracr's original circuit while being more realistic. SIIT can also train\ntransformers with larger circuits, like Indirect Object Identification (IOI).\nFinally, we use our benchmark to evaluate existing circuit discovery\ntechniques.\n","authors":["Rohan Gupta","Iván Arcuschin","Thomas Kwa","Adrià Garriga-Alonso"],"pdf_url":"https://arxiv.org/pdf/2407.14494v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.18518v2","updated":"2024-07-19T17:38:12Z","published":"2024-05-28T18:38:15Z","title":"Modeling Long Sequences in Bladder Cancer Recurrence: A Comparative\n  Evaluation of LSTM,Transformer,and Mamba","summary":"  Traditional survival analysis methods often struggle with complex\ntime-dependent data,failing to capture and interpret dynamic characteristics\nadequately.This study aims to evaluate the performance of three long-sequence\nmodels,LSTM,Transformer,and Mamba,in analyzing recurrence event data and\nintegrating them with the Cox proportional hazards model.This study integrates\nthe advantages of deep learning models for handling long-sequence data with the\nCox proportional hazards model to enhance the performance in analyzing\nrecurrent events with dynamic time information.Additionally,this study compares\nthe ability of different models to extract and utilize features from\ntime-dependent clinical recurrence data.The LSTM-Cox model outperformed both\nthe Transformer-Cox and Mamba-Cox models in prediction accuracy and model\nfit,achieving a Concordance index of up to 0.90 on the test set.Significant\npredictors of bladder cancer recurrence,such as treatment stop time,maximum\ntumor size at recurrence and recurrence frequency,were identified.The LSTM-Cox\nmodel aligned well with clinical outcomes,effectively distinguishing between\nhigh-risk and low-risk patient groups.This study demonstrates that the LSTM-Cox\nmodel is a robust and efficient method for recurrent data analysis and feature\nextraction,surpassing newer models like Transformer and Mamba.It offers a\npractical approach for integrating deep learning technologies into clinical\nrisk prediction systems,thereby improving patient management and treatment\noutcomes.\n","authors":["Runquan Zhang","Jiawen Jiang","Xiaoping Shi"],"pdf_url":"https://arxiv.org/pdf/2405.18518v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14482v1","updated":"2024-07-19T17:35:47Z","published":"2024-07-19T17:35:47Z","title":"ChatQA 2: Bridging the Gap to Proprietary LLMs in Long Context and RAG\n  Capabilities","summary":"  In this work, we introduce ChatQA 2, a Llama3-based model designed to bridge\nthe gap between open-access LLMs and leading proprietary models (e.g.,\nGPT-4-Turbo) in long-context understanding and retrieval-augmented generation\n(RAG) capabilities. These two capabilities are essential for LLMs to process\nlarge volumes of information that cannot fit into a single prompt and are\ncomplementary to each other, depending on the downstream tasks and\ncomputational budgets. We present a detailed continued training recipe to\nextend the context window of Llama3-70B-base from 8K to 128K tokens, along with\na three-stage instruction tuning process to enhance the model's\ninstruction-following, RAG performance, and long-context understanding\ncapabilities. Our results demonstrate that the Llama3-ChatQA-2-70B model\nachieves accuracy comparable to GPT-4-Turbo-2024-0409 on many long-context\nunderstanding tasks and surpasses it on the RAG benchmark. Interestingly, we\nfind that the state-of-the-art long-context retriever can alleviate the top-k\ncontext fragmentation issue in RAG, further improving RAG-based results for\nlong-context understanding tasks. We also provide extensive comparisons between\nRAG and long-context solutions using state-of-the-art long-context LLMs.\n","authors":["Peng Xu","Wei Ping","Xianchao Wu","Zihan Liu","Mohammad Shoeybi","Bryan Catanzaro"],"pdf_url":"https://arxiv.org/pdf/2407.14482v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14477v1","updated":"2024-07-19T17:27:52Z","published":"2024-07-19T17:27:52Z","title":"Data-Centric Human Preference Optimization with Rationales","summary":"  Reinforcement learning from human feedback plays a crucial role in aligning\nlanguage models towards human preferences, traditionally represented through\ncomparisons between pairs or sets of responses within a given context. While\nmany studies have enhanced algorithmic techniques to optimize learning from\nsuch data, this work shifts focus to improving preference learning through a\ndata-centric approach. Specifically, we propose enriching existing preference\ndatasets with machine-generated rationales that explain the reasons behind\nchoices. We develop a simple and principled framework to augment current\npreference learning methods with rationale information. Our comprehensive\nanalysis highlights how rationales enhance learning efficiency. Extensive\nexperiments reveal that rationale-enriched preference learning offers multiple\nadvantages: it improves data efficiency, accelerates convergence to\nhigher-performing models, and reduces verbosity bias and hallucination.\nFurthermore, this framework is versatile enough to integrate with various\npreference optimization algorithms. Overall, our findings highlight the\npotential of re-imagining data design for preference learning, demonstrating\nthat even freely available machine-generated rationales can significantly boost\nperformance across multiple dimensions. The code repository is available at\nhttps: //github.com/reds-lab/preference-learning-with-rationales\n","authors":["Hoang Anh Just","Ming Jin","Anit Sahu","Huy Phan","Ruoxi Jia"],"pdf_url":"https://arxiv.org/pdf/2407.14477v1.pdf","comment":"Preference Learning with Rationales"},{"id":"http://arxiv.org/abs/2309.16748v2","updated":"2024-07-19T17:08:00Z","published":"2023-09-28T17:55:45Z","title":"Discovering environments with XRM","summary":"  Environment annotations are essential for the success of many\nout-of-distribution (OOD) generalization methods. Unfortunately, these are\ncostly to obtain and often limited by human annotators' biases. To achieve\nrobust generalization, it is essential to develop algorithms for automatic\nenvironment discovery within datasets. Current proposals, which divide examples\nbased on their training error, suffer from one fundamental problem. These\nmethods introduce hyper-parameters and early-stopping criteria, which require a\nvalidation set with human-annotated environments, the very information subject\nto discovery. In this paper, we propose Cross-Risk-Minimization (XRM) to\naddress this issue. XRM trains twin networks, each learning from one random\nhalf of the training data, while imitating confident held-out mistakes made by\nits sibling. XRM provides a recipe for hyper-parameter tuning, does not require\nearly-stopping, and can discover environments for all training and validation\ndata. Algorithms built on top of XRM environments achieve oracle\nworst-group-accuracy, addressing a long-standing challenge in OOD\ngeneralization. Code available at\n\\url{https://github.com/facebookresearch/XRM}.\n","authors":["Mohammad Pezeshki","Diane Bouchacourt","Mark Ibrahim","Nicolas Ballas","Pascal Vincent","David Lopez-Paz"],"pdf_url":"https://arxiv.org/pdf/2309.16748v2.pdf","comment":"Oral at ICML 2024"},{"id":"http://arxiv.org/abs/2407.14463v1","updated":"2024-07-19T17:06:03Z","published":"2024-07-19T17:06:03Z","title":"SurvReLU: Inherently Interpretable Survival Analysis via Deep ReLU\n  Networks","summary":"  Survival analysis models time-to-event distributions with censorship.\nRecently, deep survival models using neural networks have dominated due to\ntheir representational power and state-of-the-art performance. However, their\n\"black-box\" nature hinders interpretability, which is crucial in real-world\napplications. In contrast, \"white-box\" tree-based survival models offer better\ninterpretability but struggle to converge to global optima due to greedy\nexpansion. In this paper, we bridge the gap between previous deep survival\nmodels and traditional tree-based survival models through deep rectified linear\nunit (ReLU) networks. We show that a deliberately constructed deep ReLU network\n(SurvReLU) can harness the interpretability of tree-based structures with the\nrepresentational power of deep survival models. Empirical studies on both\nsimulated and real survival benchmark datasets show the effectiveness of the\nproposed SurvReLU in terms of performance and interoperability. The code is\navailable at \\href{https://github.com/xs018/SurvReLU}{\\color{magenta}{\nhttps://github.com/xs018/SurvReLU}}.\n","authors":["Xiaotong Sun","Peijie Qiu","Shengfan Zhang"],"pdf_url":"https://arxiv.org/pdf/2407.14463v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14459v1","updated":"2024-07-19T17:01:41Z","published":"2024-07-19T17:01:41Z","title":"PolyFormer: Scalable Node-wise Filters via Polynomial Graph Transformer","summary":"  Spectral Graph Neural Networks have demonstrated superior performance in\ngraph representation learning. However, many current methods focus on employing\nshared polynomial coefficients for all nodes, i.e., learning node-unified\nfilters, which limits the filters' flexibility for node-level tasks. The recent\nDSF attempts to overcome this limitation by learning node-wise coefficients\nbased on positional encoding. However, the initialization and updating process\nof the positional encoding are burdensome, hindering scalability on large-scale\ngraphs. In this work, we propose a scalable node-wise filter, PolyAttn.\nLeveraging the attention mechanism, PolyAttn can directly learn node-wise\nfilters in an efficient manner, offering powerful representation capabilities.\nBuilding on PolyAttn, we introduce the whole model, named PolyFormer. In the\nlens of Graph Transformer models, PolyFormer, which calculates attention scores\nwithin nodes, shows great scalability. Moreover, the model captures spectral\ninformation, enhancing expressiveness while maintaining efficiency. With these\nadvantages, PolyFormer offers a desirable balance between scalability and\nexpressiveness for node-level tasks. Extensive experiments demonstrate that our\nproposed methods excel at learning arbitrary node-wise filters, showing\nsuperior performance on both homophilic and heterophilic graphs, and handling\ngraphs containing up to 100 million nodes. The code is available at\nhttps://github.com/air029/PolyFormer.\n","authors":["Jiahong Ma","Mingguo He","Zhewei Wei"],"pdf_url":"https://arxiv.org/pdf/2407.14459v1.pdf","comment":"ACM SIGKDD 2024"},{"id":"http://arxiv.org/abs/2405.14981v2","updated":"2024-07-19T16:10:00Z","published":"2024-05-23T18:35:46Z","title":"MaSS: Multi-attribute Selective Suppression for Utility-preserving Data\n  Transformation from an Information-theoretic Perspective","summary":"  The growing richness of large-scale datasets has been crucial in driving the\nrapid advancement and wide adoption of machine learning technologies. The\nmassive collection and usage of data, however, pose an increasing risk for\npeople's private and sensitive information due to either inadvertent\nmishandling or malicious exploitation. Besides legislative solutions, many\ntechnical approaches have been proposed towards data privacy protection.\nHowever, they bear various limitations such as leading to degraded data\navailability and utility, or relying on heuristics and lacking solid\ntheoretical bases. To overcome these limitations, we propose a formal\ninformation-theoretic definition for this utility-preserving privacy protection\nproblem, and design a data-driven learnable data transformation framework that\nis capable of selectively suppressing sensitive attributes from target datasets\nwhile preserving the other useful attributes, regardless of whether or not they\nare known in advance or explicitly annotated for preservation. We provide\nrigorous theoretical analyses on the operational bounds for our framework, and\ncarry out comprehensive experimental evaluations using datasets of a variety of\nmodalities, including facial images, voice audio clips, and human activity\nmotion sensor signals. Results demonstrate the effectiveness and\ngeneralizability of our method under various configurations on a multitude of\ntasks. Our code is available at https://github.com/jpmorganchase/MaSS.\n","authors":["Yizhuo Chen","Chun-Fu Chen","Hsiang Hsu","Shaohan Hu","Marco Pistoia","Tarek Abdelzaher"],"pdf_url":"https://arxiv.org/pdf/2405.14981v2.pdf","comment":"ICML 2024, GitHub: https://github.com/jpmorganchase/MaSS"},{"id":"http://arxiv.org/abs/2407.14435v1","updated":"2024-07-19T16:07:19Z","published":"2024-07-19T16:07:19Z","title":"Jumping Ahead: Improving Reconstruction Fidelity with JumpReLU Sparse\n  Autoencoders","summary":"  Sparse autoencoders (SAEs) are a promising unsupervised approach for\nidentifying causally relevant and interpretable linear features in a language\nmodel's (LM) activations. To be useful for downstream tasks, SAEs need to\ndecompose LM activations faithfully; yet to be interpretable the decomposition\nmust be sparse -- two objectives that are in tension. In this paper, we\nintroduce JumpReLU SAEs, which achieve state-of-the-art reconstruction fidelity\nat a given sparsity level on Gemma 2 9B activations, compared to other recent\nadvances such as Gated and TopK SAEs. We also show that this improvement does\nnot come at the cost of interpretability through manual and automated\ninterpretability studies. JumpReLU SAEs are a simple modification of vanilla\n(ReLU) SAEs -- where we replace the ReLU with a discontinuous JumpReLU\nactivation function -- and are similarly efficient to train and run. By\nutilising straight-through-estimators (STEs) in a principled manner, we show\nhow it is possible to train JumpReLU SAEs effectively despite the discontinuous\nJumpReLU function introduced in the SAE's forward pass. Similarly, we use STEs\nto directly train L0 to be sparse, instead of training on proxies such as L1,\navoiding problems like shrinkage.\n","authors":["Senthooran Rajamanoharan","Tom Lieberum","Nicolas Sonnerat","Arthur Conmy","Vikrant Varma","János Kramár","Neel Nanda"],"pdf_url":"https://arxiv.org/pdf/2407.14435v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.04375v2","updated":"2024-07-19T16:01:49Z","published":"2024-02-06T20:24:07Z","title":"Bounding the Excess Risk for Linear Models Trained on\n  Marginal-Preserving, Differentially-Private, Synthetic Data","summary":"  The growing use of machine learning (ML) has raised concerns that an ML model\nmay reveal private information about an individual who has contributed to the\ntraining dataset. To prevent leakage of sensitive data, we consider using\ndifferentially-private (DP), synthetic training data instead of real training\ndata to train an ML model. A key desirable property of synthetic data is its\nability to preserve the low-order marginals of the original distribution. Our\nmain contribution comprises novel upper and lower bounds on the excess\nempirical risk of linear models trained on such synthetic data, for continuous\nand Lipschitz loss functions. We perform extensive experimentation alongside\nour theoretical results.\n","authors":["Yvonne Zhou","Mingyu Liang","Ivan Brugere","Dana Dachman-Soled","Danial Dervovic","Antigoni Polychroniadou","Min Wu"],"pdf_url":"https://arxiv.org/pdf/2402.04375v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14430v1","updated":"2024-07-19T16:01:37Z","published":"2024-07-19T16:01:37Z","title":"The Extrapolation Power of Implicit Models","summary":"  In this paper, we investigate the extrapolation capabilities of implicit deep\nlearning models in handling unobserved data, where traditional deep neural\nnetworks may falter. Implicit models, distinguished by their adaptability in\nlayer depth and incorporation of feedback within their computational graph, are\nput to the test across various extrapolation scenarios: out-of-distribution,\ngeographical, and temporal shifts. Our experiments consistently demonstrate\nsignificant performance advantage with implicit models. Unlike their\nnon-implicit counterparts, which often rely on meticulous architectural design\nfor each task, implicit models demonstrate the ability to learn complex model\nstructures without the need for task-specific design, highlighting their\nrobustness in handling unseen data.\n","authors":["Juliette Decugis","Alicia Y. Tsai","Max Emerling","Ashwin Ganesh","Laurent El Ghaoui"],"pdf_url":"https://arxiv.org/pdf/2407.14430v1.pdf","comment":"Accepted at the Workshop on Explainable Artificial Intelligence (XAI)\n  at IJCAI 2024"},{"id":"http://arxiv.org/abs/2310.16975v2","updated":"2024-07-19T15:55:46Z","published":"2023-10-25T20:20:09Z","title":"Efficient Neural Network Approaches for Conditional Optimal Transport\n  with Applications in Bayesian Inference","summary":"  We present two neural network approaches that approximate the solutions of\nstatic and dynamic conditional optimal transport (COT) problems. Both\napproaches enable conditional sampling and conditional density estimation,\nwhich are core tasks in Bayesian inference$\\unicode{x2013}$particularly in the\nsimulation-based (\"likelihood-free\") setting. Our methods represent the target\nconditional distributions as transformations of a tractable reference\ndistribution and, therefore, fall into the framework of measure transport.\nAlthough many measure transport approaches model the transformation as COT\nmaps, obtaining the map is computationally challenging, even in moderate\ndimensions. To improve scalability, our numerical algorithms use neural\nnetworks to parameterize COT maps and further exploit the structure of the COT\nproblem. Our static approach approximates the map as the gradient of a\npartially input-convex neural network. It uses a novel numerical implementation\nto increase computational efficiency compared to state-of-the-art alternatives.\nOur dynamic approach approximates the conditional optimal transport via the\nflow map of a regularized neural ODE; compared to the static approach, it is\nslower to train but offers more modeling choices and can lead to faster\nsampling. We demonstrate both algorithms numerically, comparing them with\ncompeting state-of-the-art approaches, using benchmark datasets and\nsimulation-based Bayesian inverse problems.\n","authors":["Zheyu Oliver Wang","Ricardo Baptista","Youssef Marzouk","Lars Ruthotto","Deepanshu Verma"],"pdf_url":"https://arxiv.org/pdf/2310.16975v2.pdf","comment":"26 pages, 7 tables, 8 figures"},{"id":"http://arxiv.org/abs/2407.14417v1","updated":"2024-07-19T15:42:49Z","published":"2024-07-19T15:42:49Z","title":"Mixture of Experts with Mixture of Precisions for Tuning Quality of\n  Service","summary":"  The increasing demand for deploying large Mixture-of-Experts (MoE) models in\nresource-constrained environments necessitates efficient approaches to address\ntheir high memory and computational requirements challenges. Moreover, given\nthat tasks come in different user-defined constraints and the available\nresources change over time in multi-tenant environments, it is necessary to\ndesign an approach which provides a flexible configuration space. This paper\npresents an adaptive serving approach for the efficient deployment of MoE\nmodels, capitalizing on partial quantization of the experts. By dynamically\ndetermining the number of quantized experts and their distribution across CPU\nand GPU, our approach explores the Pareto frontier and offers a fine-grained\nrange of configurations for tuning throughput and model quality. Our evaluation\non an NVIDIA A100 GPU using a Mixtral 8x7B MoE model for three language\nmodelling benchmarks demonstrates that the throughput of token generation can\nbe adjusted from 0.63 to 13.00 token per second. This enhancement comes with a\nmarginal perplexity increase of 2.62 to 2.80, 6.48 to 7.24, and 3.24 to 3.53\nfor WikiText2, PTB, and C4 datasets respectively under maximum quantization.\nThese results highlight the practical applicability of our approach in dynamic\nand accuracy-sensitive applications where both memory usage and output quality\nare important.\n","authors":["HamidReza Imani","Abdolah Amirany","Tarek El-Ghazawi"],"pdf_url":"https://arxiv.org/pdf/2407.14417v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14414v1","updated":"2024-07-19T15:40:59Z","published":"2024-07-19T15:40:59Z","title":"System-1.x: Learning to Balance Fast and Slow Planning with Language\n  Models","summary":"  Language models can be used to solve long-horizon planning problems in two\ndistinct modes: a fast 'System-1' mode, directly generating plans without any\nexplicit search or backtracking, and a slow 'System-2' mode, planning\nstep-by-step by explicitly searching over possible actions. While System-2 is\ntypically more effective, it is also more computationally expensive, making it\ninfeasible for long plans or large action spaces. Moreover, isolated System-1\nor 2 ignores the user's end goals, failing to provide ways to control the\nmodel's behavior. To this end, we propose the System-1.x Planner, a\ncontrollable planning framework with LLMs that is capable of generating hybrid\nplans and balancing between the two planning modes based on the difficulty of\nthe problem at hand. System-1.x consists of (i) a controller, (ii) a System-1\nPlanner, and (iii) a System-2 Planner. Based on a user-specified hybridization\nfactor (x) governing the mixture between System-1 and 2, the controller\ndecomposes a problem into sub-goals, and classifies them as easy or hard to be\nsolved by either System-1 or 2, respectively. We fine-tune all three components\non top of a single base LLM, requiring only search traces as supervision.\nExperiments with two diverse planning tasks -- Maze Navigation and Blocksworld\n-- show that our System-1.x Planner outperforms a System-1 Planner, a System-2\nPlanner trained to approximate A* search, and also a symbolic planner (A*). We\ndemonstrate the following key properties of our planner: (1) controllability:\nincreasing the hybridization factor (e.g., System-1.75 vs 1.5) performs more\nsearch, improving performance, (2) flexibility: by building a neuro-symbolic\nvariant with a neural System-1 and a symbolic System-2, we can use existing\nsymbolic methods, and (3) generalizability: by being able to learn from\ndifferent search algorithms, our method is robust to the choice of search\nalgorithm.\n","authors":["Swarnadeep Saha","Archiki Prasad","Justin Chih-Yao Chen","Peter Hase","Elias Stengel-Eskin","Mohit Bansal"],"pdf_url":"https://arxiv.org/pdf/2407.14414v1.pdf","comment":"29 pages (10 tables)"},{"id":"http://arxiv.org/abs/2407.14412v1","updated":"2024-07-19T15:39:19Z","published":"2024-07-19T15:39:19Z","title":"DEAL: Disentangle and Localize Concept-level Explanations for VLMs","summary":"  Large pre-trained Vision-Language Models (VLMs) have become ubiquitous\nfoundational components of other models and downstream tasks. Although\npowerful, our empirical results reveal that such models might not be able to\nidentify fine-grained concepts. Specifically, the explanations of VLMs with\nrespect to fine-grained concepts are entangled and mislocalized. To address\nthis issue, we propose to DisEntAngle and Localize (DEAL) the concept-level\nexplanations for VLMs without human annotations. The key idea is encouraging\nthe concept-level explanations to be distinct while maintaining consistency\nwith category-level explanations. We conduct extensive experiments and ablation\nstudies on a wide range of benchmark datasets and vision-language models. Our\nempirical results demonstrate that the proposed method significantly improves\nthe concept-level explanations of the model in terms of disentanglability and\nlocalizability. Surprisingly, the improved explainability alleviates the\nmodel's reliance on spurious correlations, which further benefits the\nprediction accuracy.\n","authors":["Tang Li","Mengmeng Ma","Xi Peng"],"pdf_url":"https://arxiv.org/pdf/2407.14412v1.pdf","comment":"In Proceedings of the European Conference on Computer Vision (ECCV),\n  2024"},{"id":"http://arxiv.org/abs/2407.09375v2","updated":"2024-07-19T15:34:25Z","published":"2024-07-12T15:56:11Z","title":"HiPPO-Prophecy: State-Space Models can Provably Learn Dynamical Systems\n  in Context","summary":"  This work explores the in-context learning capabilities of State Space Models\n(SSMs) and presents, to the best of our knowledge, the first theoretical\nexplanation of a possible underlying mechanism. We introduce a novel weight\nconstruction for SSMs, enabling them to predict the next state of any dynamical\nsystem after observing previous states without parameter fine-tuning. This is\naccomplished by extending the HiPPO framework to demonstrate that continuous\nSSMs can approximate the derivative of any input signal. Specifically, we find\nan explicit weight construction for continuous SSMs and provide an asymptotic\nerror bound on the derivative approximation. The discretization of this\ncontinuous SSM subsequently yields a discrete SSM that predicts the next state.\nFinally, we demonstrate the effectiveness of our parameterization empirically.\nThis work should be an initial step toward understanding how sequence models\nbased on SSMs learn in context.\n","authors":["Federico Arangath Joseph","Kilian Konstantin Haefeli","Noah Liniger","Caglar Gulcehre"],"pdf_url":"https://arxiv.org/pdf/2407.09375v2.pdf","comment":"ICML 2024, Next Generation Sequence Modeling Architectures Workshop"},{"id":"http://arxiv.org/abs/2407.14400v1","updated":"2024-07-19T15:25:20Z","published":"2024-07-19T15:25:20Z","title":"On the Impact of PRB Load Uncertainty Forecasting for Sustainable Open\n  RAN","summary":"  The transition to sustainable Open Radio Access Network (O-RAN) architectures\nbrings new challenges for resource management, especially in predicting the\nutilization of Physical Resource Block (PRB)s. In this paper, we propose a\nnovel approach to characterize the PRB load using probabilistic forecasting\ntechniques. First, we provide background information on the O-RAN architecture\nand components and emphasize the importance of energy/power consumption models\nfor sustainable implementations. The problem statement highlights the need for\naccurate PRB load prediction to optimize resource allocation and power\nefficiency. We then investigate probabilistic forecasting techniques, including\nSimple-Feed-Forward (SFF), DeepAR, and Transformers, and discuss their\nlikelihood model assumptions. The simulation results show that DeepAR\nestimators predict the PRBs with less uncertainty and effectively capture the\ntemporal dependencies in the dataset compared to SFF- and Transformer-based\nmodels, leading to power savings. Different percentile selections can also\nincrease power savings, but at the cost of over-/under provisioning. At the\nsame time, the performance of the Long-Short Term Memory (LSTM) is shown to be\ninferior to the probabilistic estimators with respect to all error metrics.\nFinally, we outline the importance of probabilistic, prediction-based\ncharacterization for sustainable O-RAN implementations and highlight avenues\nfor future research.\n","authors":["Vaishnavi Kasuluru","Luis Blanco","Cristian J. Vaca-Rubio","Engin Zeydan"],"pdf_url":"https://arxiv.org/pdf/2407.14400v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.01607v2","updated":"2024-07-19T15:25:15Z","published":"2024-05-02T04:53:42Z","title":"Wildfire Risk Prediction: A Review","summary":"  Wildfires have significant impacts on global vegetation, wildlife, and\nhumans. They destroy plant communities and wildlife habitats and contribute to\nincreased emissions of carbon dioxide, nitrogen oxides, methane, and other\npollutants. The prediction of wildfires relies on various independent variables\ncombined with regression or machine learning methods. In this technical review,\nwe describe the options for independent variables, data processing techniques,\nmodels, independent variables collinearity and importance estimation methods,\nand model performance evaluation metrics. First, we divide the independent\nvariables into 4 aspects, including climate and meteorology conditions,\nsocio-economical factors, terrain and hydrological features, and wildfire\nhistorical records. Second, preprocessing methods are described for different\nmagnitudes, different spatial-temporal resolutions, and different formats of\ndata. Third, the collinearity and importance evaluation methods of independent\nvariables are also considered. Fourth, we discuss the application of\nstatistical models, traditional machine learning models, and deep learning\nmodels in wildfire risk prediction. In this subsection, compared with other\nreviews, this manuscript particularly discusses the evaluation metrics and\nrecent advancements in deep learning methods. Lastly, addressing the\nlimitations of current research, this paper emphasizes the need for more\neffective deep learning time series forecasting algorithms, the utilization of\nthree-dimensional data including ground and trunk fuel, extraction of more\naccurate historical fire point data, and improved model evaluation metrics.\n","authors":["Zhengsen Xu","Jonathan Li","Linlin Xu"],"pdf_url":"https://arxiv.org/pdf/2405.01607v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.04317v2","updated":"2024-07-19T15:19:55Z","published":"2024-06-06T17:57:49Z","title":"Regularized KL-Divergence for Well-Defined Function-Space Variational\n  Inference in Bayesian neural networks","summary":"  Bayesian neural networks (BNN) promise to combine the predictive performance\nof neural networks with principled uncertainty modeling important for\nsafety-critical systems and decision making. However, posterior uncertainty\nestimates depend on the choice of prior, and finding informative priors in\nweight-space has proven difficult. This has motivated variational inference\n(VI) methods that pose priors directly on the function generated by the BNN\nrather than on weights. In this paper, we address a fundamental issue with such\nfunction-space VI approaches pointed out by Burt et al. (2020), who showed that\nthe objective function (ELBO) is negative infinite for most priors of interest.\nOur solution builds on generalized VI (Knoblauch et al., 2019) with the\nregularized KL divergence (Quang, 2019) and is, to the best of our knowledge,\nthe first well-defined variational objective for function-space inference in\nBNNs with Gaussian process (GP) priors. Experiments show that our method\nincorporates the properties specified by the GP prior on synthetic and small\nreal-world data sets, and provides competitive uncertainty estimates for\nregression, classification and out-of-distribution detection compared to BNN\nbaselines with both function and weight-space priors.\n","authors":["Tristan Cinquin","Robert Bamler"],"pdf_url":"https://arxiv.org/pdf/2406.04317v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.10853v2","updated":"2024-07-19T15:16:17Z","published":"2024-05-17T15:27:52Z","title":"The Future of Large Language Model Pre-training is Federated","summary":"  Generative pre-trained large language models (LLMs) have demonstrated\nimpressive performance over a wide range of tasks, thanks to the unprecedented\namount of data they have been trained on. As established scaling laws indicate,\nLLMs' future performance improvement depends on the amount of computing and\ndata sources they can leverage for pre-training. Federated learning (FL) has\nthe potential to unleash the majority of the planet's data and computational\nresources, which are underutilized by the data-center-focused training\nmethodology of current LLM practice. Our work presents a robust, flexible,\nreproducible FL approach that enables large-scale collaboration across\ninstitutions to train LLMs. We propose a scalable deployment system called\nPhoton to enable the investigation and development of this new training\nparadigm for LLM pre-training. We show that Photon can be used by organizations\ninterested in collaborating with their private data sources and computational\nresources for pre-training LLMs with billions of parameters. This paradigm\nwould mobilize more computational and data resources while matching or\npotentially exceeding centralized performance. We further show the\neffectiveness of the federated training scales with model size and present our\napproach for training a billion-scale federated LLM using limited resources.\nFinally, we show that LLM training is highly resilient to the classical\nchallenges of federated statistical and hardware heterogeneity. Furthermore, we\nshow that convergence is robust to partial participation, opening the avenue\nfor compute-efficient collaborative training. Photon will help data-rich actors\nto become the protagonists of LLMs pre-training instead of leaving the stage to\ncompute-rich actors alone.\n","authors":["Lorenzo Sani","Alex Iacob","Zeyu Cao","Bill Marino","Yan Gao","Tomas Paulik","Wanru Zhao","William F. Shen","Preslav Aleksandrov","Xinchi Qiu","Nicholas D. Lane"],"pdf_url":"https://arxiv.org/pdf/2405.10853v2.pdf","comment":"10 pages, 4 figures, pre-print"},{"id":"http://arxiv.org/abs/2406.11544v2","updated":"2024-07-19T15:13:45Z","published":"2024-06-17T13:42:28Z","title":"Do Parameters Reveal More than Loss for Membership Inference?","summary":"  Membership inference attacks aim to infer whether an individual record was\nused to train a model, serving as a key tool for disclosure auditing. While\nsuch evaluations are useful to demonstrate risk, they are computationally\nexpensive and often make strong assumptions about potential adversaries' access\nto models and training environments, and thus do not provide very tight bounds\non leakage from potential attacks. We show how prior claims around black-box\naccess being sufficient for optimal membership inference do not hold for most\nuseful settings such as stochastic gradient descent, and that optimal\nmembership inference indeed requires white-box access. We validate our findings\nwith a new white-box inference attack IHA (Inverse Hessian Attack) that\nexplicitly uses model parameters by taking advantage of computing\ninverse-Hessian vector products. Our results show that both audits and\nadversaries may be able to benefit from access to model parameters, and we\nadvocate for further research into white-box methods for membership privacy\nauditing.\n","authors":["Anshuman Suri","Xiao Zhang","David Evans"],"pdf_url":"https://arxiv.org/pdf/2406.11544v2.pdf","comment":"Accepted at High-dimensional Learning Dynamics (HiLD) Workshop, ICML\n  2024"},{"id":"http://arxiv.org/abs/2407.14387v1","updated":"2024-07-19T15:13:22Z","published":"2024-07-19T15:13:22Z","title":"GLAudio Listens to the Sound of the Graph","summary":"  We propose GLAudio: Graph Learning on Audio representation of the node\nfeatures and the connectivity structure. This novel architecture propagates the\nnode features through the graph network according to the discrete wave equation\nand then employs a sequence learning architecture to learn the target node\nfunction from the audio wave signal. This leads to a new paradigm of learning\non graph-structured data, in which information propagation and information\nprocessing are separated into two distinct steps. We theoretically characterize\nthe expressivity of our model, introducing the notion of the receptive field of\na vertex, and investigate our model's susceptibility to over-smoothing and\nover-squashing both theoretically as well as experimentally on various graph\ndatasets.\n","authors":["Aurelio Sulser","Johann Wenckstern","Clara Kuempel"],"pdf_url":"https://arxiv.org/pdf/2407.14387v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14386v1","updated":"2024-07-19T15:11:55Z","published":"2024-07-19T15:11:55Z","title":"Frontiers of Deep Learning: From Novel Application to Real-World\n  Deployment","summary":"  Deep learning continues to re-shape numerous fields, from natural language\nprocessing and imaging to data analytics and recommendation systems. This\nreport studies two research papers that represent recent progress on deep\nlearning from two largely different aspects: The first paper applied the\ntransformer networks, which are typically used in language models, to improve\nthe quality of synthetic aperture radar image by effectively reducing the\nspeckle noise. The second paper presents an in-storage computing design\nsolution to enable cost-efficient and high-performance implementations of deep\nlearning recommendation systems. In addition to summarizing each paper in terms\nof motivation, key ideas and techniques, and evaluation results, this report\nalso presents thoughts and discussions about possible future research\ndirections. By carrying out in-depth study on these two representative papers\nand related references, this doctoral candidate has developed better\nunderstanding on the far-reaching impact and efficient implementation of deep\nlearning models.\n","authors":["Rui Xie"],"pdf_url":"https://arxiv.org/pdf/2407.14386v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14381v1","updated":"2024-07-19T15:10:46Z","published":"2024-07-19T15:10:46Z","title":"Improving GBDT Performance on Imbalanced Datasets: An Empirical Study of\n  Class-Balanced Loss Functions","summary":"  Class imbalance remains a significant challenge in machine learning,\nparticularly for tabular data classification tasks. While Gradient Boosting\nDecision Trees (GBDT) models have proven highly effective for such tasks, their\nperformance can be compromised when dealing with imbalanced datasets. This\npaper presents the first comprehensive study on adapting class-balanced loss\nfunctions to three GBDT algorithms across various tabular classification tasks,\nincluding binary, multi-class, and multi-label classification. We conduct\nextensive experiments on multiple datasets to evaluate the impact of\nclass-balanced losses on different GBDT models, establishing a valuable\nbenchmark. Our results demonstrate the potential of class-balanced loss\nfunctions to enhance GBDT performance on imbalanced datasets, offering a robust\napproach for practitioners facing class imbalance challenges in real-world\napplications. Additionally, we introduce a Python package that facilitates the\nintegration of class-balanced loss functions into GBDT workflows, making these\nadvanced techniques accessible to a wider audience.\n","authors":["Jiaqi Luo","Yuan Yuan","Shixin Xu"],"pdf_url":"https://arxiv.org/pdf/2407.14381v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14377v1","updated":"2024-07-19T15:04:15Z","published":"2024-07-19T15:04:15Z","title":"Enhancing Cloud-Native Resource Allocation with Probabilistic\n  Forecasting Techniques in O-RAN","summary":"  The need for intelligent and efficient resource provisioning for the\nproductive management of resources in real-world scenarios is growing with the\nevolution of telecommunications towards the 6G era. Technologies such as Open\nRadio Access Network (O-RAN) can help to build interoperable solutions for the\nmanagement of complex systems. Probabilistic forecasting, in contrast to\ndeterministic single-point estimators, can offer a different approach to\nresource allocation by quantifying the uncertainty of the generated\npredictions. This paper examines the cloud-native aspects of O-RAN together\nwith the radio App (rApp) deployment options. The integration of probabilistic\nforecasting techniques as a rApp in O-RAN is also emphasized, along with case\nstudies of real-world applications. Through a comparative analysis of\nforecasting models using the error metric, we show the advantages of Deep\nAutoregressive Recurrent network (DeepAR) over other deterministic\nprobabilistic estimators. Furthermore, the simplicity of Simple-Feed-Forward\n(SFF) leads to a fast runtime but does not capture the temporal dependencies of\nthe input data. Finally, we present some aspects related to the practical\napplicability of cloud-native O-RAN with probabilistic forecasting.\n","authors":["Vaishnavi Kasuluru","Luis Blanco","Engin Zeydan","Albert Bel","Angelos Antonopoulos"],"pdf_url":"https://arxiv.org/pdf/2407.14377v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14375v1","updated":"2024-07-19T15:03:38Z","published":"2024-07-19T15:03:38Z","title":"On the use of Probabilistic Forecasting for Network Analysis in Open RAN","summary":"  Unlike other single-point Artificial Intelligence (AI)-based prediction\ntechniques, such as Long-Short Term Memory (LSTM), probabilistic forecasting\ntechniques (e.g., DeepAR and Transformer) provide a range of possible outcomes\nand associated probabilities that enable decision makers to make more informed\nand robust decisions. At the same time, the architecture of Open RAN has\nemerged as a revolutionary approach for mobile networks, aiming at openness,\ninteroperability and innovation in the ecosystem of RAN. In this paper, we\npropose the use of probabilistic forecasting techniques as a radio App (rApp)\nwithin the Open RAN architecture. We investigate and compare different\nprobabilistic and single-point forecasting methods and algorithms to estimate\nthe utilization and resource demands of Physical Resource Blocks (PRBs) of\ncellular base stations. Through our evaluations, we demonstrate the numerical\nadvantages of probabilistic forecasting techniques over traditional\nsingle-point forecasting methods and show that they are capable of providing\nmore accurate and reliable estimates. In particular, DeepAR clearly outperforms\nsingle-point forecasting techniques such as LSTM and Seasonal-Naive (SN)\nbaselines and other probabilistic forecasting techniques such as\nSimple-Feed-Forward (SFF) and Transformer neural networks.\n","authors":["Vaishnavi Kasuluru","Luis Blanco","Engin Zeydan"],"pdf_url":"https://arxiv.org/pdf/2407.14375v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14372v1","updated":"2024-07-19T15:02:00Z","published":"2024-07-19T15:02:00Z","title":"SCoPE: Evaluating LLMs for Software Vulnerability Detection","summary":"  In recent years, code security has become increasingly important, especially\nwith the rise of interconnected technologies. Detecting vulnerabilities early\nin the software development process has demonstrated numerous benefits.\nConsequently, the scientific community started using machine learning for\nautomated detection of source code vulnerabilities. This work explores and\nrefines the CVEFixes dataset, which is commonly used to train models for\ncode-related tasks, specifically the C/C++ subset. To this purpose, the Source\nCode Processing Engine (SCoPE), a framework composed of strategized techniques\nthat can be used to reduce the size and normalize C/C++ functions is presented.\nThe output generated by SCoPE was used to create a new version of CVEFixes.\nThis refined dataset was then employed in a feature representation analysis to\nassess the effectiveness of the tool's code processing techniques, consisting\nof fine-tuning three pre-trained LLMs for software vulnerability detection. The\nresults show that SCoPE successfully helped to identify 905 duplicates within\nthe evaluated subset. The LLM results corroborate with the literature regarding\ntheir suitability for software vulnerability detection, with the best model\nachieving 53% F1-score.\n","authors":["José Gonçalves","Tiago Dias","Eva Maia","Isabel Praça"],"pdf_url":"https://arxiv.org/pdf/2407.14372v1.pdf","comment":"10 pages, 3 figures, 1 table, published in DCAI 24 conference"},{"id":"http://arxiv.org/abs/2407.14371v1","updated":"2024-07-19T15:01:24Z","published":"2024-07-19T15:01:24Z","title":"Open Artificial Knowledge","summary":"  The tremendous success of chat-based AI systems like ChatGPT, Claude, and\nGemini stems from Large Language Models (LLMs) trained on vast amount of\ndatasets. However, acquiring high-quality, diverse, and ethically sourced\ntraining data remains a significant challenge. We introduce the Open Artificial\nKnowledge (OAK) dataset, a large-scale resource of over 500 million tokens (at\nthe moment of writing) designed to address this issue. OAK leverages an\nensemble of state-of-the-art LLMs, including GPT4o, LLaMa3-70B, LLaMa3-8B,\nMixtral-8x7B, Gemma-7B, and Gemma-2-9B , to generate high-quality text across\ndiverse domains, guided by Wikipedia's main categories. Our methodology ensures\nbroad knowledge coverage while maintaining coherence and factual accuracy. The\nOAK dataset aims to foster the development of more capable and aligned language\nmodels while addressing critical issues of data scarcity and privacy in LLM\ntraining, and it is freely available on www.oakdataset.org.\n","authors":["Vadim Borisov","Richard H. Schreiber"],"pdf_url":"https://arxiv.org/pdf/2407.14371v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14361v1","updated":"2024-07-19T14:43:35Z","published":"2024-07-19T14:43:35Z","title":"FuzzTheREST: An Intelligent Automated Black-box RESTful API Fuzzer","summary":"  Software's pervasive impact and increasing reliance in the era of digital\ntransformation raise concerns about vulnerabilities, emphasizing the need for\nsoftware security. Fuzzy testing is a dynamic analysis software testing\ntechnique that consists of feeding faulty input data to a System Under Test\n(SUT) and observing its behavior. Specifically regarding black-box RESTful API\ntesting, recent literature has attempted to automate this technique using\nheuristics to perform the input search and using the HTTP response status codes\nfor classification. However, most approaches do not keep track of code\ncoverage, which is important to validate the solution. This work introduces a\nblack-box RESTful API fuzzy testing tool that employs Reinforcement Learning\n(RL) for vulnerability detection. The fuzzer operates via the OpenAPI\nSpecification (OAS) file and a scenarios file, which includes information to\ncommunicate with the SUT and the sequences of functionalities to test,\nrespectively. To evaluate its effectiveness, the tool was tested on the\nPetstore API. The tool found a total of six unique vulnerabilities and achieved\n55\\% code coverage.\n","authors":["Tiago Dias","Eva Maia","Isabel Praça"],"pdf_url":"https://arxiv.org/pdf/2407.14361v1.pdf","comment":"10 pages, 4 figures, published in DCAI 2024 conference, 2 tables"},{"id":"http://arxiv.org/abs/2407.14342v1","updated":"2024-07-19T14:23:20Z","published":"2024-07-19T14:23:20Z","title":"Quantifying the value of positive transfer: An experimental case study","summary":"  In traditional approaches to structural health monitoring, challenges often\narise associated with the availability of labelled data. Population-based\nstructural health monitoring seeks to overcomes these challenges by leveraging\ndata/information from similar structures via technologies such as transfer\nlearning. The current paper demonstrate a methodology for quantifying the value\nof information transfer in the context of operation and maintenance\ndecision-making. This demonstration, based on a population of laboratory-scale\naircraft models, highlights the steps required to evaluate the expected value\nof information transfer including similarity assessment and prediction of\ntransfer efficacy. Once evaluated for a given population, the value of\ninformation transfer can be used to optimise transfer-learning strategies for\nnewly-acquired target domains.\n","authors":["Aidan J. Hughes","Giulia Delo","Jack Poole","Nikolaos Dervilis","Keith Worden"],"pdf_url":"https://arxiv.org/pdf/2407.14342v1.pdf","comment":"In Proceedings of the 11th European Workshop on Structural Health\n  Monitoring (EWSHM 2024), Potsdam, Germany. July 2024. arXiv admin note: text\n  overlap with arXiv:2311.03083"},{"id":"http://arxiv.org/abs/2403.14270v2","updated":"2024-07-19T14:07:25Z","published":"2024-03-21T10:15:57Z","title":"Scene-Graph ViT: End-to-End Open-Vocabulary Visual Relationship\n  Detection","summary":"  Visual relationship detection aims to identify objects and their\nrelationships in images. Prior methods approach this task by adding separate\nrelationship modules or decoders to existing object detection architectures.\nThis separation increases complexity and hinders end-to-end training, which\nlimits performance. We propose a simple and highly efficient decoder-free\narchitecture for open-vocabulary visual relationship detection. Our model\nconsists of a Transformer-based image encoder that represents objects as tokens\nand models their relationships implicitly. To extract relationship information,\nwe introduce an attention mechanism that selects object pairs likely to form a\nrelationship. We provide a single-stage recipe to train this model on a mixture\nof object and relationship detection data. Our approach achieves\nstate-of-the-art relationship detection performance on Visual Genome and on the\nlarge-vocabulary GQA benchmark at real-time inference speeds. We provide\nablations, real-world qualitative examples, and analyses of zero-shot\nperformance.\n","authors":["Tim Salzmann","Markus Ryll","Alex Bewley","Matthias Minderer"],"pdf_url":"https://arxiv.org/pdf/2403.14270v2.pdf","comment":"ECCV camera-ready; changed to graph-constrained results"},{"id":"http://arxiv.org/abs/2407.14328v1","updated":"2024-07-19T14:06:01Z","published":"2024-07-19T14:06:01Z","title":"Modality-Order Matters! A Novel Hierarchical Feature Fusion Method for\n  CoSAm: A Code-Switched Autism Corpus","summary":"  Autism Spectrum Disorder (ASD) is a complex neuro-developmental challenge,\npresenting a spectrum of difficulties in social interaction, communication, and\nthe expression of repetitive behaviors in different situations. This increasing\nprevalence underscores the importance of ASD as a major public health concern\nand the need for comprehensive research initiatives to advance our\nunderstanding of the disorder and its early detection methods. This study\nintroduces a novel hierarchical feature fusion method aimed at enhancing the\nearly detection of ASD in children through the analysis of code-switched speech\n(English and Hindi). Employing advanced audio processing techniques, the\nresearch integrates acoustic, paralinguistic, and linguistic information using\nTransformer Encoders. This innovative fusion strategy is designed to improve\nclassification robustness and accuracy, crucial for early and precise ASD\nidentification. The methodology involves collecting a code-switched speech\ncorpus, CoSAm, from children diagnosed with ASD and a matched control group.\nThe dataset comprises 61 voice recordings from 30 children diagnosed with ASD\nand 31 from neurotypical children, aged between 3 and 13 years, resulting in a\ntotal of 159.75 minutes of voice recordings. The feature analysis focuses on\nMFCCs and extensive statistical attributes to capture speech pattern\nvariability and complexity. The best model performance is achieved using a\nhierarchical fusion technique with an accuracy of 98.75% using a combination of\nacoustic and linguistic features first, followed by paralinguistic features in\na hierarchical manner.\n","authors":["Mohd Mujtaba Akhtar"," Girish","Muskaan Singh","Orchid Chetia Phukan"],"pdf_url":"https://arxiv.org/pdf/2407.14328v1.pdf","comment":"Submitted to Computer Speech & Language"},{"id":"http://arxiv.org/abs/2407.12687v2","updated":"2024-07-19T14:03:41Z","published":"2024-05-21T19:27:59Z","title":"Towards Responsible Development of Generative AI for Education: An\n  Evaluation-Driven Approach","summary":"  A major challenge facing the world is the provision of equitable and\nuniversal access to quality education. Recent advances in generative AI (gen\nAI) have created excitement about the potential of new technologies to offer a\npersonal tutor for every learner and a teaching assistant for every teacher.\nThe full extent of this dream, however, has not yet materialised. We argue that\nthis is primarily due to the difficulties with verbalising pedagogical\nintuitions into gen AI prompts and the lack of good evaluation practices,\nreinforced by the challenges in defining excellent pedagogy. Here we present\nour work collaborating with learners and educators to translate high level\nprinciples from learning science into a pragmatic set of seven diverse\neducational benchmarks, spanning quantitative, qualitative, automatic and human\nevaluations; and to develop a new set of fine-tuning datasets to improve the\npedagogical capabilities of Gemini, introducing LearnLM-Tutor. Our evaluations\nshow that LearnLM-Tutor is consistently preferred over a prompt tuned Gemini by\neducators and learners on a number of pedagogical dimensions. We hope that this\nwork can serve as a first step towards developing a comprehensive educational\nevaluation framework, and that this can enable rapid progress within the AI and\nEdTech communities towards maximising the positive impact of gen AI in\neducation.\n","authors":["Irina Jurenka","Markus Kunesch","Kevin R. McKee","Daniel Gillick","Shaojian Zhu","Sara Wiltberger","Shubham Milind Phal","Katherine Hermann","Daniel Kasenberg","Avishkar Bhoopchand","Ankit Anand","Miruna Pîslar","Stephanie Chan","Lisa Wang","Jennifer She","Parsa Mahmoudieh","Aliya Rysbek","Wei-Jen Ko","Andrea Huber","Brett Wiltshire","Gal Elidan","Roni Rabin","Jasmin Rubinovitz","Amit Pitaru","Mac McAllister","Julia Wilkowski","David Choi","Roee Engelberg","Lidan Hackmon","Adva Levin","Rachel Griffin","Michael Sears","Filip Bar","Mia Mesar","Mana Jabbour","Arslan Chaudhry","James Cohan","Sridhar Thiagarajan","Nir Levine","Ben Brown","Dilan Gorur","Svetlana Grant","Rachel Hashimshoni","Laura Weidinger","Jieru Hu","Dawn Chen","Kuba Dolecki","Canfer Akbulut","Maxwell Bileschi","Laura Culp","Wen-Xin Dong","Nahema Marchal","Kelsie Van Deman","Hema Bajaj Misra","Michael Duah","Moran Ambar","Avi Caciularu","Sandra Lefdal","Chris Summerfield","James An","Pierre-Alexandre Kamienny","Abhinit Mohdi","Theofilos Strinopoulous","Annie Hale","Wayne Anderson","Luis C. Cobo","Niv Efron","Muktha Ananda","Shakir Mohamed","Maureen Heymans","Zoubin Ghahramani","Yossi Matias","Ben Gomes","Lila Ibrahim"],"pdf_url":"https://arxiv.org/pdf/2407.12687v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.15776v3","updated":"2024-07-19T14:00:43Z","published":"2024-02-24T09:47:46Z","title":"Truly No-Regret Learning in Constrained MDPs","summary":"  Constrained Markov decision processes (CMDPs) are a common way to model\nsafety constraints in reinforcement learning. State-of-the-art methods for\nefficiently solving CMDPs are based on primal-dual algorithms. For these\nalgorithms, all currently known regret bounds allow for error cancellations --\none can compensate for a constraint violation in one round with a strict\nconstraint satisfaction in another. This makes the online learning process\nunsafe since it only guarantees safety for the final (mixture) policy but not\nduring learning. As Efroni et al. (2020) pointed out, it is an open question\nwhether primal-dual algorithms can provably achieve sublinear regret if we do\nnot allow error cancellations. In this paper, we give the first affirmative\nanswer. We first generalize a result on last-iterate convergence of regularized\nprimal-dual schemes to CMDPs with multiple constraints. Building upon this\ninsight, we propose a model-based primal-dual algorithm to learn in an unknown\nCMDP. We prove that our algorithm achieves sublinear regret without error\ncancellations.\n","authors":["Adrian Müller","Pragnya Alatur","Volkan Cevher","Giorgia Ramponi","Niao He"],"pdf_url":"https://arxiv.org/pdf/2402.15776v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.04131v5","updated":"2024-07-19T13:57:52Z","published":"2023-11-07T16:58:51Z","title":"Towards Interpretable Sequence Continuation: Analyzing Shared Circuits\n  in Large Language Models","summary":"  While transformer models exhibit strong capabilities on linguistic tasks,\ntheir complex architectures make them difficult to interpret. Recent work has\naimed to reverse engineer transformer models into human-readable\nrepresentations called circuits that implement algorithmic functions. We extend\nthis research by analyzing and comparing circuits for similar sequence\ncontinuation tasks, which include increasing sequences of Arabic numerals,\nnumber words, and months. By applying circuit interpretability analysis, we\nidentify a key sub-circuit in both GPT-2 Small and Llama-2-7B responsible for\ndetecting sequence members and for predicting the next member in a sequence.\nOur analysis reveals that semantically related sequences rely on shared circuit\nsubgraphs with analogous roles. Additionally, we show that this sub-circuit has\neffects on various math-related prompts, such as on intervaled circuits,\nSpanish number word and months continuation, and natural language word\nproblems. Overall, documenting shared computational structures enables better\nmodel behavior predictions, identification of errors, and safer editing\nprocedures. This mechanistic understanding of transformers is a critical step\ntowards building more robust, aligned, and interpretable language models.\n","authors":["Michael Lan","Philip Torr","Fazl Barez"],"pdf_url":"https://arxiv.org/pdf/2311.04131v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14320v1","updated":"2024-07-19T13:56:57Z","published":"2024-07-19T13:56:57Z","title":"Joint or Disjoint: Mixing Training Regimes for Early-Exit Models","summary":"  Early exits are an important efficiency mechanism integrated into deep neural\nnetworks that allows for the termination of the network's forward pass before\nprocessing through all its layers. By allowing early halting of the inference\nprocess for less complex inputs that reached high confidence, early exits\nsignificantly reduce the amount of computation required. Early exit methods add\ntrainable internal classifiers which leads to more intricacy in the training\nprocess. However, there is no consistent verification of the approaches of\ntraining of early exit methods, and no unified scheme of training such models.\nMost early exit methods employ a training strategy that either simultaneously\ntrains the backbone network and the exit heads or trains the exit heads\nseparately. We propose a training approach where the backbone is initially\ntrained on its own, followed by a phase where both the backbone and the exit\nheads are trained together. Thus, we advocate for organizing early-exit\ntraining strategies into three distinct categories, and then validate them for\ntheir performance and efficiency. In this benchmark, we perform both\ntheoretical and empirical analysis of early-exit training regimes. We study the\nmethods in terms of information flow, loss landscape and numerical rank of\nactivations and gauge the suitability of regimes for various architectures and\ndatasets.\n","authors":["Bartłomiej Krzepkowski","Monika Michaluk","Franciszek Szarwacki","Piotr Kubaty","Jary Pomponi","Tomasz Trzciński","Bartosz Wójcik","Kamil Adamczewski"],"pdf_url":"https://arxiv.org/pdf/2407.14320v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14303v1","updated":"2024-07-19T13:33:38Z","published":"2024-07-19T13:33:38Z","title":"Multi-Source and Test-Time Domain Adaptation on Multivariate Signals\n  using Spatio-Temporal Monge Alignment","summary":"  Machine learning applications on signals such as computer vision or\nbiomedical data often face significant challenges due to the variability that\nexists across hardware devices or session recordings. This variability poses a\nDomain Adaptation (DA) problem, as training and testing data distributions\noften differ. In this work, we propose Spatio-Temporal Monge Alignment (STMA)\nto mitigate these variabilities. This Optimal Transport (OT) based method\nadapts the cross-power spectrum density (cross-PSD) of multivariate signals by\nmapping them to the Wasserstein barycenter of source domains (multi-source DA).\nPredictions for new domains can be done with a filtering without the need for\nretraining a model with source data (test-time DA). We also study and discuss\ntwo special cases of the method, Temporal Monge Alignment (TMA) and Spatial\nMonge Alignment (SMA). Non-asymptotic concentration bounds are derived for the\nmappings estimation, which reveals a bias-plus-variance error structure with a\nvariance decay rate of $\\mathcal{O}(n_\\ell^{-1/2})$ with $n_\\ell$ the signal\nlength. This theoretical guarantee demonstrates the efficiency of the proposed\ncomputational schema. Numerical experiments on multivariate biosignals and\nimage data show that STMA leads to significant and consistent performance gains\nbetween datasets acquired with very different settings. Notably, STMA is a\npre-processing step complementary to state-of-the-art deep learning methods.\n","authors":["Théo Gnassounou","Antoine Collas","Rémi Flamary","Karim Lounici","Alexandre Gramfort"],"pdf_url":"https://arxiv.org/pdf/2407.14303v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.11187v2","updated":"2024-07-19T13:22:02Z","published":"2024-06-17T03:49:44Z","title":"Save It All: Enabling Full Parameter Tuning for Federated Large Language\n  Models via Cycle Block Gradient Descent","summary":"  The advent of large language models (LLMs) has revolutionized the deep\nlearning paradigm, yielding impressive results across a wide array of tasks.\nHowever, the pre-training or fine-tuning of LLMs within a federated learning\n(FL) framework poses substantial challenges, including considerable\ncomputational and memory resource demands, as well as communication bottlenecks\nbetween servers and clients. Existing solutions either make the unrealistic\nassumption that the entire model is exchanged for training, or apply\nparameter-effective fine-tuning methods from centralized learning to train LLMs\nin FL which tend to underperform during training or fine-tuning stages due to\nthe limited search subspace of parameter updating. In this paper, we introduce\na novel method for the efficient training and fine-tuning of LLMs in FL, with\nminimal resource consumption. Our approach, termed FedCyBGD, utilizes Cycle\nBlock Gradient Descent to periodically update the model. In particular, we\ndesign a compression scheme for FedCyBGD, aiming to further decrease the model\ndownload cost. It enables full parameter training in FL with only selected\nblock updates and uploads, thereby reducing communication, computation, and\nmemory costs. Our method achieves state-of-the-art performance for FL LLM\ntraining, while significantly reducing associated costs. Codes are provided\nhere.\n","authors":["Lin Wang","Zhichao Wang","Xiaoying Tang"],"pdf_url":"https://arxiv.org/pdf/2406.11187v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.06800v2","updated":"2024-07-19T13:07:06Z","published":"2024-07-09T12:14:48Z","title":"Learn and Don't Forget: Adding a New Language to ASR Foundation Models","summary":"  Foundation ASR models often support many languages, e.g. 100 languages in\nWhisper. However, there has been limited work on integrating an additional,\ntypically low-resource, language, while maintaining performance on the original\nlanguage set. Fine-tuning, while simple, may degrade the accuracy of the\noriginal set. We compare three approaches that exploit adaptation parameters:\nsoft language code tuning, train only the language code; soft prompt tuning,\ntrain prepended tokens; and LoRA where a small set of additional parameters are\noptimised. Elastic Weight Consolidation (EWC) offers an alternative compromise\nwith the potential to maintain performance in specific target languages.\nResults show that direct fine-tuning yields the best performance for the new\nlanguage but degrades existing language capabilities. EWC can address this\nissue for specific languages. If only adaptation parameters are used, the\nlanguage capabilities are maintained but at the cost of performance in the new\nlanguage.\n","authors":["Mengjie Qian","Siyuan Tang","Rao Ma","Kate M. Knill","Mark J. F. Gales"],"pdf_url":"https://arxiv.org/pdf/2407.06800v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.11969v2","updated":"2024-07-19T13:03:01Z","published":"2024-07-16T17:59:55Z","title":"Does Refusal Training in LLMs Generalize to the Past Tense?","summary":"  Refusal training is widely used to prevent LLMs from generating harmful,\nundesirable, or illegal outputs. We reveal a curious generalization gap in the\ncurrent refusal training approaches: simply reformulating a harmful request in\nthe past tense (e.g., \"How to make a Molotov cocktail?\" to \"How did people make\na Molotov cocktail?\") is often sufficient to jailbreak many state-of-the-art\nLLMs. We systematically evaluate this method on Llama-3 8B, Claude-3.5 Sonnet,\nGPT-3.5 Turbo, Gemma-2 9B, Phi-3-Mini, GPT-4o mini, GPT-4o, and R2D2 models\nusing GPT-3.5 Turbo as a reformulation model. For example, the success rate of\nthis simple attack on GPT-4o increases from 1% using direct requests to 88%\nusing 20 past tense reformulation attempts on harmful requests from\nJailbreakBench with GPT-4 as a jailbreak judge. Interestingly, we also find\nthat reformulations in the future tense are less effective, suggesting that\nrefusal guardrails tend to consider past historical questions more benign than\nhypothetical future questions. Moreover, our experiments on fine-tuning GPT-3.5\nTurbo show that defending against past reformulations is feasible when past\ntense examples are explicitly included in the fine-tuning data. Overall, our\nfindings highlight that the widely used alignment techniques -- such as SFT,\nRLHF, and adversarial training -- employed to align the studied models can be\nbrittle and do not always generalize as intended. We provide code and jailbreak\nartifacts at https://github.com/tml-epfl/llm-past-tense.\n","authors":["Maksym Andriushchenko","Nicolas Flammarion"],"pdf_url":"https://arxiv.org/pdf/2407.11969v2.pdf","comment":"Update in v2: Claude-3.5 Sonnet and GPT-4o mini. We provide code and\n  jailbreak artifacts at https://github.com/tml-epfl/llm-past-tense"},{"id":"http://arxiv.org/abs/2402.10232v4","updated":"2024-07-19T12:49:42Z","published":"2024-02-10T15:37:46Z","title":"Simple, unified analysis of Johnson-Lindenstrauss with applications","summary":"  We present a simplified and unified analysis of the Johnson-Lindenstrauss\n(JL) lemma, a cornerstone of dimensionality reduction for managing\nhigh-dimensional data. Our approach simplifies understanding and unifies\nvarious constructions under the JL framework, including spherical, binary-coin,\nsparse JL, Gaussian, and sub-Gaussian models. This unification preserves the\nintrinsic geometry of data, essential for applications from streaming\nalgorithms to reinforcement learning. We provide the first rigorous proof of\nthe spherical construction's effectiveness and introduce a general class of\nsub-Gaussian constructions within this simplified framework. Central to our\ncontribution is an innovative extension of the Hanson-Wright inequality to high\ndimensions, complete with explicit constants. By using simple yet powerful\nprobabilistic tools and analytical techniques, such as an enhanced\ndiagonalization process, our analysis solidifies the theoretical foundation of\nthe JL lemma by removing an independence assumption and extends its practical\napplicability to contemporary algorithms.\n","authors":["Yingru Li"],"pdf_url":"https://arxiv.org/pdf/2402.10232v4.pdf","comment":"24 pages, presented at \"High-dimensional Learning Dynamics 2024: The\n  Emergence of Structure and Reasoning\""},{"id":"http://arxiv.org/abs/2407.14266v1","updated":"2024-07-19T12:45:21Z","published":"2024-07-19T12:45:21Z","title":"L^2CL: Embarrassingly Simple Layer-to-Layer Contrastive Learning for\n  Graph Collaborative Filtering","summary":"  Graph neural networks (GNNs) have recently emerged as an effective approach\nto model neighborhood signals in collaborative filtering. Towards this research\nline, graph contrastive learning (GCL) demonstrates robust capabilities to\naddress the supervision label shortage issue through generating massive\nself-supervised signals. Despite its effectiveness, GCL for recommendation\nsuffers seriously from two main challenges: i) GCL relies on graph augmentation\nto generate semantically different views for contrasting, which could\npotentially disrupt key information and introduce unwanted noise; ii) current\nworks for GCL primarily focus on contrasting representations using\nsophisticated networks architecture (usually deep) to capture high-order\ninteractions, which leads to increased computational complexity and suboptimal\ntraining efficiency. To this end, we propose L2CL, a principled Layer-to-Layer\nContrastive Learning framework that contrasts representations from different\nlayers. By aligning the semantic similarities between different layers, L2CL\nenables the learning of complex structural relationships and gets rid of the\nnoise perturbation in stochastic data augmentation. Surprisingly, we find that\nL2CL, using only one-hop contrastive learning paradigm, is able to capture\nintrinsic semantic structures and improve the quality of node representation,\nleading to a simple yet effective architecture. We also provide theoretical\nguarantees for L2CL in minimizing task-irrelevant information. Extensive\nexperiments on five real-world datasets demonstrate the superiority of our\nmodel over various state-of-the-art collaborative filtering methods. Our code\nis available at https://github.com/downeykking/L2CL.\n","authors":["Xinzhou Jin","Jintang Li","Liang Chen","Chenyun Yu","Yuanzhen Xie","Tao Xie","Chengxiang Zhuo","Zang Li","Zibin Zheng"],"pdf_url":"https://arxiv.org/pdf/2407.14266v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.11447v5","updated":"2024-07-19T12:42:17Z","published":"2024-01-21T09:55:47Z","title":"Sequential Model for Predicting Patient Adherence in Subcutaneous\n  Immunotherapy for Allergic Rhinitis","summary":"  Objective: Subcutaneous Immunotherapy (SCIT) is the long-lasting causal\ntreatment of allergic rhinitis (AR). How to enhance the adherence of patients\nto maximize the benefit of allergen immunotherapy (AIT) plays a crucial role in\nthe management of AIT. This study aims to leverage novel machine learning\nmodels to precisely predict the risk of non-adherence of AR patients and\nrelated local symptom scores in three years SCIT.\n  Methods: The research develops and analyzes two models, sequential\nlatent-variable model (SLVM) of Stochastic Latent Actor-Critic (SLAC) and Long\nShort-Term Memory (LSTM) evaluating them based on scoring and adherence\nprediction capabilities.\n  Results: Excluding the biased samples at the first time step, the predictive\nadherence accuracy of the SLAC models is from 60\\% to 72\\%, and for LSTM\nmodels, it is 66\\% to 84\\%, varying according to the time steps. The range of\nRoot Mean Square Error (RMSE) for SLAC models is between 0.93 and 2.22, while\nfor LSTM models it is between 1.09 and 1.77. Notably, these RMSEs are\nsignificantly lower than the random prediction error of 4.55.\n  Conclusion: We creatively apply sequential models in the long-term management\nof SCIT with promising accuracy in the prediction of SCIT nonadherence in AR\npatients. While LSTM outperforms SLAC in adherence prediction, SLAC excels in\nscore prediction for patients undergoing SCIT for AR. The state-action-based\nSLAC adds flexibility, presenting a novel and effective approach for managing\nlong-term AIT.\n","authors":["Yin Li","Yu Xiong","Wenxin Fan","Kai Wang","Qingqing Yu","Liping Si","Patrick van der Smagt","Jun Tang","Nutan Chen"],"pdf_url":"https://arxiv.org/pdf/2401.11447v5.pdf","comment":"Frontiers in Pharmacology, research topic: Methods and Metrics to\n  Measure Medication Adherence"},{"id":"http://arxiv.org/abs/2407.14262v1","updated":"2024-07-19T12:40:08Z","published":"2024-07-19T12:40:08Z","title":"Hyperparameter Optimization for Driving Strategies Based on\n  Reinforcement Learning","summary":"  This paper focuses on hyperparameter optimization for autonomous driving\nstrategies based on Reinforcement Learning. We provide a detailed description\nof training the RL agent in a simulation environment. Subsequently, we employ\nEfficient Global Optimization algorithm that uses Gaussian Process fitting for\nhyperparameter optimization in RL. Before this optimization phase, Gaussian\nprocess interpolation is applied to fit the surrogate model, for which the\nhyperparameter set is generated using Latin hypercube sampling. To accelerate\nthe evaluation, parallelization techniques are employed. Following the\nhyperparameter optimization procedure, a set of hyperparameters is identified,\nresulting in a noteworthy enhancement in overall driving performance. There is\na substantial increase of 4\\% when compared to existing manually tuned\nparameters and the hyperparameters discovered during the initialization process\nusing Latin hypercube sampling. After the optimization, we analyze the obtained\nresults thoroughly and conduct a sensitivity analysis to assess the robustness\nand generalization capabilities of the learned autonomous driving strategies.\nThe findings from this study contribute to the advancement of Gaussian process\nbased Bayesian optimization to optimize the hyperparameters for autonomous\ndriving in RL, providing valuable insights for the development of efficient and\nreliable autonomous driving systems.\n","authors":["Nihal Acharya Adde","Hanno Gottschalk","Andreas Ebert"],"pdf_url":"https://arxiv.org/pdf/2407.14262v1.pdf","comment":"Submitted and accepted by LOD24 conference\n  https://lod2024.icas.events/"},{"id":"http://arxiv.org/abs/2407.14259v1","updated":"2024-07-19T12:37:15Z","published":"2024-07-19T12:37:15Z","title":"Voices in a Crowd: Searching for Clusters of Unique Perspectives","summary":"  Language models have been shown to reproduce underlying biases existing in\ntheir training data, which is the majority perspective by default. Proposed\nsolutions aim to capture minority perspectives by either modelling annotator\ndisagreements or grouping annotators based on shared metadata, both of which\nface significant challenges. We propose a framework that trains models without\nencoding annotator metadata, extracts latent embeddings informed by annotator\nbehaviour, and creates clusters of similar opinions, that we refer to as\nvoices. Resulting clusters are validated post-hoc via internal and external\nquantitative metrics, as well a qualitative analysis to identify the type of\nvoice that each cluster represents. Our results demonstrate the strong\ngeneralisation capability of our framework, indicated by resulting clusters\nbeing adequately robust, while also capturing minority perspectives based on\ndifferent demographic factors throughout two distinct datasets.\n","authors":["Nikolas Vitsakis","Amit Parekh","Ioannis Konstas"],"pdf_url":"https://arxiv.org/pdf/2407.14259v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14251v1","updated":"2024-07-19T12:31:15Z","published":"2024-07-19T12:31:15Z","title":"Personalized Multi-tier Federated Learning","summary":"  The key challenge of personalized federated learning (PerFL) is to capture\nthe statistical heterogeneity properties of data with inexpensive\ncommunications and gain customized performance for participating devices. To\naddress these, we introduced personalized federated learning in multi-tier\narchitecture (PerMFL) to obtain optimized and personalized local models when\nthere are known team structures across devices. We provide theoretical\nguarantees of PerMFL, which offers linear convergence rates for smooth strongly\nconvex problems and sub-linear convergence rates for smooth non-convex\nproblems. We conduct numerical experiments demonstrating the robust empirical\nperformance of PerMFL, outperforming the state-of-the-art in multiple\npersonalized federated learning tasks.\n","authors":["Sourasekhar Banerjee","Ali Dadras","Alp Yurtsever","Monowar Bhuyan"],"pdf_url":"https://arxiv.org/pdf/2407.14251v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14249v1","updated":"2024-07-19T12:30:03Z","published":"2024-07-19T12:30:03Z","title":"An Attention-based Representation Distillation Baseline for Multi-Label\n  Continual Learning","summary":"  The field of Continual Learning (CL) has inspired numerous researchers over\nthe years, leading to increasingly advanced countermeasures to the issue of\ncatastrophic forgetting. Most studies have focused on the single-class\nscenario, where each example comes with a single label. The recent literature\nhas successfully tackled such a setting, with impressive results. Differently,\nwe shift our attention to the multi-label scenario, as we feel it to be more\nrepresentative of real-world open problems. In our work, we show that existing\nstate-of-the-art CL methods fail to achieve satisfactory performance, thus\nquestioning the real advance claimed in recent years. Therefore, we assess both\nold-style and novel strategies and propose, on top of them, an approach called\nSelective Class Attention Distillation (SCAD). It relies on a knowledge\ntransfer technique that seeks to align the representations of the student\nnetwork -- which trains continuously and is subject to forgetting -- with the\nteacher ones, which is pretrained and kept frozen. Importantly, our method is\nable to selectively transfer the relevant information from the teacher to the\nstudent, thereby preventing irrelevant information from harming the student's\nperformance during online training. To demonstrate the merits of our approach,\nwe conduct experiments on two different multi-label datasets, showing that our\nmethod outperforms the current state-of-the-art Continual Learning methods. Our\nfindings highlight the importance of addressing the unique challenges posed by\nmulti-label environments in the field of Continual Learning. The code of SCAD\nis available at https://github.com/aimagelab/SCAD-LOD-2024.\n","authors":["Martin Menabue","Emanuele Frascaroli","Matteo Boschini","Lorenzo Bonicelli","Angelo Porrello","Simone Calderara"],"pdf_url":"https://arxiv.org/pdf/2407.14249v1.pdf","comment":"Accepted at LOD 2024"},{"id":"http://arxiv.org/abs/2306.13990v2","updated":"2024-07-19T12:08:27Z","published":"2023-06-24T14:50:20Z","title":"Cross-Validation Is All You Need: A Statistical Approach To Label Noise\n  Estimation","summary":"  Machine learning models experience deteriorated performance when trained in\nthe presence of noisy labels. This is particularly problematic for medical\ntasks, such as survival prediction, which typically face high label noise\ncomplexity with few clear-cut solutions. Inspired by the large fluctuations\nacross folds in the cross-validation performance of survival analyses, we\ndesign Monte-Carlo experiments to show that such fluctuation could be caused by\nlabel noise. We propose two novel and straightforward label noise detection\nalgorithms that effectively identify noisy examples by pinpointing the samples\nthat more frequently contribute to inferior cross-validation results. We first\nintroduce Repeated Cross-Validation (ReCoV), a parameter-free label noise\ndetection algorithm that is robust to model choice. We further develop\nfastReCoV, a less robust but more tractable and efficient variant of ReCoV\nsuitable for deep learning applications. Through extensive experiments, we show\nthat ReCoV and fastReCoV achieve state-of-the-art label noise detection\nperformance in a wide range of modalities, models and tasks, including survival\nanalysis, which has yet to be addressed in the literature. Our code and data\nare publicly available at https://github.com/GJiananChen/ReCoV.\n","authors":["Jianan Chen","Vishwesh Ramanathan","Tony Xu","Anne L. Martel"],"pdf_url":"https://arxiv.org/pdf/2306.13990v2.pdf","comment":"Accepted by MICCAI 2024"},{"id":"http://arxiv.org/abs/2406.05391v2","updated":"2024-07-19T12:04:29Z","published":"2024-06-08T07:48:16Z","title":"DUPLEX: Dual GAT for Complex Embedding of Directed Graphs","summary":"  Current directed graph embedding methods build upon undirected techniques but\noften inadequately capture directed edge information, leading to challenges\nsuch as: (1) Suboptimal representations for nodes with low in/out-degrees, due\nto the insufficient neighbor interactions; (2) Limited inductive ability for\nrepresenting new nodes post-training; (3) Narrow generalizability, as training\nis overly coupled with specific tasks. In response, we propose DUPLEX, an\ninductive framework for complex embeddings of directed graphs. It (1) leverages\nHermitian adjacency matrix decomposition for comprehensive neighbor\nintegration, (2) employs a dual GAT encoder for directional neighbor modeling,\nand (3) features two parameter-free decoders to decouple training from\nparticular tasks. DUPLEX outperforms state-of-the-art models, especially for\nnodes with sparse connectivity, and demonstrates robust inductive capability\nand adaptability across various tasks. The code is available at\nhttps://github.com/alipay/DUPLEX.\n","authors":["Zhaoru Ke","Hang Yu","Jianguo Li","Haipeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.05391v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14231v1","updated":"2024-07-19T11:58:30Z","published":"2024-07-19T11:58:30Z","title":"Realistic Evaluation of Test-Time Adaptation Algorithms: Unsupervised\n  Hyperparameter Selection","summary":"  Test-Time Adaptation (TTA) has recently emerged as a promising strategy for\ntackling the problem of machine learning model robustness under distribution\nshifts by adapting the model during inference without access to any labels.\nBecause of task difficulty, hyperparameters strongly influence the\neffectiveness of adaptation. However, the literature has provided little\nexploration into optimal hyperparameter selection. In this work, we tackle this\nproblem by evaluating existing TTA methods using surrogate-based hp-selection\nstrategies (which do not assume access to the test labels) to obtain a more\nrealistic evaluation of their performance. We show that some of the recent\nstate-of-the-art methods exhibit inferior performance compared to the previous\nalgorithms when using our more realistic evaluation setup. Further, we show\nthat forgetting is still a problem in TTA as the only method that is robust to\nhp-selection resets the model to the initial state at every step. We analyze\ndifferent types of unsupervised selection strategies, and while they work\nreasonably well in most scenarios, the only strategies that work consistently\nwell use some kind of supervision (either by a limited number of annotated test\nsamples or by using pretraining data). Our findings underscore the need for\nfurther research with more rigorous benchmarking by explicitly stating model\nselection strategies, to facilitate which we open-source our code.\n","authors":["Sebastian Cygert","Damian Sójka","Tomasz Trzciński","Bartłomiej Twardowski"],"pdf_url":"https://arxiv.org/pdf/2407.14231v1.pdf","comment":"preprint"},{"id":"http://arxiv.org/abs/2407.14230v1","updated":"2024-07-19T11:57:56Z","published":"2024-07-19T11:57:56Z","title":"ETSCL: An Evidence Theory-Based Supervised Contrastive Learning\n  Framework for Multi-modal Glaucoma Grading","summary":"  Glaucoma is one of the leading causes of vision impairment. Digital imaging\ntechniques, such as color fundus photography (CFP) and optical coherence\ntomography (OCT), provide quantitative and noninvasive methods for glaucoma\ndiagnosis. Recently, in the field of computer-aided glaucoma diagnosis,\nmulti-modality methods that integrate the CFP and OCT modalities have achieved\ngreater diagnostic accuracy compared to single-modality methods. However, it\nremains challenging to extract reliable features due to the high similarity of\nmedical images and the unbalanced multi-modal data distribution. Moreover,\nexisting methods overlook the uncertainty estimation of different modalities,\nleading to unreliable predictions. To address these challenges, we propose a\nnovel framework, namely ETSCL, which consists of a contrastive feature\nextraction stage and a decision-level fusion stage. Specifically, the\nsupervised contrastive loss is employed to enhance the discriminative power in\nthe feature extraction process, resulting in more effective features. In\naddition, we utilize the Frangi vesselness algorithm as a preprocessing step to\nincorporate vessel information to assist in the prediction. In the\ndecision-level fusion stage, an evidence theory-based multi-modality classifier\nis employed to combine multi-source information with uncertainty estimation.\nExtensive experiments demonstrate that our method achieves state-of-the-art\nperformance. The code is available at\n\\url{https://github.com/master-Shix/ETSCL}.\n","authors":["Zhiyuan Yang","Bo Zhang","Yufei Shi","Ningze Zhong","Johnathan Loh","Huihui Fang","Yanwu Xu","Si Yong Yeo"],"pdf_url":"https://arxiv.org/pdf/2407.14230v1.pdf","comment":"Accepted by Ophthalmic Medical Image Analysis Workshop at MICCAI'24"},{"id":"http://arxiv.org/abs/2311.05550v2","updated":"2024-07-19T11:32:32Z","published":"2023-11-09T17:49:02Z","title":"Towards End-to-End Spoken Grammatical Error Correction","summary":"  Grammatical feedback is crucial for L2 learners, teachers, and testers.\nSpoken grammatical error correction (GEC) aims to supply feedback to L2\nlearners on their use of grammar when speaking. This process usually relies on\na cascaded pipeline comprising an ASR system, disfluency removal, and GEC, with\nthe associated concern of propagating errors between these individual modules.\nIn this paper, we introduce an alternative \"end-to-end\" approach to spoken GEC,\nexploiting a speech recognition foundation model, Whisper. This foundation\nmodel can be used to replace the whole framework or part of it, e.g., ASR and\ndisfluency removal. These end-to-end approaches are compared to more standard\ncascaded approaches on the data obtained from a free-speaking spoken language\nassessment test, Linguaskill. Results demonstrate that end-to-end spoken GEC is\npossible within this architecture, but the lack of available data limits\ncurrent performance compared to a system using large quantities of text-based\nGEC data. Conversely, end-to-end disfluency detection and removal, which is\neasier for the attention-based Whisper to learn, does outperform cascaded\napproaches. Additionally, the paper discusses the challenges of providing\nfeedback to candidates when using end-to-end systems for spoken GEC.\n","authors":["Stefano Bannò","Rao Ma","Mengjie Qian","Kate M. Knill","Mark J. F. Gales"],"pdf_url":"https://arxiv.org/pdf/2311.05550v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14214v1","updated":"2024-07-19T11:19:43Z","published":"2024-07-19T11:19:43Z","title":"Domain Adaptation for Industrial Time-series Forecasting via\n  Counterfactual Inference","summary":"  Industrial time-series, as a structural data responds to production process\ninformation, can be utilized to perform data-driven decision-making for\neffective monitoring of industrial production process. However, there are some\nchallenges for time-series forecasting in industry, e.g., predicting few-shot\ncaused by data shortage, and decision-confusing caused by unknown treatment\npolicy. To cope with the problems, we propose a novel causal domain adaptation\nframework, Causal Domain Adaptation (CDA) forecaster to improve the performance\non the interested domain with limited data (target). Firstly, we analyze the\ncausality existing along with treatments, and thus ensure the shared causality\nover time. Subsequently, we propose an answer-based attention mechanism to\nachieve domain-invariant representation by the shared causality in both\ndomains. Then, a novel domain-adaptation is built to model treatments and\noutcomes jointly training on source and target domain. The main insights are\nthat our designed answer-based attention mechanism allows the target domain to\nleverage the existed causality in source time-series even with different\ntreatments, and our forecaster can predict the counterfactual outcome of\nindustrial time-series, meaning a guidance in production process. Compared with\ncommonly baselines, our method on real-world and synthetic oilfield datasets\ndemonstrates the effectiveness in across-domain prediction and the practicality\nin guiding production process\n","authors":["Chao Min","Guoquan Wen","Jiangru Yuan","Jun Yi","Xing Guo"],"pdf_url":"https://arxiv.org/pdf/2407.14214v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14211v1","updated":"2024-07-19T11:17:42Z","published":"2024-07-19T11:17:42Z","title":"Enhanced Mortality Prediction in ICU Stroke Patients via Deep Learning","summary":"  Background: Stroke is second-leading cause of disability and death among\nadults. Approximately 17 million people suffer from a stroke annually, with\nabout 85% being ischemic strokes. Predicting mortality of ischemic stroke\npatients in intensive care unit (ICU) is crucial for optimizing treatment\nstrategies, allocating resources, and improving survival rates. Methods: We\nacquired data on ICU ischemic stroke patients from MIMIC-IV database, including\ndiagnoses, vital signs, laboratory tests, medications, procedures, treatments,\nand clinical notes. Stroke patients were randomly divided into training (70%,\nn=2441), test (15%, n=523), and validation (15%, n=523) sets. To address data\nimbalances, we applied Synthetic Minority Over-sampling Technique (SMOTE). We\nselected 30 features for model development, significantly reducing feature\nnumber from 1095 used in the best study. We developed a deep learning model to\nassess mortality risk and implemented several baseline machine learning models\nfor comparison. Results: XGB-DL model, combining XGBoost for feature selection\nand deep learning, effectively minimized false positives. Model AUROC improved\nfrom 0.865 (95% CI: 0.821 - 0.905) on first day to 0.903 (95% CI: 0.868 -\n0.936) by fourth day using data from 3,646 ICU mortality patients in the\nMIMIC-IV database with 0.945 AUROC (95% CI: 0.944 - 0.947) during training.\nAlthough other ML models also performed well in terms of AUROC, we chose Deep\nLearning for its higher specificity. Conclusions: Through enhanced feature\nselection and data cleaning, proposed model demonstrates a 13% AUROC\nimprovement compared to existing models while reducing feature number from 1095\nin previous studies to 30.\n","authors":["Armin Abdollahi","Xinghong Ma","Jiahao Zhang","Daijia Wu","Tongshou Wu","Zizheng Ye","Maryam Pishgar"],"pdf_url":"https://arxiv.org/pdf/2407.14211v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14210v1","updated":"2024-07-19T11:16:02Z","published":"2024-07-19T11:16:02Z","title":"Fair Overlap Number of Balls (Fair-ONB): A Data-Morphology-based\n  Undersampling Method for Bias Reduction","summary":"  Given the magnitude of data generation currently, both in quantity and speed,\nthe use of machine learning is increasingly important. When data include\nprotected features that might give rise to discrimination, special care must be\ntaken. Data quality is critical in these cases, as biases in training data can\nbe reflected in classification models. This has devastating consequences and\nfails to comply with current regulations. Data-Centric Artificial Intelligence\nproposes dataset modifications to improve its quality. Instance selection via\nundersampling can foster balanced learning of classes and protected feature\nvalues in the classifier. When such undersampling is done close to the decision\nboundary, the effect on the classifier would be bolstered. This work proposes\nFair Overlap Number of Balls (Fair-ONB), an undersampling method that harnesses\nthe data morphology of the different data groups (obtained from the combination\nof classes and protected feature values) to perform guided undersampling in the\nareas where they overlap. It employs attributes of the ball coverage of the\ngroups, such as the radius, number of covered instances and density, to select\nthe most suitable areas for undersampling and reduce bias. Results show that\nthe Fair-ONB method reduces bias with low impact on the classifier's predictive\nperformance.\n","authors":["José Daniel Pascual-Triana","Alberto Fernández","Paulo Novais","Francisco Herrera"],"pdf_url":"https://arxiv.org/pdf/2407.14210v1.pdf","comment":"16 pages, 9 tables, 10 figures"},{"id":"http://arxiv.org/abs/2407.14209v1","updated":"2024-07-19T11:15:02Z","published":"2024-07-19T11:15:02Z","title":"Unlearning Concepts from Text-to-Video Diffusion Models","summary":"  With the advancement of computer vision and natural language processing,\ntext-to-video generation, enabled by text-to-video diffusion models, has become\nmore prevalent. These models are trained using a large amount of data from the\ninternet. However, the training data often contain copyrighted content,\nincluding cartoon character icons and artist styles, private portraits, and\nunsafe videos. Since filtering the data and retraining the model is\nchallenging, methods for unlearning specific concepts from text-to-video\ndiffusion models have been investigated. However, due to the high computational\ncomplexity and relative large optimization scale, there is little work on\nunlearning methods for text-to-video diffusion models. We propose a novel\nconcept-unlearning method by transferring the unlearning capability of the text\nencoder of text-to-image diffusion models to text-to-video diffusion models.\nSpecifically, the method optimizes the text encoder using few-shot unlearning,\nwhere several generated images are used. We then use the optimized text encoder\nin text-to-video diffusion models to generate videos. Our method costs low\ncomputation resources and has small optimization scale. We discuss the\ngenerated videos after unlearning a concept. The experiments demonstrates that\nour method can unlearn copyrighted cartoon characters, artist styles, objects\nand people's facial characteristics. Our method can unlearn a concept within\nabout 100 seconds on an RTX 3070. Since there was no concept unlearning method\nfor text-to-video diffusion models before, we make concept unlearning feasible\nand more accessible in the text-to-video domain.\n","authors":["Shiqi Liu","Yihua Tan"],"pdf_url":"https://arxiv.org/pdf/2407.14209v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.10877v7","updated":"2024-07-19T11:12:08Z","published":"2024-02-16T18:29:19Z","title":"Robust agents learn causal world models","summary":"  It has long been hypothesised that causal reasoning plays a fundamental role\nin robust and general intelligence. However, it is not known if agents must\nlearn causal models in order to generalise to new domains, or if other\ninductive biases are sufficient. We answer this question, showing that any\nagent capable of satisfying a regret bound under a large set of distributional\nshifts must have learned an approximate causal model of the data generating\nprocess, which converges to the true causal model for optimal agents. We\ndiscuss the implications of this result for several research areas including\ntransfer learning and causal inference.\n","authors":["Jonathan Richens","Tom Everitt"],"pdf_url":"https://arxiv.org/pdf/2402.10877v7.pdf","comment":"ICLR 2024 (oral). Updated agents section, new corollary"},{"id":"http://arxiv.org/abs/2407.14207v1","updated":"2024-07-19T11:12:08Z","published":"2024-07-19T11:12:08Z","title":"Longhorn: State Space Models are Amortized Online Learners","summary":"  The most fundamental capability of modern AI methods such as Large Language\nModels (LLMs) is the ability to predict the next token in a long sequence of\ntokens, known as ``sequence modeling.\" Although the Transformers model is the\ncurrent dominant approach to sequence modeling, its quadratic computational\ncost with respect to sequence length is a significant drawback. State-space\nmodels (SSMs) offer a promising alternative due to their linear decoding\nefficiency and high parallelizability during training. However, existing SSMs\noften rely on seemingly ad hoc linear recurrence designs. In this work, we\nexplore SSM design through the lens of online learning, conceptualizing SSMs as\nmeta-modules for specific online learning problems. This approach links SSM\ndesign to formulating precise online learning objectives, with state transition\nrules derived from optimizing these objectives. Based on this insight, we\nintroduce a novel deep SSM architecture based on the implicit update for\noptimizing an online regression objective. Our experimental results show that\nour models outperform state-of-the-art SSMs, including the Mamba model, on\nstandard sequence modeling benchmarks and language modeling tasks.\n","authors":["Bo Liu","Rui Wang","Lemeng Wu","Yihao Feng","Peter Stone","Qiang Liu"],"pdf_url":"https://arxiv.org/pdf/2407.14207v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14206v1","updated":"2024-07-19T11:04:54Z","published":"2024-07-19T11:04:54Z","title":"Watermark Smoothing Attacks against Language Models","summary":"  Watermarking is a technique used to embed a hidden signal in the probability\ndistribution of text generated by large language models (LLMs), enabling\nattribution of the text to the originating model. We introduce smoothing\nattacks and show that existing watermarking methods are not robust against\nminor modifications of text. An adversary can use weaker language models to\nsmooth out the distribution perturbations caused by watermarks without\nsignificantly compromising the quality of the generated text. The modified text\nresulting from the smoothing attack remains close to the distribution of text\nthat the original model (without watermark) would have produced. Our attack\nreveals a fundamental limitation of a wide range of watermarking techniques.\n","authors":["Hongyan Chang","Hamed Hassani","Reza Shokri"],"pdf_url":"https://arxiv.org/pdf/2407.14206v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14194v1","updated":"2024-07-19T10:45:36Z","published":"2024-07-19T10:45:36Z","title":"Enhancing Variable Importance in Random Forests: A Novel Application of\n  Global Sensitivity Analysis","summary":"  The present work provides an application of Global Sensitivity Analysis to\nsupervised machine learning methods such as Random Forests. These methods act\nas black boxes, selecting features in high--dimensional data sets as to provide\naccurate classifiers in terms of prediction when new data are fed into the\nsystem. In supervised machine learning, predictors are generally ranked by\nimportance based on their contribution to the final prediction. Global\nSensitivity Analysis is primarily used in mathematical modelling to investigate\nthe effect of the uncertainties of the input variables on the output. We apply\nit here as a novel way to rank the input features by their importance to the\nexplainability of the data generating process, shedding light on how the\nresponse is determined by the dependence structure of its predictors. A\nsimulation study shows that our proposal can be used to explore what advances\ncan be achieved either in terms of efficiency, explanatory ability, or simply\nby way of confirming existing results.\n","authors":["Giulia Vannucci","Roberta Siciliano","Andrea Saltelli"],"pdf_url":"https://arxiv.org/pdf/2407.14194v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.01490v2","updated":"2024-07-19T10:45:21Z","published":"2024-07-01T17:26:21Z","title":"LLM See, LLM Do: Guiding Data Generation to Target Non-Differentiable\n  Objectives","summary":"  The widespread adoption of synthetic data raises new questions about how\nmodels generating the data can influence other large language models (LLMs) via\ndistilled data. To start, our work exhaustively characterizes the impact of\npassive inheritance of model properties by systematically studying the\nconsequences of synthetic data integration. We provide one of the most\ncomprehensive studies to-date of how the source of synthetic data shapes\nmodels' internal biases, calibration and generations' textual attributes and\npreferences. We find that models are surprisingly sensitive towards certain\nattributes even when the synthetic data prompts appear \"neutral\". which invites\nthe question whether this sensitivity can be exploited for good.\n  Our findings invite the question can we explicitly steer the models towards\nthe properties we want at test time by exploiting the data generation process?\nThis would have historically been considered infeasible due to the cost of\ncollecting data with a specific characteristic or objective in mind. However,\nimprovement in the quality of synthetic data, as well as a shift towards\ngeneral-purpose models designed to follow a diverse way of instructions, means\nthis question is timely. We propose active inheritance as a term to describe\nintentionally constraining synthetic data according to a non-differentiable\nobjective. We demonstrate how active inheritance can steer the generation\nprofiles of models towards desirable non-differentiable attributes, e.g. high\nlexical diversity or low toxicity.\n","authors":["Luísa Shimabucoro","Sebastian Ruder","Julia Kreutzer","Marzieh Fadaee","Sara Hooker"],"pdf_url":"https://arxiv.org/pdf/2407.01490v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.09372v3","updated":"2024-07-19T10:44:53Z","published":"2023-08-18T08:06:49Z","title":"Which Transformer to Favor: A Comparative Analysis of Efficiency in\n  Vision Transformers","summary":"  Self-attention in Transformers comes with a high computational cost because\nof their quadratic computational complexity, but their effectiveness in\naddressing problems in language and vision has sparked extensive research aimed\nat enhancing their efficiency. However, diverse experimental conditions,\nspanning multiple input domains, prevent a fair comparison based solely on\nreported results, posing challenges for model selection. To address this gap in\ncomparability, we perform a large-scale benchmark of more than 45 models for\nimage classification, evaluating key efficiency aspects, including accuracy,\nspeed, and memory usage. Our benchmark provides a standardized baseline for\nefficiency-oriented transformers. We analyze the results based on the Pareto\nfront -- the boundary of optimal models. Surprisingly, despite claims of other\nmodels being more efficient, ViT remains Pareto optimal across multiple\nmetrics. We observe that hybrid attention-CNN models exhibit remarkable\ninference memory- and parameter-efficiency. Moreover, our benchmark shows that\nusing a larger model in general is more efficient than using higher resolution\nimages. Thanks to our holistic evaluation, we provide a centralized resource\nfor practitioners and researchers, facilitating informed decisions when\nselecting or developing efficient transformers.\n","authors":["Tobias Christian Nauen","Sebastian Palacio","Federico Raue","Andreas Dengel"],"pdf_url":"https://arxiv.org/pdf/2308.09372v3.pdf","comment":"v3: new models, analysis of scaling behaviors"},{"id":"http://arxiv.org/abs/2407.09930v2","updated":"2024-07-19T10:37:03Z","published":"2024-07-13T15:53:37Z","title":"Evaluating the Impact of Different Quantum Kernels on the Classification\n  Performance of Support Vector Machine Algorithm: A Medical Dataset\n  Application","summary":"  The support vector machine algorithm with a quantum kernel estimator\n(QSVM-Kernel), as a leading example of a quantum machine learning technique,\nhas undergone significant advancements. Nevertheless, its integration with\nclassical data presents unique challenges. While quantum computers primarily\ninteract with data in quantum states, embedding classical data into quantum\nstates using feature mapping techniques is essential for leveraging quantum\nalgorithms Despite the recognized importance of feature mapping, its specific\nimpact on data classification outcomes remains largely unexplored. This study\naddresses this gap by comprehensively assessing the effects of various feature\nmapping methods on classification results, taking medical data analysis as a\ncase study. In this study, the QSVM-Kernel method was applied to classification\nproblems in two different and publicly available medical datasets, namely, the\nWisconsin Breast Cancer (original) and The Cancer Genome Atlas (TCGA) Glioma\ndatasets. In the QSVM-Kernel algorithm, quantum kernel matrices obtained from 9\ndifferent quantum feature maps were used. Thus, the effects of these quantum\nfeature maps on the classification results of the QSVM-Kernel algorithm were\nexamined in terms of both classifier performance and total execution time. As a\nresult, in the Wisconsin Breast Cancer (original) and TCGA Glioma datasets,\nwhen Rx and Ry rotational gates were used, respectively, as feature maps in the\nQSVM-Kernel algorithm, the best classification performances were achieved both\nin terms of classification performance and total execution time. The\ncontributions of this study are that (1) it highlights the significant impact\nof feature mapping techniques on medical data classification outcomes using the\nQSVM-Kernel algorithm, and (2) it also guides undertaking research for improved\nQSVM classification performance.\n","authors":["Emine Akpinar","Sardar M. N. Islam","Murat Oduncuoglu"],"pdf_url":"https://arxiv.org/pdf/2407.09930v2.pdf","comment":"10 pages, 2 figures, 1 table"},{"id":"http://arxiv.org/abs/2407.14185v1","updated":"2024-07-19T10:29:00Z","published":"2024-07-19T10:29:00Z","title":"Achieving Well-Informed Decision-Making in Drug Discovery: A\n  Comprehensive Calibration Study using Neural Network-Based Structure-Activity\n  Models","summary":"  In the drug discovery process, where experiments can be costly and\ntime-consuming, computational models that predict drug-target interactions are\nvaluable tools to accelerate the development of new therapeutic agents.\nEstimating the uncertainty inherent in these neural network predictions\nprovides valuable information that facilitates optimal decision-making when\nrisk assessment is crucial. However, such models can be poorly calibrated,\nwhich results in unreliable uncertainty estimates that do not reflect the true\npredictive uncertainty. In this study, we compare different metrics, including\naccuracy and calibration scores, used for model hyperparameter tuning to\ninvestigate which model selection strategy achieves well-calibrated models.\nFurthermore, we propose to use a computationally efficient Bayesian uncertainty\nestimation method named Bayesian Linear Probing (BLP), which generates\nHamiltonian Monte Carlo (HMC) trajectories to obtain samples for the parameters\nof a Bayesian Logistic Regression fitted to the hidden layer of the baseline\nneural network. We report that BLP improves model calibration and achieves the\nperformance of common uncertainty quantification methods by combining the\nbenefits of uncertainty estimation and probability calibration methods.\nFinally, we show that combining post hoc calibration method with\nwell-performing uncertainty quantification approaches can boost model accuracy\nand calibration.\n","authors":["Hannah Rosa Friesacher","Ola Engkvist","Lewis Mervin","Yves Moreau","Adam Arany"],"pdf_url":"https://arxiv.org/pdf/2407.14185v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14175v1","updated":"2024-07-19T10:06:01Z","published":"2024-07-19T10:06:01Z","title":"On Policy Evaluation Algorithms in Distributional Reinforcement Learning","summary":"  We introduce a novel class of algorithms to efficiently approximate the\nunknown return distributions in policy evaluation problems from distributional\nreinforcement learning (DRL). The proposed distributional dynamic programming\nalgorithms are suitable for underlying Markov decision processes (MDPs) having\nan arbitrary probabilistic reward mechanism, including continuous reward\ndistributions with unbounded support being potentially heavy-tailed.\n  For a plain instance of our proposed class of algorithms we prove error\nbounds, both within Wasserstein and Kolmogorov--Smirnov distances. Furthermore,\nfor return distributions having probability density functions the algorithms\nyield approximations for these densities; error bounds are given within\nsupremum norm. We introduce the concept of quantile-spline discretizations to\ncome up with algorithms showing promising results in simulation experiments.\n  While the performance of our algorithms can rigorously be analysed they can\nbe seen as universal black box algorithms applicable to a large class of MDPs.\nWe also derive new properties of probability metrics commonly used in DRL on\nwhich our quantitative analysis is based.\n","authors":["Julian Gerstenberg","Ralph Neininger","Denis Spiegel"],"pdf_url":"https://arxiv.org/pdf/2407.14175v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14166v1","updated":"2024-07-19T09:52:18Z","published":"2024-07-19T09:52:18Z","title":"On Maximum Entropy Linear Feature Inversion","summary":"  We revisit the classical problem of inverting dimension-reducing linear\nmappings using the maximum entropy (MaxEnt) criterion. In the literature,\nsolutions are problem-dependent, inconsistent, and use different entropy\nmeasures. We propose a new unified approach that not only specializes to the\nexisting approaches, but offers solutions to new cases, such as when data\nvalues are constrained to [0, 1], which has new applications in machine\nlearning.\n","authors":["Paul M Baggenstoss"],"pdf_url":"https://arxiv.org/pdf/2407.14166v1.pdf","comment":"Submitted IEEE-Signal Processing Letters"},{"id":"http://arxiv.org/abs/2310.02027v4","updated":"2024-07-19T09:51:59Z","published":"2023-10-03T13:10:14Z","title":"DeepHGCN: Toward Deeper Hyperbolic Graph Convolutional Networks","summary":"  Hyperbolic graph convolutional networks (HGCNs) have demonstrated significant\npotential in extracting information from hierarchical graphs. However, existing\nHGCNs are limited to shallow architectures due to the computational expense of\nhyperbolic operations and the issue of over-smoothing as depth increases.\nAlthough treatments have been applied to alleviate over-smoothing in GCNs,\ndeveloping a hyperbolic solution presents distinct challenges since operations\nmust be carefully designed to fit the hyperbolic nature. Addressing these\nchallenges, we propose DeepHGCN, the first deep multi-layer HGCN architecture\nwith dramatically improved computational efficiency and substantially reduced\nover-smoothing. DeepHGCN features two key innovations: (1) a novel hyperbolic\nfeature transformation layer that enables fast and accurate linear mappings,\nand (2) techniques such as hyperbolic residual connections and regularization\nfor both weights and features, facilitated by an efficient hyperbolic midpoint\nmethod. Extensive experiments demonstrate that DeepHGCN achieves significant\nimprovements in link prediction and node classification tasks compared to both\nEuclidean and shallow hyperbolic GCN variants.\n","authors":["Jiaxu Liu","Xinping Yi","Xiaowei Huang"],"pdf_url":"https://arxiv.org/pdf/2310.02027v4.pdf","comment":"14 pages including appendix and reference"},{"id":"http://arxiv.org/abs/2403.14200v2","updated":"2024-07-19T09:50:51Z","published":"2024-03-21T07:50:45Z","title":"Debiasing surgeon: fantastic weights and how to find them","summary":"  Nowadays an ever-growing concerning phenomenon, the emergence of algorithmic\nbiases that can lead to unfair models, emerges. Several debiasing approaches\nhave been proposed in the realm of deep learning, employing more or less\nsophisticated approaches to discourage these models from massively employing\nthese biases. However, a question emerges: is this extra complexity really\nnecessary? Is a vanilla-trained model already embodying some ``unbiased\nsub-networks'' that can be used in isolation and propose a solution without\nrelying on the algorithmic biases? In this work, we show that such a\nsub-network typically exists, and can be extracted from a vanilla-trained model\nwithout requiring additional training. We further validate that such specific\narchitecture is incapable of learning a specific bias, suggesting that there\nare possible architectural countermeasures to the problem of biases in deep\nneural networks.\n","authors":["Rémi Nahon","Ivan Luiz De Moura Matos","Van-Tam Nguyen","Enzo Tartaglione"],"pdf_url":"https://arxiv.org/pdf/2403.14200v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14158v1","updated":"2024-07-19T09:42:20Z","published":"2024-07-19T09:42:20Z","title":"Machine learning emulation of precipitation from km-scale regional\n  climate simulations using a diffusion model","summary":"  High-resolution climate simulations are very valuable for understanding\nclimate change impacts and planning adaptation measures. This has motivated use\nof regional climate models at sufficiently fine resolution to capture important\nsmall-scale atmospheric processes, such as convective storms. However, these\nregional models have very high computational costs, limiting their\napplicability. We present CPMGEM, a novel application of a generative machine\nlearning model, a diffusion model, to skilfully emulate precipitation\nsimulations from such a high-resolution model over England and Wales at much\nlower cost. This emulator enables stochastic generation of high-resolution\n(8.8km), daily-mean precipitation samples conditioned on coarse-resolution\n(60km) weather states from a global climate model. The output is fine enough\nfor use in applications such as flood inundation modelling. The emulator\nproduces precipitation predictions with realistic intensities and spatial\nstructures and captures most of the 21st century climate change signal. We show\nevidence that the emulator has skill for extreme events up to and including\n1-in-100 year intensities. Potential applications include producing\nhigh-resolution precipitation predictions for large-ensemble climate\nsimulations and downscaling different climate models and climate change\nscenarios to better sample uncertainty in climate changes at local-scale.\n","authors":["Henry Addison","Elizabeth Kendon","Suman Ravuri","Laurence Aitchison","Peter AG Watson"],"pdf_url":"https://arxiv.org/pdf/2407.14158v1.pdf","comment":"29 pages, 11 figures, 3 tables"},{"id":"http://arxiv.org/abs/2402.04298v3","updated":"2024-07-19T09:37:48Z","published":"2024-02-06T15:53:49Z","title":"Multi-View Symbolic Regression","summary":"  Symbolic regression (SR) searches for analytical expressions representing the\nrelationship between a set of explanatory and response variables. Current SR\nmethods assume a single dataset extracted from a single experiment.\nNevertheless, frequently, the researcher is confronted with multiple sets of\nresults obtained from experiments conducted with different setups. Traditional\nSR methods may fail to find the underlying expression since the parameters of\neach experiment can be different. In this work we present Multi-View Symbolic\nRegression (MvSR), which takes into account multiple datasets simultaneously,\nmimicking experimental environments, and outputs a general parametric solution.\nThis approach fits the evaluated expression to each independent dataset and\nreturns a parametric family of functions f(x; theta) simultaneously capable of\naccurately fitting all datasets. We demonstrate the effectiveness of MvSR using\ndata generated from known expressions, as well as real-world data from\nastronomy, chemistry and economy, for which an a priori analytical expression\nis not available. Results show that MvSR obtains the correct expression more\nfrequently and is robust to hyperparameters change. In real-world data, it is\nable to grasp the group behavior, recovering known expressions from the\nliterature as well as promising alternatives, thus enabling the use of SR to a\nlarge range of experimental scenarios.\n","authors":["Etienne Russeil","Fabrício Olivetti de França","Konstantin Malanchev","Bogdan Burlacu","Emille E. O. Ishida","Marion Leroux","Clément Michelin","Guillaume Moinard","Emmanuel Gangler"],"pdf_url":"https://arxiv.org/pdf/2402.04298v3.pdf","comment":"Accepted to GECCO-2024. 11 pages, 5 figures"},{"id":"http://arxiv.org/abs/2312.04985v5","updated":"2024-07-19T09:37:19Z","published":"2023-12-08T11:47:35Z","title":"SparQ Attention: Bandwidth-Efficient LLM Inference","summary":"  The computational difficulties of large language model (LLM) inference remain\na significant obstacle to their widespread deployment. The need for many\napplications to support long input sequences and process them in large batches\ntypically causes token-generation to be bottlenecked by data transfer. For this\nreason, we introduce SparQ Attention, a technique for increasing the inference\nthroughput of LLMs by utilising memory bandwidth more efficiently within the\nattention layers, through selective fetching of the cached history. Our\nproposed technique can be applied directly to off-the-shelf LLMs during\ninference, without requiring any modification to the pre-training setup or\nadditional fine-tuning. We show that SparQ Attention brings up to 8x savings in\nattention data transfers without substantial drops in accuracy, by evaluating\nLlama 2 and 3, Mistral, Gemma and Pythia models on a wide range of downstream\ntasks.\n","authors":["Luka Ribar","Ivan Chelombiev","Luke Hudlass-Galley","Charlie Blake","Carlo Luschi","Douglas Orr"],"pdf_url":"https://arxiv.org/pdf/2312.04985v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14154v1","updated":"2024-07-19T09:34:04Z","published":"2024-07-19T09:34:04Z","title":"Where is the Testbed for my Federated Learning Research?","summary":"  Progressing beyond centralized AI is of paramount importance, yet,\ndistributed AI solutions, in particular various federated learning (FL)\nalgorithms, are often not comprehensively assessed, which prevents the research\ncommunity from identifying the most promising approaches and practitioners from\nbeing convinced that a certain solution is deployment-ready. The largest hurdle\ntowards FL algorithm evaluation is the difficulty of conducting real-world\nexperiments over a variety of FL client devices and different platforms, with\ndifferent datasets and data distribution, all while assessing various\ndimensions of algorithm performance, such as inference accuracy, energy\nconsumption, and time to convergence, to name a few. In this paper, we present\nCoLExT, a real-world testbed for FL research. CoLExT is designed to streamline\nexperimentation with custom FL algorithms in a rich testbed configuration\nspace, with a large number of heterogeneous edge devices, ranging from\nsingle-board computers to smartphones, and provides real-time collection and\nvisualization of a variety of metrics through automatic instrumentation.\nAccording to our evaluation, porting FL algorithms to CoLExT requires minimal\ninvolvement from the developer, and the instrumentation introduces minimal\nresource usage overhead. Furthermore, through an initial investigation\ninvolving popular FL algorithms running on CoLExT, we reveal previously unknown\ntrade-offs, inefficiencies, and programming bugs.\n","authors":["Janez Božič","Amândio R. Faustino","Boris Radovič","Marco Canini","Veljko Pejović"],"pdf_url":"https://arxiv.org/pdf/2407.14154v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14151v1","updated":"2024-07-19T09:29:38Z","published":"2024-07-19T09:29:38Z","title":"A Comparative Study of Deep Reinforcement Learning Models: DQN vs PPO vs\n  A2C","summary":"  This study conducts a comparative analysis of three advanced Deep\nReinforcement Learning models: Deep Q-Networks (DQN), Proximal Policy\nOptimization (PPO), and Advantage Actor-Critic (A2C), within the BreakOut Atari\ngame environment. Our research assesses the performance and effectiveness of\nthese models in a controlled setting. Through rigorous experimentation, we\nexamine each model's learning efficiency, strategy development, and\nadaptability under dynamic game conditions. The findings provide critical\ninsights into the practical applications of these models in game-based learning\nenvironments and contribute to the broader understanding of their capabilities.\nThe code is publicly available at github.com/Neilus03/DRL_comparative_study.\n","authors":["Neil De La Fuente","Daniel A. Vidal Guerra"],"pdf_url":"https://arxiv.org/pdf/2407.14151v1.pdf","comment":"8 pages, Accepted at KDD 2024"},{"id":"http://arxiv.org/abs/2404.06997v2","updated":"2024-07-19T09:28:54Z","published":"2024-04-10T13:24:27Z","title":"Agent-driven Generative Semantic Communication with Cross-Modality and\n  Prediction","summary":"  In the era of 6G, with compelling visions of intelligent transportation\nsystems and digital twins, remote surveillance is poised to become a ubiquitous\npractice. Substantial data volume and frequent updates present challenges in\nwireless networks. To address these challenges, we propose a novel agent-driven\ngenerative semantic communication (A-GSC) framework based on reinforcement\nlearning. In contrast to the existing research on semantic communication\n(SemCom), which mainly focuses on either semantic extraction or semantic\nsampling, we seamlessly integrate both by jointly considering the intrinsic\nattributes of source information and the contextual information regarding the\ntask. Notably, the introduction of generative artificial intelligence (GAI)\nenables the independent design of semantic encoders and decoders. In this work,\nwe develop an agent-assisted semantic encoder with cross-modality capability,\nwhich can track the semantic changes, channel condition, to perform adaptive\nsemantic extraction and sampling. Accordingly, we design a semantic decoder\nwith both predictive and generative capabilities, consisting of two tailored\nmodules. Moreover, the effectiveness of the designed models has been verified\nusing the UA-DETRAC dataset, demonstrating the performance gains of the overall\nA-GSC framework in both energy saving and reconstruction accuracy.\n","authors":["Wanting Yang","Zehui Xiong","Yanli Yuan","Wenchao Jiang","Tony Q. S. Quek","Merouane Debbah"],"pdf_url":"https://arxiv.org/pdf/2404.06997v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.02769v2","updated":"2024-07-19T09:26:30Z","published":"2024-02-05T07:05:17Z","title":"Learning from Teaching Regularization: Generalizable Correlations Should\n  be Easy to Imitate","summary":"  Generalization remains a central challenge in machine learning. In this work,\nwe propose Learning from Teaching (LoT), a novel regularization technique for\ndeep neural networks to enhance generalization. Inspired by the human ability\nto capture concise and abstract patterns, we hypothesize that generalizable\ncorrelations are expected to be easier to imitate. LoT operationalizes this\nconcept to improve generalization of the main model with auxiliary student\nlearners. The student learners are trained by the main model and, in turn,\nprovide feedback to help the main model capture more generalizable and imitable\ncorrelations. Our experimental results across several domains, including\nComputer Vision, Natural Language Processing, and methodologies like\nReinforcement Learning, demonstrate that the introduction of LoT brings\nsignificant benefits compared to training models on the original dataset. The\nresults suggest the effectiveness and efficiency of LoT in identifying\ngeneralizable information at the right scales while discarding spurious data\ncorrelations, thus making LoT a valuable addition to current machine learning.\nCode is available at https://github.com/jincan333/LoT.\n","authors":["Can Jin","Tong Che","Hongwu Peng","Yiyuan Li","Dimitris N. Metaxas","Marco Pavone"],"pdf_url":"https://arxiv.org/pdf/2402.02769v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.12231v5","updated":"2024-07-19T09:21:04Z","published":"2024-02-19T15:36:36Z","title":"Diffusion Tempering Improves Parameter Estimation with Probabilistic\n  Integrators for Ordinary Differential Equations","summary":"  Ordinary differential equations (ODEs) are widely used to describe dynamical\nsystems in science, but identifying parameters that explain experimental\nmeasurements is challenging. In particular, although ODEs are differentiable\nand would allow for gradient-based parameter optimization, the nonlinear\ndynamics of ODEs often lead to many local minima and extreme sensitivity to\ninitial conditions. We therefore propose diffusion tempering, a novel\nregularization technique for probabilistic numerical methods which improves\nconvergence of gradient-based parameter optimization in ODEs. By iteratively\nreducing a noise parameter of the probabilistic integrator, the proposed method\nconverges more reliably to the true parameters. We demonstrate that our method\nis effective for dynamical systems of different complexity and show that it\nobtains reliable parameter estimates for a Hodgkin-Huxley model with a\npractically relevant number of parameters.\n","authors":["Jonas Beck","Nathanael Bosch","Michael Deistler","Kyra L. Kadhim","Jakob H. Macke","Philipp Hennig","Philipp Berens"],"pdf_url":"https://arxiv.org/pdf/2402.12231v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14143v1","updated":"2024-07-19T09:20:33Z","published":"2024-07-19T09:20:33Z","title":"Class-Incremental Learning with CLIP: Adaptive Representation Adjustment\n  and Parameter Fusion","summary":"  Class-incremental learning is a challenging problem, where the goal is to\ntrain a model that can classify data from an increasing number of classes over\ntime. With the advancement of vision-language pre-trained models such as CLIP,\nthey demonstrate good generalization ability that allows them to excel in\nclass-incremental learning with completely frozen parameters. However, further\nadaptation to downstream tasks by simply fine-tuning the model leads to severe\nforgetting. Most existing works with pre-trained models assume that the\nforgetting of old classes is uniform when the model acquires new knowledge. In\nthis paper, we propose a method named Adaptive Representation Adjustment and\nParameter Fusion (RAPF). During training for new data, we measure the influence\nof new classes on old ones and adjust the representations, using textual\nfeatures. After training, we employ a decomposed parameter fusion to further\nmitigate forgetting during adapter module fine-tuning. Experiments on several\nconventional benchmarks show that our method achieves state-of-the-art results.\nOur code is available at \\url{https://github.com/linlany/RAPF}.\n","authors":["Linlan Huang","Xusheng Cao","Haori Lu","Xialei Liu"],"pdf_url":"https://arxiv.org/pdf/2407.14143v1.pdf","comment":"Accepted at ECCV 2024"},{"id":"http://arxiv.org/abs/2407.14142v1","updated":"2024-07-19T09:19:29Z","published":"2024-07-19T09:19:29Z","title":"Early Preparation Pays Off: New Classifier Pre-tuning for Class\n  Incremental Semantic Segmentation","summary":"  Class incremental semantic segmentation aims to preserve old knowledge while\nlearning new tasks, however, it is impeded by catastrophic forgetting and\nbackground shift issues. Prior works indicate the pivotal importance of\ninitializing new classifiers and mainly focus on transferring knowledge from\nthe background classifier or preparing classifiers for future classes,\nneglecting the flexibility and variance of new classifiers. In this paper, we\npropose a new classifier pre-tuning~(NeST) method applied before the formal\ntraining process, learning a transformation from old classifiers to generate\nnew classifiers for initialization rather than directly tuning the parameters\nof new classifiers. Our method can make new classifiers align with the backbone\nand adapt to the new data, preventing drastic changes in the feature extractor\nwhen learning new classes. Besides, we design a strategy considering the\ncross-task class similarity to initialize matrices used in the transformation,\nhelping achieve the stability-plasticity trade-off. Experiments on Pascal VOC\n2012 and ADE20K datasets show that the proposed strategy can significantly\nimprove the performance of previous methods. The code is available at\n\\url{https://github.com/zhengyuan-xie/ECCV24_NeST}.\n","authors":["Zhengyuan Xie","Haiquan Lu","Jia-wen Xiao","Enguang Wang","Le Zhang","Xialei Liu"],"pdf_url":"https://arxiv.org/pdf/2407.14142v1.pdf","comment":"Accepted at ECCV 2024"},{"id":"http://arxiv.org/abs/2407.14129v1","updated":"2024-07-19T08:59:00Z","published":"2024-07-19T08:59:00Z","title":"Comparing and Contrasting Deep Learning Weather Prediction Backbones on\n  Navier-Stokes and Atmospheric Dynamics","summary":"  Remarkable progress in the development of Deep Learning Weather Prediction\n(DLWP) models positions them to become competitive with traditional numerical\nweather prediction (NWP) models. Indeed, a wide number of DLWP architectures --\nbased on various backbones, including U-Net, Transformer, Graph Neural Network\n(GNN), and Fourier Neural Operator (FNO) -- have demonstrated their potential\nat forecasting atmospheric states. However, due to differences in training\nprotocols, forecast horizons, and data choices, it remains unclear which (if\nany) of these methods and architectures are most suitable for weather\nforecasting and for future model development. Here, we step back and provide a\ndetailed empirical analysis, under controlled conditions, comparing and\ncontrasting the most prominent DLWP models, along with their backbones. We\naccomplish this by predicting synthetic two-dimensional incompressible\nNavier-Stokes and real-world global weather dynamics. In terms of accuracy,\nmemory consumption, and runtime, our results illustrate various tradeoffs. For\nexample, on synthetic data, we observe favorable performance of FNO; and on the\nreal-world WeatherBench dataset, our results demonstrate the suitability of\nConvLSTM and SwinTransformer for short-to-mid-ranged forecasts. For long-ranged\nweather rollouts of up to 365 days, we observe superior stability and physical\nsoundness in architectures that formulate a spherical data representation,\ni.e., GraphCast and Spherical FNO. In addition, we observe that all of these\nmodel backbones ``saturate,'' i.e., none of them exhibit so-called neural\nscaling, which highlights an important direction for future work on these and\nrelated models.\n","authors":["Matthias Karlbauer","Danielle C. Maddix","Abdul Fatir Ansari","Boran Han","Gaurav Gupta","Yuyang Wang","Andrew Stuart","Michael W. Mahoney"],"pdf_url":"https://arxiv.org/pdf/2407.14129v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.04519v2","updated":"2024-07-19T08:53:18Z","published":"2023-10-06T18:28:33Z","title":"SPADE: Sparsity-Guided Debugging for Deep Neural Networks","summary":"  It is known that sparsity can improve interpretability for deep neural\nnetworks. However, existing methods in the area either require networks that\nare pre-trained with sparsity constraints, or impose sparsity after the fact,\naltering the network's general behavior. In this paper, we demonstrate, for the\nfirst time, that sparsity can instead be incorporated into the interpretation\nprocess itself, as a sample-specific preprocessing step. Unlike previous work,\nthis approach, which we call SPADE, does not place constraints on the trained\nmodel and does not affect its behavior during inference on the sample. Given a\ntrained model and a target sample, SPADE uses sample-targeted pruning to\nprovide a \"trace\" of the network's execution on the sample, reducing the\nnetwork to the most important connections prior to computing an interpretation.\nWe demonstrate that preprocessing with SPADE significantly increases the\naccuracy of image saliency maps across several interpretability methods.\nAdditionally, SPADE improves the usefulness of neuron visualizations, aiding\nhumans in reasoning about network behavior. Our code is available at\nhttps://github.com/IST-DASLab/SPADE.\n","authors":["Arshia Soltani Moakhar","Eugenia Iofinova","Elias Frantar","Dan Alistarh"],"pdf_url":"https://arxiv.org/pdf/2310.04519v2.pdf","comment":"Published at ICML 2024. 33 pages"},{"id":"http://arxiv.org/abs/2402.07613v2","updated":"2024-07-19T08:50:31Z","published":"2024-02-12T12:38:20Z","title":"Global optimality under amenable symmetry constraints","summary":"  Consider a convex function that is invariant under an group of\ntransformations. If it has a minimizer, does it also have an invariant\nminimizer? Variants of this problem appear in nonparametric statistics and in a\nnumber of adjacent fields. The answer depends on the choice of function, and on\nwhat one may loosely call the geometry of the problem -- the interplay between\nconvexity, the group, and the underlying vector space, which is typically\ninfinite-dimensional. We observe that this geometry is completely encoded in\nthe smallest closed convex invariant subsets of the space, and proceed to study\nthese sets, for groups that are amenable but not necessarily compact. We then\napply this toolkit to the invariant optimality problem. It yields new results\non invariant kernel mean embeddings and risk-optimal invariant couplings, and\nclarifies relations between seemingly distinct ideas, such as the summation\ntrick used in machine learning to construct equivariant neural networks and the\nclassic Hunt-Stein theorem of statistics.\n","authors":["Peter Orbanz"],"pdf_url":"https://arxiv.org/pdf/2402.07613v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14119v1","updated":"2024-07-19T08:36:25Z","published":"2024-07-19T08:36:25Z","title":"Shape and Style GAN-based Multispectral Data Augmentation for Crop/Weed\n  Segmentation in Precision Farming","summary":"  The use of deep learning methods for precision farming is gaining increasing\ninterest. However, collecting training data in this application field is\nparticularly challenging and costly due to the need of acquiring information\nduring the different growing stages of the cultivation of interest. In this\npaper, we present a method for data augmentation that uses two GANs to create\nartificial images to augment the training data. To obtain a higher image\nquality, instead of re-creating the entire scene, we take original images and\nreplace only the patches containing objects of interest with artificial ones\ncontaining new objects with different shapes and styles. In doing this, we take\ninto account both the foreground (i.e., crop samples) and the background (i.e.,\nthe soil) of the patches. Quantitative experiments, conducted on publicly\navailable datasets, demonstrate the effectiveness of the proposed approach. The\nsource code and data discussed in this work are available as open source.\n","authors":["Mulham Fawakherji","Vincenzo Suriani","Daniele Nardi","Domenico Daniele Bloisi"],"pdf_url":"https://arxiv.org/pdf/2407.14119v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14116v1","updated":"2024-07-19T08:33:07Z","published":"2024-07-19T08:33:07Z","title":"AuditNet: A Conversational AI-based Security Assistant [DEMO]","summary":"  In the age of information overload, professionals across various fields face\nthe challenge of navigating vast amounts of documentation and ever-evolving\nstandards. Ensuring compliance with standards, regulations, and contractual\nobligations is a critical yet complex task across various professional fields.\nWe propose a versatile conversational AI assistant framework designed to\nfacilitate compliance checking on the go, in diverse domains, including but not\nlimited to network infrastructure, legal contracts, educational standards,\nenvironmental regulations, and government policies. By leveraging\nretrieval-augmented generation using large language models, our framework\nautomates the review, indexing, and retrieval of relevant, context-aware\ninformation, streamlining the process of verifying adherence to established\nguidelines and requirements. This AI assistant not only reduces the manual\neffort involved in compliance checks but also enhances accuracy and efficiency,\nsupporting professionals in maintaining high standards of practice and ensuring\nregulatory compliance in their respective fields. We propose and demonstrate\nAuditNet, the first conversational AI security assistant designed to assist IoT\nnetwork security experts by providing instant access to security standards,\npolicies, and regulations.\n","authors":["Shohreh Deldari","Mohammad Goudarzi","Aditya Joshi","Arash Shaghaghi","Simon Finn","Flora D. Salim","Sanjay Jha"],"pdf_url":"https://arxiv.org/pdf/2407.14116v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14111v1","updated":"2024-07-19T08:29:12Z","published":"2024-07-19T08:29:12Z","title":"A Mirror Descent-Based Algorithm for Corruption-Tolerant Distributed\n  Gradient Descent","summary":"  Distributed gradient descent algorithms have come to the fore in modern\nmachine learning, especially in parallelizing the handling of large datasets\nthat are distributed across several workers. However, scant attention has been\npaid to analyzing the behavior of distributed gradient descent algorithms in\nthe presence of adversarial corruptions instead of random noise. In this paper,\nwe formulate a novel problem in which adversarial corruptions are present in a\ndistributed learning system. We show how to use ideas from (lazy) mirror\ndescent to design a corruption-tolerant distributed optimization algorithm.\nExtensive convergence analysis for (strongly) convex loss functions is provided\nfor different choices of the stepsize. We carefully optimize the stepsize\nschedule to accelerate the convergence of the algorithm, while at the same time\namortizing the effect of the corruption over time. Experiments based on linear\nregression, support vector classification, and softmax classification on the\nMNIST dataset corroborate our theoretical findings.\n","authors":["Shuche Wang","Vincent Y. F. Tan"],"pdf_url":"https://arxiv.org/pdf/2407.14111v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14106v1","updated":"2024-07-19T08:21:42Z","published":"2024-07-19T08:21:42Z","title":"TorchGT: A Holistic System for Large-scale Graph Transformer Training","summary":"  Graph Transformer is a new architecture that surpasses GNNs in graph\nlearning. While there emerge inspiring algorithm advancements, their practical\nadoption is still limited, particularly on real-world graphs involving up to\nmillions of nodes. We observe existing graph transformers fail on large-scale\ngraphs mainly due to heavy computation, limited scalability and inferior model\nquality. Motivated by these observations, we propose TorchGT, the first\nefficient, scalable, and accurate graph transformer training system. TorchGT\noptimizes training at different levels. At algorithm level, by harnessing the\ngraph sparsity, TorchGT introduces a Dual-interleaved Attention which is\ncomputation-efficient and accuracy-maintained. At runtime level, TorchGT scales\ntraining across workers with a communication-light Cluster-aware Graph\nParallelism. At kernel level, an Elastic Computation Reformation further\noptimizes the computation by reducing memory access latency in a dynamic way.\nExtensive experiments demonstrate that TorchGT boosts training by up to 62.7x\nand supports graph sequence lengths of up to 1M.\n","authors":["Meng Zhang","Jie Sun","Qinghao Hu","Peng Sun","Zeke Wang","Yonggang Wen","Tianwei Zhang"],"pdf_url":"https://arxiv.org/pdf/2407.14106v1.pdf","comment":"Proceedings of the International Conference for High Performance\n  Computing, Networking, Storage and Analysis (SC), 2024"},{"id":"http://arxiv.org/abs/2407.14100v1","updated":"2024-07-19T08:12:41Z","published":"2024-07-19T08:12:41Z","title":"ParamsDrag: Interactive Parameter Space Exploration via Image-Space\n  Dragging","summary":"  Numerical simulation serves as a cornerstone in scientific modeling, yet the\nprocess of fine-tuning simulation parameters poses significant challenges.\nConventionally, parameter adjustment relies on extensive numerical simulations,\ndata analysis, and expert insights, resulting in substantial computational\ncosts and low efficiency. The emergence of deep learning in recent years has\nprovided promising avenues for more efficient exploration of parameter spaces.\nHowever, existing approaches often lack intuitive methods for precise parameter\nadjustment and optimization. To tackle these challenges, we introduce\nParamsDrag, a model that facilitates parameter space exploration through direct\ninteraction with visualizations. Inspired by DragGAN, our ParamsDrag model\noperates in three steps. First, the generative component of ParamsDrag\ngenerates visualizations based on the input simulation parameters. Second, by\ndirectly dragging structure-related features in the visualizations, users can\nintuitively understand the controlling effect of different parameters. Third,\nwith the understanding from the earlier step, users can steer ParamsDrag to\nproduce dynamic visual outcomes. Through experiments conducted on real-world\nsimulations and comparisons with state-of-the-art deep learning-based\napproaches, we demonstrate the efficacy of our solution.\n","authors":["Guan Li","Yang Liu","Guihua Shan","Shiyu Cheng","Weiqun Cao","Junpeng Wang","Ko-Chih Wang"],"pdf_url":"https://arxiv.org/pdf/2407.14100v1.pdf","comment":"To be published in Proc. IEEE VIS 2024"},{"id":"http://arxiv.org/abs/2303.15845v3","updated":"2024-07-19T08:11:11Z","published":"2023-03-28T09:36:14Z","title":"Conditional Generative Models are Provably Robust: Pointwise Guarantees\n  for Bayesian Inverse Problems","summary":"  Conditional generative models became a very powerful tool to sample from\nBayesian inverse problem posteriors. It is well-known in classical Bayesian\nliterature that posterior measures are quite robust with respect to\nperturbations of both the prior measure and the negative log-likelihood, which\nincludes perturbations of the observations. However, to the best of our\nknowledge, the robustness of conditional generative models with respect to\nperturbations of the observations has not been investigated yet. In this paper,\nwe prove for the first time that appropriately learned conditional generative\nmodels provide robust results for single observations.\n","authors":["Fabian Altekrüger","Paul Hagemann","Gabriele Steidl"],"pdf_url":"https://arxiv.org/pdf/2303.15845v3.pdf","comment":"Accepted and published in Transactions on Machine Learning Research\n  (07/2023)"},{"id":"http://arxiv.org/abs/2407.14097v1","updated":"2024-07-19T08:08:17Z","published":"2024-07-19T08:08:17Z","title":"On the Robustness of Fully-Spiking Neural Networks in Open-World\n  Scenarios using Forward-Only Learning Algorithms","summary":"  In the last decade, Artificial Intelligence (AI) models have rapidly\nintegrated into production pipelines propelled by their excellent modeling\nperformance. However, the development of these models has not been matched by\nadvancements in algorithms ensuring their safety, failing to guarantee robust\nbehavior against Out-of-Distribution (OoD) inputs outside their learning\ndomain. Furthermore, there is a growing concern with the sustainability of AI\nmodels and their required energy consumption in both training and inference\nphases. To mitigate these issues, this work explores the use of the\nForward-Forward Algorithm (FFA), a biologically plausible alternative to\nBackpropagation, adapted to the spiking domain to enhance the overall energy\nefficiency of the model. By capitalizing on the highly expressive topology\nemerging from the latent space of models trained with FFA, we develop a novel\nFF-SCP algorithm for OoD Detection. Our approach measures the likelihood of a\nsample belonging to the in-distribution (ID) data by using the distance from\nthe latent representation of samples to class-representative manifolds.\nAdditionally, to provide deeper insights into our OoD pipeline, we propose a\ngradient-free attribution technique that highlights the features of a sample\npushing it away from the distribution of any class. Multiple experiments using\nour spiking FFA adaptation demonstrate that the achieved accuracy levels are\ncomparable to those seen in analog networks trained via back-propagation.\nFurthermore, OoD detection experiments on multiple datasets prove that FF-SCP\noutperforms avant-garde OoD detectors within the spiking domain in terms of\nseveral metrics used in this area. We also present a qualitative analysis of\nour explainability technique, exposing the precision by which the method\ndetects OoD features, such as embedded artifacts or missing regions.\n","authors":["Erik B. Terres-Escudero","Javier Del Ser","Aitor Martínez-Seras","Pablo Garcia-Bringas"],"pdf_url":"https://arxiv.org/pdf/2407.14097v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14094v1","updated":"2024-07-19T07:58:26Z","published":"2024-07-19T07:58:26Z","title":"User-Creator Feature Dynamics in Recommender Systems with Dual Influence","summary":"  Recommender systems present relevant contents to users and help content\ncreators reach their target audience. The dual nature of these systems\ninfluences both users and creators: users' preferences are affected by the\nitems they are recommended, while creators are incentivized to alter their\ncontents such that it is recommended more frequently. We define a model, called\nuser-creator feature dynamics, to capture the dual influences of recommender\nsystems. We prove that a recommender system with dual influence is guaranteed\nto polarize, causing diversity loss in the system. We then investigate, both\ntheoretically and empirically, approaches for mitigating polarization and\npromoting diversity in recommender systems. Unexpectedly, we find that common\ndiversity-promoting approaches do not work in the presence of dual influence,\nwhile relevancy-optimizing methods like top-$k$ recommendation can prevent\npolarization and improve diversity of the system.\n","authors":["Tao Lin","Kun Jin","Andrew Estornell","Xiaoying Zhang","Yiling Chen","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2407.14094v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.12833v2","updated":"2024-07-19T07:38:35Z","published":"2024-07-03T15:41:54Z","title":"ESQA: Event Sequences Question Answering","summary":"  Event sequences (ESs) arise in many practical domains including finance,\nretail, social networks, and healthcare. In the context of machine learning,\nevent sequences can be seen as a special type of tabular data with annotated\ntimestamps. Despite the importance of ESs modeling and analysis, little effort\nwas made in adapting large language models (LLMs) to the ESs domain. In this\npaper, we highlight the common difficulties of ESs processing and propose a\nnovel solution capable of solving multiple downstream tasks with little or no\nfinetuning. In particular, we solve the problem of working with long sequences\nand improve time and numeric features processing. The resulting method, called\nESQA, effectively utilizes the power of LLMs and, according to extensive\nexperiments, achieves state-of-the-art results in the ESs domain.\n","authors":["Irina Abdullaeva","Andrei Filatov","Mikhail Orlov","Ivan Karpukhin","Viacheslav Vasilev","Denis Dimitrov","Andrey Kuznetsov","Ivan Kireev","Andrey Savchenko"],"pdf_url":"https://arxiv.org/pdf/2407.12833v2.pdf","comment":"25 pages, 3 figures"},{"id":"http://arxiv.org/abs/2407.14081v1","updated":"2024-07-19T07:31:32Z","published":"2024-07-19T07:31:32Z","title":"DisenSemi: Semi-supervised Graph Classification via Disentangled\n  Representation Learning","summary":"  Graph classification is a critical task in numerous multimedia applications,\nwhere graphs are employed to represent diverse types of multimedia data,\nincluding images, videos, and social networks. Nevertheless, in real-world\nscenarios, labeled graph data can be limited or scarce. To address this issue,\nwe focus on the problem of semi-supervised graph classification, which involves\nboth supervised and unsupervised models learning from labeled and unlabeled\ndata. In contrast to recent approaches that transfer the entire knowledge from\nthe unsupervised model to the supervised one, we argue that an effective\ntransfer should only retain the relevant semantics that align well with the\nsupervised task. In this paper, we propose a novel framework named DisenSemi,\nwhich learns disentangled representation for semi-supervised graph\nclassification. Specifically, a disentangled graph encoder is proposed to\ngenerate factor-wise graph representations for both supervised and unsupervised\nmodels. Then we train two models via supervised objective and mutual\ninformation (MI)-based constraints respectively. To ensure the meaningful\ntransfer of knowledge from the unsupervised encoder to the supervised one, we\nfurther define an MI-based disentangled consistency regularization between two\nmodels and identify the corresponding rationale that aligns well with the\ncurrent graph classification task. Experimental results on a range of publicly\naccessible datasets reveal the effectiveness of our DisenSemi.\n","authors":["Yifan Wang","Xiao Luo","Chong Chen","Xian-Sheng Hua","Ming Zhang","Wei Ju"],"pdf_url":"https://arxiv.org/pdf/2407.14081v1.pdf","comment":"Accepted by IEEE Transactions on Neural Networks and Learning Systems\n  (TNNLS 2024)"},{"id":"http://arxiv.org/abs/2407.12370v2","updated":"2024-07-19T07:27:14Z","published":"2024-07-17T07:46:53Z","title":"Temporal receptive field in dynamic graph learning: A comprehensive\n  analysis","summary":"  Dynamic link prediction is a critical task in the analysis of evolving\nnetworks, with applications ranging from recommender systems to economic\nexchanges. However, the concept of the temporal receptive field, which refers\nto the temporal context that models use for making predictions, has been\nlargely overlooked and insufficiently analyzed in existing research. In this\nstudy, we present a comprehensive analysis of the temporal receptive field in\ndynamic graph learning. By examining multiple datasets and models, we formalize\nthe role of temporal receptive field and highlight their crucial influence on\npredictive accuracy. Our results demonstrate that appropriately chosen temporal\nreceptive field can significantly enhance model performance, while for some\nmodels, overly large windows may introduce noise and reduce accuracy. We\nconduct extensive benchmarking to validate our findings, ensuring that all\nexperiments are fully reproducible. Code is available at\nhttps://github.com/ykrmm/BenchmarkTW .\n","authors":["Yannis Karmim","Leshanshui Yang","Raphaël Fournier S'Niehotta","Clément Chatelain","Sébastien Adam","Nicolas Thome"],"pdf_url":"https://arxiv.org/pdf/2407.12370v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14076v1","updated":"2024-07-19T07:12:43Z","published":"2024-07-19T07:12:43Z","title":"Domain-Specific Pretraining of Language Models: A Comparative Study in\n  the Medical Field","summary":"  There are many cases where LLMs are used for specific tasks in a single\ndomain. These usually require less general, but more domain-specific knowledge.\nHighly capable, general-purpose state-of-the-art language models like GPT-4 or\nClaude-3-opus can often be used for such tasks, but they are very large and\ncannot be run locally, even if they were not proprietary. This can be a problem\nwhen working with sensitive data. This paper focuses on domain-specific and\nmixed-domain pretraining as potentially more efficient methods than general\npretraining for specialized language models. We will take a look at work\nrelated to domain-specific pretraining, specifically in the medical area, and\ncompare benchmark results of specialized language models to general-purpose\nlanguage models.\n","authors":["Tobias Kerner"],"pdf_url":"https://arxiv.org/pdf/2407.14076v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14066v1","updated":"2024-07-19T06:50:24Z","published":"2024-07-19T06:50:24Z","title":"360VFI: A Dataset and Benchmark for Omnidirectional Video Frame\n  Interpolation","summary":"  With the development of VR-related techniques, viewers can enjoy a realistic\nand immersive experience through a head-mounted display, while omnidirectional\nvideo with a low frame rate can lead to user dizziness. However, the prevailing\nplane frame interpolation methodologies are unsuitable for Omnidirectional\nVideo Interpolation, chiefly due to the lack of models tailored to such videos\nwith strong distortion, compounded by the scarcity of valuable datasets for\nOmnidirectional Video Frame Interpolation. In this paper, we introduce the\nbenchmark dataset, 360VFI, for Omnidirectional Video Frame Interpolation. We\npresent a practical implementation that introduces a distortion prior from\nomnidirectional video into the network to modulate distortions. We especially\npropose a pyramid distortion-sensitive feature extractor that uses the unique\ncharacteristics of equirectangular projection (ERP) format as prior\ninformation. Moreover, we devise a decoder that uses an affine transformation\nto facilitate the synthesis of intermediate frames further. 360VFI is the first\ndataset and benchmark that explores the challenge of Omnidirectional Video\nFrame Interpolation. Through our benchmark analysis, we presented four\ndifferent distortion conditions scenes in the proposed 360VFI dataset to\nevaluate the challenge triggered by distortion during interpolation. Besides,\nexperimental results demonstrate that Omnidirectional Video Interpolation can\nbe effectively improved by modeling for omnidirectional distortion.\n","authors":["Wenxuan Lu","Mengshun Hu","Yansheng Qiu","Liang Liao","Zheng Wang"],"pdf_url":"https://arxiv.org/pdf/2407.14066v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2302.03453,\n  arXiv:1804.02815 by other authors"},{"id":"http://arxiv.org/abs/2407.14065v1","updated":"2024-07-19T06:42:41Z","published":"2024-07-19T06:42:41Z","title":"MSCT: Addressing Time-Varying Confounding with Marginal Structural\n  Causal Transformer for Counterfactual Post-Crash Traffic Prediction","summary":"  Traffic crashes profoundly impede traffic efficiency and pose economic\nchallenges. Accurate prediction of post-crash traffic status provides essential\ninformation for evaluating traffic perturbations and developing effective\nsolutions. Previous studies have established a series of deep learning models\nto predict post-crash traffic conditions, however, these correlation-based\nmethods cannot accommodate the biases caused by time-varying confounders and\nthe heterogeneous effects of crashes. The post-crash traffic prediction model\nneeds to estimate the counterfactual traffic speed response to hypothetical\ncrashes under various conditions, which demonstrates the necessity of\nunderstanding the causal relationship between traffic factors. Therefore, this\npaper presents the Marginal Structural Causal Transformer (MSCT), a novel deep\nlearning model designed for counterfactual post-crash traffic prediction. To\naddress the issue of time-varying confounding bias, MSCT incorporates a\nstructure inspired by Marginal Structural Models and introduces a balanced loss\nfunction to facilitate learning of invariant causal features. The proposed\nmodel is treatment-aware, with a specific focus on comprehending and predicting\ntraffic speed under hypothetical crash intervention strategies. In the absence\nof ground-truth data, a synthetic data generation procedure is proposed to\nemulate the causal mechanism between traffic speed, crashes, and covariates.\nThe model is validated using both synthetic and real-world data, demonstrating\nthat MSCT outperforms state-of-the-art models in multi-step-ahead prediction\nperformance. This study also systematically analyzes the impact of time-varying\nconfounding bias and dataset distribution on model performance, contributing\nvaluable insights into counterfactual prediction for intelligent transportation\nsystems.\n","authors":["Shuang Li","Ziyuan Pu","Nan Zhang","Duxin Chen","Lu Dong","Daniel J. Graham","Yinhai Wang"],"pdf_url":"https://arxiv.org/pdf/2407.14065v1.pdf","comment":"13 pages, 9 figures"},{"id":"http://arxiv.org/abs/2407.14058v1","updated":"2024-07-19T06:35:49Z","published":"2024-07-19T06:35:49Z","title":"On the Causal Sufficiency and Necessity of Multi-Modal Representation\n  Learning","summary":"  An effective paradigm of multi-modal learning (MML) is to learn unified\nrepresentations among modalities. From a causal perspective, constraining the\nconsistency between different modalities can mine causal representations that\nconvey primary events. However, such simple consistency may face the risk of\nlearning insufficient or unnecessary information: a necessary but insufficient\ncause is invariant across modalities but may not have the required accuracy; a\nsufficient but unnecessary cause tends to adapt well to specific modalities but\nmay be hard to adapt to new data. To address this issue, in this paper, we aim\nto learn representations that are both causal sufficient and necessary, i.e.,\nCausal Complete Cause ($C^3$), for MML. Firstly, we define the concept of $C^3$\nfor MML, which reflects the probability of being causal sufficiency and\nnecessity. We also propose the identifiability and measurement of $C^3$, i.e.,\n$C^3$ risk, to ensure calculating the learned representations' $C^3$ scores in\npractice. Then, we theoretically prove the effectiveness of $C^3$ risk by\nestablishing the performance guarantee of MML with a tight generalization\nbound. Based on these theoretical results, we propose a plug-and-play method,\nnamely Causal Complete Cause Regularization ($C^3$R), to learn causal complete\nrepresentations by constraining the $C^3$ risk bound. Extensive experiments\nconducted on various benchmark datasets empirically demonstrate the\neffectiveness of $C^3$R.\n","authors":["Jingyao Wang","Wenwen Qiang","Jiangmeng Li","Lingyu Si","Changwen Zheng","Bing Su"],"pdf_url":"https://arxiv.org/pdf/2407.14058v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14057v1","updated":"2024-07-19T06:34:45Z","published":"2024-07-19T06:34:45Z","title":"LazyLLM: Dynamic Token Pruning for Efficient Long Context LLM Inference","summary":"  The inference of transformer-based large language models consists of two\nsequential stages: 1) a prefilling stage to compute the KV cache of prompts and\ngenerate the first token, and 2) a decoding stage to generate subsequent\ntokens. For long prompts, the KV cache must be computed for all tokens during\nthe prefilling stage, which can significantly increase the time needed to\ngenerate the first token. Consequently, the prefilling stage may become a\nbottleneck in the generation process. An open question remains whether all\nprompt tokens are essential for generating the first token. To answer this, we\nintroduce a novel method, LazyLLM, that selectively computes the KV for tokens\nimportant for the next token prediction in both the prefilling and decoding\nstages. Contrary to static pruning approaches that prune the prompt at once,\nLazyLLM allows language models to dynamically select different subsets of\ntokens from the context in different generation steps, even though they might\nbe pruned in previous steps. Extensive experiments on standard datasets across\nvarious tasks demonstrate that LazyLLM is a generic method that can be\nseamlessly integrated with existing language models to significantly accelerate\nthe generation without fine-tuning. For instance, in the multi-document\nquestion-answering task, LazyLLM accelerates the prefilling stage of the LLama\n2 7B model by 2.34x while maintaining accuracy.\n","authors":["Qichen Fu","Minsik Cho","Thomas Merth","Sachin Mehta","Mohammad Rastegari","Mahyar Najibi"],"pdf_url":"https://arxiv.org/pdf/2407.14057v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14056v1","updated":"2024-07-19T06:33:10Z","published":"2024-07-19T06:33:10Z","title":"Rasa: Building Expressive Speech Synthesis Systems for Indian Languages\n  in Low-resource Settings","summary":"  We release Rasa, the first multilingual expressive TTS dataset for any Indian\nlanguage, which contains 10 hours of neutral speech and 1-3 hours of expressive\nspeech for each of the 6 Ekman emotions covering 3 languages: Assamese,\nBengali, & Tamil. Our ablation studies reveal that just 1 hour of neutral and\n30 minutes of expressive data can yield a Fair system as indicated by MUSHRA\nscores. Increasing neutral data to 10 hours, with minimal expressive data,\nsignificantly enhances expressiveness. This offers a practical recipe for\nresource-constrained languages, prioritizing easily obtainable neutral data\nalongside smaller amounts of expressive data. We show the importance of\nsyllabically balanced data and pooling emotions to enhance expressiveness. We\nalso highlight challenges in generating specific emotions, e.g., fear and\nsurprise.\n","authors":["Praveen Srinivasa Varadhan","Ashwin Sankar","Giri Raju","Mitesh M. Khapra"],"pdf_url":"https://arxiv.org/pdf/2407.14056v1.pdf","comment":"Accepted at INTERSPEECH 2024. First two authors listed contributed\n  equally"},{"id":"http://arxiv.org/abs/2407.14055v1","updated":"2024-07-19T06:31:22Z","published":"2024-07-19T06:31:22Z","title":"Quantum Hamiltonian Embedding of Images for Data Reuploading Classifiers","summary":"  When applying quantum computing to machine learning tasks, one of the first\nconsiderations is the design of the quantum machine learning model itself.\nConventionally, the design of quantum machine learning algorithms relies on the\n``quantisation\" of classical learning algorithms, such as using quantum linear\nalgebra to implement important subroutines of classical algorithms, if not the\nentire algorithm, seeking to achieve quantum advantage through possible\nrun-time accelerations brought by quantum computing. However, recent research\nhas started questioning whether quantum advantage via speedup is the right goal\nfor quantum machine learning [1]. Research also has been undertaken to exploit\nproperties that are unique to quantum systems, such as quantum contextuality,\nto better design quantum machine learning models [2]. In this paper, we take an\nalternative approach by incorporating the heuristics and empirical evidences\nfrom the design of classical deep learning algorithms to the design of quantum\nneural networks. We first construct a model based on the data reuploading\ncircuit [3] with the quantum Hamiltonian data embedding unitary [4]. Through\nnumerical experiments on images datasets, including the famous MNIST and\nFashionMNIST datasets, we demonstrate that our model outperforms the quantum\nconvolutional neural network (QCNN)[5] by a large margin (up to over 40% on\nMNIST test set). Based on the model design process and numerical results, we\nthen laid out six principles for designing quantum machine learning models,\nespecially quantum neural networks.\n","authors":["Peiyong Wang","Casey R. Myers","Lloyd C. L. Hollenberg","Udaya Parampalli"],"pdf_url":"https://arxiv.org/pdf/2407.14055v1.pdf","comment":"11 figures, 31 pages. Code available on\n  https://github.com/peiyong-addwater/HamEmbedding"},{"id":"http://arxiv.org/abs/2407.13703v2","updated":"2024-07-19T05:57:16Z","published":"2024-06-26T08:59:49Z","title":"Energy-Efficient Channel Decoding for Wireless Federated Learning:\n  Convergence Analysis and Adaptive Design","summary":"  One of the most critical challenges for deploying distributed learning\nsolutions, such as federated learning (FL), in wireless networks is the limited\nbattery capacity of mobile clients. While it is a common belief that the major\nenergy consumption of mobile clients comes from the uplink data transmission,\nthis paper presents a novel finding, namely the channel decoding operation also\ncontributes significantly to the overall energy consumption of mobile clients\nin FL. Motivated by this new observation, we propose an energy-efficient\nadaptive channel decoding scheme that leverages the intrinsic robustness of FL\nto model errors. In particular, the robustness is exploited to reduce the\nenergy consumption of channel decoders at mobile clients by adaptively\nadjusting the number of decoding iterations. We theoretically prove that\nwireless FL with communication errors can converge at the same rate as the case\nwith error-free communication as long as the bit error rate (BER) is properly\nconstrained. An adaptive channel decoding scheme is then proposed to improve\nthe energy efficiency of wireless FL systems. Experimental results demonstrate\nthat the proposed method maintains the same learning accuracy while reducing\nthe channel decoding energy consumption by 20% when compared to existing\napproaches.\n","authors":["Linping Qu","Yuyi Mao","Shenghui Song","Chi-Ying Tsui"],"pdf_url":"https://arxiv.org/pdf/2407.13703v2.pdf","comment":"This work has been submitted to the IEEE TWC for possible\n  publication. Copyright may be transferred without notice, after which this\n  version may no longer be accessible"},{"id":"http://arxiv.org/abs/2407.14040v1","updated":"2024-07-19T05:34:08Z","published":"2024-07-19T05:34:08Z","title":"Generative Language Model for Catalyst Discovery","summary":"  Discovery of novel and promising materials is a critical challenge in the\nfield of chemistry and material science, traditionally approached through\nmethodologies ranging from trial-and-error to machine learning-driven inverse\ndesign. Recent studies suggest that transformer-based language models can be\nutilized as material generative models to expand chemical space and explore\nmaterials with desired properties. In this work, we introduce the Catalyst\nGenerative Pretrained Transformer (CatGPT), trained to generate string\nrepresentations of inorganic catalyst structures from a vast chemical space.\nCatGPT not only demonstrates high performance in generating valid and accurate\ncatalyst structures but also serves as a foundation model for generating\ndesired types of catalysts by fine-tuning with sparse and specified datasets.\nAs an example, we fine-tuned the pretrained CatGPT using a binary alloy\ncatalyst dataset designed for screening two-electron oxygen reduction reaction\n(2e-ORR) catalyst and generate catalyst structures specialized for 2e-ORR. Our\nwork demonstrates the potential of language models as generative tools for\ncatalyst discovery.\n","authors":["Dong Hyeon Mok","Seoin Back"],"pdf_url":"https://arxiv.org/pdf/2407.14040v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14039v1","updated":"2024-07-19T05:33:09Z","published":"2024-07-19T05:33:09Z","title":"BERTer: The Efficient One","summary":"  We explore advanced fine-tuning techniques to boost BERT's performance in\nsentiment analysis, paraphrase detection, and semantic textual similarity. Our\napproach leverages SMART regularization to combat overfitting, improves\nhyperparameter choices, employs a cross-embedding Siamese architecture for\nimproved sentence embeddings, and introduces innovative early exiting methods.\nOur fine-tuning findings currently reveal substantial improvements in model\nefficiency and effectiveness when combining multiple fine-tuning architectures,\nachieving a state-of-the-art performance score of on the test set, surpassing\ncurrent benchmarks and highlighting BERT's adaptability in multifaceted\nlinguistic tasks.\n","authors":["Pradyumna Saligram","Andrew Lanpouthakoun"],"pdf_url":"https://arxiv.org/pdf/2407.14039v1.pdf","comment":"10 pages, 4 figures"},{"id":"http://arxiv.org/abs/2404.14047v2","updated":"2024-07-19T05:31:53Z","published":"2024-04-22T10:03:03Z","title":"An Empirical Study of LLaMA3 Quantization: From LLMs to MLLMs","summary":"  The LLaMA family has become one of the most powerful open-source Large\nLanguage Models (LLMs) and the popular LLM backbones of Multimodal Large\nLanguage Models (MLLMs), widely applied in Computer Vision (CV) and Natural\nLanguage Understanding (NLU) tasks. Notably, LLaMA3 models have recently been\nreleased and achieve impressive performance across various with super-large\nscale pre-training on over 15T tokens of data. Given the wide application of\nlow-bit quantization for LLMs in resource-limited scenarios, we explore\nLLaMA3's capabilities when quantized to low bit-width. This exploration can\npotentially unveil new insights and challenges for low-bit quantization of\nLLaMA3 and other forthcoming LLMs, especially in addressing performance\ndegradation problems that suffer in LLM compression. Specifically, we\ncomprehensively evaluate the 10 existing post-training quantization and\nLoRA-finetuning methods of LLaMA3 on 1-8 bits and diverse datasets to reveal\nLLaMA3's low-bit quantization performance. To uncover the capabilities of\nlow-bit quantized MLLM, we assessed the performance of the LLaMA3-based\nLLaVA-Next-8B model under 2-4 ultra-low bits with post-training quantization\nmethods. Our experimental results indicate that LLaMA3 still suffers\nnon-negligent degradation in linguistic and visual contexts, particularly under\nultra-low bit widths. This highlights the significant performance gap under low\nbit-width that needs to be bridged in future developments. We expect that this\nempirical study will prove valuable in advancing future models, driving LLMs\nand MLLMs to achieve higher accuracy at lower bit to enhance practicality.\n","authors":["Wei Huang","Xingyu Zheng","Xudong Ma","Haotong Qin","Chengtao Lv","Hong Chen","Jie Luo","Xiaojuan Qi","Xianglong Liu","Michele Magno"],"pdf_url":"https://arxiv.org/pdf/2404.14047v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.12870v2","updated":"2024-07-19T05:26:06Z","published":"2024-07-14T04:41:16Z","title":"Revisiting Adaptive Cellular Recognition Under Domain Shifts: A\n  Contextual Correspondence View","summary":"  Cellular nuclei recognition serves as a fundamental and essential step in the\nworkflow of digital pathology. However, with disparate source organs and\nstaining procedures among histology image clusters, the scanned tiles\ninherently conform to a non-uniform data distribution, which induces\ndeteriorated promises for general cross-cohort usages. Despite the latest\nefforts leveraging domain adaptation to mitigate distributional discrepancy,\nthose methods are subjected to modeling the morphological characteristics of\neach cell individually, disregarding the hierarchical latent structure and\nintrinsic contextual correspondences across the tumor micro-environment. In\nthis work, we identify the importance of implicit correspondences across\nbiological contexts for exploiting domain-invariant pathological composition\nand thereby propose to exploit the dependence over various biological\nstructures for domain adaptive cellular recognition. We discover those\nhigh-level correspondences via unsupervised contextual modeling and use them as\nbridges to facilitate adaptation over diverse organs and stains. In addition,\nto further exploit the rich spatial contexts embedded amongst nuclear\ncommunities, we propose self-adaptive dynamic distillation to secure\ninstance-aware trade-offs across different model constituents. The proposed\nmethod is extensively evaluated on a broad spectrum of cross-domain settings\nunder miscellaneous data distribution shifts and outperforms the\nstate-of-the-art methods by a substantial margin. Code is available at\nhttps://github.com/camwew/CellularRecognition_DA_CC.\n","authors":["Jianan Fan","Dongnan Liu","Canran Li","Hang Chang","Heng Huang","Filip Braet","Mei Chen","Weidong Cai"],"pdf_url":"https://arxiv.org/pdf/2407.12870v2.pdf","comment":"ECCV 2024 main conference"},{"id":"http://arxiv.org/abs/2310.05990v2","updated":"2024-07-19T05:23:28Z","published":"2023-10-08T04:54:12Z","title":"Cross-Task Data Augmentation by Pseudo-label Generation for Region Based\n  Coronary Artery Instance Segmentation","summary":"  Coronary Artery Diseases (CADs) although preventable, are one of the leading\ncauses of death and disability. Diagnosis of these diseases is often difficult\nand resource intensive. Angiographic imaging segmentation of the arteries has\nevolved as a tool of assistance that helps clinicians make an accurate\ndiagnosis. However, due to the limited amount of data and the difficulty in\ncurating a dataset, the task of segmentation has proven challenging. In this\nstudy, we introduce the use of pseudo-labels to address the issue of limited\ndata in the angiographic dataset to enhance the performance of the baseline\nYOLO model. Unlike existing data augmentation techniques that improve the model\nconstrained to a fixed dataset, we introduce the use of pseudo-labels generated\non a dataset of separate related task to diversify and improve model\nperformance. This method increases the baseline F1 score by 9% in the\nvalidation data set and by 3% in the test data set.\n","authors":["Sandesh Pokhrel","Sanjay Bhandari","Eduard Vazquez","Yash Raj Shrestha","Binod Bhattarai"],"pdf_url":"https://arxiv.org/pdf/2310.05990v2.pdf","comment":"arXiv admin note: text overlap with arXiv:2310.04749"},{"id":"http://arxiv.org/abs/2407.14030v1","updated":"2024-07-19T05:04:24Z","published":"2024-07-19T05:04:24Z","title":"HeCiX: Integrating Knowledge Graphs and Large Language Models for\n  Biomedical Research","summary":"  Despite advancements in drug development strategies, 90% of clinical trials\nfail. This suggests overlooked aspects in target validation and drug\noptimization. In order to address this, we introduce HeCiX-KG,\nHetionet-Clinicaltrials neXus Knowledge Graph, a novel fusion of data from\nClinicalTrials.gov and Hetionet in a single knowledge graph. HeCiX-KG combines\ndata on previously conducted clinical trials from ClinicalTrials.gov, and\ndomain expertise on diseases and genes from Hetionet. This offers a thorough\nresource for clinical researchers. Further, we introduce HeCiX, a system that\nuses LangChain to integrate HeCiX-KG with GPT-4, and increase its usability.\nHeCiX shows high performance during evaluation against a range of clinically\nrelevant issues, proving this model to be promising for enhancing the\neffectiveness of clinical research. Thus, this approach provides a more\nholistic view of clinical trials and existing biological data.\n","authors":["Prerana Sanjay Kulkarni","Muskaan Jain","Disha Sheshanarayana","Srinivasan Parthiban"],"pdf_url":"https://arxiv.org/pdf/2407.14030v1.pdf","comment":"8 pages, 3 figures, under review"},{"id":"http://arxiv.org/abs/2407.14029v1","updated":"2024-07-19T05:03:16Z","published":"2024-07-19T05:03:16Z","title":"PASS++: A Dual Bias Reduction Framework for Non-Exemplar\n  Class-Incremental Learning","summary":"  Class-incremental learning (CIL) aims to recognize new classes incrementally\nwhile maintaining the discriminability of old classes. Most existing CIL\nmethods are exemplar-based, i.e., storing a part of old data for retraining.\nWithout relearning old data, those methods suffer from catastrophic forgetting.\nIn this paper, we figure out two inherent problems in CIL, i.e., representation\nbias and classifier bias, that cause catastrophic forgetting of old knowledge.\nTo address these two biases, we present a simple and novel dual bias reduction\nframework that employs self-supervised transformation (SST) in input space and\nprototype augmentation (protoAug) in deep feature space. On the one hand, SST\nalleviates the representation bias by learning generic and diverse\nrepresentations that can transfer across different tasks. On the other hand,\nprotoAug overcomes the classifier bias by explicitly or implicitly augmenting\nprototypes of old classes in the deep feature space, which poses tighter\nconstraints to maintain previously learned decision boundaries. We further\npropose hardness-aware prototype augmentation and multi-view ensemble\nstrategies, leading to significant improvements. The proposed framework can be\neasily integrated with pre-trained models. Without storing any samples of old\nclasses, our method can perform comparably with state-of-the-art exemplar-based\napproaches which store plenty of old data. We hope to draw the attention of\nresearchers back to non-exemplar CIL by rethinking the necessity of storing old\nsamples in CIL.\n","authors":["Fei Zhu","Xu-Yao Zhang","Zhen Cheng","Cheng-Lin Liu"],"pdf_url":"https://arxiv.org/pdf/2407.14029v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14022v1","updated":"2024-07-19T04:46:58Z","published":"2024-07-19T04:46:58Z","title":"Causal Inference with Complex Treatments: A Survey","summary":"  Causal inference plays an important role in explanatory analysis and decision\nmaking across various fields like statistics, marketing, health care, and\neducation. Its main task is to estimate treatment effects and make intervention\npolicies. Traditionally, most of the previous works typically focus on the\nbinary treatment setting that there is only one treatment for a unit to adopt\nor not. However, in practice, the treatment can be much more complex,\nencompassing multi-valued, continuous, or bundle options. In this paper, we\nrefer to these as complex treatments and systematically and comprehensively\nreview the causal inference methods for addressing them. First, we formally\nrevisit the problem definition, the basic assumptions, and their possible\nvariations under specific conditions. Second, we sequentially review the\nrelated methods for multi-valued, continuous, and bundled treatment settings.\nIn each situation, we tentatively divide the methods into two categories: those\nconforming to the unconfoundedness assumption and those violating it.\nSubsequently, we discuss the available datasets and open-source codes. Finally,\nwe provide a brief summary of these works and suggest potential directions for\nfuture research.\n","authors":["Yingrong Wang","Haoxuan Li","Minqin Zhu","Anpeng Wu","Ruoxuan Xiong","Fei Wu","Kun Kuang"],"pdf_url":"https://arxiv.org/pdf/2407.14022v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14020v1","updated":"2024-07-19T04:42:52Z","published":"2024-07-19T04:42:52Z","title":"NeuroBind: Towards Unified Multimodal Representations for Neural Signals","summary":"  Understanding neural activity and information representation is crucial for\nadvancing knowledge of brain function and cognition. Neural activity, measured\nthrough techniques like electrophysiology and neuroimaging, reflects various\naspects of information processing. Recent advances in deep neural networks\noffer new approaches to analyzing these signals using pre-trained models.\nHowever, challenges arise due to discrepancies between different neural signal\nmodalities and the limited scale of high-quality neural data. To address these\nchallenges, we present NeuroBind, a general representation that unifies\nmultiple brain signal types, including EEG, fMRI, calcium imaging, and spiking\ndata. To achieve this, we align neural signals in these image-paired neural\ndatasets to pre-trained vision-language embeddings. Neurobind is the first\nmodel that studies different neural modalities interconnectedly and is able to\nleverage high-resource modality models for various neuroscience tasks. We also\nshowed that by combining information from different neural signal modalities,\nNeuroBind enhances downstream performance, demonstrating the effectiveness of\nthe complementary strengths of different neural modalities. As a result, we can\nleverage multiple types of neural signals mapped to the same space to improve\ndownstream tasks, and demonstrate the complementary strengths of different\nneural modalities. This approach holds significant potential for advancing\nneuroscience research, improving AI systems, and developing neuroprosthetics\nand brain-computer interfaces.\n","authors":["Fengyu Yang","Chao Feng","Daniel Wang","Tianye Wang","Ziyao Zeng","Zhiyang Xu","Hyoungseob Park","Pengliang Ji","Hanbin Zhao","Yuanning Li","Alex Wong"],"pdf_url":"https://arxiv.org/pdf/2407.14020v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07955v2","updated":"2024-07-19T04:31:38Z","published":"2024-03-12T07:24:17Z","title":"Towards Faithful Explanations: Boosting Rationalization with Shortcuts\n  Discovery","summary":"  The remarkable success in neural networks provokes the selective\nrationalization. It explains the prediction results by identifying a small\nsubset of the inputs sufficient to support them. Since existing methods still\nsuffer from adopting the shortcuts in data to compose rationales and limited\nlarge-scale annotated rationales by human, in this paper, we propose a\nShortcuts-fused Selective Rationalization (SSR) method, which boosts the\nrationalization by discovering and exploiting potential shortcuts.\nSpecifically, SSR first designs a shortcuts discovery approach to detect\nseveral potential shortcuts. Then, by introducing the identified shortcuts, we\npropose two strategies to mitigate the problem of utilizing shortcuts to\ncompose rationales. Finally, we develop two data augmentations methods to close\nthe gap in the number of annotated rationales. Extensive experimental results\non real-world datasets clearly validate the effectiveness of our proposed\nmethod.\n","authors":["Linan Yue","Qi Liu","Yichao Du","Li Wang","Weibo Gao","Yanqing An"],"pdf_url":"https://arxiv.org/pdf/2403.07955v2.pdf","comment":"Accepted to ICLR 2024"},{"id":"http://arxiv.org/abs/2312.15566v4","updated":"2024-07-19T04:29:39Z","published":"2023-12-24T23:34:01Z","title":"Deep Copula-Based Survival Analysis for Dependent Censoring with\n  Identifiability Guarantees","summary":"  Censoring is the central problem in survival analysis where either the\ntime-to-event (for instance, death), or the time-tocensoring (such as loss of\nfollow-up) is observed for each sample. The majority of existing machine\nlearning-based survival analysis methods assume that survival is conditionally\nindependent of censoring given a set of covariates; an assumption that cannot\nbe verified since only marginal distributions is available from the data. The\nexistence of dependent censoring, along with the inherent bias in current\nestimators has been demonstrated in a variety of applications, accentuating the\nneed for a more nuanced approach. However, existing methods that adjust for\ndependent censoring require practitioners to specify the ground truth copula.\nThis requirement poses a significant challenge for practical applications, as\nmodel misspecification can lead to substantial bias. In this work, we propose a\nflexible deep learning-based survival analysis method that simultaneously\naccommodate for dependent censoring and eliminates the requirement for\nspecifying the ground truth copula. We theoretically prove the identifiability\nof our model under a broad family of copulas and survival distributions.\nExperiments results from a wide range of datasets demonstrate that our approach\nsuccessfully discerns the underlying dependency structure and significantly\nreduces survival estimation bias when compared to existing methods.\n","authors":["Weijia Zhang","Chun Kai Ling","Xuanhui Zhang"],"pdf_url":"https://arxiv.org/pdf/2312.15566v4.pdf","comment":"To appear in AAAI 2024"},{"id":"http://arxiv.org/abs/2402.09025v5","updated":"2024-07-19T04:13:59Z","published":"2024-02-14T09:01:13Z","title":"SLEB: Streamlining LLMs through Redundancy Verification and Elimination\n  of Transformer Blocks","summary":"  Large language models (LLMs) have proven to be highly effective across\nvarious natural language processing tasks. However, their large number of\nparameters poses significant challenges for practical deployment. Pruning, a\ntechnique aimed at reducing the size and complexity of LLMs, offers a potential\nsolution by removing redundant components from the network. Despite the promise\nof pruning, existing methods often struggle to achieve substantial end-to-end\nLLM inference speedup. In this paper, we introduce SLEB, a novel approach\ndesigned to streamline LLMs by eliminating redundant transformer blocks. We\nchoose the transformer block as the fundamental unit for pruning, because LLMs\nexhibit block-level redundancy with high similarity between the outputs of\nneighboring blocks. This choice allows us to effectively enhance the processing\nspeed of LLMs. Our experimental results demonstrate that SLEB outperforms\nprevious LLM pruning methods in accelerating LLM inference while also\nmaintaining superior perplexity and accuracy, making SLEB as a promising\ntechnique for enhancing the efficiency of LLMs. The code is available at:\nhttps://github.com/jiwonsong-dev/SLEB.\n","authors":["Jiwon Song","Kyungseok Oh","Taesu Kim","Hyungjun Kim","Yulhwa Kim","Jae-Joon Kim"],"pdf_url":"https://arxiv.org/pdf/2402.09025v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.01473v2","updated":"2024-07-19T04:06:19Z","published":"2023-11-01T06:55:09Z","title":"Adversarial Examples in the Physical World: A Survey","summary":"  Deep neural networks (DNNs) have demonstrated high vulnerability to\nadversarial examples, raising broad security concerns about their applications.\nBesides the attacks in the digital world, the practical implications of\nadversarial examples in the physical world present significant challenges and\nsafety concerns. However, current research on physical adversarial examples\n(PAEs) lacks a comprehensive understanding of their unique characteristics,\nleading to limited significance and understanding. In this paper, we address\nthis gap by thoroughly examining the characteristics of PAEs within a practical\nworkflow encompassing training, manufacturing, and re-sampling processes. By\nanalyzing the links between physical adversarial attacks, we identify\nmanufacturing and re-sampling as the primary sources of distinct attributes and\nparticularities in PAEs. Leveraging this knowledge, we develop a comprehensive\nanalysis and classification framework for PAEs based on their specific\ncharacteristics, covering over 100 studies on physical-world adversarial\nexamples. Furthermore, we investigate defense strategies against PAEs and\nidentify open challenges and opportunities for future research. We aim to\nprovide a fresh, thorough, and systematic understanding of PAEs, thereby\npromoting the development of robust adversarial learning and its application in\nopen-world scenarios to provide the community with a continuously updated list\nof physical world adversarial sample resources, including papers, code, \\etc,\nwithin the proposed framework\n","authors":["Jiakai Wang","Xianglong Liu","Jin Hu","Donghua Wang","Siyang Wu","Tingsong Jiang","Yuanfang Guo","Aishan Liu","Aishan Liu","Jiantao Zhou"],"pdf_url":"https://arxiv.org/pdf/2311.01473v2.pdf","comment":"Adversarial examples, physical-world scenarios, attacks and defenses"},{"id":"http://arxiv.org/abs/2407.14008v1","updated":"2024-07-19T03:45:27Z","published":"2024-07-19T03:45:27Z","title":"Investigating the Indirect Object Identification circuit in Mamb","summary":"  How well will current interpretability techniques generalize to future\nmodels? A relevant case study is Mamba, a recent recurrent architecture with\nscaling comparable to Transformers. We adapt pre-Mamba techniques to Mamba and\npartially reverse-engineer the circuit responsible for the Indirect Object\nIdentification (IOI) task. Our techniques provide evidence that 1) Layer 39 is\na key bottleneck, 2) Convolutions in layer 39 shift names one position forward,\nand 3) The name entities are stored linearly in Layer 39's SSM. Finally, we\nadapt an automatic circuit discovery tool, positional Edge Attribution\nPatching, to identify a Mamba IOI circuit. Our contributions provide initial\nevidence that circuit-based mechanistic interpretability tools work well for\nthe Mamba architecture.\n","authors":["Danielle Ensign","Adrià Garriga-Alonso"],"pdf_url":"https://arxiv.org/pdf/2407.14008v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.16828v2","updated":"2024-07-19T03:31:34Z","published":"2024-04-25T17:59:56Z","title":"Made to Order: Discovering monotonic temporal changes via\n  self-supervised video ordering","summary":"  Our objective is to discover and localize monotonic temporal changes in a\nsequence of images. To achieve this, we exploit a simple proxy task of ordering\na shuffled image sequence, with `time' serving as a supervisory signal, since\nonly changes that are monotonic with time can give rise to the correct\nordering. We also introduce a transformer-based model for ordering of image\nsequences of arbitrary length with built-in attribution maps. After training,\nthe model successfully discovers and localizes monotonic changes while ignoring\ncyclic and stochastic ones. We demonstrate applications of the model in\nmultiple domains covering different scene and object types, discovering both\nobject-level and environmental changes in unseen sequences. We also demonstrate\nthat the attention-based attribution maps function as effective prompts for\nsegmenting the changing regions, and that the learned representations can be\nused for downstream applications. Finally, we show that the model achieves the\nstate-of-the-art on standard benchmarks for image ordering.\n","authors":["Charig Yang","Weidi Xie","Andrew Zisserman"],"pdf_url":"https://arxiv.org/pdf/2404.16828v2.pdf","comment":"ECCV 2024. Project page: https://charigyang.github.io/order/"},{"id":"http://arxiv.org/abs/2407.14003v1","updated":"2024-07-19T03:24:20Z","published":"2024-07-19T03:24:20Z","title":"Time Series Generative Learning with Application to Brain Imaging\n  Analysis","summary":"  This paper focuses on the analysis of sequential image data, particularly\nbrain imaging data such as MRI, fMRI, CT, with the motivation of understanding\nthe brain aging process and neurodegenerative diseases. To achieve this goal,\nwe investigate image generation in a time series context. Specifically, we\nformulate a min-max problem derived from the $f$-divergence between neighboring\npairs to learn a time series generator in a nonparametric manner. The generator\nenables us to generate future images by transforming prior lag-k observations\nand a random vector from a reference distribution. With a deep neural network\nlearned generator, we prove that the joint distribution of the generated\nsequence converges to the latent truth under a Markov and a conditional\ninvariance condition. Furthermore, we extend our generation mechanism to a\npanel data scenario to accommodate multiple samples. The effectiveness of our\nmechanism is evaluated by generating real brain MRI sequences from the\nAlzheimer's Disease Neuroimaging Initiative. These generated image sequences\ncan be used as data augmentation to enhance the performance of further\ndownstream tasks, such as Alzheimer's disease detection.\n","authors":["Zhenghao Li","Sanyou Wu","Long Feng"],"pdf_url":"https://arxiv.org/pdf/2407.14003v1.pdf","comment":"45 pages"},{"id":"http://arxiv.org/abs/2407.14000v1","updated":"2024-07-19T03:12:10Z","published":"2024-07-19T03:12:10Z","title":"Clinical Reading Comprehension with Encoder-Decoder Models Enhanced by\n  Direct Preference Optimization","summary":"  Extractive question answering over clinical text is a crucial need to help\ndeal with the deluge of clinical text generated in hospitals. While encoder\nmodels (e.g., BERT) have been popular for this reading comprehension task,\nrecently encoder-decoder models (e.g., T5) are on the rise. There is also the\nemergence of preference optimization techniques to align decoder-only LLMs with\nhuman preferences. In this paper, we combine encoder-decoder models with the\ndirect preference optimization (DPO) method to improve over prior state of the\nart for the RadQA radiology question answering task by 12-15 F1 points. To the\nbest of our knowledge, this effort is the first to show that DPO method also\nworks for reading comprehension via novel heuristics to generate preference\ndata without human inputs.\n","authors":["Md Sultan Al Nahian","Ramakanth Kavuluru"],"pdf_url":"https://arxiv.org/pdf/2407.14000v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10419v2","updated":"2024-07-19T03:00:16Z","published":"2024-07-15T03:48:16Z","title":"Omni-Dimensional Frequency Learner for General Time Series Analysis","summary":"  Frequency domain representation of time series feature offers a concise\nrepresentation for handling real-world time series data with inherent\ncomplexity and dynamic nature. However, current frequency-based methods with\ncomplex operations still fall short of state-of-the-art time domain methods for\ngeneral time series analysis. In this work, we present Omni-Dimensional\nFrequency Learner (ODFL) model based on a in depth analysis among all the three\naspects of the spectrum feature: channel redundancy property among the\nfrequency dimension, the sparse and un-salient frequency energy distribution\namong the frequency dimension, and the semantic diversity among the variable\ndimension. Technically, our method is composed of a semantic-adaptive global\nfilter with attention to the un-salient frequency bands and partial operation\namong the channel dimension. Empirical results show that ODFL achieves\nconsistent state-of-the-art in five mainstream time series analysis tasks,\nincluding short- and long-term forecasting, imputation, classification, and\nanomaly detection, offering a promising foundation for time series analysis.\n","authors":["Xianing Chen","Hanting Chen","Hailin Hu"],"pdf_url":"https://arxiv.org/pdf/2407.10419v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07827v2","updated":"2024-07-19T02:51:01Z","published":"2024-07-10T16:50:59Z","title":"Estimating the stability number of a random graph using convolutional\n  neural networks","summary":"  Graph combinatorial optimization problems are widely applicable and\nnotoriously difficult to compute; for example, consider the traveling salesman\nor facility location problems. In this paper, we explore the feasibility of\nusing convolutional neural networks (CNNs) on graph images to predict the\ncardinality of combinatorial properties of random graphs and networks.\nSpecifically, we use image representations of modified adjacency matrices of\nrandom graphs as training samples for a CNN model to predict the stability\nnumber of random graphs; where the stability number is the cardinality of a\nmaximum set of vertices in a graph that contains no pairwise adjacency between\nvertices. The model and results presented in this study suggest potential for\napplying deep learning in combinatorial optimization problems previously not\nconsidered by simple deep learning techniques.\n","authors":["Randy Davila"],"pdf_url":"https://arxiv.org/pdf/2407.07827v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08762v2","updated":"2024-07-19T02:36:08Z","published":"2024-07-09T19:31:49Z","title":"Commute-Time-Optimised Graphs for GNNs","summary":"  We explore graph rewiring methods that optimise commute time. Recent graph\nrewiring approaches facilitate long-range interactions in sparse graphs, making\nsuch rewirings commute-time-optimal $\\textit{on average}$. However, when an\nexpert prior exists on which node pairs should or should not interact, a\nsuperior rewiring would favour short commute times between these privileged\nnode pairs. We construct two synthetic datasets with known priors reflecting\nrealistic settings, and use these to motivate two bespoke rewiring methods that\nincorporate the known prior. We investigate the regimes where our rewiring\nimproves test performance on the synthetic datasets. Finally, we perform a case\nstudy on a real-world citation graph to investigate the practical implications\nof our work.\n","authors":["Igor Sterner","Shiye Su","Petar Veličković"],"pdf_url":"https://arxiv.org/pdf/2407.08762v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13989v1","updated":"2024-07-19T02:34:10Z","published":"2024-07-19T02:34:10Z","title":"Enhancing Data-Limited Graph Neural Networks by Actively Distilling\n  Knowledge from Large Language Models","summary":"  Graphs have emerged as critical data structures for content analysis in\nvarious domains, such as social network analysis, bioinformatics, and\nrecommendation systems. Node classification, a fundamental task in this\ncontext, is typically tackled using graph neural networks (GNNs).\nUnfortunately, conventional GNNs still face challenges in scenarios with few\nlabeled nodes, despite the prevalence of few-shot node classification tasks in\nreal-world applications. To address this challenge, various approaches have\nbeen proposed, including graph meta-learning, transfer learning, and methods\nbased on Large Language Models (LLMs). However, traditional meta-learning and\ntransfer learning methods often require prior knowledge from base classes or\nfail to exploit the potential advantages of unlabeled nodes. Meanwhile,\nLLM-based methods may overlook the zero-shot capabilities of LLMs and rely\nheavily on the quality of generated contexts. In this paper, we propose a novel\napproach that integrates LLMs and GNNs, leveraging the zero-shot inference and\nreasoning capabilities of LLMs and employing a Graph-LLM-based active learning\nparadigm to enhance GNNs' performance. Extensive experiments demonstrate the\neffectiveness of our model in improving node classification accuracy with\nconsiderably limited labeled data, surpassing state-of-the-art baselines by\nsignificant margins.\n","authors":["Quan Li","Tianxiang Zhao","Lingwei Chen","Junjie Xu","Suhang Wang"],"pdf_url":"https://arxiv.org/pdf/2407.13989v1.pdf","comment":"10 pages, 3 Figures"},{"id":"http://arxiv.org/abs/2407.13981v1","updated":"2024-07-19T02:12:25Z","published":"2024-07-19T02:12:25Z","title":"Decomposed Direct Preference Optimization for Structure-Based Drug\n  Design","summary":"  Diffusion models have achieved promising results for Structure-Based Drug\nDesign (SBDD). Nevertheless, high-quality protein subpocket and ligand data are\nrelatively scarce, which hinders the models' generation capabilities. Recently,\nDirect Preference Optimization (DPO) has emerged as a pivotal tool for the\nalignment of generative models such as large language models and diffusion\nmodels, providing greater flexibility and accuracy by directly aligning model\noutputs with human preferences. Building on this advancement, we introduce DPO\nto SBDD in this paper. We tailor diffusion models to pharmaceutical needs by\naligning them with elaborately designed chemical score functions. We propose a\nnew structure-based molecular optimization method called DecompDPO, which\ndecomposes the molecule into arms and scaffolds and performs preference\noptimization at both local substructure and global molecule levels, allowing\nfor more precise control with fine-grained preferences. Notably, DecompDPO can\nbe effectively used for two main purposes: (1) fine-tuning pretrained diffusion\nmodels for molecule generation across various protein families, and (2)\nmolecular optimization given a specific protein subpocket after generation.\nExtensive experiments on the CrossDocked2020 benchmark show that DecompDPO\nsignificantly improves model performance in both molecule generation and\noptimization, with up to 100% Median High Affinity and a 54.9% Success Rate.\n","authors":["Xiwei Cheng","Xiangxin Zhou","Yuwei Yang","Yu Bao","Quanquan Gu"],"pdf_url":"https://arxiv.org/pdf/2407.13981v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13980v1","updated":"2024-07-19T02:11:26Z","published":"2024-07-19T02:11:26Z","title":"Byzantine-tolerant distributed learning of finite mixture models","summary":"  This paper proposes two split-and-conquer (SC) learning estimators for finite\nmixture models that are tolerant to Byzantine failures. In SC learning,\nindividual machines obtain local estimates, which are then transmitted to a\ncentral server for aggregation. During this communication, the server may\nreceive malicious or incorrect information from some local machines, a scenario\nknown as Byzantine failures. While SC learning approaches have been devised to\nmitigate Byzantine failures in statistical models with Euclidean parameters,\ndeveloping Byzantine-tolerant methods for finite mixture models with\nnon-Euclidean parameters requires a distinct strategy. Our proposed\ndistance-based methods are hyperparameter tuning free, unlike existing methods,\nand are resilient to Byzantine failures while achieving high statistical\nefficiency. We validate the effectiveness of our methods both theoretically and\nempirically via experiments on simulated and real data from machine learning\napplications for digit recognition. The code for the experiment can be found at\nhttps://github.com/SarahQiong/RobustSCGMM.\n","authors":["Qiong Zhang","Jiahua Chen"],"pdf_url":"https://arxiv.org/pdf/2407.13980v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.00161v2","updated":"2024-07-19T02:11:04Z","published":"2023-09-29T21:56:37Z","title":"Region-centric Image-Language Pretraining for Open-Vocabulary Detection","summary":"  We present a new open-vocabulary detection approach based on region-centric\nimage-language pretraining to bridge the gap between image-level pretraining\nand open-vocabulary object detection. At the pretraining phase, we incorporate\nthe detector architecture on top of the classification backbone, which better\nserves the region-level recognition needs of detection by enabling the detector\nheads to learn from large-scale image-text pairs. Using only standard\ncontrastive loss and no pseudo-labeling, our approach is a simple yet effective\nextension of the contrastive learning method to learn emergent object-semantic\ncues. In addition, we propose a shifted-window learning approach upon window\nattention to make the backbone representation more robust,\ntranslation-invariant, and less biased by the window pattern. On the popular\nLVIS open-vocabulary detection benchmark, our approach sets a new state of the\nart of 37.6 mask APr using the common ViT-L backbone and public LAION dataset,\nand 40.5 mask APr using the DataComp-1B dataset, significantly outperforming\nthe best existing approach by +3.7 mask APr at system level. On the COCO\nbenchmark, we achieve very competitive 39.6 novel AP without pseudo labeling or\nweak supervision. In addition, we evaluate our approach on the transfer\ndetection setup, where it demonstrates notable improvement over the baseline.\nVisualization reveals emerging object locality from the pretraining recipes\ncompared to the baseline.\n","authors":["Dahun Kim","Anelia Angelova","Weicheng Kuo"],"pdf_url":"https://arxiv.org/pdf/2310.00161v2.pdf","comment":"ECCV 2024, project page at\n  https://github.com/google-research/google-research/tree/master/fvlm/dito"},{"id":"http://arxiv.org/abs/2407.13979v1","updated":"2024-07-19T02:07:55Z","published":"2024-07-19T02:07:55Z","title":"Truthfulness of Calibration Measures","summary":"  We initiate the study of the truthfulness of calibration measures in\nsequential prediction. A calibration measure is said to be truthful if the\nforecaster (approximately) minimizes the expected penalty by predicting the\nconditional expectation of the next outcome, given the prior distribution of\noutcomes. Truthfulness is an important property of calibration measures,\nensuring that the forecaster is not incentivized to exploit the system with\ndeliberate poor forecasts. This makes it an essential desideratum for\ncalibration measures, alongside typical requirements, such as soundness and\ncompleteness.\n  We conduct a taxonomy of existing calibration measures and their\ntruthfulness. Perhaps surprisingly, we find that all of them are far from being\ntruthful. That is, under existing calibration measures, there are simple\ndistributions on which a polylogarithmic (or even zero) penalty is achievable,\nwhile truthful prediction leads to a polynomial penalty. Our main contribution\nis the introduction of a new calibration measure termed the Subsampled Smooth\nCalibration Error (SSCE) under which truthful prediction is optimal up to a\nconstant multiplicative factor.\n","authors":["Nika Haghtalab","Mingda Qiao","Kunhe Yang","Eric Zhao"],"pdf_url":"https://arxiv.org/pdf/2407.13979v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13978v1","updated":"2024-07-19T02:06:41Z","published":"2024-07-19T02:06:41Z","title":"Double Gradient Reversal Network for Single-Source Domain Generalization\n  in Multi-mode Fault Diagnosis","summary":"  Domain generalization achieves fault diagnosis on unseen modes. In process\nindustrial systems, fault samples are limited, and only single-mode fault data\ncan be obtained. Extracting domain-invariant fault features from single-mode\ndata for unseen mode fault diagnosis poses challenges. Existing methods utilize\na generator module to simulate samples of unseen modes. However, multi-mode\nsamples contain complex spatiotemporal information, which brings significant\ndifficulties to accurate sample generation. Therefore, double gradient reversal\nnetwork (DGRN) is proposed. First, the model is pre-trained to acquire fault\nknowledge from the single seen mode. Then, pseudo-fault feature generation\nstrategy is designed by Adaptive instance normalization, to simulate fault\nfeatures of unseen mode. The dual adversarial training strategy is created to\nenhance the diversity of pseudo-fault features, which models unseen modes with\nsignificant distribution differences. Subsequently, domain-invariant feature\nextraction strategy is constructed by contrastive learning and adversarial\nlearning. This strategy extracts common features of faults and helps multi-mode\nfault diagnosis. Finally, the experiments were conducted on Tennessee Eastman\nprocess and continuous stirred-tank reactor. The experiments demonstrate that\nDGRN achieves high classification accuracy on unseen modes while maintaining a\nsmall model size.\n","authors":["Guangqiang Li","M. Amine Atoui","Xiangshun Li"],"pdf_url":"https://arxiv.org/pdf/2407.13978v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13977v1","updated":"2024-07-19T02:06:08Z","published":"2024-07-19T02:06:08Z","title":"A Unified Confidence Sequence for Generalized Linear Models, with\n  Applications to Bandits","summary":"  We present a unified likelihood ratio-based confidence sequence (CS) for any\n(self-concordant) generalized linear models (GLMs) that is guaranteed to be\nconvex and numerically tight. We show that this is on par or improves upon\nknown CSs for various GLMs, including Gaussian, Bernoulli, and Poisson. In\nparticular, for the first time, our CS for Bernoulli has a poly(S)-free radius\nwhere S is the norm of the unknown parameter. Our first technical novelty is\nits derivation, which utilizes a time-uniform PAC-Bayesian bound with a uniform\nprior/posterior, despite the latter being a rather unpopular choice for\nderiving CSs. As a direct application of our new CS, we propose a simple and\nnatural optimistic algorithm called OFUGLB applicable to any generalized linear\nbandits (GLB; Filippi et al. (2010)). Our analysis shows that the celebrated\noptimistic approach simultaneously attains state-of-the-art regrets for various\nself-concordant (not necessarily bounded) GLBs, and even poly(S)-free for\nbounded GLBs, including logistic bandits. The regret analysis, our second\ntechnical novelty, follows from combining our new CS with a new proof technique\nthat completely avoids the previously widely used self-concordant control lemma\n(Faury et al., 2020, Lemma 9). Finally, we verify numerically that OFUGLB\nsignificantly outperforms the prior state-of-the-art (Lee et al., 2024) for\nlogistic bandits.\n","authors":["Junghyun Lee","Se-Young Yun","Kwang-Sung Jun"],"pdf_url":"https://arxiv.org/pdf/2407.13977v1.pdf","comment":"31 pages, 1 figure, 2 tables"},{"id":"http://arxiv.org/abs/2407.13146v2","updated":"2024-07-19T02:00:01Z","published":"2024-07-18T04:18:52Z","title":"PG-Rainbow: Using Distributional Reinforcement Learning in Policy\n  Gradient Methods","summary":"  This paper introduces PG-Rainbow, a novel algorithm that incorporates a\ndistributional reinforcement learning framework with a policy gradient\nalgorithm. Existing policy gradient methods are sample inefficient and rely on\nthe mean of returns when calculating the state-action value function,\nneglecting the distributional nature of returns in reinforcement learning\ntasks. To address this issue, we use an Implicit Quantile Network that provides\nthe quantile information of the distribution of rewards to the critic network\nof the Proximal Policy Optimization algorithm. We show empirical results that\nthrough the integration of reward distribution information into the policy\nnetwork, the policy agent acquires enhanced capabilities to comprehensively\nevaluate the consequences of potential actions in a given state, facilitating\nmore sophisticated and informed decision-making processes. We evaluate the\nperformance of the proposed algorithm in the Atari-2600 game suite, simulated\nvia the Arcade Learning Environment (ALE).\n","authors":["WooJae Jeon","KangJun Lee","Jeewoo Lee"],"pdf_url":"https://arxiv.org/pdf/2407.13146v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.04192v2","updated":"2024-07-19T01:36:34Z","published":"2024-07-05T00:38:49Z","title":"KAN-ODEs: Kolmogorov-Arnold Network Ordinary Differential Equations for\n  Learning Dynamical Systems and Hidden Physics","summary":"  Kolmogorov-Arnold networks (KANs) as an alternative to multi-layer\nperceptrons (MLPs) are a recent development demonstrating strong potential for\ndata-driven modeling. This work applies KANs as the backbone of a neural\nordinary differential equation (ODE) framework, generalizing their use to the\ntime-dependent and temporal grid-sensitive cases often seen in dynamical\nsystems and scientific machine learning applications. The proposed KAN-ODEs\nretain the flexible dynamical system modeling framework of Neural ODEs while\nleveraging the many benefits of KANs compared to MLPs, including higher\naccuracy and faster neural scaling, stronger interpretability and\ngeneralizability, and lower parameter counts. First, we quantitatively\ndemonstrated these improvements in a comprehensive study of the classical\nLotka-Volterra predator-prey model. We then showcased the KAN-ODE framework's\nability to learn symbolic source terms and complete solution profiles in\nhigher-complexity and data-lean scenarios including wave propagation and shock\nformation, the complex Schr\\\"odinger equation, and the Allen-Cahn phase\nseparation equation. The successful training of KAN-ODEs, and their improved\nperformance compared to traditional Neural ODEs, implies significant potential\nin leveraging this novel network architecture in myriad scientific machine\nlearning applications for discovering hidden physics and predicting dynamic\nevolution.\n","authors":["Benjamin C. Koenig","Suyong Kim","Sili Deng"],"pdf_url":"https://arxiv.org/pdf/2407.04192v2.pdf","comment":"B.C.K. and S.K. contributed equally to this work. 20 pages, 10\n  figures, and 4 tables. Revised upload includes additional examples and\n  extended discussion of existing examples"},{"id":"http://arxiv.org/abs/2405.20405v3","updated":"2024-07-19T01:35:15Z","published":"2024-05-30T18:20:35Z","title":"Private Mean Estimation with Person-Level Differential Privacy","summary":"  We study person-level differentially private (DP) mean estimation in the case\nwhere each person holds multiple samples. DP here requires the usual notion of\ndistributional stability when $\\textit{all}$ of a person's datapoints can be\nmodified. Informally, if $n$ people each have $m$ samples from an unknown\n$d$-dimensional distribution with bounded $k$-th moments, we show that \\[n =\n\\tilde \\Theta\\left(\\frac{d}{\\alpha^2 m} + \\frac{d}{\\alpha m^{1/2} \\varepsilon}\n+ \\frac{d}{\\alpha^{k/(k-1)} m \\varepsilon} + \\frac{d}{\\varepsilon}\\right)\\]\npeople are necessary and sufficient to estimate the mean up to distance\n$\\alpha$ in $\\ell_2$-norm under $\\varepsilon$-differential privacy (and its\ncommon relaxations). In the multivariate setting, we give computationally\nefficient algorithms under approximate-DP and computationally inefficient\nalgorithms under pure DP, and our nearly matching lower bounds hold for the\nmost permissive case of approximate DP. Our computationally efficient\nestimators are based on the standard clip-and-noise framework, but the analysis\nfor our setting requires both new algorithmic techniques and new analyses. In\nparticular, our new bounds on the tails of sums of independent, vector-valued,\nbounded-moments random variables may be of interest.\n","authors":["Sushant Agarwal","Gautam Kamath","Mahbod Majid","Argyris Mouzakis","Rose Silver","Jonathan Ullman"],"pdf_url":"https://arxiv.org/pdf/2405.20405v3.pdf","comment":"72 pages, 3 figures"},{"id":"http://arxiv.org/abs/2211.09619v3","updated":"2024-07-19T00:46:18Z","published":"2022-11-17T16:12:45Z","title":"Introduction to Online Nonstochastic Control","summary":"  This text presents an introduction to an emerging paradigm in control of\ndynamical systems and differentiable reinforcement learning called online\nnonstochastic control. The new approach applies techniques from online convex\noptimization and convex relaxations to obtain new methods with provable\nguarantees for classical settings in optimal and robust control.\n  The primary distinction between online nonstochastic control and other\nframeworks is the objective. In optimal control, robust control, and other\ncontrol methodologies that assume stochastic noise, the goal is to perform\ncomparably to an offline optimal strategy. In online nonstochastic control,\nboth the cost functions as well as the perturbations from the assumed dynamical\nmodel are chosen by an adversary. Thus the optimal policy is not defined a\npriori. Rather, the target is to attain low regret against the best policy in\nhindsight from a benchmark class of policies.\n  This objective suggests the use of the decision making framework of online\nconvex optimization as an algorithmic methodology. The resulting methods are\nbased on iterative mathematical optimization algorithms, and are accompanied by\nfinite-time regret and computational complexity guarantees.\n","authors":["Elad Hazan","Karan Singh"],"pdf_url":"https://arxiv.org/pdf/2211.09619v3.pdf","comment":"Draft; comments/suggestions welcome at\n  nonstochastic.control@gmail.com"},{"id":"http://arxiv.org/abs/2407.13957v1","updated":"2024-07-19T00:34:03Z","published":"2024-07-19T00:34:03Z","title":"The Group Robustness is in the Details: Revisiting Finetuning under\n  Spurious Correlations","summary":"  Modern machine learning models are prone to over-reliance on spurious\ncorrelations, which can often lead to poor performance on minority groups. In\nthis paper, we identify surprising and nuanced behavior of finetuned models on\nworst-group accuracy via comprehensive experiments on four well-established\nbenchmarks across vision and language tasks. We first show that the commonly\nused class-balancing techniques of mini-batch upsampling and loss upweighting\ncan induce a decrease in worst-group accuracy (WGA) with training epochs,\nleading to performance no better than without class-balancing. While in some\nscenarios, removing data to create a class-balanced subset is more effective,\nwe show this depends on group structure and propose a mixture method which can\noutperform both techniques. Next, we show that scaling pretrained models is\ngenerally beneficial for worst-group accuracy, but only in conjuction with\nappropriate class-balancing. Finally, we identify spectral imbalance in\nfinetuning features as a potential source of group disparities -- minority\ngroup covariance matrices incur a larger spectral norm than majority groups\nonce conditioned on the classes. Our results show more nuanced interactions of\nmodern finetuned models with group robustness than was previously known. Our\ncode is available at https://github.com/tmlabonte/revisiting-finetuning.\n","authors":["Tyler LaBonte","John C. Hill","Xinchen Zhang","Vidya Muthukumar","Abhishek Kumar"],"pdf_url":"https://arxiv.org/pdf/2407.13957v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.09542v3","updated":"2024-07-19T00:33:49Z","published":"2024-02-14T19:34:28Z","title":"Layerwise Proximal Replay: A Proximal Point Method for Online Continual\n  Learning","summary":"  In online continual learning, a neural network incrementally learns from a\nnon-i.i.d. data stream. Nearly all online continual learning methods employ\nexperience replay to simultaneously prevent catastrophic forgetting and\nunderfitting on past data. Our work demonstrates a limitation of this approach:\nneural networks trained with experience replay tend to have unstable\noptimization trajectories, impeding their overall accuracy. Surprisingly, these\ninstabilities persist even when the replay buffer stores all previous training\nexamples, suggesting that this issue is orthogonal to catastrophic forgetting.\nWe minimize these instabilities through a simple modification of the\noptimization geometry. Our solution, Layerwise Proximal Replay (LPR), balances\nlearning from new and replay data while only allowing for gradual changes in\nthe hidden activation of past data. We demonstrate that LPR consistently\nimproves replay-based online continual learning methods across multiple problem\nsettings, regardless of the amount of available replay memory.\n","authors":["Jason Yoo","Yunpeng Liu","Frank Wood","Geoff Pleiss"],"pdf_url":"https://arxiv.org/pdf/2402.09542v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.00741v4","updated":"2024-07-19T00:30:01Z","published":"2024-06-30T16:05:31Z","title":"Diffusion Models for Offline Multi-agent Reinforcement Learning with\n  Safety Constraints","summary":"  In recent advancements in Multi-agent Reinforcement Learning (MARL), its\napplication has extended to various safety-critical scenarios. However, most\nmethods focus on online learning, which presents substantial risks when\ndeployed in real-world settings. Addressing this challenge, we introduce an\ninnovative framework integrating diffusion models within the MARL paradigm.\nThis approach notably enhances the safety of actions taken by multiple agents\nthrough risk mitigation while modeling coordinated action. Our framework is\ngrounded in the Centralized Training with Decentralized Execution (CTDE)\narchitecture, augmented by a Diffusion Model for prediction trajectory\ngeneration. Additionally, we incorporate a specialized algorithm to further\nensure operational safety. We evaluate our model against baselines on the DSRL\nbenchmark. Experiment results demonstrate that our model not only adheres to\nstringent safety constraints but also achieves superior performance compared to\nexisting methodologies. This underscores the potential of our approach in\nadvancing the safety and efficacy of MARL in real-world applications.\n","authors":["Jianuo Huang"],"pdf_url":"https://arxiv.org/pdf/2407.00741v4.pdf","comment":"arXiv admin note: text overlap with arXiv:2101.05436 by other authors"},{"id":"http://arxiv.org/abs/2305.15759v5","updated":"2024-07-19T00:21:06Z","published":"2023-05-25T06:18:31Z","title":"Differentially Private Latent Diffusion Models","summary":"  Diffusion models (DMs) are one of the most widely used generative models for\nproducing high quality images. However, a flurry of recent papers points out\nthat DMs are least private forms of image generators, by extracting a\nsignificant number of near-identical replicas of training images from DMs.\nExisting privacy-enhancing techniques for DMs, unfortunately, do not provide a\ngood privacy-utility tradeoff. In this paper, we aim to improve the current\nstate of DMs with differential privacy (DP) by adopting the \\textit{Latent}\nDiffusion Models (LDMs). LDMs are equipped with powerful pre-trained\nautoencoders that map the high-dimensional pixels into lower-dimensional latent\nrepresentations, in which DMs are trained, yielding a more efficient and fast\ntraining of DMs. Rather than fine-tuning the entire LDMs, we fine-tune only the\n$\\textit{attention}$ modules of LDMs with DP-SGD, reducing the number of\ntrainable parameters by roughly $90\\%$ and achieving a better privacy-accuracy\ntrade-off. Our approach allows us to generate realistic, high-dimensional\nimages (256x256) conditioned on text prompts with DP guarantees, which, to the\nbest of our knowledge, has not been attempted before. Our approach provides a\npromising direction for training more powerful, yet training-efficient\ndifferentially private DMs, producing high-quality DP images. Our code is\navailable at https://anonymous.4open.science/r/DP-LDM-4525.\n","authors":["Michael F. Liu","Saiyue Lyu","Margarita Vinaroz","Mijung Park"],"pdf_url":"https://arxiv.org/pdf/2305.15759v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.15017v2","updated":"2024-07-19T00:18:40Z","published":"2023-07-27T17:19:37Z","title":"Samplable Anonymous Aggregation for Private Federated Data Analysis","summary":"  We revisit the problem of designing scalable protocols for private statistics\nand private federated learning when each device holds its private data. Locally\ndifferentially private algorithms require little trust but are (provably)\nlimited in their utility. Centrally differentially private algorithms can allow\nsignificantly better utility but require a trusted curator. This gap has led to\nsignificant interest in the design and implementation of simple cryptographic\nprimitives, that can allow central-like utility guarantees without having to\ntrust a central server.\n  Our first contribution is to propose a new primitive that allows for\nefficient implementation of several commonly used algorithms, and allows for\nprivacy accounting that is close to that in the central setting without\nrequiring the strong trust assumptions it entails. {\\em Shuffling} and {\\em\naggregation} primitives that have been proposed in earlier works enable this\nfor some algorithms, but have significant limitations as primitives. We propose\na {\\em Samplable Anonymous Aggregation} primitive, which computes an aggregate\nover a random subset of the inputs and show that it leads to better\nprivacy-utility trade-offs for various fundamental tasks. Secondly, we propose\na system architecture that implements this primitive and perform a security\nanalysis of the proposed system. Our design combines additive secret-sharing\nwith anonymization and authentication infrastructures.\n","authors":["Kunal Talwar","Shan Wang","Audra McMillan","Vojta Jina","Vitaly Feldman","Pansy Bansal","Bailey Basile","Aine Cahill","Yi Sheng Chan","Mike Chatzidakis","Junye Chen","Oliver Chick","Mona Chitnis","Suman Ganta","Yusuf Goren","Filip Granqvist","Kristine Guo","Frederic Jacobs","Omid Javidbakht","Albert Liu","Richard Low","Dan Mascenik","Steve Myers","David Park","Wonhee Park","Gianni Parsa","Tommy Pauly","Christian Priebe","Rehan Rishi","Guy Rothblum","Michael Scaria","Linmao Song","Congzheng Song","Karl Tarbe","Sebastian Vogt","Luke Winstrom","Shundong Zhou"],"pdf_url":"https://arxiv.org/pdf/2307.15017v2.pdf","comment":"34 pages"},{"id":"http://arxiv.org/abs/2310.07506v2","updated":"2024-07-19T00:14:59Z","published":"2023-10-11T14:02:11Z","title":"Leveraging Hierarchical Feature Sharing for Efficient Dataset\n  Condensation","summary":"  Given a real-world dataset, data condensation (DC) aims to synthesize a small\nsynthetic dataset that captures the knowledge of a natural dataset while being\nusable for training models with comparable accuracy. Recent works propose to\nenhance DC with data parameterization, which condenses data into very compact\nparameterized data containers instead of images. The intuition behind data\nparameterization is to encode shared features of images to avoid additional\nstorage costs. In this paper, we recognize that images share common features in\na hierarchical way due to the inherent hierarchical structure of the\nclassification system, which is overlooked by current data parameterization\nmethods. To better align DC with this hierarchical nature and encourage more\nefficient information sharing inside data containers, we propose a novel data\nparameterization architecture, Hierarchical Memory Network (HMN). HMN stores\ncondensed data in a three-tier structure, representing the dataset-level,\nclass-level, and instance-level features. Another helpful property of the\nhierarchical architecture is that HMN naturally ensures good independence among\nimages despite achieving information sharing. This enables instance-level\npruning for HMN to reduce redundant information, thereby further minimizing\nredundancy and enhancing performance. We evaluate HMN on five public datasets\nand show that our proposed method outperforms all baselines.\n","authors":["Haizhong Zheng","Jiachen Sun","Shutong Wu","Bhavya Kailkhura","Zhuoqing Mao","Chaowei Xiao","Atul Prakash"],"pdf_url":"https://arxiv.org/pdf/2310.07506v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13954v1","updated":"2024-07-19T00:10:56Z","published":"2024-07-19T00:10:56Z","title":"Neural topology optimization: the good, the bad, and the ugly","summary":"  Neural networks (NNs) hold great promise for advancing inverse design via\ntopology optimization (TO), yet misconceptions about their application persist.\nThis article focuses on neural topology optimization (neural TO), which\nleverages NNs to reparameterize the decision space and reshape the optimization\nlandscape. While the method is still in its infancy, our analysis tools reveal\ncritical insights into the NNs' impact on the optimization process. We\ndemonstrate that the choice of NN architecture significantly influences the\nobjective landscape and the optimizer's path to an optimum. Notably, NNs\nintroduce non-convexities even in otherwise convex landscapes, potentially\ndelaying convergence in convex problems but enhancing exploration for\nnon-convex problems. This analysis lays the groundwork for future advancements\nby highlighting: 1) the potential of neural TO for non-convex problems and\ndedicated GPU hardware (the \"good\"), 2) the limitations in smooth landscapes\n(the \"bad\"), and 3) the complex challenge of selecting optimal NN architectures\nand hyperparameters for superior performance (the \"ugly\").\n","authors":["Suryanarayanan Manoj Sanu","Alejandro M. Aragon","Miguel A. Bessa"],"pdf_url":"https://arxiv.org/pdf/2407.13954v1.pdf","comment":"33 pages, 19 figures, includes Supporting Information in the same PDF"},{"id":"http://arxiv.org/abs/2306.11986v2","updated":"2024-07-19T00:03:11Z","published":"2023-06-21T02:42:37Z","title":"Sequential Recommendation with Controllable Diversification:\n  Representation Degeneration and Diversity","summary":"  Sequential recommendation (SR) models the dynamic user preferences and\ngenerates the next-item prediction as the affinity between the sequence and\nitems, in a joint latent space with low dimensions (i.e., the sequence and item\nembedding space). Both sequence and item representations suffer from the\nrepresentation degeneration issue due to the user/item long-tail distributions,\nwhere tail users/ items are indistinguishably distributed as a narrow cone in\nthe latent space. We argue that the representation degeneration issue is the\nroot cause of insufficient recommendation diversity in existing SR methods,\nimpairing the user potential exploration and further worsening the echo chamber\nissue.\n  In this work, we first disclose the connection between the representation\ndegeneration and recommendation diversity, in which severer representation\ndegeneration indicates lower recommendation diversity. We then propose a novel\nSingular sPectrum sMoothing regularization for Recommendation (SPMRec), which\nacts as a controllable surrogate to alleviate the degeneration and achieve the\nbalance between recommendation diversity and performance. The proposed\nsmoothing regularization alleviates the degeneration by maximizing the area\nunder the singular value curve, which is also the diversity surrogate. We\nconduct experiments on four benchmark datasets to demonstrate the superiority\nof SPMRec, and show that the proposed singular spectrum smoothing can control\nthe balance of recommendation performance and diversity simultaneously.\n","authors":["Ziwei Fan","Zhiwei Liu","Hao Peng","Philip S. Yu"],"pdf_url":"https://arxiv.org/pdf/2306.11986v2.pdf","comment":"12 pages"}],"Multimedia":[{"id":"http://arxiv.org/abs/2407.14364v1","updated":"2024-07-19T14:52:11Z","published":"2024-07-19T14:52:11Z","title":"Towards Assessing Data Replication in Music Generation with Music\n  Similarity Metrics on Raw Audio","summary":"  Recent advancements in music generation are raising multiple concerns about\nthe implications of AI in creative music processes, current business models and\nimpacts related to intellectual property management. A relevant challenge is\nthe potential replication and plagiarism of the training set in AI-generated\nmusic, which could lead to misuse of data and intellectual property rights\nviolations. To tackle this issue, we present the Music Replication Assessment\n(MiRA) tool: a model-independent open evaluation method based on diverse audio\nmusic similarity metrics to assess data replication of the training set. We\nevaluate the ability of five metrics to identify exact replication, by\nconducting a controlled replication experiment in different music genres based\non synthetic samples. Our results show that the proposed methodology can\nestimate exact data replication with a proportion higher than 10%. By\nintroducing the MiRA tool, we intend to encourage the open evaluation of music\ngenerative models by researchers, developers and users concerning data\nreplication, highlighting the importance of ethical, social, legal and economic\nconsequences of generative AI in the music domain.\n","authors":["Roser Batlle-Roca","Wei-Hisang Liao","Xavier Serra","Yuki Mitsufuji","Emilia Gómez"],"pdf_url":"https://arxiv.org/pdf/2407.14364v1.pdf","comment":"Accepted at ISMIR 2024"},{"id":"http://arxiv.org/abs/2407.14321v1","updated":"2024-07-19T13:57:11Z","published":"2024-07-19T13:57:11Z","title":"Multimodal Misinformation Detection using Large Vision-Language Models","summary":"  The increasing proliferation of misinformation and its alarming impact have\nmotivated both industry and academia to develop approaches for misinformation\ndetection and fact checking. Recent advances on large language models (LLMs)\nhave shown remarkable performance in various tasks, but whether and how LLMs\ncould help with misinformation detection remains relatively underexplored. Most\nof existing state-of-the-art approaches either do not consider evidence and\nsolely focus on claim related features or assume the evidence to be provided.\nFew approaches consider evidence retrieval as part of the misinformation\ndetection but rely on fine-tuning models. In this paper, we investigate the\npotential of LLMs for misinformation detection in a zero-shot setting. We\nincorporate an evidence retrieval component into the process as it is crucial\nto gather pertinent information from various sources to detect the veracity of\nclaims. To this end, we propose a novel re-ranking approach for multimodal\nevidence retrieval using both LLMs and large vision-language models (LVLM). The\nretrieved evidence samples (images and texts) serve as the input for an\nLVLM-based approach for multimodal fact verification (LVLM4FV). To enable a\nfair evaluation, we address the issue of incomplete ground truth for evidence\nsamples in an existing evidence retrieval dataset by annotating a more complete\nset of evidence samples for both image and text retrieval. Our experimental\nresults on two datasets demonstrate the superiority of the proposed approach in\nboth evidence retrieval and fact verification tasks and also better\ngeneralization capability across dataset compared to the supervised baseline.\n","authors":["Sahar Tahmasebi","Eric Müller-Budack","Ralph Ewerth"],"pdf_url":"https://arxiv.org/pdf/2407.14321v1.pdf","comment":"Accepted for publication in: Conference on Information and Knowledge\n  Management (CIKM) 2024"},{"id":"http://arxiv.org/abs/2407.14242v1","updated":"2024-07-19T12:22:32Z","published":"2024-07-19T12:22:32Z","title":"Continual Panoptic Perception: Towards Multi-modal Incremental\n  Interpretation of Remote Sensing Images","summary":"  Continual learning (CL) breaks off the one-way training manner and enables a\nmodel to adapt to new data, semantics and tasks continuously. However, current\nCL methods mainly focus on single tasks. Besides, CL models are plagued by\ncatastrophic forgetting and semantic drift since the lack of old data, which\noften occurs in remote-sensing interpretation due to the intricate fine-grained\nsemantics. In this paper, we propose Continual Panoptic Perception (CPP), a\nunified continual learning model that leverages multi-task joint learning\ncovering pixel-level classification, instance-level segmentation and\nimage-level perception for universal interpretation in remote sensing images.\nConcretely, we propose a collaborative cross-modal encoder (CCE) to extract the\ninput image features, which supports pixel classification and caption\ngeneration synchronously. To inherit the knowledge from the old model without\nexemplar memory, we propose a task-interactive knowledge distillation (TKD)\nmethod, which leverages cross-modal optimization and task-asymmetric\npseudo-labeling (TPL) to alleviate catastrophic forgetting. Furthermore, we\nalso propose a joint optimization mechanism to achieve end-to-end multi-modal\npanoptic perception. Experimental results on the fine-grained panoptic\nperception dataset validate the effectiveness of the proposed model, and also\nprove that joint optimization can boost sub-task CL efficiency with over 13\\%\nrelative improvement on panoptic quality.\n","authors":["Bo Yuan","Danpei Zhao","Zhuoran Liu","Wentao Li","Tian Li"],"pdf_url":"https://arxiv.org/pdf/2407.14242v1.pdf","comment":"Accepted in ACMMM 2024"},{"id":"http://arxiv.org/abs/2403.10107v2","updated":"2024-07-19T09:38:18Z","published":"2024-03-15T08:51:15Z","title":"Enhancing Human-Centered Dynamic Scene Understanding via Multiple LLMs\n  Collaborated Reasoning","summary":"  Human-centered dynamic scene understanding plays a pivotal role in enhancing\nthe capability of robotic and autonomous systems, in which Video-based\nHuman-Object Interaction (V-HOI) detection is a crucial task in semantic scene\nunderstanding, aimed at comprehensively understanding HOI relationships within\na video to benefit the behavioral decisions of mobile robots and autonomous\ndriving systems. Although previous V-HOI detection models have made significant\nstrides in accurate detection on specific datasets, they still lack the general\nreasoning ability like human beings to effectively induce HOI relationships. In\nthis study, we propose V-HOI Multi-LLMs Collaborated Reasoning (V-HOI MLCR), a\nnovel framework consisting of a series of plug-and-play modules that could\nfacilitate the performance of current V-HOI detection models by leveraging the\nstrong reasoning ability of different off-the-shelf pre-trained large language\nmodels (LLMs). We design a two-stage collaboration system of different LLMs for\nthe V-HOI task. Specifically, in the first stage, we design a Cross-Agents\nReasoning scheme to leverage the LLM conduct reasoning from different aspects.\nIn the second stage, we perform Multi-LLMs Debate to get the final reasoning\nanswer based on the different knowledge in different LLMs. Additionally, we\ndevise an auxiliary training strategy that utilizes CLIP, a large\nvision-language model to enhance the base V-HOI models' discriminative ability\nto better cooperate with LLMs. We validate the superiority of our design by\ndemonstrating its effectiveness in improving the prediction accuracy of the\nbase V-HOI model via reasoning from multiple perspectives.\n","authors":["Hang Zhang","Wenxiao Zhang","Haoxuan Qu","Jun Liu"],"pdf_url":"https://arxiv.org/pdf/2403.10107v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14146v1","updated":"2024-07-19T09:24:43Z","published":"2024-07-19T09:24:43Z","title":"Fine-grained Knowledge Graph-driven Video-Language Learning for Action\n  Recognition","summary":"  Recent work has explored video action recognition as a video-text matching\nproblem and several effective methods have been proposed based on large-scale\npre-trained vision-language models. However, these approaches primarily operate\nat a coarse-grained level without the detailed and semantic understanding of\naction concepts by exploiting fine-grained semantic connections between actions\nand body movements. To address this gap, we propose a contrastive\nvideo-language learning framework guided by a knowledge graph, termed KG-CLIP,\nwhich incorporates structured information into the CLIP model in the video\ndomain. Specifically, we construct a multi-modal knowledge graph composed of\nmulti-grained concepts by parsing actions based on compositional learning. By\nimplementing a triplet encoder and deviation compensation to adaptively\noptimize the margin in the entity distance function, our model aims to improve\nalignment of entities in the knowledge graph to better suit complex\nrelationship learning. This allows for enhanced video action recognition\ncapabilities by accommodating nuanced associations between graph components. We\ncomprehensively evaluate KG-CLIP on Kinetics-TPS, a large-scale action parsing\ndataset, demonstrating its effectiveness compared to competitive baselines.\nEspecially, our method excels at action recognition with few sample frames or\nlimited training data, which exhibits excellent data utilization and learning\ncapabilities.\n","authors":["Rui Zhang","Yafen Lu","Pengli Ji","Junxiao Xue","Xiaoran Yan"],"pdf_url":"https://arxiv.org/pdf/2407.14146v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14093v1","updated":"2024-07-19T07:57:48Z","published":"2024-07-19T07:57:48Z","title":"Not All Attention is Needed: Parameter and Computation Efficient Tuning\n  for Multi-modal Large Language Models via Effective Attention Skipping","summary":"  Recently, mixture of experts (MoE) has become a popular paradigm for\nachieving the trade-off between modal capacity and efficiency of multi-modal\nlarge language models (MLLMs). Different from previous efforts, we are\ndedicated to exploring the dynamic expert path in an already exist MLLM and\nshow that a standard MLLM can be also a mixture of experts. To approach this\ntarget, we propose a novel dynamic expert scheme for MLLMs, termed Routing\nExperts (RoE), which can achieve example-dependent optimal path routing without\nobvious structure tweaks. Meanwhile, a new regularization of structure sparsity\nis also introduced to enforce MLLMs to learn more short-cut inference, ensuring\nthe efficiency. In addition, we also realize the first attempt of aligning the\ntraining and inference schemes of MLLMs in terms of network routing. To\nvalidate RoE, we apply it to a set of latest MLLMs, including LLaVA-1.5,\nLLaVA-HR and VILA, and conduct extensive experiments on a bunch of VL\nbenchmarks. The experiment results not only show the great advantages of our\nRoE in improving MLLMs' efficiency, but also yield obvious advantages than\nMoE-LLaVA in both performance and speed, e.g., an average performance gain of\n3.3% on 5 benchmarks while being faster.\n","authors":["Qiong Wu","Zhaoxi Ke","Yiyi Zhou","Gen Luo","Xiaoshuai Sun","Rongrong Ji"],"pdf_url":"https://arxiv.org/pdf/2407.14093v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14066v1","updated":"2024-07-19T06:50:24Z","published":"2024-07-19T06:50:24Z","title":"360VFI: A Dataset and Benchmark for Omnidirectional Video Frame\n  Interpolation","summary":"  With the development of VR-related techniques, viewers can enjoy a realistic\nand immersive experience through a head-mounted display, while omnidirectional\nvideo with a low frame rate can lead to user dizziness. However, the prevailing\nplane frame interpolation methodologies are unsuitable for Omnidirectional\nVideo Interpolation, chiefly due to the lack of models tailored to such videos\nwith strong distortion, compounded by the scarcity of valuable datasets for\nOmnidirectional Video Frame Interpolation. In this paper, we introduce the\nbenchmark dataset, 360VFI, for Omnidirectional Video Frame Interpolation. We\npresent a practical implementation that introduces a distortion prior from\nomnidirectional video into the network to modulate distortions. We especially\npropose a pyramid distortion-sensitive feature extractor that uses the unique\ncharacteristics of equirectangular projection (ERP) format as prior\ninformation. Moreover, we devise a decoder that uses an affine transformation\nto facilitate the synthesis of intermediate frames further. 360VFI is the first\ndataset and benchmark that explores the challenge of Omnidirectional Video\nFrame Interpolation. Through our benchmark analysis, we presented four\ndifferent distortion conditions scenes in the proposed 360VFI dataset to\nevaluate the challenge triggered by distortion during interpolation. Besides,\nexperimental results demonstrate that Omnidirectional Video Interpolation can\nbe effectively improved by modeling for omnidirectional distortion.\n","authors":["Wenxuan Lu","Mengshun Hu","Yansheng Qiu","Liang Liao","Zheng Wang"],"pdf_url":"https://arxiv.org/pdf/2407.14066v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2302.03453,\n  arXiv:1804.02815 by other authors"},{"id":"http://arxiv.org/abs/2407.14093v1","updated":"2024-07-19T07:57:48Z","published":"2024-07-19T07:57:48Z","title":"Routing Experts: Learning to Route Dynamic Experts in Multi-modal Large\n  Language Models","summary":"  Recently, mixture of experts (MoE) has become a popular paradigm for\nachieving the trade-off between modal capacity and efficiency of multi-modal\nlarge language models (MLLMs). Different from previous efforts, we are\ndedicated to exploring the dynamic expert path in an already exist MLLM and\nshow that a standard MLLM can be also a mixture of experts. To approach this\ntarget, we propose a novel dynamic expert scheme for MLLMs, termed Routing\nExperts (RoE), which can achieve example-dependent optimal path routing without\nobvious structure tweaks. Meanwhile, a new regularization of structure sparsity\nis also introduced to enforce MLLMs to learn more short-cut inference, ensuring\nthe efficiency. In addition, we also realize the first attempt of aligning the\ntraining and inference schemes of MLLMs in terms of network routing. To\nvalidate RoE, we apply it to a set of latest MLLMs, including LLaVA-1.5,\nLLaVA-HR and VILA, and conduct extensive experiments on a bunch of VL\nbenchmarks. The experiment results not only show the great advantages of our\nRoE in improving MLLMs' efficiency, but also yield obvious advantages than\nMoE-LLaVA in both performance and speed, e.g., an average performance gain of\n3.3% on 5 benchmarks while being faster.\n","authors":["Qiong Wu","Zhaoxi Ke","Yiyi Zhou","Gen Luo","Xiaoshuai Sun","Rongrong Ji"],"pdf_url":"https://arxiv.org/pdf/2407.14093v1.pdf","comment":null}]},"2024-07-22T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2407.14296v2","updated":"2024-07-22T17:55:26Z","published":"2024-07-19T13:26:52Z","title":"Foundation Models for Autonomous Robots in Unstructured Environments","summary":"  Automating activities through robots in unstructured environments, such as\nconstruction sites, has been a long-standing desire. However, the high degree\nof unpredictable events in these settings has resulted in far less adoption\ncompared to more structured settings, such as manufacturing, where robots can\nbe hard-coded or trained on narrowly defined datasets. Recently, pretrained\nfoundation models, such as Large Language Models (LLMs), have demonstrated\nsuperior generalization capabilities by providing zero-shot solutions for\nproblems do not present in the training data, proposing them as a potential\nsolution for introducing robots to unstructured environments. To this end, this\nstudy investigates potential opportunities and challenges of pretrained\nfoundation models from a multi-dimensional perspective. The study\nsystematically reviews application of foundation models in two field of robotic\nand unstructured environment and then synthesized them with deliberative acting\ntheory. Findings showed that linguistic capabilities of LLMs have been utilized\nmore than other features for improving perception in human-robot interactions.\nOn the other hand, findings showed that the use of LLMs demonstrated more\napplications in project management and safety in construction, and natural\nhazard detection in disaster management. Synthesizing these findings, we\nlocated the current state-of-the-art in this field on a five-level scale of\nautomation, placing them at conditional automation. This assessment was then\nused to envision future scenarios, challenges, and solutions toward autonomous\nsafe unstructured environments. Our study can be seen as a benchmark to track\nour progress toward that future.\n","authors":["Hossein Naderi","Alireza Shojaei","Lifu Huang"],"pdf_url":"https://arxiv.org/pdf/2407.14296v2.pdf","comment":"arXiv admin note: text overlap with arXiv:2312.07843,\n  arXiv:2402.05741 by other authors"},{"id":"http://arxiv.org/abs/2407.15835v1","updated":"2024-07-22T17:51:53Z","published":"2024-07-22T17:51:53Z","title":"dMel: Speech Tokenization made Simple","summary":"  Large language models have revolutionized natural language processing by\nleveraging self-supervised pretraining on vast textual data. Inspired by this\nsuccess, researchers have investigated complicated speech tokenization methods\nto discretize continuous speech signals so that language modeling techniques\ncan be applied to speech data. However, existing approaches either model\nsemantic tokens, potentially losing acoustic information, or model acoustic\ntokens, risking the loss of semantic information. Having multiple token types\nalso complicates the architecture and requires additional pretraining. Here we\nshow that discretizing mel-filterbank channels into discrete intensity bins\nproduces a simple representation (dMel), that performs better than other\nexisting speech tokenization methods. Using a transformer decoder-only\narchitecture for speech-text modeling, we comprehensively evaluate different\nspeech tokenization methods on speech recognition (ASR), speech synthesis\n(TTS). Our results demonstrate the effectiveness of dMel in achieving high\nperformance on both tasks within a unified framework, paving the way for\nefficient and effective joint modeling of speech and text.\n","authors":["He Bai","Tatiana Likhomanenko","Ruixiang Zhang","Zijin Gu","Zakaria Aldeneh","Navdeep Jaitly"],"pdf_url":"https://arxiv.org/pdf/2407.15835v1.pdf","comment":"under review"},{"id":"http://arxiv.org/abs/2407.15828v1","updated":"2024-07-22T17:46:50Z","published":"2024-07-22T17:46:50Z","title":"J-CHAT: Japanese Large-scale Spoken Dialogue Corpus for Spoken Dialogue\n  Language Modeling","summary":"  Spoken dialogue plays a crucial role in human-AI interactions, necessitating\ndialogue-oriented spoken language models (SLMs). To develop versatile SLMs,\nlarge-scale and diverse speech datasets are essential. Additionally, to ensure\nhiqh-quality speech generation, the data must be spontaneous like in-wild data\nand must be acoustically clean with noise removed. Despite the critical need,\nno open-source corpus meeting all these criteria has been available. This study\naddresses this gap by constructing and releasing a large-scale spoken dialogue\ncorpus, named Japanese Corpus for Human-AI Talks (J-CHAT), which is publicly\naccessible. Furthermore, this paper presents a language-independent method for\ncorpus construction and describes experiments on dialogue generation using SLMs\ntrained on J-CHAT. Experimental results indicate that the collected data from\nmultiple domains by our method improve the naturalness and meaningfulness of\ndialogue generation.\n","authors":["Wataru Nakata","Kentaro Seki","Hitomi Yanaka","Yuki Saito","Shinnosuke Takamichi","Hiroshi Saruwatari"],"pdf_url":"https://arxiv.org/pdf/2407.15828v1.pdf","comment":"8 pages, 6 figures"},{"id":"http://arxiv.org/abs/2402.16822v2","updated":"2024-07-22T17:31:43Z","published":"2024-02-26T18:47:27Z","title":"Rainbow Teaming: Open-Ended Generation of Diverse Adversarial Prompts","summary":"  As large language models (LLMs) become increasingly prevalent across many\nreal-world applications, understanding and enhancing their robustness to\nadversarial attacks is of paramount importance. Existing methods for\nidentifying adversarial prompts tend to focus on specific domains, lack\ndiversity, or require extensive human annotations. To address these\nlimitations, we present Rainbow Teaming, a novel black-box approach for\nproducing a diverse collection of adversarial prompts. Rainbow Teaming casts\nadversarial prompt generation as a quality-diversity problem, and uses\nopen-ended search to generate prompts that are both effective and diverse.\nFocusing on the safety domain, we use Rainbow Teaming to target various\nstate-of-the-art LLMs, including the Llama 2 and Llama 3 models. Our approach\nreveals hundreds of effective adversarial prompts, with an attack success rate\nexceeding 90% across all tested models. Furthermore, we demonstrate that\nfine-tuning models with synthetic data generated by the Rainbow Teaming method\nsignificantly enhances their safety without sacrificing general performance or\nhelpfulness. We additionally explore the versatility of Rainbow Teaming by\napplying it to question answering and cybersecurity, showcasing its potential\nto drive robust open-ended self-improvement in a wide range of applications.\n","authors":["Mikayel Samvelyan","Sharath Chandra Raparthy","Andrei Lupu","Eric Hambro","Aram H. Markosyan","Manish Bhatt","Yuning Mao","Minqi Jiang","Jack Parker-Holder","Jakob Foerster","Tim Rocktäschel","Roberta Raileanu"],"pdf_url":"https://arxiv.org/pdf/2402.16822v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15814v1","updated":"2024-07-22T17:26:12Z","published":"2024-07-22T17:26:12Z","title":"Perceptions of Linguistic Uncertainty by Language Models and Humans","summary":"  Uncertainty expressions such as ``probably'' or ``highly unlikely'' are\npervasive in human language. While prior work has established that there is\npopulation-level agreement in terms of how humans interpret these expressions,\nthere has been little inquiry into the abilities of language models to\ninterpret such expressions. In this paper, we investigate how language models\nmap linguistic expressions of uncertainty to numerical responses. Our approach\nassesses whether language models can employ theory of mind in this setting:\nunderstanding the uncertainty of another agent about a particular statement,\nindependently of the model's own certainty about that statement. We evaluate\nboth humans and 10 popular language models on a task created to assess these\nabilities. Unexpectedly, we find that 8 out of 10 models are able to map\nuncertainty expressions to probabilistic responses in a human-like manner.\nHowever, we observe systematically different behavior depending on whether a\nstatement is actually true or false. This sensitivity indicates that language\nmodels are substantially more susceptible to bias based on their prior\nknowledge (as compared to humans). These findings raise important questions and\nhave broad implications for human-AI alignment and AI-AI communication.\n","authors":["Catarina G Belem","Markelle Kelly","Mark Steyvers","Sameer Singh","Padhraic Smyth"],"pdf_url":"https://arxiv.org/pdf/2407.15814v1.pdf","comment":"In submission"},{"id":"http://arxiv.org/abs/2403.20262v2","updated":"2024-07-22T17:24:14Z","published":"2024-03-29T16:13:31Z","title":"ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language\n  Models","summary":"  Research on Large Language Models (LLMs) has recently witnessed an increasing\ninterest in extending models' context size to better capture dependencies\nwithin long documents. While benchmarks have been proposed to assess long-range\nabilities, existing efforts primarily considered generic tasks that are not\nnecessarily aligned with real-world applications. In contrast, our work\nproposes a new benchmark for long-context LLMs focused on a practical meeting\nassistant scenario. In this scenario, the long contexts consist of transcripts\nobtained by automatic speech recognition, presenting unique challenges for LLMs\ndue to the inherent noisiness and oral nature of such data. Our benchmark,\nnamed ELITR-Bench, augments the existing ELITR corpus' transcripts with 271\nmanually crafted questions and their ground-truth answers. Our experiments with\nrecent long-context LLMs on ELITR-Bench highlight a gap between open-source and\nproprietary models, especially when questions are asked sequentially within a\nconversation. We also provide a thorough analysis of our GPT-4-based evaluation\nmethod, encompassing insights from a crowdsourcing study. Our findings suggest\nthat while GPT-4's evaluation scores are correlated with human judges', its\nability to differentiate among more than three score levels may be limited.\n","authors":["Thibaut Thonet","Jos Rozen","Laurent Besacier"],"pdf_url":"https://arxiv.org/pdf/2403.20262v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15806v1","updated":"2024-07-22T17:20:22Z","published":"2024-07-22T17:20:22Z","title":"FSboard: Over 3 million characters of ASL fingerspelling collected via\n  smartphones","summary":"  Progress in machine understanding of sign languages has been slow and\nhampered by limited data. In this paper, we present FSboard, an American Sign\nLanguage fingerspelling dataset situated in a mobile text entry use case,\ncollected from 147 paid and consenting Deaf signers using Pixel 4A selfie\ncameras in a variety of environments. Fingerspelling recognition is an\nincomplete solution that is only one small part of sign language translation,\nbut it could provide some immediate benefit to Deaf/Hard of Hearing signers as\nmore broadly capable technology develops. At >3 million characters in length\nand >250 hours in duration, FSboard is the largest fingerspelling recognition\ndataset to date by a factor of >10x. As a simple baseline, we finetune 30 Hz\nMediaPipe Holistic landmark inputs into ByT5-Small and achieve 11.1% Character\nError Rate (CER) on a test set with unique phrases and signers. This quality\ndegrades gracefully when decreasing frame rate and excluding face/body\nlandmarks: plausible optimizations to help models run on device in real time.\n","authors":["Manfred Georg","Garrett Tanzer","Saad Hassan","Maximus Shengelia","Esha Uboweja","Sam Sepah","Sean Forbes","Thad Starner"],"pdf_url":"https://arxiv.org/pdf/2407.15806v1.pdf","comment":"Access FSboard at https://www.kaggle.com/datasets/googleai/fsboard"},{"id":"http://arxiv.org/abs/2407.15788v1","updated":"2024-07-22T16:47:31Z","published":"2024-07-22T16:47:31Z","title":"Extracting Structured Insights from Financial News: An Augmented LLM\n  Driven Approach","summary":"  Financial news plays a crucial role in decision-making processes across the\nfinancial sector, yet the efficient processing of this information into a\nstructured format remains challenging. This paper presents a novel approach to\nfinancial news processing that leverages Large Language Models (LLMs) to\novercome limitations that previously prevented the extraction of structured\ndata from unstructured financial news. We introduce a system that extracts\nrelevant company tickers from raw news article content, performs sentiment\nanalysis at the company level, and generates summaries, all without relying on\npre-structured data feeds. Our methodology combines the generative capabilities\nof LLMs, and recent prompting techniques, with a robust validation framework\nthat uses a tailored string similarity approach. Evaluation on a dataset of\n5530 financial news articles demonstrates the effectiveness of our approach,\nwith 90% of articles not missing any tickers compared with current data\nproviders, and 22% of articles having additional relevant tickers. In addition\nto this paper, the methodology has been implemented at scale with the resulting\nprocessed data made available through a live API endpoint, which is updated in\nreal-time with the latest news. To the best of our knowledge, we are the first\ndata provider to offer granular, per-company sentiment analysis from news\narticles, enhancing the depth of information available to market participants.\nWe also release the evaluation dataset of 5530 processed articles as a static\nfile, which we hope will facilitate further research leveraging financial news.\n","authors":["Rian Dolphin","Joe Dursun","Jonathan Chow","Jarrett Blankenship","Katie Adams","Quinton Pike"],"pdf_url":"https://arxiv.org/pdf/2407.15788v1.pdf","comment":"7 pages, 6 figures"},{"id":"http://arxiv.org/abs/2203.10560v3","updated":"2024-07-22T16:46:59Z","published":"2022-03-20T14:26:20Z","title":"Who Shares Fake News? Uncovering Insights from Social Media Users' Post\n  Histories","summary":"  We propose that social-media users' own post histories are an underused yet\nvaluable resource for studying fake-news sharing. By extracting textual cues\nfrom their prior posts, and contrasting their prevalence against random\nsocial-media users and others (e.g., those with similar socio-demographics,\npolitical news-sharers, and fact-check sharers), researchers can identify cues\nthat distinguish fake-news sharers, predict those most likely to share fake\nnews, and identify promising constructs to build interventions. Our research\nincludes studies along these lines. In Study 1, we explore the distinctive\nlanguage patterns of fake-news sharers, highlighting elements such as their\nhigher use of anger and power-related words. In Study 2, we show that adding\ntextual cues into predictive models enhances their accuracy in predicting\nfake-news sharers. In Study 3, we explore the contrasting role of trait and\nsituational anger, and show trait anger is associated with a greater propensity\nto share both true and fake news. In Study 4, we introduce a way to\nauthenticate Twitter accounts in surveys, before using it to explore how\ncrafting an ad copy that resonates with users' sense of power encourages the\nadoption of fact-checking tools. We hope to encourage the use of novel research\nmethods for marketers and misinformation researchers.\n","authors":["Verena Schoenmueller","Simon J. Blanchard","Gita V. Johar"],"pdf_url":"https://arxiv.org/pdf/2203.10560v3.pdf","comment":"108 pages"},{"id":"http://arxiv.org/abs/2407.15762v1","updated":"2024-07-22T16:13:38Z","published":"2024-07-22T16:13:38Z","title":"Conditioned Language Policy: A General Framework for Steerable\n  Multi-Objective Finetuning","summary":"  Reward-based finetuning is crucial for aligning language policies with\nintended behaviors (e.g., creativity and safety). A key challenge here is to\ndevelop steerable language models that trade-off multiple (conflicting)\nobjectives in a flexible and efficient manner. This paper presents Conditioned\nLanguage Policy (CLP), a general framework for finetuning language models on\nmultiple objectives. Building on techniques from multi-task training and\nparameter-efficient finetuning, CLP can learn steerable models that effectively\ntrade-off conflicting objectives at inference time. Notably, this does not\nrequire training or maintaining multiple models to achieve different trade-offs\nbetween the objectives. Through an extensive set of experiments and ablations,\nwe show that the CLP framework learns steerable models that outperform and\nPareto-dominate the current state-of-the-art approaches for multi-objective\nfinetuning.\n","authors":["Kaiwen Wang","Rahul Kidambi","Ryan Sullivan","Alekh Agarwal","Christoph Dann","Andrea Michi","Marco Gelmi","Yunxuan Li","Raghav Gupta","Avinava Dubey","Alexandre Ramé","Johan Ferret","Geoffrey Cideron","Le Hou","Hongkun Yu","Amr Ahmed","Aranyak Mehta","Léonard Hussenot","Olivier Bachem","Edouard Leurent"],"pdf_url":"https://arxiv.org/pdf/2407.15762v1.pdf","comment":"40 pages"},{"id":"http://arxiv.org/abs/2407.15754v1","updated":"2024-07-22T16:00:55Z","published":"2024-07-22T16:00:55Z","title":"LongVideoBench: A Benchmark for Long-context Interleaved Video-Language\n  Understanding","summary":"  Large multimodal models (LMMs) are processing increasingly longer and richer\ninputs. Albeit the progress, few public benchmark is available to measure such\ndevelopment. To mitigate this gap, we introduce LongVideoBench, a\nquestion-answering benchmark that features video-language interleaved inputs up\nto an hour long. Our benchmark includes 3,763 varying-length web-collected\nvideos with their subtitles across diverse themes, designed to comprehensively\nevaluate LMMs on long-term multimodal understanding. To achieve this, we\ninterpret the primary challenge as to accurately retrieve and reason over\ndetailed multimodal information from long inputs. As such, we formulate a novel\nvideo question-answering task termed referring reasoning. Specifically, as part\nof the question, it contains a referring query that references related video\ncontexts, called referred context. The model is then required to reason over\nrelevant video details from the referred context. Following the paradigm of\nreferring reasoning, we curate 6,678 human-annotated multiple-choice questions\nin 17 fine-grained categories, establishing one of the most comprehensive\nbenchmarks for long-form video understanding. Evaluations suggest that the\nLongVideoBench presents significant challenges even for the most advanced\nproprietary models (e.g. GPT-4o, Gemini-1.5-Pro, GPT-4-Turbo), while their\nopen-source counterparts show an even larger performance gap. In addition, our\nresults indicate that model performance on the benchmark improves only when\nthey are capable of processing more frames, positioning LongVideoBench as a\nvaluable benchmark for evaluating future-generation long-context LMMs.\n","authors":["Haoning Wu","Dongxu Li","Bei Chen","Junnan Li"],"pdf_url":"https://arxiv.org/pdf/2407.15754v1.pdf","comment":"29 pages"},{"id":"http://arxiv.org/abs/2407.15736v1","updated":"2024-07-22T15:40:17Z","published":"2024-07-22T15:40:17Z","title":"OMoS-QA: A Dataset for Cross-Lingual Extractive Question Answering in a\n  German Migration Context","summary":"  When immigrating to a new country, it is easy to feel overwhelmed by the need\nto obtain information on financial support, housing, schooling, language\ncourses, and other issues. If relocation is rushed or even forced, the\nnecessity for high-quality answers to such questions is all the more urgent.\nOfficial immigration counselors are usually overbooked, and online systems\ncould guide newcomers to the requested information or a suitable counseling\nservice.\n  To this end, we present OMoS-QA, a dataset of German and English questions\npaired with relevant trustworthy documents and manually annotated answers,\nspecifically tailored to this scenario. Questions are automatically generated\nwith an open-source large language model (LLM) and answer sentences are\nselected by crowd workers with high agreement. With our data, we conduct a\ncomparison of 5 pretrained LLMs on the task of extractive question answering\n(QA) in German and English. Across all models and both languages, we find high\nprecision and low-to-mid recall in selecting answer sentences, which is a\nfavorable trade-off to avoid misleading users. This performance even holds up\nwhen the question language does not match the document language. When it comes\nto identifying unanswerable questions given a context, there are larger\ndifferences between the two languages.\n","authors":["Steffen Kleinle","Jakob Prange","Annemarie Friedrich"],"pdf_url":"https://arxiv.org/pdf/2407.15736v1.pdf","comment":"Accepted to KONVENS 2024"},{"id":"http://arxiv.org/abs/2403.07805v3","updated":"2024-07-22T15:29:00Z","published":"2024-03-12T16:42:44Z","title":"Beyond Memorization: The Challenge of Random Memory Access in Language\n  Models","summary":"  Recent developments in Language Models (LMs) have shown their effectiveness\nin NLP tasks, particularly in knowledge-intensive tasks. However, the\nmechanisms underlying knowledge storage and memory access within their\nparameters remain elusive. In this paper, we investigate whether a generative\nLM (e.g., GPT-2) is able to access its memory sequentially or randomly. Through\ncarefully-designed synthetic tasks, covering the scenarios of full recitation,\nselective recitation and grounded question answering, we reveal that LMs manage\nto sequentially access their memory while encountering challenges in randomly\naccessing memorized content. We find that techniques including recitation and\npermutation improve the random memory access capability of LMs. Furthermore, by\napplying this intervention to realistic scenarios of open-domain question\nanswering, we validate that enhancing random access by recitation leads to\nnotable improvements in question answering. The code to reproduce our\nexperiments can be found at https://github.com/sail-sg/lm-random-memory-access.\n","authors":["Tongyao Zhu","Qian Liu","Liang Pang","Zhengbao Jiang","Min-Yen Kan","Min Lin"],"pdf_url":"https://arxiv.org/pdf/2403.07805v3.pdf","comment":"9 pages, 4 figures; accepted by ACL 2024 (oral)"},{"id":"http://arxiv.org/abs/2407.15723v1","updated":"2024-07-22T15:27:55Z","published":"2024-07-22T15:27:55Z","title":"DStruct2Design: Data and Benchmarks for Data Structure Driven Generative\n  Floor Plan Design","summary":"  Text conditioned generative models for images have yielded impressive\nresults. Text conditioned floorplan generation as a special type of raster\nimage generation task also received particular attention. However there are\nmany use cases in floorpla generation where numerical properties of the\ngenerated result are more important than the aesthetics. For instance, one\nmight want to specify sizes for certain rooms in a floorplan and compare the\ngenerated floorplan with given specifications Current approaches, datasets and\ncommonly used evaluations do not support these kinds of constraints. As such,\nan attractive strategy is to generate an intermediate data structure that\ncontains numerical properties of a floorplan which can be used to generate the\nfinal floorplan image. To explore this setting we (1) construct a new dataset\nfor this data-structure to data-structure formulation of floorplan generation\nusing two popular image based floorplan datasets RPLAN and ProcTHOR-10k, and\nprovide the tools to convert further procedurally generated ProcTHOR floorplan\ndata into our format. (2) We explore the task of floorplan generation given a\npartial or complete set of constraints and we design a series of metrics and\nbenchmarks to enable evaluating how well samples generated from models respect\nthe constraints. (3) We create multiple baselines by finetuning a large\nlanguage model (LLM), Llama3, and demonstrate the feasibility of using\nfloorplan data structure conditioned LLMs for the problem of floorplan\ngeneration respecting numerical constraints. We hope that our new datasets and\nbenchmarks will encourage further research on different ways to improve the\nperformance of LLMs and other generative modelling techniques for generating\ndesigns where quantitative constraints are only partially specified, but must\nbe respected.\n","authors":["Zhi Hao Luo","Luis Lara","Ge Ya Luo","Florian Golemo","Christopher Beckham","Christopher Pal"],"pdf_url":"https://arxiv.org/pdf/2407.15723v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15720v1","updated":"2024-07-22T15:22:34Z","published":"2024-07-22T15:22:34Z","title":"Do Large Language Models Have Compositional Ability? An Investigation\n  into Limitations and Scalability","summary":"  Large language models (LLMs) have emerged as powerful tools for many AI\nproblems and exhibit remarkable in-context learning (ICL) capabilities.\nCompositional ability, solving unseen complex tasks that combine two or more\nsimple tasks, is an essential reasoning ability for Artificial General\nIntelligence. Despite LLM's tremendous success, how they approach composite\ntasks, especially those not encountered during the pretraining phase, remains\nan open question and largely ununderstood. In this study, we delve into the ICL\ncapabilities of LLMs on composite tasks, with only simple tasks as in-context\nexamples. We develop a test suite of composite tasks that include linguistic\nand logical challenges and perform empirical studies across different LLM\nfamilies. We observe that models exhibit divergent behaviors: (1) For simpler\ncomposite tasks that apply distinct mapping mechanisms to different input\nsegments, the models demonstrate decent compositional ability, while scaling up\nthe model enhances this ability; (2) for more complex composite tasks that\ninvolving reasoning multiple steps, where each step represent one task, models\ntypically underperform, and scaling up generally provide no improvements. We\noffer theoretical analysis in a simplified setting, explaining that models\nexhibit compositional capability when the task handles different input parts\nseparately. We believe our work sheds new light on the capabilities of LLMs in\nsolving composite tasks regarding the nature of the tasks and model scale. Our\ndataset and code are available at\n{\\url{https://github.com/OliverXUZY/LLM_Compose}}.\n","authors":["Zhuoyan Xu","Zhenmei Shi","Yingyu Liang"],"pdf_url":"https://arxiv.org/pdf/2407.15720v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15711v1","updated":"2024-07-22T15:18:45Z","published":"2024-07-22T15:18:45Z","title":"AssistantBench: Can Web Agents Solve Realistic and Time-Consuming Tasks?","summary":"  Language agents, built on top of language models (LMs), are systems that can\ninteract with complex environments, such as the open web. In this work, we\nexamine whether such agents can perform realistic and time-consuming tasks on\nthe web, e.g., monitoring real-estate markets or locating relevant nearby\nbusinesses. We introduce AssistantBench, a challenging new benchmark consisting\nof 214 realistic tasks that can be automatically evaluated, covering different\nscenarios and domains. We find that AssistantBench exposes the limitations of\ncurrent systems, including language models and retrieval-augmented language\nmodels, as no model reaches an accuracy of more than 25 points. While\nclosed-book LMs perform well, they exhibit low precision since they tend to\nhallucinate facts. State-of-the-art web agents reach a score of near zero.\nAdditionally, we introduce SeePlanAct (SPA), a new web agent that significantly\noutperforms previous agents, and an ensemble of SPA and closed-book models\nreaches the best overall performance. Moreover, we analyze failures of current\nsystems and highlight that web navigation remains a major challenge.\n","authors":["Ori Yoran","Samuel Joseph Amouyal","Chaitanya Malaviya","Ben Bogin","Ofir Press","Jonathan Berant"],"pdf_url":"https://arxiv.org/pdf/2407.15711v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15695v1","updated":"2024-07-22T15:01:45Z","published":"2024-07-22T15:01:45Z","title":"Supporting the Digital Autonomy of Elders Through LLM Assistance","summary":"  The internet offers tremendous access to services, social connections, and\nneeded products. However, to those without sufficient experience, engaging with\nbusinesses and friends across the internet can be daunting due to the ever\npresent danger of scammers and thieves, to say nothing of the myriad of\npotential computer viruses. Like a forest rich with both edible and poisonous\nplants, those familiar with the norms inhabit it safely with ease while\nnewcomers need a guide. However, reliance on a human digital guide can be\ntaxing and often impractical. We propose and pilot a simple but unexplored\nidea: could an LLM provide the necessary support to help the elderly who are\nseparated by the digital divide safely achieve digital autonomy?\n","authors":["Jesse Roberts","Lindsey Roberts","Alice Reed"],"pdf_url":"https://arxiv.org/pdf/2407.15695v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15694v1","updated":"2024-07-22T15:00:23Z","published":"2024-07-22T15:00:23Z","title":"Counter Turing Test ($CT^2$): Investigating AI-Generated Text Detection\n  for Hindi -- Ranking LLMs based on Hindi AI Detectability Index ($ADI_{hi}$)","summary":"  The widespread adoption of large language models (LLMs) and awareness around\nmultilingual LLMs have raised concerns regarding the potential risks and\nrepercussions linked to the misapplication of AI-generated text, necessitating\nincreased vigilance. While these models are primarily trained for English,\ntheir extensive training on vast datasets covering almost the entire web,\nequips them with capabilities to perform well in numerous other languages.\nAI-Generated Text Detection (AGTD) has emerged as a topic that has already\nreceived immediate attention in research, with some initial methods having been\nproposed, soon followed by the emergence of techniques to bypass detection. In\nthis paper, we report our investigation on AGTD for an indic language Hindi.\nOur major contributions are in four folds: i) examined 26 LLMs to evaluate\ntheir proficiency in generating Hindi text, ii) introducing the AI-generated\nnews article in Hindi ($AG_{hi}$) dataset, iii) evaluated the effectiveness of\nfive recently proposed AGTD techniques: ConDA, J-Guard, RADAR, RAIDAR and\nIntrinsic Dimension Estimation for detecting AI-generated Hindi text, iv)\nproposed Hindi AI Detectability Index ($ADI_{hi}$) which shows a spectrum to\nunderstand the evolving landscape of eloquence of AI-generated text in Hindi.\nWe will make the codes and datasets available to encourage further research.\n","authors":["Ishan Kavathekar","Anku Rani","Ashmit Chamoli","Ponnurangam Kumaraguru","Amit Sheth","Amitava Das"],"pdf_url":"https://arxiv.org/pdf/2407.15694v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02938v2","updated":"2024-07-22T14:34:04Z","published":"2024-01-01T23:10:23Z","title":"Fast and Effective Weight Update for Pruned Large Language Models","summary":"  Pruning large language models (LLMs) is a challenging task due to their\nenormous size. The primary difficulty is fine-tuning the model after pruning,\nwhich is needed to recover the lost performance caused by dropping weights.\nRecent approaches have either ignored fine-tuning entirely, focusing on\nefficient pruning criteria, or attempted layer-wise weight updates, preserving\nthe behavior of each layer. However, even layer-wise weight updates can be\ncostly for LLMs, and previous works have resorted to various approximations.\n  In our paper, we propose a fast and effective weight update algorithm for\npruned layers based on the Alternating Direction Method of Multipliers (ADMM).\nWe further extend it with a simple gradual pruning mask selection and achieve\nstate-of-the-art pruning performance across a wide range of LLMs. Code is\navailable at https://github.com/fmfi-compbio/admm-pruning.\n","authors":["Vladimír Boža"],"pdf_url":"https://arxiv.org/pdf/2401.02938v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06613v2","updated":"2024-07-22T14:32:33Z","published":"2024-06-07T00:28:43Z","title":"GameBench: Evaluating Strategic Reasoning Abilities of LLM Agents","summary":"  Large language models have demonstrated remarkable few-shot performance on\nmany natural language understanding tasks. Despite several demonstrations of\nusing large language models in complex, strategic scenarios, there lacks a\ncomprehensive framework for evaluating agents' performance across various types\nof reasoning found in games. To address this gap, we introduce GameBench, a\ncross-domain benchmark for evaluating strategic reasoning abilities of LLM\nagents. We focus on 9 different game environments, where each covers at least\none axis of key reasoning skill identified in strategy games, and select games\nfor which strategy explanations are unlikely to form a significant portion of\nmodels' pretraining corpuses. Our evaluations use GPT-3 and GPT-4 in their base\nform along with two scaffolding frameworks designed to enhance strategic\nreasoning ability: Chain-of-Thought (CoT) prompting and Reasoning Via Planning\n(RAP). Our results show that none of the tested models match human performance,\nand at worst GPT-4 performs worse than random action. CoT and RAP both improve\nscores but not comparable to human levels.\n","authors":["Anthony Costarelli","Mat Allen","Roman Hauksson","Grace Sodunke","Suhas Hariharan","Carlson Cheng","Wenjie Li","Joshua Clymer","Arjun Yadav"],"pdf_url":"https://arxiv.org/pdf/2406.06613v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.04474v3","updated":"2024-07-22T14:27:56Z","published":"2023-12-07T17:51:43Z","title":"Chain of Code: Reasoning with a Language Model-Augmented Code Emulator","summary":"  Code provides a general syntactic structure to build complex programs and\nperform precise computations when paired with a code interpreter - we\nhypothesize that language models (LMs) can leverage code-writing to improve\nChain of Thought reasoning not only for logic and arithmetic tasks, but also\nfor semantic ones (and in particular, those that are a mix of both). For\nexample, consider prompting an LM to write code that counts the number of times\nit detects sarcasm in an essay: the LM may struggle to write an implementation\nfor \"detect_sarcasm(string)\" that can be executed by the interpreter (handling\nthe edge cases would be insurmountable). However, LMs may still produce a valid\nsolution if they not only write code, but also selectively \"emulate\" the\ninterpreter by generating the expected output of \"detect_sarcasm(string)\". In\nthis work, we propose Chain of Code (CoC), a simple yet surprisingly effective\nextension that improves LM code-driven reasoning. The key idea is to encourage\nLMs to format semantic sub-tasks in a program as flexible pseudocode that the\ninterpreter can explicitly catch undefined behaviors and hand off to simulate\nwith an LM (as an \"LMulator\"). Experiments demonstrate that Chain of Code\noutperforms Chain of Thought and other baselines across a variety of\nbenchmarks; on BIG-Bench Hard, Chain of Code achieves 84%, a gain of 12% over\nChain of Thought. In a nutshell, CoC broadens the scope of reasoning questions\nthat LMs can answer by \"thinking in code\".\n","authors":["Chengshu Li","Jacky Liang","Andy Zeng","Xinyun Chen","Karol Hausman","Dorsa Sadigh","Sergey Levine","Li Fei-Fei","Fei Xia","Brian Ichter"],"pdf_url":"https://arxiv.org/pdf/2312.04474v3.pdf","comment":"ICML 2024 Oral; Project webpage: https://chain-of-code.github.io"},{"id":"http://arxiv.org/abs/2407.15645v1","updated":"2024-07-22T14:02:59Z","published":"2024-07-22T14:02:59Z","title":"Psychometric Alignment: Capturing Human Knowledge Distributions via\n  Language Models","summary":"  Language models (LMs) are increasingly used to simulate human-like responses\nin scenarios where accurately mimicking a population's behavior can guide\ndecision-making, such as in developing educational materials and designing\npublic policies. The objective of these simulations is for LMs to capture the\nvariations in human responses, rather than merely providing the expected\ncorrect answers. Prior work has shown that LMs often generate unrealistically\naccurate responses, but there are no established metrics to quantify how\nclosely the knowledge distribution of LMs aligns with that of humans. To\naddress this, we introduce \"psychometric alignment,\" a metric that measures the\nextent to which LMs reflect human knowledge distribution. Assessing this\nalignment involves collecting responses from both LMs and humans to the same\nset of test items and using Item Response Theory to analyze the differences in\nitem functioning between the groups. We demonstrate that our metric can capture\nimportant variations in populations that traditional metrics, like differences\nin accuracy, fail to capture. We apply this metric to assess existing LMs for\ntheir alignment with human knowledge distributions across three real-world\ndomains. We find significant misalignment between LMs and human populations,\nthough using persona-based prompts can improve alignment. Interestingly,\nsmaller LMs tend to achieve greater psychometric alignment than larger LMs.\nFurther, training LMs on human response data from the target distribution\nenhances their psychometric alignment on unseen test items, but the\neffectiveness of such training varies across domains.\n","authors":["Joy He-Yueya","Wanjing Anya Ma","Kanishk Gandhi","Benjamin W. Domingue","Emma Brunskill","Noah D. Goodman"],"pdf_url":"https://arxiv.org/pdf/2407.15645v1.pdf","comment":"Code and data: https://github.com/joyheyueya/psychometric-alignment"},{"id":"http://arxiv.org/abs/2402.19379v6","updated":"2024-07-22T13:50:27Z","published":"2024-02-29T17:27:59Z","title":"Wisdom of the Silicon Crowd: LLM Ensemble Prediction Capabilities Rival\n  Human Crowd Accuracy","summary":"  Human forecasting accuracy in practice relies on the 'wisdom of the crowd'\neffect, in which predictions about future events are significantly improved by\naggregating across a crowd of individual forecasters. Past work on the\nforecasting ability of large language models (LLMs) suggests that frontier\nLLMs, as individual forecasters, underperform compared to the gold standard of\na human crowd forecasting tournament aggregate. In Study 1, we expand this\nresearch by using an LLM ensemble approach consisting of a crowd of twelve\nLLMs. We compare the aggregated LLM predictions on 31 binary questions to that\nof a crowd of 925 human forecasters from a three-month forecasting tournament.\nOur preregistered main analysis shows that the LLM crowd outperforms a simple\nno-information benchmark and is not statistically different from the human\ncrowd. In exploratory analyses, we find that these two approaches are\nequivalent with respect to medium-effect-size equivalence bounds. We also\nobserve an acquiescence effect, with mean model predictions being significantly\nabove 50%, despite an almost even split of positive and negative resolutions.\nMoreover, in Study 2, we test whether LLM predictions (of GPT-4 and Claude 2)\ncan be improved by drawing on human cognitive output. We find that both models'\nforecasting accuracy benefits from exposure to the median human prediction as\ninformation, improving accuracy by between 17% and 28%: though this leads to\nless accurate predictions than simply averaging human and machine forecasts.\nOur results suggest that LLMs can achieve forecasting accuracy rivaling that of\nhuman crowd forecasting tournaments: via the simple, practically applicable\nmethod of forecast aggregation. This replicates the 'wisdom of the crowd'\neffect for LLMs, and opens up their use for a variety of applications\nthroughout society.\n","authors":["Philipp Schoenegger","Indre Tuminauskaite","Peter S. Park","Philip E. Tetlock"],"pdf_url":"https://arxiv.org/pdf/2402.19379v6.pdf","comment":"20 pages; 13 visualizations (nine figures, four tables)"},{"id":"http://arxiv.org/abs/2407.11591v2","updated":"2024-07-22T13:47:08Z","published":"2024-07-16T10:50:39Z","title":"AdaptEval: Evaluating Large Language Models on Domain Adaptation for\n  Text Summarization","summary":"  Despite the advances in the abstractive summarization task using Large\nLanguage Models (LLM), there is a lack of research that asses their abilities\nto easily adapt to different domains. We evaluate the domain adaptation\nabilities of a wide range of LLMs on the summarization task across various\ndomains in both fine-tuning and in-context learning settings. We also present\nAdaptEval, the first domain adaptation evaluation suite. AdaptEval includes a\ndomain benchmark and a set of metrics to facilitate the analysis of domain\nadaptation. Our results demonstrate that LLMs exhibit comparable performance in\nthe in-context learning setting, regardless of their parameter scale.\n","authors":["Anum Afzal","Ribin Chalumattu","Florian Matthes","Laura Mascarell"],"pdf_url":"https://arxiv.org/pdf/2407.11591v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15621v1","updated":"2024-07-22T13:29:56Z","published":"2024-07-22T13:29:56Z","title":"RadioRAG: Factual Large Language Models for Enhanced Diagnostics in\n  Radiology Using Dynamic Retrieval Augmented Generation","summary":"  Large language models (LLMs) have advanced the field of artificial\nintelligence (AI) in medicine. However LLMs often generate outdated or\ninaccurate information based on static training datasets. Retrieval augmented\ngeneration (RAG) mitigates this by integrating outside data sources. While\nprevious RAG systems used pre-assembled, fixed databases with limited\nflexibility, we have developed Radiology RAG (RadioRAG) as an end-to-end\nframework that retrieves data from authoritative radiologic online sources in\nreal-time. RadioRAG is evaluated using a dedicated radiologic\nquestion-and-answer dataset (RadioQA). We evaluate the diagnostic accuracy of\nvarious LLMs when answering radiology-specific questions with and without\naccess to additional online information via RAG. Using 80 questions from RSNA\nCase Collection across radiologic subspecialties and 24 additional\nexpert-curated questions, for which the correct gold-standard answers were\navailable, LLMs (GPT-3.5-turbo, GPT-4, Mistral-7B, Mixtral-8x7B, and Llama3 [8B\nand 70B]) were prompted with and without RadioRAG. RadioRAG retrieved\ncontext-specific information from www.radiopaedia.org in real-time and\nincorporated them into its reply. RadioRAG consistently improved diagnostic\naccuracy across all LLMs, with relative improvements ranging from 2% to 54%. It\nmatched or exceeded question answering without RAG across radiologic\nsubspecialties, particularly in breast imaging and emergency radiology.\nHowever, degree of improvement varied among models; GPT-3.5-turbo and\nMixtral-8x7B-instruct-v0.1 saw notable gains, while Mistral-7B-instruct-v0.2\nshowed no improvement, highlighting variability in its effectiveness. LLMs\nbenefit when provided access to domain-specific data beyond their training\ndata. For radiology, RadioRAG establishes a robust framework that substantially\nimproves diagnostic accuracy and factuality in radiological question answering.\n","authors":["Soroosh Tayebi Arasteh","Mahshad Lotfinia","Keno Bressem","Robert Siepmann","Dyke Ferber","Christiane Kuhl","Jakob Nikolas Kather","Sven Nebelung","Daniel Truhn"],"pdf_url":"https://arxiv.org/pdf/2407.15621v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15612v1","updated":"2024-07-22T13:14:27Z","published":"2024-07-22T13:14:27Z","title":"Can GPT-4 learn to analyze moves in research article abstracts?","summary":"  One of the most powerful and enduring ideas in written discourse analysis is\nthat genres can be described in terms of the moves which structure a writer's\npurpose. Considerable research has sought to identify these distinct\ncommunicative acts, but analyses have been beset by problems of subjectivity,\nreliability and the time-consuming need for multiple coders to confirm\nanalyses. In this paper we employ the affordances of GPT-4 to automate the\nannotation process by using natural language prompts. Focusing on abstracts\nfrom articles in four applied linguistics journals, we devise prompts which\nenable the model to identify moves effectively. The annotated outputs of these\nprompts were evaluated by two assessors with a third addressing disagreements.\nThe results show that an 8-shot prompt was more effective than one using two,\nconfirming that the inclusion of examples illustrating areas of variability can\nenhance GPT-4's ability to recognize multiple moves in a single sentence and\nreduce bias related to textual position. We suggest that GPT-4 offers\nconsiderable potential in automating this annotation process, when human actors\nwith domain specific linguistic expertise inform the prompting process.\n","authors":["Danni Yu","Marina Bondi","Ken Hylannd"],"pdf_url":"https://arxiv.org/pdf/2407.15612v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15608v1","updated":"2024-07-22T13:08:30Z","published":"2024-07-22T13:08:30Z","title":"StylusAI: Stylistic Adaptation for Robust German Handwritten Text\n  Generation","summary":"  In this study, we introduce StylusAI, a novel architecture leveraging\ndiffusion models in the domain of handwriting style generation. StylusAI is\nspecifically designed to adapt and integrate the stylistic nuances of one\nlanguage's handwriting into another, particularly focusing on blending English\nhandwriting styles into the context of the German writing system. This approach\nenables the generation of German text in English handwriting styles and German\nhandwriting styles into English, enriching machine-generated handwriting\ndiversity while ensuring that the generated text remains legible across both\nlanguages. To support the development and evaluation of StylusAI, we present\nthe \\lq{Deutscher Handschriften-Datensatz}\\rq~(DHSD), a comprehensive dataset\nencompassing 37 distinct handwriting styles within the German language. This\ndataset provides a fundamental resource for training and benchmarking in the\nrealm of handwritten text generation. Our results demonstrate that StylusAI not\nonly introduces a new method for style adaptation in handwritten text\ngeneration but also surpasses existing models in generating handwriting samples\nthat improve both text quality and stylistic fidelity, evidenced by its\nperformance on the IAM database and our newly proposed DHSD. Thus, StylusAI\nrepresents a significant advancement in the field of handwriting style\ngeneration, offering promising avenues for future research and applications in\ncross-linguistic style adaptation for languages with similar scripts.\n","authors":["Nauman Riaz","Saifullah Saifullah","Stefan Agne","Andreas Dengel","Sheraz Ahmed"],"pdf_url":"https://arxiv.org/pdf/2407.15608v1.pdf","comment":"Accepted in ICDAR 2024"},{"id":"http://arxiv.org/abs/2407.09417v2","updated":"2024-07-22T12:28:05Z","published":"2024-07-12T16:47:34Z","title":"Mitigating Entity-Level Hallucination in Large Language Models","summary":"  The emergence of Large Language Models (LLMs) has revolutionized how users\naccess information, shifting from traditional search engines to direct\nquestion-and-answer interactions with LLMs. However, the widespread adoption of\nLLMs has revealed a significant challenge known as hallucination, wherein LLMs\ngenerate coherent yet factually inaccurate responses. This hallucination\nphenomenon has led to users' distrust in information retrieval systems based on\nLLMs. To tackle this challenge, this paper proposes Dynamic Retrieval\nAugmentation based on hallucination Detection (DRAD) as a novel method to\ndetect and mitigate hallucinations in LLMs. DRAD improves upon traditional\nretrieval augmentation by dynamically adapting the retrieval process based on\nreal-time hallucination detection. It features two main components: Real-time\nHallucination Detection (RHD) for identifying potential hallucinations without\nexternal models, and Self-correction based on External Knowledge (SEK) for\ncorrecting these errors using external knowledge. Experiment results show that\nDRAD demonstrates superior performance in both detecting and mitigating\nhallucinations in LLMs. All of our code and data are open-sourced at\nhttps://github.com/oneal2000/EntityHallucination.\n","authors":["Weihang Su","Yichen Tang","Qingyao Ai","Changyue Wang","Zhijing Wu","Yiqun Liu"],"pdf_url":"https://arxiv.org/pdf/2407.09417v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15588v1","updated":"2024-07-22T12:25:48Z","published":"2024-07-22T12:25:48Z","title":"Unsupervised Robust Cross-Lingual Entity Alignment via Joint Modeling of\n  Entity and Relation Texts","summary":"  Cross-lingual entity alignment (EA) enables the integration of multiple\nknowledge graphs (KGs) across different languages, providing users with\nseamless access to diverse and comprehensive knowledge.Existing methods, mostly\nsupervised, face challenges in obtaining labeled entity pairs. To address this,\nrecent studies have shifted towards a self-supervised and unsupervised\nframeworks. Despite their effectiveness, these approaches have limitations: (1)\nthey mainly focus on entity features, neglecting the semantic information of\nrelations, (2) they assume isomorphism between source and target graphs,\nleading to noise and reduced alignment accuracy, and (3) they are susceptible\nto noise in the textual features, especially when encountering inconsistent\ntranslations or Out-Of-Vocabulary (OOV) problems.\n  In this paper, we propose ERAlign, an unsupervised and robust cross-lingual\nEA framework that jointly performs Entity-level and Relation-level Alignment\nusing semantic textual features of relations and entities. Its refinement\nprocess iteratively enhances results by fusing entity-level and relation-level\nalignments based on neighbor triple matching. The additional verification\nprocess examines the entities' neighbor triples as the linearized text. This\n\\textit{Align-and-Verify} pipeline that rigorously assesses alignment results,\nachieving near-perfect alignment even in the presence of noisy textual features\nof entities. Our extensive experiments demonstrate that robustness and general\napplicability of \\proposed improved the accuracy and effectiveness of EA tasks,\ncontributing significantly to knowledge-oriented applications.\n","authors":["Soojin Yoon","Sungho Ko","Tongyoung Kim","SeongKu Kang","Jinyoung Yeo","Dongha Lee"],"pdf_url":"https://arxiv.org/pdf/2407.15588v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.05112v7","updated":"2024-07-22T12:16:30Z","published":"2023-11-09T02:55:58Z","title":"A Survey of Large Language Models in Medicine: Progress, Application,\n  and Challenge","summary":"  Large language models (LLMs), such as ChatGPT, have received substantial\nattention due to their capabilities for understanding and generating human\nlanguage. While there has been a burgeoning trend in research focusing on the\nemployment of LLMs in supporting different medical tasks (e.g., enhancing\nclinical diagnostics and providing medical education), a review of these\nefforts, particularly their development, practical applications, and outcomes\nin medicine, remains scarce. Therefore, this review aims to provide a detailed\noverview of the development and deployment of LLMs in medicine, including the\nchallenges and opportunities they face. In terms of development, we provide a\ndetailed introduction to the principles of existing medical LLMs, including\ntheir basic model structures, number of parameters, and sources and scales of\ndata used for model development. It serves as a guide for practitioners in\ndeveloping medical LLMs tailored to their specific needs. In terms of\ndeployment, we offer a comparison of the performance of different LLMs across\nvarious medical tasks, and further compare them with state-of-the-art\nlightweight models, aiming to provide an understanding of the advantages and\nlimitations of LLMs in medicine. Overall, in this review, we address the\nfollowing questions: 1) What are the practices for developing medical LLMs 2)\nHow to measure the medical task performance of LLMs in a medical setting? 3)\nHow have medical LLMs been employed in real-world practice? 4) What challenges\narise from the use of medical LLMs? and 5) How to more effectively develop and\ndeploy medical LLMs? By answering these questions, this review aims to provide\ninsights into the opportunities for LLMs in medicine and serve as a practical\nresource. We also maintain a regularly updated list of practical guides on\nmedical LLMs at https://github.com/AI-in-Health/MedLLMsPracticalGuide\n","authors":["Hongjian Zhou","Fenglin Liu","Boyang Gu","Xinyu Zou","Jinfa Huang","Jinge Wu","Yiru Li","Sam S. Chen","Peilin Zhou","Junling Liu","Yining Hua","Chengfeng Mao","Chenyu You","Xian Wu","Yefeng Zheng","Lei Clifton","Zheng Li","Jiebo Luo","David A. Clifton"],"pdf_url":"https://arxiv.org/pdf/2311.05112v7.pdf","comment":"Preprint. Version 6. Update Figures 1-5; Tables 2-3; 31 pages"},{"id":"http://arxiv.org/abs/2401.04152v2","updated":"2024-07-22T12:14:07Z","published":"2024-01-08T16:37:45Z","title":"Cross-Speaker Encoding Network for Multi-Talker Speech Recognition","summary":"  End-to-end multi-talker speech recognition has garnered great interest as an\neffective approach to directly transcribe overlapped speech from multiple\nspeakers. Current methods typically adopt either 1) single-input\nmultiple-output (SIMO) models with a branched encoder, or 2) single-input\nsingle-output (SISO) models based on attention-based encoder-decoder\narchitecture with serialized output training (SOT). In this work, we propose a\nCross-Speaker Encoding (CSE) network to address the limitations of SIMO models\nby aggregating cross-speaker representations. Furthermore, the CSE model is\nintegrated with SOT to leverage both the advantages of SIMO and SISO while\nmitigating their drawbacks. To the best of our knowledge, this work represents\nan early effort to integrate SIMO and SISO for multi-talker speech recognition.\nExperiments on the two-speaker LibrispeechMix dataset show that the CES model\nreduces word error rate (WER) by 8% over the SIMO baseline. The CSE-SOT model\nreduces WER by 10% overall and by 16% on high-overlap speech compared to the\nSOT model. Code is available at https://github.com/kjw11/CSEnet-ASR.\n","authors":["Jiawen Kang","Lingwei Meng","Mingyu Cui","Haohan Guo","Xixin Wu","Xunying Liu","Helen Meng"],"pdf_url":"https://arxiv.org/pdf/2401.04152v2.pdf","comment":"Accepted by ICASSP2024"},{"id":"http://arxiv.org/abs/2407.12707v2","updated":"2024-07-22T12:08:35Z","published":"2024-07-17T16:30:27Z","title":"TTSDS -- Text-to-Speech Distribution Score","summary":"  Many recently published Text-to-Speech (TTS) systems produce audio close to\nreal speech. However, TTS evaluation needs to be revisited to make sense of the\nresults obtained with the new architectures, approaches and datasets. We\npropose evaluating the quality of synthetic speech as a combination of multiple\nfactors such as prosody, speaker identity, and intelligibility. Our approach\nassesses how well synthetic speech mirrors real speech by obtaining correlates\nof each factor and measuring their distance from both real speech datasets and\nnoise datasets. We benchmark 35 TTS systems developed between 2008 and 2024 and\nshow that our score computed as an unweighted average of factors strongly\ncorrelates with the human evaluations from each time period.\n","authors":["Christoph Minixhofer","Ondřej Klejch","Peter Bell"],"pdf_url":"https://arxiv.org/pdf/2407.12707v2.pdf","comment":"Under review for SLT 2024"},{"id":"http://arxiv.org/abs/2406.11260v2","updated":"2024-07-22T11:56:44Z","published":"2024-06-17T07:00:41Z","title":"Adversarial Style Augmentation via Large Language Model for Robust Fake\n  News Detection","summary":"  The spread of fake news negatively impacts individuals and is regarded as a\nsignificant social challenge that needs to be addressed. A number of\nalgorithmic and insightful features have been identified for detecting fake\nnews. However, with the recent LLMs and their advanced generation capabilities,\nmany of the detectable features (e.g., style-conversion attacks) can be\naltered, making it more challenging to distinguish from real news. This study\nproposes adversarial style augmentation, AdStyle, to train a fake news detector\nthat remains robust against various style-conversion attacks. Our model's key\nmechanism is the careful use of LLMs to automatically generate a diverse yet\ncoherent range of style-conversion attack prompts. This improves the generation\nof prompts that are particularly difficult for the detector to handle.\nExperiments show that our augmentation strategy improves robustness and\ndetection performance when tested on fake news benchmark datasets.\n","authors":["Sungwon Park","Sungwon Han","Meeyoung Cha"],"pdf_url":"https://arxiv.org/pdf/2406.11260v2.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2407.15569v1","updated":"2024-07-22T11:55:14Z","published":"2024-07-22T11:55:14Z","title":"An Empirical Study of Retrieval Augmented Generation with\n  Chain-of-Thought","summary":"  Since the launch of ChatGPT at the end of 2022, generative dialogue models\nrepresented by ChatGPT have quickly become essential tools in daily life. As\nuser expectations increase, enhancing the capability of generative dialogue\nmodels to solve complex problems has become a focal point of current research.\nThis paper delves into the effectiveness of the RAFT (Retrieval Augmented\nFine-Tuning) method in improving the performance of Generative dialogue models.\nRAFT combines chain-of-thought with model supervised fine-tuning (SFT) and\nretrieval augmented generation (RAG), which significantly enhanced the model's\ninformation extraction and logical reasoning abilities. We evaluated the RAFT\nmethod across multiple datasets and analysed its performance in various\nreasoning tasks, including long-form QA and short-form QA tasks, tasks in both\nChinese and English, and supportive and comparison reasoning tasks. Notably, it\naddresses the gaps in previous research regarding long-form QA tasks and\nChinese datasets. Moreover, we also evaluate the benefit of the\nchain-of-thought (CoT) in the RAFT method. This work offers valuable insights\nfor studies focused on enhancing the performance of generative dialogue models.\n","authors":["Yuetong Zhao","Hongyu Cao","Xianyu Zhao","Zhijian Ou"],"pdf_url":"https://arxiv.org/pdf/2407.15569v1.pdf","comment":"5 pages, 4 figures"},{"id":"http://arxiv.org/abs/2407.15556v1","updated":"2024-07-22T11:34:48Z","published":"2024-07-22T11:34:48Z","title":"SETTP: Style Extraction and Tunable Inference via Dual-level\n  Transferable Prompt Learning","summary":"  Text style transfer, an important research direction in natural language\nprocessing, aims to adapt the text to various preferences but often faces\nchallenges with limited resources. In this work, we introduce a novel method\ntermed Style Extraction and Tunable Inference via Dual-level Transferable\nPrompt Learning (SETTP) for effective style transfer in low-resource scenarios.\nFirst, SETTP learns source style-level prompts containing fundamental style\ncharacteristics from high-resource style transfer. During training, the source\nstyle-level prompts are transferred through an attention module to derive a\ntarget style-level prompt for beneficial knowledge provision in low-resource\nstyle transfer. Additionally, we propose instance-level prompts obtained by\nclustering the target resources based on the semantic content to reduce\nsemantic bias. We also propose an automated evaluation approach of style\nsimilarity based on alignment with human evaluations using ChatGPT-4. Our\nexperiments across three resourceful styles show that SETTP requires only\n1/20th of the data volume to achieve performance comparable to state-of-the-art\nmethods. In tasks involving scarce data like writing style and role style,\nSETTP outperforms previous methods by 16.24\\%.\n","authors":["Chunzhen Jin","Yongfeng Huang","Yaqi Wang","Peng Cao","Osmar Zaiane"],"pdf_url":"https://arxiv.org/pdf/2407.15556v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14246v2","updated":"2024-07-22T11:22:30Z","published":"2024-07-19T12:28:22Z","title":"Unipa-GPT: Large Language Models for university-oriented QA in Italian","summary":"  This paper illustrates the architecture and training of Unipa-GPT, a chatbot\nrelying on a Large Language Model, developed for assisting students in choosing\na bachelor/master degree course at the University of Palermo. Unipa-GPT relies\non gpt-3.5-turbo, it was presented in the context of the European Researchers'\nNight (SHARPER night). In our experiments we adopted both the Retrieval\nAugmented Generation (RAG) approach and fine-tuning to develop the system. The\nwhole architecture of Unipa-GPT is presented, both the RAG and the fine-tuned\nsystems are compared, and a brief discussion on their performance is reported.\nFurther comparison with other Large Language Models and the experimental\nresults during the SHARPER night are illustrated.\n","authors":["Irene Siragusa","Roberto Pirrone"],"pdf_url":"https://arxiv.org/pdf/2407.14246v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15549v1","updated":"2024-07-22T11:19:14Z","published":"2024-07-22T11:19:14Z","title":"Targeted Latent Adversarial Training Improves Robustness to Persistent\n  Harmful Behaviors in LLMs","summary":"  Large language models (LLMs) can often be made to behave in undesirable ways\nthat they are explicitly fine-tuned not to. For example, the LLM red-teaming\nliterature has produced a wide variety of `jailbreaking' techniques to elicit\nharmful text from models that were fine-tuned to be harmless. Recent work on\nred-teaming, model editing, and interpretability suggests that this challenge\nstems from how (adversarial) fine-tuning largely serves to suppress rather than\nremove undesirable capabilities from LLMs. Prior work has introduced latent\nadversarial training (LAT) as a way to improve robustness to broad classes of\nfailures. These prior works have considered untargeted latent space attacks\nwhere the adversary perturbs latent activations to maximize loss on examples of\ndesirable behavior. Untargeted LAT can provide a generic type of robustness but\ndoes not leverage information about specific failure modes. Here, we experiment\nwith targeted LAT where the adversary seeks to minimize loss on a specific\ncompeting task. We find that it can augment a wide variety of state-of-the-art\nmethods. First, we use targeted LAT to improve robustness to jailbreaks,\noutperforming a strong R2D2 baseline with orders of magnitude less compute.\nSecond, we use it to more effectively remove backdoors with no knowledge of the\ntrigger. Finally, we use it to more effectively unlearn knowledge for specific\nundesirable tasks in a way that is also more robust to re-learning. Overall,\nour results suggest that targeted LAT can be an effective tool for defending\nagainst harmful behaviors from LLMs.\n","authors":["Abhay Sheshadri","Aidan Ewart","Phillip Guo","Aengus Lynch","Cindy Wu","Vivek Hebbar","Henry Sleight","Asa Cooper Stickland","Ethan Perez","Dylan Hadfield-Menell","Stephen Casper"],"pdf_url":"https://arxiv.org/pdf/2407.15549v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.07598v3","updated":"2024-07-22T11:13:54Z","published":"2024-01-15T11:06:43Z","title":"MAPLE: Multilingual Evaluation of Parameter Efficient Finetuning of\n  Large Language Models","summary":"  Parameter Efficient Finetuning (PEFT) has emerged as a viable solution for\nimproving the performance of Large Language Models (LLMs) without requiring\nmassive resources and compute. Prior work on multilingual evaluation has shown\nthat there is a large gap between the performance of LLMs on English and other\nlanguages. Further, there is also a large gap between the performance of\nsmaller open-source models and larger LLMs. Finetuning can be an effective way\nto bridge this gap and make language models more equitable. In this work, we\nfinetune the LLama-2-7B and Mistral-7B models on two synthetic multilingual\ninstruction tuning datasets to determine its effect on model performance on six\ndownstream tasks covering forty languages in all. Additionally, we experiment\nwith various parameters, such as rank for low-rank adaptation and values of\nquantisation to determine their effects on downstream performance and find that\nhigher rank and higher quantisation values benefit low-resource languages. We\nfind that PEFT of smaller open-source models sometimes bridges the gap between\nthe performance of these models and the larger ones, however, English\nperformance can take a hit. We also find that finetuning sometimes improves\nperformance on low-resource languages, while degrading performance on\nhigh-resource languages.\n","authors":["Divyanshu Aggarwal","Ashutosh Sathe","Ishaan Watts","Sunayana Sitaram"],"pdf_url":"https://arxiv.org/pdf/2401.07598v3.pdf","comment":"46 pages, 23 figures, 45 tables. Accepted in ACL 2024 findings"},{"id":"http://arxiv.org/abs/2310.20204v4","updated":"2024-07-22T11:01:54Z","published":"2023-10-31T06:04:18Z","title":"General-Purpose Retrieval-Enhanced Medical Prediction Model Using\n  Near-Infinite History","summary":"  Machine learning (ML) has recently shown promising results in medical\npredictions using electronic health records (EHRs). However, since ML models\ntypically have a limited capability in terms of input sizes, selecting specific\nmedical events from EHRs for use as input is necessary. This selection process,\noften relying on expert opinion, can cause bottlenecks in development. We\npropose Retrieval-Enhanced Medical prediction model (REMed) to address such\nchallenges. REMed can essentially evaluate unlimited medical events, select the\nrelevant ones, and make predictions. This allows for an unrestricted input\nsize, eliminating the need for manual event selection. We verified these\nproperties through experiments involving 27 clinical prediction tasks across\nfour independent cohorts, where REMed outperformed the baselines. Notably, we\nfound that the preferences of REMed align closely with those of medical\nexperts. We expect our approach to significantly expedite the development of\nEHR prediction models by minimizing clinicians' need for manual involvement.\n","authors":["Junu Kim","Chaeeun Shim","Bosco Seong Kyu Yang","Chami Im","Sung Yoon Lim","Han-Gil Jeong","Edward Choi"],"pdf_url":"https://arxiv.org/pdf/2310.20204v4.pdf","comment":"The source codes corresponding to this paper are available at:\n  https://github.com/starmpcc/REMed"},{"id":"http://arxiv.org/abs/2407.10167v2","updated":"2024-07-22T10:26:23Z","published":"2024-07-14T11:41:03Z","title":"Key-Point-Driven Mathematical Reasoning Distillation of Large Language\n  Model","summary":"  Large Language Models (LLMs) have demonstrated exceptional proficiency in\nmathematical reasoning tasks due to their extensive parameter counts and\ntraining on vast datasets. Despite these capabilities, deploying LLMs is\nhindered by their computational demands. Distilling LLM mathematical reasoning\ninto Smaller Language Models (SLMs) has emerged as a solution to this\nchallenge, although these smaller models often suffer from errors in\ncalculation and semantic understanding. Prior work has proposed\nProgram-of-Thought Distillation (PoTD) to avoid calculation error. To further\naddress semantic understanding errors, we propose Key-Point-Driven Mathematical\nReasoning Distillation (KPDD). KPDD enhances the reasoning performance of SLMs\nby breaking down the problem-solving process into three stages: Core Question\nExtraction, Problem-Solving Information Extraction, and Step-by-Step Solution.\nThis method is further divided into KPDD-CoT, which generates Chain-of-Thought\nrationales, and KPDD-PoT, which creates Program-of-Thought rationales. The\nexperiment results show that KPDD-CoT significantly improves reasoning\nabilities, while KPDD-PoT achieves state-of-the-art performance in mathematical\nreasoning tasks. Our approach effectively mitigates misunderstanding errors,\nadvancing the deployment of efficient and capable SLMs.\n","authors":["Xunyu Zhu","Jian Li","Can Ma","Weiping Wang"],"pdf_url":"https://arxiv.org/pdf/2407.10167v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15516v1","updated":"2024-07-22T10:09:05Z","published":"2024-07-22T10:09:05Z","title":"Attention Is All You Need But You Don't Need All Of It For Inference of\n  Large Language Models","summary":"  The inference demand for LLMs has skyrocketed in recent months, and serving\nmodels with low latencies remains challenging due to the quadratic input length\ncomplexity of the attention layers. In this work, we investigate the effect of\ndropping MLP and attention layers at inference time on the performance of\nLlama-v2 models. We find that dropping dreeper attention layers only marginally\ndecreases performance but leads to the best speedups alongside dropping entire\nlayers. For example, removing 33\\% of attention layers in a 13B Llama2 model\nresults in a 1.8\\% drop in average performance over the OpenLLM benchmark. We\nalso observe that skipping layers except the latter layers reduces performances\nfor more layers skipped, except for skipping the attention layers.\n","authors":["Georgy Tyukin","Gbetondji J-S Dovonon","Jean Kaddour","Pasquale Minervini"],"pdf_url":"https://arxiv.org/pdf/2407.15516v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15508v1","updated":"2024-07-22T09:45:16Z","published":"2024-07-22T09:45:16Z","title":"Compensate Quantization Errors+: Quantized Models Are Inquisitive\n  Learners","summary":"  Large Language Models (LLMs) showcase remarkable performance and robust\ndeductive capabilities, yet their expansive size complicates deployment and\nraises environmental concerns due to substantial resource consumption. The\nrecent development of a quantization technique known as Learnable\nSingular-value Increment (LSI) has addressed some of these quantization\nchallenges. Leveraging insights from LSI and our extensive research, we have\ndeveloped innovative methods that enhance the performance of quantized LLMs,\nparticularly in low-bit settings. Our methods consistently deliver\nstate-of-the-art results across various quantization scenarios and offer deep\ntheoretical insights into the quantization process, elucidating the potential\nof quantized models for widespread application.\n","authors":["Yifei Gao","Jie Ou","Lei Wang","Fanhua Shang","Jaji Wu","Jun Cheng"],"pdf_url":"https://arxiv.org/pdf/2407.15508v1.pdf","comment":"Effecient Quantization Methods for LLMs"},{"id":"http://arxiv.org/abs/2407.15504v1","updated":"2024-07-22T09:40:13Z","published":"2024-07-22T09:40:13Z","title":"Fundamental Limits of Prompt Compression: A Rate-Distortion Framework\n  for Black-Box Language Models","summary":"  We formalize the problem of prompt compression for large language models\n(LLMs) and present a framework to unify token-level prompt compression methods\nwhich create hard prompts for black-box models. We derive the distortion-rate\nfunction for this setup as a linear program, and provide an efficient algorithm\nto compute this fundamental limit via the dual of the linear program. Using the\ndistortion-rate function as the baseline, we study the performance of existing\ncompression schemes on a synthetic dataset consisting of prompts generated from\na Markov chain, natural language queries, and their respective answers. Our\nempirical analysis demonstrates the criticality of query-aware prompt\ncompression, where the compressor has knowledge of the downstream task/query\nfor the black-box LLM. We show that there is a large gap between the\nperformance of current prompt compression methods and the optimal strategy, and\npropose a query-aware, variable-rate adaptation of a prior work to close the\ngap. We extend our experiments to a small natural language dataset to further\nconfirm our findings on our synthetic dataset.\n","authors":["Adway Girish","Alliot Nagle","Marco Bondaschi","Michael Gastpar","Ashok Vardhan Makkuva","Hyeji Kim"],"pdf_url":"https://arxiv.org/pdf/2407.15504v1.pdf","comment":"40 pages, 15 figures. Under review"},{"id":"http://arxiv.org/abs/2402.18458v2","updated":"2024-07-22T09:35:08Z","published":"2024-02-28T16:35:52Z","title":"Meta-Task Prompting Elicits Embeddings from Large Language Models","summary":"  We introduce a new unsupervised text embedding method, Meta-Task Prompting\nwith Explicit One-Word Limitation (MetaEOL), for generating high-quality\nsentence embeddings from Large Language Models (LLMs) without the need for\nmodel fine-tuning. Leveraging meta-task prompting, MetaEOL guides LLMs to\nproduce embeddings through a series of carefully designed prompts that address\nmultiple representational aspects. Our comprehensive experiments demonstrate\nthat embeddings averaged from various meta-tasks are versatile embeddings that\nyield competitive performance on Semantic Textual Similarity (STS) benchmarks\nand excel in downstream tasks, surpassing contrastive-trained models. Our\nfindings suggest a new scaling law, offering a versatile and resource-efficient\napproach for embedding generation across diverse scenarios.\n","authors":["Yibin Lei","Di Wu","Tianyi Zhou","Tao Shen","Yu Cao","Chongyang Tao","Andrew Yates"],"pdf_url":"https://arxiv.org/pdf/2402.18458v2.pdf","comment":"ACL 2024"},{"id":"http://arxiv.org/abs/2407.15498v1","updated":"2024-07-22T09:26:35Z","published":"2024-07-22T09:26:35Z","title":"Refining Corpora from a Model Calibration Perspective for Chinese\n  Spelling Correction","summary":"  Chinese Spelling Correction (CSC) commonly lacks large-scale high-quality\ncorpora, due to the labor-intensive labeling of spelling errors in real-life\nhuman writing or typing scenarios. Two data augmentation methods are widely\nadopted: (1) \\textit{Random Replacement} with the guidance of confusion sets\nand (2) \\textit{OCR/ASR-based Generation} that simulates character misusing.\nHowever, both methods inevitably introduce noisy data (e.g., false spelling\nerrors), potentially leading to over-correction. By carefully analyzing the two\ntypes of corpora, we find that though the latter achieves more robust\ngeneralization performance, the former yields better-calibrated CSC models. We\nthen provide a theoretical analysis of this empirical observation, based on\nwhich a corpus refining strategy is proposed. Specifically, OCR/ASR-based data\nsamples are fed into a well-calibrated CSC model trained on random\nreplacement-based corpora and then filtered based on prediction confidence. By\nlearning a simple BERT-based model on the refined OCR/ASR-based corpus, we set\nup impressive state-of-the-art performance on three widely-used benchmarks,\nwhile significantly alleviating over-correction (e.g., lowering false positive\npredictions).\n","authors":["Dingyao Yu","Yang An","Wei Ye","Xiongfeng Xiao","Shaoguang Mao","Tao Ge","Shikun Zhang"],"pdf_url":"https://arxiv.org/pdf/2407.15498v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.02180v2","updated":"2024-07-22T09:22:23Z","published":"2022-03-04T08:21:27Z","title":"EAG: Extract and Generate Multi-way Aligned Corpus for Complete\n  Multi-lingual Neural Machine Translation","summary":"  Complete Multi-lingual Neural Machine Translation (C-MNMT) achieves superior\nperformance against the conventional MNMT by constructing multi-way aligned\ncorpus, i.e., aligning bilingual training examples from different language\npairs when either their source or target sides are identical. However, since\nexactly identical sentences from different language pairs are scarce, the power\nof the multi-way aligned corpus is limited by its scale. To handle this\nproblem, this paper proposes \"Extract and Generate\" (EAG), a two-step approach\nto construct large-scale and high-quality multi-way aligned corpus from\nbilingual data. Specifically, we first extract candidate aligned examples by\npairing the bilingual examples from different language pairs with highly\nsimilar source or target sentences; and then generate the final aligned\nexamples from the candidates with a well-trained generation model. With this\ntwo-step pipeline, EAG can construct a large-scale and multi-way aligned corpus\nwhose diversity is almost identical to the original bilingual corpus.\nExperiments on two publicly available datasets i.e., WMT-5 and OPUS-100, show\nthat the proposed method achieves significant improvements over strong\nbaselines, with +1.1 and +1.4 BLEU points improvements on the two datasets\nrespectively.\n","authors":["Yulin Xu","Zhen Yang","Fandong Meng"," JieZhou"],"pdf_url":"https://arxiv.org/pdf/2203.02180v2.pdf","comment":"Accepted as a long paper at ACL 2022"},{"id":"http://arxiv.org/abs/2407.15489v1","updated":"2024-07-22T09:16:30Z","published":"2024-07-22T09:16:30Z","title":"Two Stacks Are Better Than One: A Comparison of Language Modeling and\n  Translation as Multilingual Pretraining Objectives","summary":"  Pretrained language models (PLMs) display impressive performances and have\ncaptured the attention of the NLP community. Establishing the best practices in\npretraining has therefore become a major point of focus for much of NLP\nresearch -- especially since the insights developed for monolingual English\nmodels need not carry to more complex multilingual. One significant caveat of\nthe current state of the art is that different works are rarely comparable:\nthey often discuss different parameter counts, training data, and evaluation\nmethodology.\n  This paper proposes a comparison of multilingual pretraining objectives in a\ncontrolled methodological environment. We ensure that training data and model\narchitectures are comparable, and discuss the downstream performances across 6\nlanguages that we observe in probing and fine-tuning scenarios. We make two key\nobservations: (1) the architecture dictates which pretraining objective is\noptimal; (2) multilingual translation is a very effective pre-training\nobjective under the right conditions. We make our code, data, and model weights\navailable at \\texttt{\\url{https://github.com/Helsinki-NLP/lm-vs-mt}}.\n","authors":["Zihao Li","Shaoxiong Ji","Timothee Mickus","Vincent Segonne","Jörg Tiedemann"],"pdf_url":"https://arxiv.org/pdf/2407.15489v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2209.02552v3","updated":"2024-07-22T09:10:34Z","published":"2022-09-06T15:01:06Z","title":"From Black Boxes to Conversations: Incorporating XAI in a Conversational\n  Agent","summary":"  The goal of Explainable AI (XAI) is to design methods to provide insights\ninto the reasoning process of black-box models, such as deep neural networks,\nin order to explain them to humans. Social science research states that such\nexplanations should be conversational, similar to human-to-human explanations.\nIn this work, we show how to incorporate XAI in a conversational agent, using a\nstandard design for the agent comprising natural language understanding and\ngeneration components. We build upon an XAI question bank, which we extend by\nquality-controlled paraphrases, to understand the user's information needs. We\nfurther systematically survey the literature for suitable explanation methods\nthat provide the information to answer those questions, and present a\ncomprehensive list of suggestions. Our work is the first step towards truly\nnatural conversations about machine learning models with an explanation agent.\nThe comprehensive list of XAI questions and the corresponding explanation\nmethods may support other researchers in providing the necessary information to\naddress users' demands. To facilitate future work, we release our source code\nand data.\n","authors":["Van Bach Nguyen","Jörg Schlötterer","Christin Seifert"],"pdf_url":"https://arxiv.org/pdf/2209.02552v3.pdf","comment":"Accepted at The World Conference on eXplainable Artificial\n  Intelligence 2023 (XAI-2023)"},{"id":"http://arxiv.org/abs/2311.04378v3","updated":"2024-07-22T09:00:18Z","published":"2023-11-07T22:52:54Z","title":"Watermarks in the Sand: Impossibility of Strong Watermarking for\n  Generative Models","summary":"  Watermarking generative models consists of planting a statistical signal\n(watermark) in a model's output so that it can be later verified that the\noutput was generated by the given model. A strong watermarking scheme satisfies\nthe property that a computationally bounded attacker cannot erase the watermark\nwithout causing significant quality degradation. In this paper, we study the\n(im)possibility of strong watermarking schemes. We prove that, under\nwell-specified and natural assumptions, strong watermarking is impossible to\nachieve. This holds even in the private detection algorithm setting, where the\nwatermark insertion and detection algorithms share a secret key, unknown to the\nattacker. To prove this result, we introduce a generic efficient watermark\nattack; the attacker is not required to know the private key of the scheme or\neven which scheme is used. Our attack is based on two assumptions: (1) The\nattacker has access to a \"quality oracle\" that can evaluate whether a candidate\noutput is a high-quality response to a prompt, and (2) The attacker has access\nto a \"perturbation oracle\" which can modify an output with a nontrivial\nprobability of maintaining quality, and which induces an efficiently mixing\nrandom walk on high-quality outputs. We argue that both assumptions can be\nsatisfied in practice by an attacker with weaker computational capabilities\nthan the watermarked model itself, to which the attacker has only black-box\naccess. Furthermore, our assumptions will likely only be easier to satisfy over\ntime as models grow in capabilities and modalities. We demonstrate the\nfeasibility of our attack by instantiating it to attack three existing\nwatermarking schemes for large language models: Kirchenbauer et al. (2023),\nKuditipudi et al. (2023), and Zhao et al. (2023). The same attack successfully\nremoves the watermarks planted by all three schemes, with only minor quality\ndegradation.\n","authors":["Hanlin Zhang","Benjamin L. Edelman","Danilo Francati","Daniele Venturi","Giuseppe Ateniese","Boaz Barak"],"pdf_url":"https://arxiv.org/pdf/2311.04378v3.pdf","comment":"ICML 2024. Website: https://hanlin-zhang.com/impossibility-watermarks"},{"id":"http://arxiv.org/abs/2407.10114v2","updated":"2024-07-22T08:59:07Z","published":"2024-07-14T08:07:50Z","title":"TokenSHAP: Interpreting Large Language Models with Monte Carlo Shapley\n  Value Estimation","summary":"  As large language models (LLMs) become increasingly prevalent in critical\napplications, the need for interpretable AI has grown. We introduce TokenSHAP,\na novel method for interpreting LLMs by attributing importance to individual\ntokens or substrings within input prompts. This approach adapts Shapley values\nfrom cooperative game theory to natural language processing, offering a\nrigorous framework for understanding how different parts of an input contribute\nto a model's response. TokenSHAP leverages Monte Carlo sampling for\ncomputational efficiency, providing interpretable, quantitative measures of\ntoken importance. We demonstrate its efficacy across diverse prompts and LLM\narchitectures, showing consistent improvements over existing baselines in\nalignment with human judgments, faithfulness to model behavior, and\nconsistency.\n  Our method's ability to capture nuanced interactions between tokens provides\nvaluable insights into LLM behavior, enhancing model transparency, improving\nprompt engineering, and aiding in the development of more reliable AI systems.\nTokenSHAP represents a significant step towards the necessary interpretability\nfor responsible AI deployment, contributing to the broader goal of creating\nmore transparent, accountable, and trustworthy AI systems.\n","authors":["Roni Goldshmidt","Miriam Horovicz"],"pdf_url":"https://arxiv.org/pdf/2407.10114v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15459v1","updated":"2024-07-22T08:15:02Z","published":"2024-07-22T08:15:02Z","title":"Text-to-Battery Recipe: A language modeling-based protocol for automatic\n  battery recipe extraction and retrieval","summary":"  Recent studies have increasingly applied natural language processing (NLP) to\nautomatically extract experimental research data from the extensive battery\nmaterials literature. Despite the complex process involved in battery\nmanufacturing -- from material synthesis to cell assembly -- there has been no\ncomprehensive study systematically organizing this information. In response, we\npropose a language modeling-based protocol, Text-to-Battery Recipe (T2BR), for\nthe automatic extraction of end-to-end battery recipes, validated using a case\nstudy on batteries containing LiFePO4 cathode material. We report machine\nlearning-based paper filtering models, screening 2,174 relevant papers from the\nkeyword-based search results, and unsupervised topic models to identify 2,876\nparagraphs related to cathode synthesis and 2,958 paragraphs related to cell\nassembly. Then, focusing on the two topics, two deep learning-based named\nentity recognition models are developed to extract a total of 30 entities --\nincluding precursors, active materials, and synthesis methods -- achieving F1\nscores of 88.18% and 94.61%. The accurate extraction of entities enables the\nsystematic generation of 165 end-toend recipes of LiFePO4 batteries. Our\nprotocol and results offer valuable insights into specific trends, such as\nassociations between precursor materials and synthesis methods, or combinations\nbetween different precursor materials. We anticipate that our findings will\nserve as a foundational knowledge base for facilitating battery-recipe\ninformation retrieval. The proposed protocol will significantly accelerate the\nreview of battery material literature and catalyze innovations in battery\ndesign and development.\n","authors":["Daeun Lee","Jaewoong Choi","Hiroshi Mizuseki","Byungju Lee"],"pdf_url":"https://arxiv.org/pdf/2407.15459v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15441v1","updated":"2024-07-22T07:48:30Z","published":"2024-07-22T07:48:30Z","title":"Developing a Reliable, General-Purpose Hallucination Detection and\n  Mitigation Service: Insights and Lessons Learned","summary":"  Hallucination, a phenomenon where large language models (LLMs) produce output\nthat is factually incorrect or unrelated to the input, is a major challenge for\nLLM applications that require accuracy and dependability. In this paper, we\nintroduce a reliable and high-speed production system aimed at detecting and\nrectifying the hallucination issue within LLMs. Our system encompasses named\nentity recognition (NER), natural language inference (NLI), span-based\ndetection (SBD), and an intricate decision tree-based process to reliably\ndetect a wide range of hallucinations in LLM responses. Furthermore, our team\nhas crafted a rewriting mechanism that maintains an optimal mix of precision,\nresponse time, and cost-effectiveness. We detail the core elements of our\nframework and underscore the paramount challenges tied to response time,\navailability, and performance metrics, which are crucial for real-world\ndeployment of these technologies. Our extensive evaluation, utilizing offline\ndata and live production traffic, confirms the efficacy of our proposed\nframework and service.\n","authors":["Song Wang","Xun Wang","Jie Mei","Yujia Xie","Sean Muarray","Zhang Li","Lingfeng Wu","Si-Qing Chen","Wayne Xiong"],"pdf_url":"https://arxiv.org/pdf/2407.15441v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.10051v3","updated":"2024-07-22T07:16:54Z","published":"2024-05-16T12:40:01Z","title":"MarkLLM: An Open-Source Toolkit for LLM Watermarking","summary":"  LLM watermarking, which embeds imperceptible yet algorithmically detectable\nsignals in model outputs to identify LLM-generated text, has become crucial in\nmitigating the potential misuse of large language models. However, the\nabundance of LLM watermarking algorithms, their intricate mechanisms, and the\ncomplex evaluation procedures and perspectives pose challenges for researchers\nand the community to easily experiment with, understand, and assess the latest\nadvancements. To address these issues, we introduce MarkLLM, an open-source\ntoolkit for LLM watermarking. MarkLLM offers a unified and extensible framework\nfor implementing LLM watermarking algorithms, while providing user-friendly\ninterfaces to ensure ease of access. Furthermore, it enhances understanding by\nsupporting automatic visualization of the underlying mechanisms of these\nalgorithms. For evaluation, MarkLLM offers a comprehensive suite of 12 tools\nspanning three perspectives, along with two types of automated evaluation\npipelines. Through MarkLLM, we aim to support researchers while improving the\ncomprehension and involvement of the general public in LLM watermarking\ntechnology, fostering consensus and driving further advancements in research\nand application. Our code is available at https://github.com/THU-BPM/MarkLLM.\n","authors":["Leyi Pan","Aiwei Liu","Zhiwei He","Zitian Gao","Xuandong Zhao","Yijian Lu","Binglin Zhou","Shuliang Liu","Hanlin Zhang","Xuming Hu","Lijie Wen","Irwin King"],"pdf_url":"https://arxiv.org/pdf/2405.10051v3.pdf","comment":"16 pages, 5 figures, 6 tables"},{"id":"http://arxiv.org/abs/2404.07584v3","updated":"2024-07-22T07:07:06Z","published":"2024-04-11T09:17:12Z","title":"UltraEval: A Lightweight Platform for Flexible and Comprehensive\n  Evaluation for LLMs","summary":"  Evaluation is pivotal for refining Large Language Models (LLMs), pinpointing\ntheir capabilities, and guiding enhancements. The rapid development of LLMs\ncalls for a lightweight and easy-to-use framework for swift evaluation\ndeployment. However, considering various implementation details, developing a\ncomprehensive evaluation platform is never easy. Existing platforms are often\ncomplex and poorly modularized, hindering seamless incorporation into research\nworkflows. This paper introduces UltraEval, a user-friendly evaluation\nframework characterized by its lightweight nature, comprehensiveness,\nmodularity, and efficiency. We identify and reimplement three core components\nof model evaluation (models, data, and metrics). The resulting composability\nallows for the free combination of different models, tasks, prompts,\nbenchmarks, and metrics within a unified evaluation workflow. Additionally,\nUltraEval supports diverse models owing to a unified HTTP service and provides\nsufficient inference acceleration. UltraEval is now available for researchers\npublicly.\n","authors":["Chaoqun He","Renjie Luo","Shengding Hu","Yuanqian Zhao","Jie Zhou","Hanghao Wu","Jiajie Zhang","Xu Han","Zhiyuan Liu","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2404.07584v3.pdf","comment":"Accepted by ACL 2024 System Demostration Track, update"},{"id":"http://arxiv.org/abs/2407.15425v1","updated":"2024-07-22T07:02:15Z","published":"2024-07-22T07:02:15Z","title":"Empirical Capacity Model for Self-Attention Neural Networks","summary":"  Large pretrained self-attention neural networks, or transformers, have been\nvery successful in various tasks recently. The performance of a model on a\ngiven task depends on its ability to memorize and generalize the training data.\nLarge transformer models, which may have billions of parameters, in theory have\na huge capacity to memorize content. However, the current algorithms for the\noptimization fall short of the theoretical capacity, and the capacity is also\nhighly dependent on the content. In this paper, we focus on the memory capacity\nof these models obtained using common training algorithms and synthetic\ntraining data. Based on the results, we derive an empirical capacity model\n(ECM) for a generic transformer. The ECM can be used to design task-specific\ntransformer models with an optimal number of parameters in cases where the\ntarget memorization capability of the task can be defined.\n","authors":["Aki Härmä","Marcin Pietrasik","Anna Wilbik"],"pdf_url":"https://arxiv.org/pdf/2407.15425v1.pdf","comment":"Submitted to BNAIC'24, 14 pages + refs"},{"id":"http://arxiv.org/abs/2407.15415v1","updated":"2024-07-22T06:42:00Z","published":"2024-07-22T06:42:00Z","title":"LLaST: Improved End-to-end Speech Translation System Leveraged by Large\n  Language Models","summary":"  We introduces LLaST, a framework for building high-performance Large Language\nmodel based Speech-to-text Translation systems. We address the limitations of\nend-to-end speech translation(E2E ST) models by exploring model architecture\ndesign and optimization techniques tailored for LLMs. Our approach includes\nLLM-based speech translation architecture design, ASR-augmented training,\nmultilingual data augmentation, and dual-LoRA optimization. Our approach\ndemonstrates superior performance on the CoVoST-2 benchmark and showcases\nexceptional scaling capabilities powered by LLMs. We believe this effective\nmethod will serve as a strong baseline for speech translation and provide\ninsights for future improvements of the LLM-based speech translation framework.\nWe release the data, code and models in https://github.com/openaudiolab/LLaST.\n","authors":["Xi Chen","Songyang Zhang","Qibing Bai","Kai Chen","Satoshi Nakamura"],"pdf_url":"https://arxiv.org/pdf/2407.15415v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.17432v3","updated":"2024-07-22T06:25:20Z","published":"2023-12-29T01:56:17Z","title":"Video Understanding with Large Language Models: A Survey","summary":"  With the burgeoning growth of online video platforms and the escalating\nvolume of video content, the demand for proficient video understanding tools\nhas intensified markedly. Given the remarkable capabilities of large language\nmodels (LLMs) in language and multimodal tasks, this survey provides a detailed\noverview of recent advancements in video understanding that harness the power\nof LLMs (Vid-LLMs). The emergent capabilities of Vid-LLMs are surprisingly\nadvanced, particularly their ability for open-ended multi-granularity (general,\ntemporal, and spatiotemporal) reasoning combined with commonsense knowledge,\nsuggesting a promising path for future video understanding. We examine the\nunique characteristics and capabilities of Vid-LLMs, categorizing the\napproaches into three main types: Video Analyzer x LLM, Video Embedder x LLM,\nand (Analyzer + Embedder) x LLM. Furthermore, we identify five sub-types based\non the functions of LLMs in Vid-LLMs: LLM as Summarizer, LLM as Manager, LLM as\nText Decoder, LLM as Regressor, and LLM as Hidden Layer. Furthermore, this\nsurvey presents a comprehensive study of the tasks, datasets, benchmarks, and\nevaluation methodologies for Vid-LLMs. Additionally, it explores the expansive\napplications of Vid-LLMs across various domains, highlighting their remarkable\nscalability and versatility in real-world video understanding challenges.\nFinally, it summarizes the limitations of existing Vid-LLMs and outlines\ndirections for future research. For more information, readers are recommended\nto visit the repository at\nhttps://github.com/yunlong10/Awesome-LLMs-for-Video-Understanding.\n","authors":["Yunlong Tang","Jing Bi","Siting Xu","Luchuan Song","Susan Liang","Teng Wang","Daoan Zhang","Jie An","Jingyang Lin","Rongyi Zhu","Ali Vosoughi","Chao Huang","Zeliang Zhang","Pinxin Liu","Mingqian Feng","Feng Zheng","Jianguo Zhang","Ping Luo","Jiebo Luo","Chenliang Xu"],"pdf_url":"https://arxiv.org/pdf/2312.17432v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15017v1","updated":"2024-07-22T06:15:59Z","published":"2024-07-22T06:15:59Z","title":"Knowledge Mechanisms in Large Language Models: A Survey and Perspective","summary":"  Understanding knowledge mechanisms in Large Language Models (LLMs) is crucial\nfor advancing towards trustworthy AGI. This paper reviews knowledge mechanism\nanalysis from a novel taxonomy including knowledge utilization and evolution.\nKnowledge utilization delves into the mechanism of memorization, comprehension\nand application, and creation. Knowledge evolution focuses on the dynamic\nprogression of knowledge within individual and group LLMs. Moreover, we discuss\nwhat knowledge LLMs have learned, the reasons for the fragility of parametric\nknowledge, and the potential dark knowledge (hypothesis) that will be\nchallenging to address. We hope this work can help understand knowledge in LLMs\nand provide insights for future research.\n","authors":["Mengru Wang","Yunzhi Yao","Ziwen Xu","Shuofei Qiao","Shumin Deng","Peng Wang","Xiang Chen","Jia-Chen Gu","Yong Jiang","Pengjun Xie","Fei Huang","Huajun Chen","Ningyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2407.15017v1.pdf","comment":"Ongoing work (v1); 34 pages, 5 figures"},{"id":"http://arxiv.org/abs/2407.15399v1","updated":"2024-07-22T06:04:29Z","published":"2024-07-22T06:04:29Z","title":"Imposter.AI: Adversarial Attacks with Hidden Intentions towards Aligned\n  Large Language Models","summary":"  With the development of large language models (LLMs) like ChatGPT, both their\nvast applications and potential vulnerabilities have come to the forefront.\nWhile developers have integrated multiple safety mechanisms to mitigate their\nmisuse, a risk remains, particularly when models encounter adversarial inputs.\nThis study unveils an attack mechanism that capitalizes on human conversation\nstrategies to extract harmful information from LLMs. We delineate three pivotal\nstrategies: (i) decomposing malicious questions into seemingly innocent\nsub-questions; (ii) rewriting overtly malicious questions into more covert,\nbenign-sounding ones; (iii) enhancing the harmfulness of responses by prompting\nmodels for illustrative examples. Unlike conventional methods that target\nexplicit malicious responses, our approach delves deeper into the nature of the\ninformation provided in responses. Through our experiments conducted on\nGPT-3.5-turbo, GPT-4, and Llama2, our method has demonstrated a marked efficacy\ncompared to conventional attack methods. In summary, this work introduces a\nnovel attack method that outperforms previous approaches, raising an important\nquestion: How to discern whether the ultimate intent in a dialogue is\nmalicious?\n","authors":["Xiao Liu","Liangzhi Li","Tong Xiang","Fuying Ye","Lu Wei","Wangyue Li","Noa Garcia"],"pdf_url":"https://arxiv.org/pdf/2407.15399v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15390v1","updated":"2024-07-22T05:35:17Z","published":"2024-07-22T05:35:17Z","title":"ALLaM: Large Language Models for Arabic and English","summary":"  We present ALLaM: Arabic Large Language Model, a series of large language\nmodels to support the ecosystem of Arabic Language Technologies (ALT). ALLaM is\ncarefully trained considering the values of language alignment and knowledge\ntransfer at scale. Our autoregressive decoder-only architecture models\ndemonstrate how second-language acquisition via vocabulary expansion and\npretraining on a mixture of Arabic and English text can steer a model towards a\nnew language (Arabic) without any catastrophic forgetting in the original\nlanguage (English). Furthermore, we highlight the effectiveness of using\nparallel/translated data to aid the process of knowledge alignment between\nlanguages. Finally, we show that extensive alignment with human preferences can\nsignificantly enhance the performance of a language model compared to models of\na larger scale with lower quality alignment. ALLaM achieves state-of-the-art\nperformance in various Arabic benchmarks, including MMLU Arabic, ACVA, and\nArabic Exams. Our aligned models improve both in Arabic and English from their\nbase aligned models.\n","authors":["M Saiful Bari","Yazeed Alnumay","Norah A. Alzahrani","Nouf M. Alotaibi","Hisham A. Alyahya","Sultan AlRashed","Faisal A. Mirza","Shaykhah Z. Alsubaie","Hassan A. Alahmed","Ghadah Alabduljabbar","Raghad Alkhathran","Yousef Almushayqih","Raneem Alnajim","Salman Alsubaihi","Maryam Al Mansour","Majed Alrubaian","Ali Alammari","Zaki Alawami","Abdulmohsen Al-Thubaity","Ahmed Abdelali","Jeril Kuriakose","Abdalghani Abujabal","Nora Al-Twairesh","Areeb Alowisheq","Haidar Khan"],"pdf_url":"https://arxiv.org/pdf/2407.15390v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.09727v3","updated":"2024-07-22T05:33:51Z","published":"2024-02-15T05:40:21Z","title":"A Human-Inspired Reading Agent with Gist Memory of Very Long Contexts","summary":"  Current Large Language Models (LLMs) are not only limited to some maximum\ncontext length, but also are not able to robustly consume long inputs. To\naddress these limitations, we propose ReadAgent, an LLM agent system that\nincreases effective context length up to 20x in our experiments. Inspired by\nhow humans interactively read long documents, we implement ReadAgent as a\nsimple prompting system that uses the advanced language capabilities of LLMs to\n(1) decide what content to store together in a memory episode, (2) compress\nthose memory episodes into short episodic memories called gist memories, and\n(3) take actions to look up passages in the original text if ReadAgent needs to\nremind itself of relevant details to complete a task. We evaluate ReadAgent\nagainst baselines using retrieval methods, using the original long contexts,\nand using the gist memories. These evaluations are performed on three\nlong-document reading comprehension tasks: QuALITY, NarrativeQA, and QMSum.\nReadAgent outperforms the baselines on all three tasks while extending the\neffective context window by 3.5-20x.\n","authors":["Kuang-Huei Lee","Xinyun Chen","Hiroki Furuta","John Canny","Ian Fischer"],"pdf_url":"https://arxiv.org/pdf/2402.09727v3.pdf","comment":"Website: https://read-agent.github.io"},{"id":"http://arxiv.org/abs/2407.15375v1","updated":"2024-07-22T04:51:33Z","published":"2024-07-22T04:51:33Z","title":"The Development of a Comprehensive Spanish Dictionary for Phonetic and\n  Lexical Tagging in Socio-phonetic Research (ESPADA)","summary":"  Pronunciation dictionaries are an important component in the process of\nspeech forced alignment. The accuracy of these dictionaries has a strong effect\non the aligned speech data since they help the mapping between orthographic\ntranscriptions and acoustic signals. In this paper, I present the creation of a\ncomprehensive pronunciation dictionary in Spanish (ESPADA) that can be used in\nmost of the dialect variants of Spanish data. Current dictionaries focus on\nspecific regional variants, but with the flexible nature of our tool, it can be\nreadily applied to capture the most common phonetic differences across major\ndialectal variants. We propose improvements to current pronunciation\ndictionaries as well as mapping other relevant annotations such as\nmorphological and lexical information. In terms of size, it is currently the\nmost complete dictionary with more than 628,000 entries, representing words\nfrom 16 countries. All entries come with their corresponding pronunciations,\nmorphological and lexical tagging, and other relevant information for phonetic\nanalysis: stress patterns, phonotactics, IPA transcriptions, and more. This\naims to equip socio-phonetic researchers with a complete open-source tool that\nenhances dialectal research within socio-phonetic frameworks in the Spanish\nlanguage.\n","authors":["Simon Gonzalez"],"pdf_url":"https://arxiv.org/pdf/2407.15375v1.pdf","comment":"Proceedings of the 16th Linguistic Annotation Workshop (LAW-XVI)\n  within LREC2022"},{"id":"http://arxiv.org/abs/2407.15374v1","updated":"2024-07-22T04:48:04Z","published":"2024-07-22T04:48:04Z","title":"ILiAD: An Interactive Corpus for Linguistic Annotated Data from Twitter\n  Posts","summary":"  Social Media platforms have offered invaluable opportunities for linguistic\nresearch. The availability of up-to-date data, coming from any part in the\nworld, and coming from natural contexts, has allowed researchers to study\nlanguage in real time. One of the fields that has made great use of social\nmedia platforms is Corpus Linguistics. There is currently a wide range of\nprojects which have been able to successfully create corpora from social media.\nIn this paper, we present the development and deployment of a linguistic corpus\nfrom Twitter posts in English, coming from 26 news agencies and 27 individuals.\nThe main goal was to create a fully annotated English corpus for linguistic\nanalysis. We include information on morphology and syntax, as well as NLP\nfeatures such as tokenization, lemmas, and n- grams. The information is\npresented through a range of powerful visualisations for users to explore\nlinguistic patterns in the corpus. With this tool, we aim to contribute to the\narea of language technologies applied to linguistic research.\n","authors":["Simon Gonzalez"],"pdf_url":"https://arxiv.org/pdf/2407.15374v1.pdf","comment":"Conference on Language Technologies & Digital Humanities Ljubljana,\n  2022"},{"id":"http://arxiv.org/abs/2407.00908v3","updated":"2024-07-22T04:45:11Z","published":"2024-07-01T02:20:28Z","title":"FineSurE: Fine-grained Summarization Evaluation using LLMs","summary":"  Automated evaluation is crucial for streamlining text summarization\nbenchmarking and model development, given the costly and time-consuming nature\nof human evaluation. Traditional methods like ROUGE do not correlate well with\nhuman judgment, while recently proposed LLM-based metrics provide only\nsummary-level assessment using Likert-scale scores. This limits deeper model\nanalysis, e.g., we can only assign one hallucination score at the summary\nlevel, while at the sentence level, we can count sentences containing\nhallucinations. To remedy those limitations, we propose FineSurE, a\nfine-grained evaluator specifically tailored for the summarization task using\nlarge language models (LLMs). It also employs completeness and conciseness\ncriteria, in addition to faithfulness, enabling multi-dimensional assessment.\nWe compare various open-source and proprietary LLMs as backbones for FineSurE.\nIn addition, we conduct extensive benchmarking of FineSurE against SOTA methods\nincluding NLI-, QA-, and LLM-based methods, showing improved performance\nespecially on the completeness and conciseness dimensions. The code is\navailable at https://github.com/DISL-Lab/FineSurE-ACL24.\n","authors":["Hwanjun Song","Hang Su","Igor Shalyminov","Jason Cai","Saab Mansour"],"pdf_url":"https://arxiv.org/pdf/2407.00908v3.pdf","comment":"Accepted at ACL 2024 (main, long)"},{"id":"http://arxiv.org/abs/2407.15370v1","updated":"2024-07-22T04:40:45Z","published":"2024-07-22T04:40:45Z","title":"A Network Analysis Approach to Conlang Research Literature","summary":"  The field of conlang has evidenced an important growth in the last decades.\nThis has been the product of a wide interest in the use and study of conlangs\nfor artistic purposes. However, one important question is what it is happening\nwith conlang in the academic world. This paper aims to have an overall\nunderstanding of the literature on conlang research. With this we aim to give a\nrealistic picture of the field in present days. We have implemented a\ncomputational linguistic approach, combining bibliometrics and network analysis\nto examine all publications available in the Scopus database. Analysing over\n2300 academic publications since 1927 until 2022, we have found that Esperanto\nis by far the most documented conlang. Three main authors have contributed to\nthis: Garv\\'ia R., Fiedler S., and Blanke D. The 1970s and 1980s have been the\ndecades where the foundations of current research have been built. In terms of\nmethodologies, language learning and experimental linguistics are the ones\ncontributing to most to the preferred approaches of study in the field. We\npresent the results and discuss our limitations and future work.\n","authors":["Simon Gonzalez"],"pdf_url":"https://arxiv.org/pdf/2407.15370v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15366v1","updated":"2024-07-22T04:25:01Z","published":"2024-07-22T04:25:01Z","title":"Walking in Others' Shoes: How Perspective-Taking Guides Large Language\n  Models in Reducing Toxicity and Bias","summary":"  The common toxicity and societal bias in contents generated by large language\nmodels (LLMs) necessitate strategies to reduce harm. Present solutions often\ndemand white-box access to the model or substantial training, which is\nimpractical for cutting-edge commercial LLMs. Moreover, prevailing prompting\nmethods depend on external tool feedback and fail to simultaneously lessen\ntoxicity and bias. Motivated by social psychology principles, we propose a\nnovel strategy named \\textbf{perspective-taking prompting (\\textsc{PeT})} that\ninspires LLMs to integrate diverse human perspectives and self-regulate their\nresponses. This self-correction mechanism can significantly diminish toxicity\n(up to $89\\%$) and bias (up to $73\\%$) in LLMs' responses. Rigorous evaluations\nand ablation studies are conducted on two commercial LLMs (ChatGPT and GLM) and\nthree open-source LLMs, revealing \\textsc{PeT}'s superiority in producing less\nharmful responses, outperforming five strong baselines.\n","authors":["Rongwu Xu","Zi'an Zhou","Tianwei Zhang","Zehan Qi","Su Yao","Ke Xu","Wei Xu","Han Qiu"],"pdf_url":"https://arxiv.org/pdf/2407.15366v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15360v1","updated":"2024-07-22T04:07:26Z","published":"2024-07-22T04:07:26Z","title":"Dissecting Multiplication in Transformers: Insights into LLMs","summary":"  Transformer-based large language models have achieved remarkable performance\nacross various natural language processing tasks. However, they often struggle\nwith seemingly easy tasks like arithmetic despite their vast capabilities. This\nstark disparity raise human's concerns about their safe and ethical use, hinder\ntheir widespread adoption.In this paper, we focus on a typical arithmetic task,\ninteger multiplication, to explore and explain the imperfection of transformers\nin this domain. We provide comprehensive analysis of a vanilla transformer\ntrained to perform n-digit integer multiplication. Our observations indicate\nthat the model decomposes multiplication task into multiple parallel subtasks,\nsequentially optimizing each subtask for each digit to complete the final\nmultiplication. Based on observation and analysis, we infer the reasons of\ntransformers deficiencies in multiplication tasks lies in their difficulty in\ncalculating successive carryovers and caching intermediate results, and\nconfirmed this inference through experiments. Guided by these findings, we\npropose improvements to enhance transformers performance on multiplication\ntasks. These enhancements are validated through rigorous testing and\nmathematical modeling, not only enhance transformer's interpretability, but\nalso improve its performance, e.g., we achieve over 99.9% accuracy on 5-digit\ninteger multiplication with a tiny transformer, outperform LLMs GPT-4. Our\nmethod contributes to the broader fields of model understanding and\ninterpretability, paving the way for analyzing more complex tasks and\nTransformer models. This work underscores the importance of explainable AI,\nhelping to build trust in large language models and promoting their adoption in\ncritical applications.\n","authors":["Luyu Qiu","Jianing Li","Chi Su","Chen Jason Zhang","Lei Chen"],"pdf_url":"https://arxiv.org/pdf/2407.15360v1.pdf","comment":"8 pages, 5 figures"},{"id":"http://arxiv.org/abs/2407.15359v1","updated":"2024-07-22T04:02:45Z","published":"2024-07-22T04:02:45Z","title":"UF-HOBI at \"Discharge Me!\": A Hybrid Solution for Discharge Summary\n  Generation Through Prompt-based Tuning of GatorTronGPT Models","summary":"  Automatic generation of discharge summaries presents significant challenges\ndue to the length of clinical documentation, the dispersed nature of patient\ninformation, and the diverse terminology used in healthcare. This paper\npresents a hybrid solution for generating discharge summary sections as part of\nour participation in the \"Discharge Me!\" Challenge at the BioNLP 2024 Shared\nTask. We developed a two-stage generation method using both extractive and\nabstractive techniques, in which we first apply name entity recognition (NER)\nto extract key clinical concepts, which are then used as input for a\nprompt-tuning-based GatorTronGPT model to generate coherent text for two\nimportant sections including \"Brief Hospital Course\" and \"Discharge\nInstructions\". Our system was ranked 5th in this challenge, achieving an\noverall score of 0.284. The results demonstrate the effectiveness of our hybrid\nsolution in improving the quality of automated discharge section generation.\n","authors":["Mengxian Lyu","Cheng Peng","Daniel Paredes","Ziyi Chen","Aokun Chen","Jiang Bian","Yonghui Wu"],"pdf_url":"https://arxiv.org/pdf/2407.15359v1.pdf","comment":"BIONLP 2024 and Shared Tasks @ ACL 2024"},{"id":"http://arxiv.org/abs/2403.00067v3","updated":"2024-07-22T03:53:32Z","published":"2024-02-29T19:00:47Z","title":"Query-OPT: Optimizing Inference of Large Language Models via Multi-Query\n  Instructions in Meeting Summarization","summary":"  This work focuses on the task of query-based meeting summarization in which\nthe summary of a context (meeting transcript) is generated in response to a\nspecific query. When using Large Language Models (LLMs) for this task, usually\na new call to the LLM inference endpoint/API is triggered for each new query,\neven if the context stays the same. However, repeated calls to the LLM\ninference endpoints would significantly increase the costs of using them in\nproduction, making LLMs impractical for many real-world use cases. To address\nthis problem, in this paper, we investigate whether combining the queries for\nthe same input context in a single prompt to minimize repeated calls can be\nsuccessfully used in meeting summarization. In this regard, we conduct\nextensive experiments by comparing the performance of various popular LLMs:\nGPT-4, Gemini, Claude-3, LLaMA-2, Mistral, Phi-3, and Qwen-2 in single-query\nand multi-query settings. We observe that 100% reliability in generating the\nresponse in the expected format is usually limited to certain closed-source\nLLMs, with most open-source LLMs lagging behind (except a few 7B parameters\nLLMs like Mistral and Phi-3). We conclude that multi-query prompting could be\nuseful to significantly optimize the inference costs in meeting summarization.\n","authors":["Md Tahmid Rahman Laskar","Elena Khasanova","Xue-Yong Fu","Cheng Chen","Shashi Bhushan TN"],"pdf_url":"https://arxiv.org/pdf/2403.00067v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15353v1","updated":"2024-07-22T03:44:27Z","published":"2024-07-22T03:44:27Z","title":"Customized Retrieval Augmented Generation and Benchmarking for EDA Tool\n  Documentation QA","summary":"  Retrieval augmented generation (RAG) enhances the accuracy and reliability of\ngenerative AI models by sourcing factual information from external databases,\nwhich is extensively employed in document-grounded question-answering (QA)\ntasks. Off-the-shelf RAG flows are well pretrained on general-purpose\ndocuments, yet they encounter significant challenges when being applied to\nknowledge-intensive vertical domains, such as electronic design automation\n(EDA). This paper addresses such issue by proposing a customized RAG framework\nalong with three domain-specific techniques for EDA tool documentation QA,\nincluding a contrastive learning scheme for text embedding model fine-tuning, a\nreranker distilled from proprietary LLM, and a generative LLM fine-tuned with\nhigh-quality domain corpus. Furthermore, we have developed and released a\ndocumentation QA evaluation benchmark, ORD-QA, for OpenROAD, an advanced\nRTL-to-GDSII design platform. Experimental results demonstrate that our\nproposed RAG flow and techniques have achieved superior performance on ORD-QA\nas well as on a commercial tool, compared with state-of-the-arts. The ORD-QA\nbenchmark and the training dataset for our customized RAG flow are open-source\nat https://github.com/lesliepy99/RAG-EDA.\n","authors":["Yuan Pu","Zhuolun He","Tairu Qiu","Haoyuan Wu","Bei Yu"],"pdf_url":"https://arxiv.org/pdf/2407.15353v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15352v1","updated":"2024-07-22T03:43:46Z","published":"2024-07-22T03:43:46Z","title":"MAVEN-Fact: A Large-scale Event Factuality Detection Dataset","summary":"  Event Factuality Detection (EFD) task determines the factuality of textual\nevents, i.e., classifying whether an event is a fact, possibility, or\nimpossibility, which is essential for faithfully understanding and utilizing\nevent knowledge. However, due to the lack of high-quality large-scale data,\nevent factuality detection is under-explored in event understanding research,\nwhich limits the development of EFD community. To address these issues and\nprovide faithful event understanding, we introduce MAVEN-Fact, a large-scale\nand high-quality EFD dataset based on the MAVEN dataset. MAVEN-Fact includes\nfactuality annotations of 112,276 events, making it the largest EFD dataset.\nExtensive experiments demonstrate that MAVEN-Fact is challenging for both\nconventional fine-tuned models and large language models (LLMs). Thanks to the\ncomprehensive annotations of event arguments and relations in MAVEN, MAVEN-Fact\nalso supports some further analyses and we find that adopting event arguments\nand relations helps in event factuality detection for fine-tuned models but\ndoes not benefit LLMs. Furthermore, we preliminarily study an application case\nof event factuality detection and find it helps in mitigating event-related\nhallucination in LLMs. Our dataset and codes can be obtained from\n\\url{https://github.com/lcy2723/MAVEN-FACT}\n","authors":["Chunyang Li","Hao Peng","Xiaozhi Wang","Yunjia Qi","Lei Hou","Bin Xu","Juanzi Li"],"pdf_url":"https://arxiv.org/pdf/2407.15352v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2407.15351v1","updated":"2024-07-22T03:36:38Z","published":"2024-07-22T03:36:38Z","title":"LLMExplainer: Large Language Model based Bayesian Inference for Graph\n  Explanation Generation","summary":"  Recent studies seek to provide Graph Neural Network (GNN) interpretability\nvia multiple unsupervised learning models. Due to the scarcity of datasets,\ncurrent methods easily suffer from learning bias. To solve this problem, we\nembed a Large Language Model (LLM) as knowledge into the GNN explanation\nnetwork to avoid the learning bias problem. We inject LLM as a Bayesian\nInference (BI) module to mitigate learning bias. The efficacy of the BI module\nhas been proven both theoretically and experimentally. We conduct experiments\non both synthetic and real-world datasets. The innovation of our work lies in\ntwo parts: 1. We provide a novel view of the possibility of an LLM functioning\nas a Bayesian inference to improve the performance of existing algorithms; 2.\nWe are the first to discuss the learning bias issues in the GNN explanation\nproblem.\n","authors":["Jiaxing Zhang","Jiayi Liu","Dongsheng Luo","Jennifer Neville","Hua Wei"],"pdf_url":"https://arxiv.org/pdf/2407.15351v1.pdf","comment":"Preprint Paper with 13 pages"},{"id":"http://arxiv.org/abs/2407.15346v1","updated":"2024-07-22T03:05:32Z","published":"2024-07-22T03:05:32Z","title":"Knowledge Acquisition Disentanglement for Knowledge-based Visual\n  Question Answering with Large Language Models","summary":"  Knowledge-based Visual Question Answering (KVQA) requires both image and\nworld knowledge to answer questions. Current methods first retrieve knowledge\nfrom the image and external knowledge base with the original complex question,\nthen generate answers with Large Language Models (LLMs). However, since the\noriginal question contains complex elements that require knowledge from\ndifferent sources, acquiring different kinds of knowledge in a coupled manner\nmay confuse models and hinder them from retrieving precise knowledge.\nFurthermore, the ``forward-only'' answering process fails to explicitly capture\nthe knowledge needs of LLMs, which can further hurt answering quality. To cope\nwith the above limitations, we propose DKA: Disentangled Knowledge Acquisition\nfrom LLM feedback, a training-free framework that disentangles knowledge\nacquisition to avoid confusion and uses LLM's feedback to specify the required\nknowledge. Specifically, DKA requires LLMs to specify what knowledge they need\nto answer the question and decompose the original complex question into two\nsimple sub-questions: Image-based sub-question and Knowledge-based\nsub-question. Then we use the two sub-questions to retrieve knowledge from the\nimage and knowledge base, respectively. In this way, two knowledge acquisition\nmodels can focus on the content that corresponds to them and avoid disturbance\nof irrelevant elements in the original complex question, which can help to\nprovide more precise knowledge and better align the knowledge needs of LLMs to\nyield correct answers. Experiments on benchmark datasets show that DKA\nsignificantly outperforms SOTA models. To facilitate future research, our data\nand code are available at \\url{https://github.com/Lackel/DKA}.\n","authors":["Wenbin An","Feng Tian","Jiahao Nie","Wenkai Shi","Haonan Lin","Yan Chen","QianYing Wang","Yaqiang Wu","Guang Dai","Ping Chen"],"pdf_url":"https://arxiv.org/pdf/2407.15346v1.pdf","comment":"Pre-print"},{"id":"http://arxiv.org/abs/2407.15343v1","updated":"2024-07-22T02:57:10Z","published":"2024-07-22T02:57:10Z","title":"Improving Minimum Bayes Risk Decoding with Multi-Prompt","summary":"  While instruction fine-tuned LLMs are effective text generators, sensitivity\nto prompt construction makes performance unstable and sub-optimal in practice.\nRelying on a single \"best\" prompt cannot capture all differing approaches to a\ngeneration problem. Using this observation, we propose multi-prompt decoding,\nwhere many candidate generations are decoded from a prompt bank at\ninference-time. To ensemble candidates, we use Minimum Bayes Risk (MBR)\ndecoding, which selects a final output using a trained value metric. We show\nmulti-prompt improves MBR across a comprehensive set of conditional generation\ntasks, and show this is a result of estimating a more diverse and higher\nquality candidate space than that of a single prompt. Further experiments\nconfirm multi-prompt improves generation across tasks, models and metrics.\n","authors":["David Heineman","Yao Dou","Wei Xu"],"pdf_url":"https://arxiv.org/pdf/2407.15343v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15341v1","updated":"2024-07-22T02:54:46Z","published":"2024-07-22T02:54:46Z","title":"ZZU-NLP at SIGHAN-2024 dimABSA Task: Aspect-Based Sentiment Analysis\n  with Coarse-to-Fine In-context Learning","summary":"  The DimABSA task requires fine-grained sentiment intensity prediction for\nrestaurant reviews, including scores for Valence and Arousal dimensions for\neach Aspect Term. In this study, we propose a Coarse-to-Fine In-context\nLearning(CFICL) method based on the Baichuan2-7B model for the DimABSA task in\nthe SIGHAN 2024 workshop. Our method improves prediction accuracy through a\ntwo-stage optimization process. In the first stage, we use fixed in-context\nexamples and prompt templates to enhance the model's sentiment recognition\ncapability and provide initial predictions for the test data. In the second\nstage, we encode the Opinion field using BERT and select the most similar\ntraining data as new in-context examples based on similarity. These examples\ninclude the Opinion field and its scores, as well as related opinion words and\ntheir average scores. By filtering for sentiment polarity, we ensure that the\nexamples are consistent with the test data. Our method significantly improves\nprediction accuracy and consistency by effectively utilizing training data and\noptimizing in-context examples, as validated by experimental results.\n","authors":["Senbin Zhu","Hanjie Zhao","Xingren Wang","Shanhong Liu","Yuxiang Jia","Hongying Zan"],"pdf_url":"https://arxiv.org/pdf/2407.15341v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15339v1","updated":"2024-07-22T02:53:18Z","published":"2024-07-22T02:53:18Z","title":"Deep Learning for Economists","summary":"  Deep learning provides powerful methods to impute structured information from\nlarge-scale, unstructured text and image datasets. For example, economists\nmight wish to detect the presence of economic activity in satellite images, or\nto measure the topics or entities mentioned in social media, the congressional\nrecord, or firm filings. This review introduces deep neural networks, covering\nmethods such as classifiers, regression models, generative AI, and embedding\nmodels. Applications include classification, document digitization, record\nlinkage, and methods for data exploration in massive scale text and image\ncorpora. When suitable methods are used, deep learning models can be cheap to\ntune and can scale affordably to problems involving millions or billions of\ndata points.. The review is accompanied by a companion website, EconDL, with\nuser-friendly demo notebooks, software resources, and a knowledge base that\nprovides technical details and additional applications.\n","authors":["Melissa Dell"],"pdf_url":"https://arxiv.org/pdf/2407.15339v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09781v4","updated":"2024-07-22T02:47:56Z","published":"2023-12-15T13:33:18Z","title":"GSQA: An End-to-End Model for Generative Spoken Question Answering","summary":"  In recent advancements in spoken question answering (QA), end-to-end models\nhave made significant strides. However, previous research has primarily focused\non extractive span selection. While this extractive-based approach is effective\nwhen answers are present directly within the input, it falls short in\naddressing abstractive questions, where answers are not directly extracted but\ninferred from the given information. To bridge this gap, we introduce the first\nend-to-end Generative Spoken Question Answering (GSQA) model that empowers the\nsystem to engage in abstractive reasoning. The challenge in training our GSQA\nmodel lies in the absence of a spoken abstractive QA dataset. We propose using\ntext models for initialization and leveraging the extractive QA dataset to\ntransfer knowledge from the text generative model to the spoken generative\nmodel. Experimental results indicate that our model surpasses the previous\nextractive model by 3% on extractive QA datasets. Furthermore, the GSQA model\nhas only been fine-tuned on the spoken extractive QA dataset. Despite not\nhaving seen any spoken abstractive QA data, it can still closely match the\nperformance of the cascade model. In conclusion, our GSQA model shows the\npotential to generalize to a broad spectrum of questions, thus further\nexpanding the spoken question answering capabilities of abstractive QA. Our\ncode is available at https://voidful.github.io/GSQA\n","authors":["Min-Han Shih","Ho-Lam Chung","Yu-Chi Pai","Ming-Hao Hsu","Guan-Ting Lin","Shang-Wen Li","Hung-yi Lee"],"pdf_url":"https://arxiv.org/pdf/2312.09781v4.pdf","comment":"5 pages, 2 figures, Interspeech 2024"},{"id":"http://arxiv.org/abs/2401.09967v4","updated":"2024-07-22T01:05:29Z","published":"2024-01-18T13:31:24Z","title":"Sketch-Guided Constrained Decoding for Boosting Blackbox Large Language\n  Models without Logit Access","summary":"  Constrained decoding, a technique for enforcing constraints on language model\noutputs, offers a way to control text generation without retraining or\narchitectural modifications. Its application is, however, typically restricted\nto models that give users access to next-token distributions (usually via\nsoftmax logits), which poses a limitation with blackbox large language models\n(LLMs). This paper introduces sketch-guided constrained decoding (SGCD), a\nnovel approach to constrained decoding for blackbox LLMs, which operates\nwithout access to the logits of the blackbox LLM. SGCD utilizes a locally\nhosted auxiliary model to refine the output of an unconstrained blackbox LLM,\neffectively treating this initial output as a \"sketch\" for further elaboration.\nThis approach is complementary to traditional logit-based techniques and\nenables the application of constrained decoding in settings where full model\ntransparency is unavailable. We demonstrate the efficacy of SGCD through\nexperiments in closed information extraction and constituency parsing, showing\nhow it enhances the utility and flexibility of blackbox LLMs for complex NLP\ntasks.\n","authors":["Saibo Geng","Berkay Döner","Chris Wendler","Martin Josifoski","Robert West"],"pdf_url":"https://arxiv.org/pdf/2401.09967v4.pdf","comment":"Accepted to ACL 2024 Oral"},{"id":"http://arxiv.org/abs/2403.18327v2","updated":"2024-07-22T00:41:38Z","published":"2024-03-27T08:08:00Z","title":"$\\forall$uto$\\exists$val: Autonomous Assessment of LLMs in Formal\n  Synthesis and Interpretation Tasks","summary":"  This paper presents $\\forall$uto$\\exists$val, a new approach for scaling LLM\nassessment in translating formal syntax -- such as first-order logic, regular\nexpressions, etc -- to natural language (interpretation) or vice versa\n(compilation), thereby facilitating their use in applications such as\ngenerating/explaining logic and control flow for programs etc. Existing\napproaches for LLM assessment in these areas require labor-intensive\nground-truth creation, the availability of which undermines the separation of\ntraining and test sets. Furthermore, such datasets typically include relatively\nfew hand-coded test cases over which LLM accuracy is determined, thus making\nthem inadequate for determining the safety or correctness of their generated\noutputs. We introduce a new approach that utilizes context-free grammars (CFGs)\nto generate out-of-distribution datasets on the fly and perform closed-loop\ntesting of LLM capabilities using formal verifiers to guarantee the correctness\nof LLM outputs without any human intervention. We release our dataset and\nbenchmark as open-source code at\n\\url{https://github.com/AAIR-lab/auto-llm-assessment}. We also conduct an\nassessment of several SOTA closed and open-source LLMs to showcase the\nfeasibility and scalability of this paradigm. Our experiments reveal that SOTA\nLLMs are unable to solve the formal translation task adequately.\n","authors":["Rushang Karia","Daniel Bramblett","Daksh Dobhal","Pulkit Verma","Siddharth Srivastava"],"pdf_url":"https://arxiv.org/pdf/2403.18327v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.07687v3","updated":"2024-07-22T00:40:17Z","published":"2023-04-16T03:49:50Z","title":"MLRegTest: A Benchmark for the Machine Learning of Regular Languages","summary":"  Synthetic datasets constructed from formal languages allow fine-grained\nexamination of the learning and generalization capabilities of machine learning\nsystems for sequence classification. This article presents a new benchmark for\nmachine learning systems on sequence classification called MLRegTest, which\ncontains training, development, and test sets from 1,800 regular languages.\nDifferent kinds of formal languages represent different kinds of long-distance\ndependencies, and correctly identifying long-distance dependencies in sequences\nis a known challenge for ML systems to generalize successfully. MLRegTest\norganizes its languages according to their logical complexity (monadic second\norder, first order, propositional, or monomial expressions) and the kind of\nlogical literals (string, tier-string, subsequence, or combinations thereof).\nThe logical complexity and choice of literal provides a systematic way to\nunderstand different kinds of long-distance dependencies in regular languages,\nand therefore to understand the capacities of different ML systems to learn\nsuch long-distance dependencies. Finally, the performance of different neural\nnetworks (simple RNN, LSTM, GRU, transformer) on MLRegTest is examined. The\nmain conclusion is that performance depends significantly on the kind of test\nset, the class of language, and the neural network architecture.\n","authors":["Sam van der Poel","Dakotah Lambert","Kalina Kostyszyn","Tiantian Gao","Rahul Verma","Derek Andersen","Joanne Chau","Emily Peterson","Cody St. Clair","Paul Fodor","Chihiro Shibata","Jeffrey Heinz"],"pdf_url":"https://arxiv.org/pdf/2304.07687v3.pdf","comment":"43 pages, MLRegTest benchmark available at\n  https://doi.org/10.5061/dryad.dncjsxm4h , associated code at\n  https://github.com/heinz-jeffrey/subregular-learning"},{"id":"http://arxiv.org/abs/2404.04735v2","updated":"2024-07-22T22:37:40Z","published":"2024-04-06T21:39:01Z","title":"MACM: Utilizing a Multi-Agent System for Condition Mining in Solving\n  Complex Mathematical Problems","summary":"  Recent advancements in large language models, such as GPT-4, have\ndemonstrated remarkable capabilities in processing standard queries. Despite\nthese advancements, their performance substantially declines in\n\\textbf{advanced mathematical problems requiring complex, multi-step logical\nreasoning}. To enhance their inferential capabilities, current research has\ndelved into \\textit{prompting engineering}, exemplified by methodologies such\nas the Tree of Thought and Graph of Thought. Nonetheless, these existing\napproaches encounter two significant limitations. Firstly, their effectiveness\nin tackling complex mathematical problems is somewhat constrained. Secondly,\nthe necessity to design distinct prompts for individual problems hampers their\ngeneralizability. In response to these limitations, this paper introduces the\n\\textit{Multi-Agent System for conditional Mining} (\\textbf{MACM}) prompting\nmethod. It not only resolves intricate mathematical problems but also\ndemonstrates strong generalization capabilities across various mathematical\ncontexts. With the assistance of MACM, the accuracy of GPT-4 Turbo on the most\nchallenging level five mathematical problems in the MATH dataset increase from\n$\\mathbf{54.68\\%} \\text{ to } \\mathbf{76.73\\%}$. The code is available in\n\\url{https://github.com/bin123apple/MACM}.\n","authors":["Bin Lei","Yi Zhang","Shan Zuo","Ali Payani","Caiwen Ding"],"pdf_url":"https://arxiv.org/pdf/2404.04735v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16073v1","updated":"2024-07-22T22:14:56Z","published":"2024-07-22T22:14:56Z","title":"KaPQA: Knowledge-Augmented Product Question-Answering","summary":"  Question-answering for domain-specific applications has recently attracted\nmuch interest due to the latest advancements in large language models (LLMs).\nHowever, accurately assessing the performance of these applications remains a\nchallenge, mainly due to the lack of suitable benchmarks that effectively\nsimulate real-world scenarios. To address this challenge, we introduce two\nproduct question-answering (QA) datasets focused on Adobe Acrobat and Photoshop\nproducts to help evaluate the performance of existing models on domain-specific\nproduct QA tasks. Additionally, we propose a novel knowledge-driven RAG-QA\nframework to enhance the performance of the models in the product QA task. Our\nexperiments demonstrated that inducing domain knowledge through query\nreformulation allowed for increased retrieval and generative performance when\ncompared to standard RAG-QA methods. This improvement, however, is slight, and\nthus illustrates the challenge posed by the datasets introduced.\n","authors":["Swetha Eppalapally","Daksh Dangi","Chaithra Bhat","Ankita Gupta","Ruiyi Zhang","Shubham Agarwal","Karishma Bagga","Seunghyun Yoon","Nedim Lipka","Ryan A. Rossi","Franck Dernoncourt"],"pdf_url":"https://arxiv.org/pdf/2407.16073v1.pdf","comment":"Accepted at the ACL 2024 Workshop on Knowledge Augmented Methods for\n  NLP"},{"id":"http://arxiv.org/abs/2407.16047v1","updated":"2024-07-22T20:54:35Z","published":"2024-07-22T20:54:35Z","title":"Leveraging Large Language Models to Geolocate Linguistic Variations in\n  Social Media Posts","summary":"  Geolocalization of social media content is the task of determining the\ngeographical location of a user based on textual data, that may show linguistic\nvariations and informal language. In this project, we address the GeoLingIt\nchallenge of geolocalizing tweets written in Italian by leveraging large\nlanguage models (LLMs). GeoLingIt requires the prediction of both the region\nand the precise coordinates of the tweet. Our approach involves fine-tuning\npre-trained LLMs to simultaneously predict these geolocalization aspects. By\nintegrating innovative methodologies, we enhance the models' ability to\nunderstand the nuances of Italian social media text to improve the\nstate-of-the-art in this domain. This work is conducted as part of the Large\nLanguage Models course at the Bertinoro International Spring School 2024. We\nmake our code publicly available on GitHub\nhttps://github.com/dawoz/geolingit-biss2024.\n","authors":["Davide Savarro","Davide Zago","Stefano Zoia"],"pdf_url":"https://arxiv.org/pdf/2407.16047v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14666v2","updated":"2024-07-22T20:37:55Z","published":"2024-03-03T03:01:14Z","title":"SyllabusQA: A Course Logistics Question Answering Dataset","summary":"  Automated teaching assistants and chatbots have significant potential to\nreduce the workload of human instructors, especially for logistics-related\nquestion answering, which is important to students yet repetitive for\ninstructors. However, due to privacy concerns, there is a lack of publicly\navailable datasets. We introduce SyllabusQA, an open-source dataset with 63\nreal course syllabi covering 36 majors, containing 5,078 open-ended course\nlogistics-related question-answer pairs that are diverse in both question types\nand answer formats. Since many logistics-related questions contain critical\ninformation like the date of an exam, it is important to evaluate the\nfactuality of answers. We benchmark several strong baselines on this task, from\nlarge language model prompting to retrieval-augmented generation. We introduce\nFact-QA, an LLM-based (GPT-4) evaluation metric to evaluate the factuality of\npredicted answers. We find that despite performing close to humans on\ntraditional metrics of textual similarity, there remains a significant gap\nbetween automated approaches and humans in terms of fact precision.\n","authors":["Nigel Fernandez","Alexander Scarlatos","Andrew Lan"],"pdf_url":"https://arxiv.org/pdf/2403.14666v2.pdf","comment":"ACL 2024: The 62nd Annual Meeting of the Association for\n  Computational Linguistics"},{"id":"http://arxiv.org/abs/2407.16030v1","updated":"2024-07-22T20:13:10Z","published":"2024-07-22T20:13:10Z","title":"Enhancing Temporal Understanding in LLMs for Semi-structured Tables","summary":"  Temporal reasoning over tabular data presents substantial challenges for\nlarge language models (LLMs), as evidenced by recent research. In this study,\nwe conduct a comprehensive analysis of temporal datasets to pinpoint the\nspecific limitations of LLMs. Our investigation leads to enhancements in\nTempTabQA, a dataset specifically designed for tabular temporal question\nanswering. We provide critical insights for improving LLM performance in\ntemporal reasoning tasks with tabular data. Furthermore, we introduce a novel\napproach, C.L.E.A.R to strengthen LLM capabilities in this domain. Our findings\ndemonstrate that our method significantly improves evidence-based reasoning\nacross various models. Additionally, our experimental results reveal that\nindirect supervision with auxiliary data substantially boosts model performance\nin these tasks. This work contributes to a deeper understanding of LLMs'\ntemporal reasoning abilities over tabular data and promotes advancements in\ntheir application across diverse fields.\n","authors":["Irwin Deng","Kushagra Dixit","Vivek Gupta","Dan Roth"],"pdf_url":"https://arxiv.org/pdf/2407.16030v1.pdf","comment":"Total Pages 18, Total Tables 6, Total figures 7"},{"id":"http://arxiv.org/abs/2407.06314v3","updated":"2024-07-22T19:27:20Z","published":"2024-07-08T18:27:54Z","title":"Personality Analysis for Social Media Users using Arabic language and\n  its Effect on Sentiment Analysis","summary":"  Social media is heading towards more and more personalization, where\nindividuals reveal their beliefs, interests, habits, and activities, simply\noffering glimpses into their personality traits. This study, explores the\ncorrelation between the use of Arabic language on twitter, personality traits\nand its impact on sentiment analysis. We indicated the personality traits of\nusers based on the information extracted from their profile activities, and the\ncontent of their tweets. Our analysis incorporated linguistic features, profile\nstatistics (including gender, age, bio, etc.), as well as additional features\nlike emoticons. To obtain personality data, we crawled the timelines and\nprofiles of users who took the 16personalities test in Arabic on\n16personalities.com. Our dataset, \"AraPers\", comprised 3,250 users who shared\ntheir personality results on twitter. We implemented various machine learning\ntechniques, to reveal personality traits and developed a dedicated model for\nthis purpose, achieving a 74.86% accuracy rate with BERT, analysis of this\ndataset proved that linguistic features, profile features and derived model can\nbe used to differentiate between different personality traits. Furthermore, our\nfindings demonstrated that personality affect sentiment in social media. This\nresearch contributes to the ongoing efforts in developing robust understanding\nof the relation between human behaviour on social media and personality\nfeatures for real-world applications, such as political discourse analysis, and\npublic opinion tracking.\n","authors":["Mokhaiber Dandash","Masoud Asadpour"],"pdf_url":"https://arxiv.org/pdf/2407.06314v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16008v1","updated":"2024-07-22T19:21:55Z","published":"2024-07-22T19:21:55Z","title":"Boosting Reward Model with Preference-Conditional Multi-Aspect Synthetic\n  Data Generation","summary":"  Reward models (RMs) are crucial for aligning large language models (LLMs)\nwith human preferences. They are trained using preference datasets where each\nexample consists of one input prompt, two responses, and a preference label. As\ncurating a high-quality human labeled preference dataset is both time-consuming\nand expensive, people often rely on existing powerful LLMs for preference label\ngeneration. This can potentially introduce noise and impede RM training. In\nthis work, we present RMBoost, a novel synthetic preference data generation\nparadigm to boost reward model quality. Unlike traditional methods, which\ngenerate two responses before obtaining the preference label, RMBoost first\ngenerates one response and selects a preference label, followed by generating\nthe second more (or less) preferred response conditioned on the pre-selected\npreference label and the first response. This approach offers two main\nadvantages. First, RMBoost reduces labeling noise since preference pairs are\nconstructed intentionally. Second, RMBoost facilitates the creation of more\ndiverse responses by incorporating various quality aspects (e.g., helpfulness,\nrelevance, completeness) into the prompts. We conduct extensive experiments\nacross three diverse datasets and demonstrate that RMBoost outperforms other\nsynthetic preference data generation techniques and significantly boosts the\nperformance of four distinct reward models.\n","authors":["Jiaming Shen","Ran Xu","Yennie Jun","Zhen Qin","Tianqi Liu","Carl Yang","Yi Liang","Simon Baumgartner","Michael Bendersky"],"pdf_url":"https://arxiv.org/pdf/2407.16008v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16007v1","updated":"2024-07-22T19:21:01Z","published":"2024-07-22T19:21:01Z","title":"SocialQuotes: Learning Contextual Roles of Social Media Quotes on the\n  Web","summary":"  Web authors frequently embed social media to support and enrich their\ncontent, creating the potential to derive web-based, cross-platform social\nmedia representations that can enable more effective social media retrieval\nsystems and richer scientific analyses. As step toward such capabilities, we\nintroduce a novel language modeling framework that enables automatic annotation\nof roles that social media entities play in their embedded web context. Using\nrelated communication theory, we liken social media embeddings to quotes,\nformalize the page context as structured natural language signals, and identify\na taxonomy of roles for quotes within the page context. We release\nSocialQuotes, a new data set built from the Common Crawl of over 32 million\nsocial quotes, 8.3k of them with crowdsourced quote annotations. Using\nSocialQuotes and the accompanying annotations, we provide a role classification\ncase study, showing reasonable performance with modern-day LLMs, and exposing\nexplainable aspects of our framework via page content ablations. We also\nclassify a large batch of un-annotated quotes, revealing interesting\ncross-domain, cross-platform role distributions on the web.\n","authors":["John Palowitch","Hamidreza Alvari","Mehran Kazemi","Tanvir Amin","Filip Radlinski"],"pdf_url":"https://arxiv.org/pdf/2407.16007v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15992v1","updated":"2024-07-22T19:00:11Z","published":"2024-07-22T19:00:11Z","title":"Multimodal Input Aids a Bayesian Model of Phonetic Learning","summary":"  One of the many tasks facing the typically-developing child language learner\nis learning to discriminate between the distinctive sounds that make up words\nin their native language. Here we investigate whether multimodal\ninformation--specifically adult speech coupled with video frames of speakers'\nfaces--benefits a computational model of phonetic learning. We introduce a\nmethod for creating high-quality synthetic videos of speakers' faces for an\nexisting audio corpus. Our learning model, when both trained and tested on\naudiovisual inputs, achieves up to a 8.1% relative improvement on a phoneme\ndiscrimination battery compared to a model trained and tested on audio-only\ninput. It also outperforms the audio model by up to 3.9% when both are tested\non audio-only data, suggesting that visual information facilitates the\nacquisition of acoustic distinctions. Visual information is especially\nbeneficial in noisy audio environments, where an audiovisual model closes 67%\nof the loss in discrimination performance of the audio model in noise relative\nto a non-noisy environment. These results demonstrate that visual information\nbenefits an ideal learner and illustrate some of the ways that children might\nbe able to leverage visual cues when learning to discriminate speech sounds.\n","authors":["Sophia Zhi","Roger P. Levy","Stephan C. Meylan"],"pdf_url":"https://arxiv.org/pdf/2407.15992v1.pdf","comment":"12 pages, 5 figures"},{"id":"http://arxiv.org/abs/2405.01458v2","updated":"2024-07-22T18:46:11Z","published":"2024-05-02T16:44:31Z","title":"UQA: Corpus for Urdu Question Answering","summary":"  This paper introduces UQA, a novel dataset for question answering and text\ncomprehension in Urdu, a low-resource language with over 70 million native\nspeakers. UQA is generated by translating the Stanford Question Answering\nDataset (SQuAD2.0), a large-scale English QA dataset, using a technique called\nEATS (Enclose to Anchor, Translate, Seek), which preserves the answer spans in\nthe translated context paragraphs. The paper describes the process of selecting\nand evaluating the best translation model among two candidates: Google\nTranslator and Seamless M4T. The paper also benchmarks several state-of-the-art\nmultilingual QA models on UQA, including mBERT, XLM-RoBERTa, and mT5, and\nreports promising results. For XLM-RoBERTa-XL, we have an F1 score of 85.99 and\n74.56 EM. UQA is a valuable resource for developing and testing multilingual\nNLP systems for Urdu and for enhancing the cross-lingual transferability of\nexisting models. Further, the paper demonstrates the effectiveness of EATS for\ncreating high-quality datasets for other languages and domains. The UQA dataset\nand the code are publicly available at www.github.com/sameearif/UQA.\n","authors":["Samee Arif","Sualeha Farid","Awais Athar","Agha Ali Raza"],"pdf_url":"https://arxiv.org/pdf/2405.01458v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08488v2","updated":"2024-07-22T18:41:53Z","published":"2024-07-11T13:22:17Z","title":"Lynx: An Open Source Hallucination Evaluation Model","summary":"  Retrieval Augmented Generation (RAG) techniques aim to mitigate\nhallucinations in Large Language Models (LLMs). However, LLMs can still produce\ninformation that is unsupported or contradictory to the retrieved contexts. We\nintroduce LYNX, a SOTA hallucination detection LLM that is capable of advanced\nreasoning on challenging real-world hallucination scenarios. To evaluate LYNX,\nwe present HaluBench, a comprehensive hallucination evaluation benchmark,\nconsisting of 15k samples sourced from various real-world domains. Our\nexperiment results show that LYNX outperforms GPT-4o, Claude-3-Sonnet, and\nclosed and open-source LLM-as-a-judge models on HaluBench. We release LYNX,\nHaluBench and our evaluation code for public access.\n","authors":["Selvan Sunitha Ravi","Bartosz Mielczarek","Anand Kannappan","Douwe Kiela","Rebecca Qian"],"pdf_url":"https://arxiv.org/pdf/2407.08488v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15975v1","updated":"2024-07-22T18:37:53Z","published":"2024-07-22T18:37:53Z","title":"Multilingual Fine-Grained News Headline Hallucination Detection","summary":"  The popularity of automated news headline generation has surged with\nadvancements in pre-trained language models. However, these models often suffer\nfrom the ``hallucination'' problem, where the generated headline is not fully\nsupported by its source article. Efforts to address this issue have\npredominantly focused on English, using over-simplistic classification schemes\nthat overlook nuanced hallucination types. In this study, we introduce the\nfirst multilingual, fine-grained news headline hallucination detection dataset\nthat contains over 11 thousand pairs in 5 languages, each annotated with\ndetailed hallucination types by experts. We conduct extensive experiments on\nthis dataset under two settings. First, we implement several supervised\nfine-tuning approaches as preparatory solutions and demonstrate this dataset's\nchallenges and utilities. Second, we test various large language models'\nin-context learning abilities and propose two novel techniques,\nlanguage-dependent demonstration selection and coarse-to-fine prompting, to\nboost the few-shot hallucination detection performance in terms of the\nexample-F1 metric. We release this dataset to foster further research in\nmultilingual, fine-grained headline hallucination detection.\n","authors":["Jiaming Shen","Tianqi Liu","Jialu Liu","Zhen Qin","Jay Pavagadhi","Simon Baumgartner","Michael Bendersky"],"pdf_url":"https://arxiv.org/pdf/2407.15975v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.14336v4","updated":"2024-07-22T18:22:08Z","published":"2023-05-23T17:58:10Z","title":"Schema-Driven Information Extraction from Heterogeneous Tables","summary":"  In this paper, we explore the question of whether large language models can\nsupport cost-efficient information extraction from tables. We introduce\nschema-driven information extraction, a new task that transforms tabular data\ninto structured records following a human-authored schema. To assess various\nLLM's capabilities on this task, we present a benchmark comprised of tables\nfrom four diverse domains: machine learning papers, chemistry literature,\nmaterial science journals, and webpages. We use this collection of annotated\ntables to evaluate the ability of open-source and API-based language models to\nextract information from tables covering diverse domains and data formats. Our\nexperiments demonstrate that surprisingly competitive performance can be\nachieved without requiring task-specific pipelines or labels, achieving F1\nscores ranging from 74.2 to 96.1, while maintaining cost efficiency. Moreover,\nthrough detailed ablation studies and analyses, we investigate the factors\ncontributing to model success and validate the practicality of distilling\ncompact models to reduce API reliance.\n","authors":["Fan Bai","Junmo Kang","Gabriel Stanovsky","Dayne Freitag","Mark Dredze","Alan Ritter"],"pdf_url":"https://arxiv.org/pdf/2305.14336v4.pdf","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2407.15850v1","updated":"2024-07-22T17:59:56Z","published":"2024-07-22T17:59:56Z","title":"AutoAD-Zero: A Training-Free Framework for Zero-Shot Audio Description","summary":"  Our objective is to generate Audio Descriptions (ADs) for both movies and TV\nseries in a training-free manner. We use the power of off-the-shelf\nVisual-Language Models (VLMs) and Large Language Models (LLMs), and develop\nvisual and text prompting strategies for this task. Our contributions are\nthree-fold: (i) We demonstrate that a VLM can successfully name and refer to\ncharacters if directly prompted with character information through visual\nindications without requiring any fine-tuning; (ii) A two-stage process is\ndeveloped to generate ADs, with the first stage asking the VLM to\ncomprehensively describe the video, followed by a second stage utilising a LLM\nto summarise dense textual information into one succinct AD sentence; (iii) A\nnew dataset for TV audio description is formulated. Our approach, named\nAutoAD-Zero, demonstrates outstanding performance (even competitive with some\nmodels fine-tuned on ground truth ADs) in AD generation for both movies and TV\nseries, achieving state-of-the-art CRITIC scores.\n","authors":["Junyu Xie","Tengda Han","Max Bain","Arsha Nagrani","Gül Varol","Weidi Xie","Andrew Zisserman"],"pdf_url":"https://arxiv.org/pdf/2407.15850v1.pdf","comment":"Project Page: https://www.robots.ox.ac.uk/~vgg/research/autoad-zero/"},{"id":"http://arxiv.org/abs/2407.15848v1","updated":"2024-07-22T17:59:46Z","published":"2024-07-22T17:59:46Z","title":"BoostMVSNeRFs: Boosting MVS-based NeRFs to Generalizable View Synthesis\n  in Large-scale Scenes","summary":"  While Neural Radiance Fields (NeRFs) have demonstrated exceptional quality,\ntheir protracted training duration remains a limitation. Generalizable and\nMVS-based NeRFs, although capable of mitigating training time, often incur\ntradeoffs in quality. This paper presents a novel approach called BoostMVSNeRFs\nto enhance the rendering quality of MVS-based NeRFs in large-scale scenes. We\nfirst identify limitations in MVS-based NeRF methods, such as restricted\nviewport coverage and artifacts due to limited input views. Then, we address\nthese limitations by proposing a new method that selects and combines multiple\ncost volumes during volume rendering. Our method does not require training and\ncan adapt to any MVS-based NeRF methods in a feed-forward fashion to improve\nrendering quality. Furthermore, our approach is also end-to-end trainable,\nallowing fine-tuning on specific scenes. We demonstrate the effectiveness of\nour method through experiments on large-scale datasets, showing significant\nrendering quality improvements in large-scale scenes and unbounded outdoor\nscenarios. We release the source code of BoostMVSNeRFs at\nhttps://su-terry.github.io/BoostMVSNeRFs/.\n","authors":["Chih-Hai Su","Chih-Yao Hu","Shr-Ruei Tsai","Jie-Ying Lee","Chin-Yang Lin","Yu-Lun Liu"],"pdf_url":"https://arxiv.org/pdf/2407.15848v1.pdf","comment":"SIGGRAPH 2024 Conference Papers. Project page:\n  https://su-terry.github.io/BoostMVSNeRFs/"},{"id":"http://arxiv.org/abs/2407.15845v1","updated":"2024-07-22T17:59:10Z","published":"2024-07-22T17:59:10Z","title":"Reconstructing Training Data From Real World Models Trained with\n  Transfer Learning","summary":"  Current methods for reconstructing training data from trained classifiers are\nrestricted to very small models, limited training set sizes, and low-resolution\nimages. Such restrictions hinder their applicability to real-world scenarios.\nIn this paper, we present a novel approach enabling data reconstruction in\nrealistic settings for models trained on high-resolution images. Our method\nadapts the reconstruction scheme of arXiv:2206.07758 to real-world scenarios --\nspecifically, targeting models trained via transfer learning over image\nembeddings of large pre-trained models like DINO-ViT and CLIP. Our work employs\ndata reconstruction in the embedding space rather than in the image space,\nshowcasing its applicability beyond visual data. Moreover, we introduce a novel\nclustering-based method to identify good reconstructions from thousands of\ncandidates. This significantly improves on previous works that relied on\nknowledge of the training set to identify good reconstructed images. Our\nfindings shed light on a potential privacy risk for data leakage from models\ntrained using transfer learning.\n","authors":["Yakir Oz","Gilad Yehudai","Gal Vardi","Itai Antebi","Michal Irani","Niv Haim"],"pdf_url":"https://arxiv.org/pdf/2407.15845v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15843v1","updated":"2024-07-22T17:59:01Z","published":"2024-07-22T17:59:01Z","title":"CarFormer: Self-Driving with Learned Object-Centric Representations","summary":"  The choice of representation plays a key role in self-driving. Bird's eye\nview (BEV) representations have shown remarkable performance in recent years.\nIn this paper, we propose to learn object-centric representations in BEV to\ndistill a complex scene into more actionable information for self-driving. We\nfirst learn to place objects into slots with a slot attention model on BEV\nsequences. Based on these object-centric representations, we then train a\ntransformer to learn to drive as well as reason about the future of other\nvehicles. We found that object-centric slot representations outperform both\nscene-level and object-level approaches that use the exact attributes of\nobjects. Slot representations naturally incorporate information about objects\nfrom their spatial and temporal context such as position, heading, and speed\nwithout explicitly providing it. Our model with slots achieves an increased\ncompletion rate of the provided routes and, consequently, a higher driving\nscore, with a lower variance across multiple runs, affirming slots as a\nreliable alternative in object-centric approaches. Additionally, we validate\nour model's performance as a world model through forecasting experiments,\ndemonstrating its capability to predict future slot representations accurately.\nThe code and the pre-trained models can be found at\nhttps://kuis-ai.github.io/CarFormer/.\n","authors":["Shadi Hamdan","Fatma Güney"],"pdf_url":"https://arxiv.org/pdf/2407.15843v1.pdf","comment":"Accepted to ECCV 2024, code and the pre-trained models can be found\n  at https://kuis-ai.github.io/CarFormer/"},{"id":"http://arxiv.org/abs/2407.15844v1","updated":"2024-07-22T17:59:01Z","published":"2024-07-22T17:59:01Z","title":"HandDGP: Camera-Space Hand Mesh Prediction with Differentiable Global\n  Positioning","summary":"  Predicting camera-space hand meshes from single RGB images is crucial for\nenabling realistic hand interactions in 3D virtual and augmented worlds.\nPrevious work typically divided the task into two stages: given a cropped image\nof the hand, predict meshes in relative coordinates, followed by lifting these\npredictions into camera space in a separate and independent stage, often\nresulting in the loss of valuable contextual and scale information. To prevent\nthe loss of these cues, we propose unifying these two stages into an end-to-end\nsolution that addresses the 2D-3D correspondence problem. This solution enables\nback-propagation from camera space outputs to the rest of the network through a\nnew differentiable global positioning module. We also introduce an image\nrectification step that harmonizes both the training dataset and the input\nimage as if they were acquired with the same camera, helping to alleviate the\ninherent scale-depth ambiguity of the problem. We validate the effectiveness of\nour framework in evaluations against several baselines and state-of-the-art\napproaches across three public benchmarks.\n","authors":["Eugene Valassakis","Guillermo Garcia-Hernando"],"pdf_url":"https://arxiv.org/pdf/2407.15844v1.pdf","comment":"To be presented at ECCV 2024"},{"id":"http://arxiv.org/abs/2407.15842v1","updated":"2024-07-22T17:58:05Z","published":"2024-07-22T17:58:05Z","title":"Artist: Aesthetically Controllable Text-Driven Stylization without\n  Training","summary":"  Diffusion models entangle content and style generation during the denoising\nprocess, leading to undesired content modification when directly applied to\nstylization tasks. Existing methods struggle to effectively control the\ndiffusion model to meet the aesthetic-level requirements for stylization. In\nthis paper, we introduce \\textbf{Artist}, a training-free approach that\naesthetically controls the content and style generation of a pretrained\ndiffusion model for text-driven stylization. Our key insight is to disentangle\nthe denoising of content and style into separate diffusion processes while\nsharing information between them. We propose simple yet effective content and\nstyle control methods that suppress style-irrelevant content generation,\nresulting in harmonious stylization results. Extensive experiments demonstrate\nthat our method excels at achieving aesthetic-level stylization requirements,\npreserving intricate details in the content image and aligning well with the\nstyle prompt. Furthermore, we showcase the highly controllability of the\nstylization strength from various perspectives. Code will be released, project\nhome page: https://DiffusionArtist.github.io\n","authors":["Ruixiang Jiang","Changwen Chen"],"pdf_url":"https://arxiv.org/pdf/2407.15842v1.pdf","comment":"WIP,webpage: https://DiffusionArtist.github.io"},{"id":"http://arxiv.org/abs/2407.15841v1","updated":"2024-07-22T17:58:04Z","published":"2024-07-22T17:58:04Z","title":"SlowFast-LLaVA: A Strong Training-Free Baseline for Video Large Language\n  Models","summary":"  We propose SlowFast-LLaVA (or SF-LLaVA for short), a training-free video\nlarge language model (LLM) that can jointly capture the detailed spatial\nsemantics and long-range temporal context without exceeding the token budget of\ncommonly used LLMs. This is realized by using a two-stream SlowFast design of\ninputs for Video LLMs to aggregate features from sampled video frames in an\neffective way. Specifically, the Slow pathway extracts features at a low frame\nrate while keeping as many spatial details as possible (e.g., with 24x24\ntokens), and the Fast pathway operates on a high frame rate but uses a larger\nspatial pooling stride (e.g., downsampling 6x) to focus on the motion cues. As\na result, this design allows us to adequately capture both spatial and temporal\nfeatures that are beneficial for understanding details along the video.\nExperimental results show that SF-LLaVA outperforms existing training-free\nmethods on a wide range of video tasks. On some benchmarks, it achieves\ncomparable or even better performance compared to state-of-the-art Video LLMs\nthat are fine-tuned on video datasets.\n","authors":["Mingze Xu","Mingfei Gao","Zhe Gan","Hong-You Chen","Zhengfeng Lai","Haiming Gang","Kai Kang","Afshin Dehghan"],"pdf_url":"https://arxiv.org/pdf/2407.15841v1.pdf","comment":"Technical report"},{"id":"http://arxiv.org/abs/2407.15838v1","updated":"2024-07-22T17:55:22Z","published":"2024-07-22T17:55:22Z","title":"MMInstruct: A High-Quality Multi-Modal Instruction Tuning Dataset with\n  Extensive Diversity","summary":"  Despite the effectiveness of vision-language supervised fine-tuning in\nenhancing the performance of Vision Large Language Models (VLLMs). However,\nexisting visual instruction tuning datasets include the following limitations:\n(1) Instruction annotation quality: despite existing VLLMs exhibiting strong\nperformance, instructions generated by those advanced VLLMs may still suffer\nfrom inaccuracies, such as hallucinations. (2) Instructions and image\ndiversity: the limited range of instruction types and the lack of diversity in\nimage data may impact the model's ability to generate diversified and closer to\nreal-world scenarios outputs. To address these challenges, we construct a\nhigh-quality, diverse visual instruction tuning dataset MMInstruct, which\nconsists of 973K instructions from 24 domains. There are four instruction\ntypes: Judgement, Multiple-Choice, Long Visual Question Answering and Short\nVisual Question Answering. To construct MMInstruct, we propose an instruction\ngeneration data engine that leverages GPT-4V, GPT-3.5, and manual correction.\nOur instruction generation engine enables semi-automatic, low-cost, and\nmulti-domain instruction generation at 1/6 the cost of manual construction.\nThrough extensive experiment validation and ablation experiments, we\ndemonstrate that MMInstruct could significantly improve the performance of\nVLLMs, e.g., the model fine-tuning on MMInstruct achieves new state-of-the-art\nperformance on 10 out of 12 benchmarks. The code and data shall be available at\nhttps://github.com/yuecao0119/MMInstruct.\n","authors":["Yangzhou Liu","Yue Cao","Zhangwei Gao","Weiyun Wang","Zhe Chen","Wenhai Wang","Hao Tian","Lewei Lu","Xizhou Zhu","Tong Lu","Yu Qiao","Jifeng Dai"],"pdf_url":"https://arxiv.org/pdf/2407.15838v1.pdf","comment":"18 pages, 8 figures, technical report"},{"id":"http://arxiv.org/abs/2407.15837v1","updated":"2024-07-22T17:54:41Z","published":"2024-07-22T17:54:41Z","title":"Towards Latent Masked Image Modeling for Self-Supervised Visual\n  Representation Learning","summary":"  Masked Image Modeling (MIM) has emerged as a promising method for deriving\nvisual representations from unlabeled image data by predicting missing pixels\nfrom masked portions of images. It excels in region-aware learning and provides\nstrong initializations for various tasks, but struggles to capture high-level\nsemantics without further supervised fine-tuning, likely due to the low-level\nnature of its pixel reconstruction objective. A promising yet unrealized\nframework is learning representations through masked reconstruction in latent\nspace, combining the locality of MIM with the high-level targets. However, this\napproach poses significant training challenges as the reconstruction targets\nare learned in conjunction with the model, potentially leading to trivial or\nsuboptimal solutions.Our study is among the first to thoroughly analyze and\naddress the challenges of such framework, which we refer to as Latent MIM.\nThrough a series of carefully designed experiments and extensive analysis, we\nidentify the source of these challenges, including representation collapsing\nfor joint online/target optimization, learning objectives, the high region\ncorrelation in latent space and decoding conditioning. By sequentially\naddressing these issues, we demonstrate that Latent MIM can indeed learn\nhigh-level representations while retaining the benefits of MIM models.\n","authors":["Yibing Wei","Abhinav Gupta","Pedro Morgado"],"pdf_url":"https://arxiv.org/pdf/2407.15837v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.12057v2","updated":"2024-07-22T17:52:47Z","published":"2024-05-20T14:26:07Z","title":"NPLMV-PS: Neural Point-Light Multi-View Photometric Stereo","summary":"  In this work we present a novel multi-view photometric stereo (MVPS) method.\nLike many works in 3D reconstruction we are leveraging neural shape\nrepresentations and learnt renderers. However, our work differs from the\nstate-of-the-art multi-view PS methods such as PS-NeRF or Supernormal in that\nwe explicitly leverage per-pixel intensity renderings rather than relying\nmainly on estimated normals.\n  We model point light attenuation and explicitly raytrace cast shadows in\norder to best approximate the incoming radiance for each point. The estimated\nincoming radiance is used as input to a fully neural material renderer that\nuses minimal prior assumptions and it is jointly optimised with the surface.\nEstimated normals and segmentation maps are also incorporated in order to\nmaximise the surface accuracy.\n  Our method is among the first (along with Supernormal) to outperform the\nclassical MVPS approach proposed by the DiLiGenT-MV benchmark and achieves\naverage 0.2mm Chamfer distance for objects imaged at approx 1.5m distance away\nwith approximate 400x400 resolution. Moreover, our method shows high robustness\nto the sparse MVPS setup (6 views, 6 lights) greatly outperforming the SOTA\ncompetitor (0.38mm vs 0.61mm), illustrating the importance of neural rendering\nin multi-view photometric stereo.\n","authors":["Fotios Logothetis","Ignas Budvytis","Roberto Cipolla"],"pdf_url":"https://arxiv.org/pdf/2405.12057v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18451v2","updated":"2024-07-22T17:52:19Z","published":"2024-06-26T16:00:35Z","title":"Detecting Brittle Decisions for Free: Leveraging Margin Consistency in\n  Deep Robust Classifiers","summary":"  Despite extensive research on adversarial training strategies to improve\nrobustness, the decisions of even the most robust deep learning models can\nstill be quite sensitive to imperceptible perturbations, creating serious risks\nwhen deploying them for high-stakes real-world applications. While detecting\nsuch cases may be critical, evaluating a model's vulnerability at a\nper-instance level using adversarial attacks is computationally too intensive\nand unsuitable for real-time deployment scenarios. The input space margin is\nthe exact score to detect non-robust samples and is intractable for deep neural\nnetworks. This paper introduces the concept of margin consistency -- a property\nthat links the input space margins and the logit margins in robust models --\nfor efficient detection of vulnerable samples. First, we establish that margin\nconsistency is a necessary and sufficient condition to use a model's logit\nmargin as a score for identifying non-robust samples. Next, through\ncomprehensive empirical analysis of various robustly trained models on CIFAR10\nand CIFAR100 datasets, we show that they indicate strong margin consistency\nwith a strong correlation between their input space margins and the logit\nmargins. Then, we show that we can effectively use the logit margin to\nconfidently detect brittle decisions with such models and accurately estimate\nrobust accuracy on an arbitrarily large test set by estimating the input\nmargins only on a small subset. Finally, we address cases where the model is\nnot sufficiently margin-consistent by learning a pseudo-margin from the feature\nrepresentation. Our findings highlight the potential of leveraging deep\nrepresentations to efficiently assess adversarial vulnerability in deployment\nscenarios.\n","authors":["Jonas Ngnawé","Sabyasachi Sahoo","Yann Pequignot","Frédéric Precioso","Christian Gagné"],"pdf_url":"https://arxiv.org/pdf/2406.18451v2.pdf","comment":"11 pages, 7 figures, 2 tables, 1 algorithm. Version Update: Figure 6"},{"id":"http://arxiv.org/abs/2407.15819v1","updated":"2024-07-22T17:33:49Z","published":"2024-07-22T17:33:49Z","title":"Accelerating Pre-training of Multimodal LLMs via Chain-of-Sight","summary":"  This paper introduces Chain-of-Sight, a vision-language bridge module that\naccelerates the pre-training of Multimodal Large Language Models (MLLMs). Our\napproach employs a sequence of visual resamplers that capture visual details at\nvarious spacial scales. This architecture not only leverages global and local\nvisual contexts effectively, but also facilitates the flexible extension of\nvisual tokens through a compound token scaling strategy, allowing up to a 16x\nincrease in the token count post pre-training. Consequently, Chain-of-Sight\nrequires significantly fewer visual tokens in the pre-training phase compared\nto the fine-tuning phase. This intentional reduction of visual tokens during\npre-training notably accelerates the pre-training process, cutting down the\nwall-clock training time by ~73%. Empirical results on a series of\nvision-language benchmarks reveal that the pre-train acceleration through\nChain-of-Sight is achieved without sacrificing performance, matching or\nsurpassing the standard pipeline of utilizing all visual tokens throughout the\nentire training process. Further scaling up the number of visual tokens for\npre-training leads to stronger performances, competitive to existing approaches\nin a series of benchmarks.\n","authors":["Ziyuan Huang","Kaixiang Ji","Biao Gong","Zhiwu Qing","Qinglong Zhang","Kecheng Zheng","Jian Wang","Jingdong Chen","Ming Yang"],"pdf_url":"https://arxiv.org/pdf/2407.15819v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15817v1","updated":"2024-07-22T17:32:06Z","published":"2024-07-22T17:32:06Z","title":"Enhancing Cell Instance Segmentation in Scanning Electron Microscopy\n  Images via a Deep Contour Closing Operator","summary":"  Accurately segmenting and individualizing cells in SEM images is a highly\npromising technique for elucidating tissue architecture in oncology. While\ncurrent AI-based methods are effective, errors persist, necessitating\ntime-consuming manual corrections, particularly in areas where the quality of\ncell contours in the image is poor and requires gap filling. This study\npresents a novel AI-driven approach for refining cell boundary delineation to\nimprove instance-based cell segmentation in SEM images, also reducing the\nnecessity for residual manual correction. A CNN COp-Net is introduced to\naddress gaps in cell contours, effectively filling in regions with deficient or\nabsent information. The network takes as input cell contour probability maps\nwith potentially inadequate or missing information and outputs corrected cell\ncontour delineations. The lack of training data was addressed by generating low\nintegrity probability maps using a tailored PDE. We showcase the efficacy of\nour approach in augmenting cell boundary precision using both private SEM\nimages from PDX hepatoblastoma tissues and publicly accessible images datasets.\nThe proposed cell contour closing operator exhibits a notable improvement in\ntested datasets, achieving respectively close to 50% (private data) and 10%\n(public data) increase in the accurately-delineated cell proportion compared to\nstate-of-the-art methods. Additionally, the need for manual corrections was\nsignificantly reduced, therefore facilitating the overall digitalization\nprocess. Our results demonstrate a notable enhancement in the accuracy of cell\ninstance segmentation, particularly in highly challenging regions where image\nquality compromises the integrity of cell boundaries, necessitating gap\nfilling. Therefore, our work should ultimately facilitate the study of tumour\ntissue bioarchitecture in onconanotomy field.\n","authors":["Florian Robert","Alexia Calovoulos","Laurent Facq","Fanny Decoeur","Etienne Gontier","Christophe F. Grosset","Baudouin Denis de Senneville"],"pdf_url":"https://arxiv.org/pdf/2407.15817v1.pdf","comment":"13 pages, 8 figures, 2 tables"},{"id":"http://arxiv.org/abs/2407.15816v1","updated":"2024-07-22T17:31:57Z","published":"2024-07-22T17:31:57Z","title":"Efficient and generalizable prediction of molecular alterations in\n  multiple cancer cohorts using H&E whole slide images","summary":"  Molecular testing of tumor samples for targetable biomarkers is restricted by\na lack of standardization, turnaround-time, cost, and tissue availability\nacross cancer types. Additionally, targetable alterations of low prevalence may\nnot be tested in routine workflows. Algorithms that predict DNA alterations\nfrom routinely generated hematoxylin and eosin (H&E)-stained images could\nprioritize samples for confirmatory molecular testing. Costs and the necessity\nof a large number of samples containing mutations limit approaches that train\nindividual algorithms for each alteration. In this work, models were trained\nfor simultaneous prediction of multiple DNA alterations from H&E images using a\nmulti-task approach. Compared to biomarker-specific models, this approach\nperformed better on average, with pronounced gains for rare mutations. The\nmodels reasonably generalized to independent temporal-holdout,\nexternally-stained, and multi-site TCGA test sets. Additionally, whole slide\nimage embeddings derived using multi-task models demonstrated strong\nperformance in downstream tasks that were not a part of training. Overall, this\nis a promising approach to develop clinically useful algorithms that provide\nmultiple actionable predictions from a single slide.\n","authors":["Kshitij Ingale","Sun Hae Hong","Qiyuan Hu","Renyu Zhang","Bo Osinski","Mina Khoshdeli","Josh Och","Kunal Nagpal","Martin C. Stumpe","Rohan P. Joshi"],"pdf_url":"https://arxiv.org/pdf/2407.15816v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.01836v3","updated":"2024-07-22T17:29:58Z","published":"2023-07-04T17:28:58Z","title":"On the Matrix Form of the Quaternion Fourier Transform and Quaternion\n  Convolution","summary":"  We study matrix forms of quaternionic versions of the Fourier Transform and\nConvolution operations. Quaternions offer a powerful representation unit,\nhowever they are related to difficulties in their use that stem foremost from\nnon-commutativity of quaternion multiplication, and due to that $\\mu^2 = -1$\npossesses infinite solutions in the quaternion domain. Handling of quaternionic\nmatrices is consequently complicated in several aspects (definition of\neigenstructure, determinant, etc.). Our research findings clarify the relation\nof the Quaternion Fourier Transform matrix to the standard (complex) Discrete\nFourier Transform matrix, and the extend on which well-known complex-domain\ntheorems extend to quaternions. We focus especially on the relation of\nQuaternion Fourier Transform matrices to Quaternion Circulant matrices\n(representing quaternionic convolution), and the eigenstructure of the latter.\nA proof-of-concept application that makes direct use of our theoretical results\nis presented, where we present a method to bound the Lipschitz constant of a\nQuaternionic Convolutional Neural Network. Code is publicly available at:\n\\url{https://github.com/sfikas/quaternion-fourier-convolution-matrix}.\n","authors":["Giorgos Sfikas","George Retsinas"],"pdf_url":"https://arxiv.org/pdf/2307.01836v3.pdf","comment":"25 pages, 4 figures"},{"id":"http://arxiv.org/abs/2407.15815v1","updated":"2024-07-22T17:29:02Z","published":"2024-07-22T17:29:02Z","title":"Learning to Manipulate Anywhere: A Visual Generalizable Framework For\n  Reinforcement Learning","summary":"  Can we endow visuomotor robots with generalization capabilities to operate in\ndiverse open-world scenarios? In this paper, we propose \\textbf{Maniwhere}, a\ngeneralizable framework tailored for visual reinforcement learning, enabling\nthe trained robot policies to generalize across a combination of multiple\nvisual disturbance types. Specifically, we introduce a multi-view\nrepresentation learning approach fused with Spatial Transformer Network (STN)\nmodule to capture shared semantic information and correspondences among\ndifferent viewpoints. In addition, we employ a curriculum-based randomization\nand augmentation approach to stabilize the RL training process and strengthen\nthe visual generalization ability. To exhibit the effectiveness of Maniwhere,\nwe meticulously design 8 tasks encompassing articulate objects, bi-manual, and\ndexterous hand manipulation tasks, demonstrating Maniwhere's strong visual\ngeneralization and sim2real transfer abilities across 3 hardware platforms. Our\nexperiments show that Maniwhere significantly outperforms existing\nstate-of-the-art methods. Videos are provided at\nhttps://gemcollector.github.io/maniwhere/.\n","authors":["Zhecheng Yuan","Tianming Wei","Shuiqi Cheng","Gu Zhang","Yuanpei Chen","Huazhe Xu"],"pdf_url":"https://arxiv.org/pdf/2407.15815v1.pdf","comment":"Webpage: https://gemcollector.github.io/maniwhere/"},{"id":"http://arxiv.org/abs/2407.15811v1","updated":"2024-07-22T17:23:28Z","published":"2024-07-22T17:23:28Z","title":"Stretching Each Dollar: Diffusion Training from Scratch on a\n  Micro-Budget","summary":"  As scaling laws in generative AI push performance, they also simultaneously\nconcentrate the development of these models among actors with large\ncomputational resources. With a focus on text-to-image (T2I) generative models,\nwe aim to address this bottleneck by demonstrating very low-cost training of\nlarge-scale T2I diffusion transformer models. As the computational cost of\ntransformers increases with the number of patches in each image, we propose to\nrandomly mask up to 75% of the image patches during training. We propose a\ndeferred masking strategy that preprocesses all patches using a patch-mixer\nbefore masking, thus significantly reducing the performance degradation with\nmasking, making it superior to model downscaling in reducing computational\ncost. We also incorporate the latest improvements in transformer architecture,\nsuch as the use of mixture-of-experts layers, to improve performance and\nfurther identify the critical benefit of using synthetic images in micro-budget\ntraining. Finally, using only 37M publicly available real and synthetic images,\nwe train a 1.16 billion parameter sparse transformer with only \\$1,890\neconomical cost and achieve a 12.7 FID in zero-shot generation on the COCO\ndataset. Notably, our model achieves competitive FID and high-quality\ngenerations while incurring 118$\\times$ lower cost than stable diffusion models\nand 14$\\times$ lower cost than the current state-of-the-art approach that costs\n\\$28,400. We aim to release our end-to-end training pipeline to further\ndemocratize the training of large-scale diffusion models on micro-budgets.\n","authors":["Vikash Sehwag","Xianghao Kong","Jingtao Li","Michael Spranger","Lingjuan Lyu"],"pdf_url":"https://arxiv.org/pdf/2407.15811v1.pdf","comment":"41 pages, 28 figures, 5 tables"},{"id":"http://arxiv.org/abs/2407.15810v1","updated":"2024-07-22T17:22:04Z","published":"2024-07-22T17:22:04Z","title":"Breaking the Global North Stereotype: A Global South-centric Benchmark\n  Dataset for Auditing and Mitigating Biases in Facial Recognition Systems","summary":"  Facial Recognition Systems (FRSs) are being developed and deployed globally\nat unprecedented rates. Most platforms are designed in a limited set of\ncountries but deployed in worldwide, without adequate checkpoints. This is\nespecially problematic for Global South countries which lack strong legislation\nto safeguard persons facing disparate performance of these systems. A\ncombination of unavailability of datasets, lack of understanding of FRS\nfunctionality and low-resource bias mitigation measures accentuate the problem.\nIn this work, we propose a new face dataset composed of 6,579 unique male and\nfemale sportspersons from eight countries around the world. More than 50% of\nthe dataset comprises individuals from the Global South countries and is\ndemographically diverse. To aid adversarial audits and robust model training,\neach image has four adversarial variants, totaling over 40,000 images. We also\nbenchmark five popular FRSs, both commercial and open-source, for the task of\ngender prediction (and country prediction for one of the open-source models as\nan example of red-teaming). Experiments on industrial FRSs reveal accuracies\nranging from 98.2%--38.1%, with a large disparity between males and females in\nthe Global South (max difference of 38.5%). Biases are also observed in all\nFRSs between females of the Global North and South (max difference of ~50%).\nGrad-CAM analysis identifies the nose, forehead and mouth as the regions of\ninterest on one of the open-source FRSs. Utilizing this insight, we design\nsimple, low-resource bias mitigation solutions using few-shot and novel\ncontrastive learning techniques significantly improving the accuracy with\ndisparity between males and females reducing from 50% to 1.5% in one of the\nsettings. In the red-teaming experiment with the open-source Deepface model,\ncontrastive learning proves more effective than simple fine-tuning.\n","authors":["Siddharth D Jaiswal","Animesh Ganai","Abhisek Dash","Saptarshi Ghosh","Animesh Mukherjee"],"pdf_url":"https://arxiv.org/pdf/2407.15810v1.pdf","comment":"This work has been accepted for publication at AAAI/ACM AIES 2024"},{"id":"http://arxiv.org/abs/2407.15806v1","updated":"2024-07-22T17:20:22Z","published":"2024-07-22T17:20:22Z","title":"FSboard: Over 3 million characters of ASL fingerspelling collected via\n  smartphones","summary":"  Progress in machine understanding of sign languages has been slow and\nhampered by limited data. In this paper, we present FSboard, an American Sign\nLanguage fingerspelling dataset situated in a mobile text entry use case,\ncollected from 147 paid and consenting Deaf signers using Pixel 4A selfie\ncameras in a variety of environments. Fingerspelling recognition is an\nincomplete solution that is only one small part of sign language translation,\nbut it could provide some immediate benefit to Deaf/Hard of Hearing signers as\nmore broadly capable technology develops. At >3 million characters in length\nand >250 hours in duration, FSboard is the largest fingerspelling recognition\ndataset to date by a factor of >10x. As a simple baseline, we finetune 30 Hz\nMediaPipe Holistic landmark inputs into ByT5-Small and achieve 11.1% Character\nError Rate (CER) on a test set with unique phrases and signers. This quality\ndegrades gracefully when decreasing frame rate and excluding face/body\nlandmarks: plausible optimizations to help models run on device in real time.\n","authors":["Manfred Georg","Garrett Tanzer","Saad Hassan","Maximus Shengelia","Esha Uboweja","Sam Sepah","Sean Forbes","Thad Starner"],"pdf_url":"https://arxiv.org/pdf/2407.15806v1.pdf","comment":"Access FSboard at https://www.kaggle.com/datasets/googleai/fsboard"},{"id":"http://arxiv.org/abs/2307.15220v3","updated":"2024-07-22T17:12:10Z","published":"2023-07-27T22:38:12Z","title":"Learning Multi-modal Representations by Watching Hundreds of Surgical\n  Video Lectures","summary":"  Recent advancements in surgical computer vision have been driven by\nvision-only models, which lack language semantics, relying on manually\nannotated videos to predict fixed object categories. This limits their\ngeneralizability to unseen surgical procedures and tasks. We propose leveraging\nsurgical video lectures from e-learning platforms to provide effective vision\nand language supervisory signals for multi-modal representation learning,\nbypassing manual annotations. We address surgery-specific linguistic challenges\nusing multiple automatic speech recognition systems for text transcriptions. We\nintroduce SurgVLP - Surgical Vision Language Pre-training - a novel method for\nmulti-modal representation learning. SurgVLP employs a new contrastive learning\nobjective, aligning video clip embeddings with corresponding multiple text\nembeddings in a joint latent space. We demonstrate the representational\ncapability of this space through several vision-and-language surgical tasks and\nvision-only tasks specific to surgery. Unlike current fully supervised\napproaches, SurgVLP adapts to different surgical procedures and tasks without\nspecific fine-tuning, achieving zero-shot adaptation to tasks such as surgical\ntool, phase, and triplet recognition without manual annotation. These results\nhighlight the transferability and versatility of the learned multi-modal\nrepresentations in surgical video analysis. The code is available at\nhttps://github.com/CAMMA-public/SurgVLP\n","authors":["Kun Yuan","Vinkle Srivastav","Tong Yu","Joel L. Lavanchy","Pietro Mascagni","Nassir Navab","Nicolas Padoy"],"pdf_url":"https://arxiv.org/pdf/2307.15220v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15799v1","updated":"2024-07-22T17:04:21Z","published":"2024-07-22T17:04:21Z","title":"Adaptive Extensions of Unbiased Risk Estimators for Unsupervised\n  Magnetic Resonance Image Denoising","summary":"  The application of Deep Neural Networks (DNNs) to image denoising has notably\nchallenged traditional denoising methods, particularly within complex noise\nscenarios prevalent in medical imaging. Despite the effectiveness of\ntraditional and some DNN-based methods, their reliance on high-quality,\nnoiseless ground truth images limits their practical utility. In response to\nthis, our work introduces and benchmarks innovative unsupervised learning\nstrategies, notably Stein's Unbiased Risk Estimator (SURE), its extension\n(eSURE), and our novel implementation, the Extended Poisson Unbiased Risk\nEstimator (ePURE), within medical imaging frameworks.\n  This paper presents a comprehensive evaluation of these methods on MRI data\nafflicted with Gaussian and Poisson noise types, a scenario typical in medical\nimaging but challenging for most denoising algorithms. Our main contribution\nlies in the effective adaptation and implementation of the SURE, eSURE, and\nparticularly the ePURE frameworks for medical images, showcasing their\nrobustness and efficacy in environments where traditional noiseless ground\ntruth cannot be obtained.\n","authors":["Reeshad Khan","Dr. John Gauch","Dr. Ukash Nakarmi"],"pdf_url":"https://arxiv.org/pdf/2407.15799v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15798v1","updated":"2024-07-22T17:00:02Z","published":"2024-07-22T17:00:02Z","title":"Robust Facial Reactions Generation: An Emotion-Aware Framework with\n  Modality Compensation","summary":"  The objective of the Multiple Appropriate Facial Reaction Generation (MAFRG)\ntask is to produce contextually appropriate and diverse listener facial\nbehavioural responses based on the multimodal behavioural data of the\nconversational partner (i.e., the speaker). Current methodologies typically\nassume continuous availability of speech and facial modality data, neglecting\nreal-world scenarios where these data may be intermittently unavailable, which\noften results in model failures. Furthermore, despite utilising advanced deep\nlearning models to extract information from the speaker's multimodal inputs,\nthese models fail to adequately leverage the speaker's emotional context, which\nis vital for eliciting appropriate facial reactions from human listeners. To\naddress these limitations, we propose an Emotion-aware Modality Compensatory\n(EMC) framework. This versatile solution can be seamlessly integrated into\nexisting models, thereby preserving their advantages while significantly\nenhancing performance and robustness in scenarios with missing modalities. Our\nframework ensures resilience when faced with missing modality data through the\nCompensatory Modality Alignment (CMA) module. It also generates more\nappropriate emotion-aware reactions via the Emotion-aware Attention (EA)\nmodule, which incorporates the speaker's emotional information throughout the\nentire encoding and decoding process. Experimental results demonstrate that our\nframework improves the appropriateness metric FRCorr by an average of 57.2\\%\ncompared to the original model structure. In scenarios where speech modality\ndata is missing, the performance of appropriate generation shows an\nimprovement, and when facial data is missing, it only exhibits minimal\ndegradation.\n","authors":["Guanyu Hu","Jie Wei","Siyang Song","Dimitrios Kollias","Xinyu Yang","Zhonglin Sun","Odysseus Kaloidas"],"pdf_url":"https://arxiv.org/pdf/2407.15798v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15797v1","updated":"2024-07-22T16:59:49Z","published":"2024-07-22T16:59:49Z","title":"MILAN: Milli-Annotations for Lidar Semantic Segmentation","summary":"  Annotating lidar point clouds for autonomous driving is a notoriously\nexpensive and time-consuming task. In this work, we show that the quality of\nrecent self-supervised lidar scan representations allows a great reduction of\nthe annotation cost. Our method has two main steps. First, we show that\nself-supervised representations allow a simple and direct selection of highly\ninformative lidar scans to annotate: training a network on these selected scans\nleads to much better results than a random selection of scans and, more\ninterestingly, to results on par with selections made by SOTA active learning\nmethods. In a second step, we leverage the same self-supervised representations\nto cluster points in our selected scans. Asking the annotator to classify each\ncluster, with a single click per cluster, then permits us to close the gap with\nfully-annotated training sets, while only requiring one thousandth of the point\nlabels.\n","authors":["Nermin Samet","Gilles Puy","Oriane Siméoni","Renaud Marlet"],"pdf_url":"https://arxiv.org/pdf/2407.15797v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15795v1","updated":"2024-07-22T16:52:37Z","published":"2024-07-22T16:52:37Z","title":"AdaCLIP: Adapting CLIP with Hybrid Learnable Prompts for Zero-Shot\n  Anomaly Detection","summary":"  Zero-shot anomaly detection (ZSAD) targets the identification of anomalies\nwithin images from arbitrary novel categories. This study introduces AdaCLIP\nfor the ZSAD task, leveraging a pre-trained vision-language model (VLM), CLIP.\nAdaCLIP incorporates learnable prompts into CLIP and optimizes them through\ntraining on auxiliary annotated anomaly detection data. Two types of learnable\nprompts are proposed: static and dynamic. Static prompts are shared across all\nimages, serving to preliminarily adapt CLIP for ZSAD. In contrast, dynamic\nprompts are generated for each test image, providing CLIP with dynamic\nadaptation capabilities. The combination of static and dynamic prompts is\nreferred to as hybrid prompts, and yields enhanced ZSAD performance. Extensive\nexperiments conducted across 14 real-world anomaly detection datasets from\nindustrial and medical domains indicate that AdaCLIP outperforms other ZSAD\nmethods and can generalize better to different categories and even domains.\nFinally, our analysis highlights the importance of diverse auxiliary data and\noptimized prompts for enhanced generalization capacity. Code is available at\nhttps://github.com/caoyunkang/AdaCLIP.\n","authors":["Yunkang Cao","Jiangning Zhang","Luca Frittoli","Yuqi Cheng","Weiming Shen","Giacomo Boracchi"],"pdf_url":"https://arxiv.org/pdf/2407.15795v1.pdf","comment":"Accepted by ECCV 2024"},{"id":"http://arxiv.org/abs/2407.15794v1","updated":"2024-07-22T16:52:32Z","published":"2024-07-22T16:52:32Z","title":"Disentangling spatio-temporal knowledge for weakly supervised object\n  detection and segmentation in surgical video","summary":"  Weakly supervised video object segmentation (WSVOS) enables the\nidentification of segmentation maps without requiring an extensive training\ndataset of object masks, relying instead on coarse video labels indicating\nobject presence. Current state-of-the-art methods either require multiple\nindependent stages of processing that employ motion cues or, in the case of\nend-to-end trainable networks, lack in segmentation accuracy, in part due to\nthe difficulty of learning segmentation maps from videos with transient object\npresence. This limits the application of WSVOS for semantic annotation of\nsurgical videos where multiple surgical tools frequently move in and out of the\nfield of view, a problem that is more difficult than typically encountered in\nWSVOS. This paper introduces Video Spatio-Temporal Disentanglement Networks\n(VDST-Net), a framework to disentangle spatiotemporal information using\nsemi-decoupled knowledge distillation to predict high-quality class activation\nmaps (CAMs). A teacher network designed to resolve temporal conflicts when\nspecifics about object location and timing in the video are not provided works\nwith a student network that integrates information over time by leveraging\ntemporal dependencies. We demonstrate the efficacy of our framework on a public\nreference dataset and on a more challenging surgical video dataset where\nobjects are, on average, present in less than 60\\% of annotated frames. Our\nmethod outperforms state-of-the-art techniques and generates superior\nsegmentation masks under video-level weak supervision.\n","authors":["Guiqiu Liao","Matjaz Jogan","Sai Koushik","Eric Eaton","Daniel A. Hashimoto"],"pdf_url":"https://arxiv.org/pdf/2407.15794v1.pdf","comment":"13 pages, 6 figures, 8 tables"},{"id":"http://arxiv.org/abs/2407.15793v1","updated":"2024-07-22T16:51:28Z","published":"2024-07-22T16:51:28Z","title":"CLIP with Generative Latent Replay: a Strong Baseline for Incremental\n  Learning","summary":"  With the emergence of Transformers and Vision-Language Models (VLMs) such as\nCLIP, large pre-trained models have become a common strategy to enhance\nperformance in Continual Learning scenarios. This led to the development of\nnumerous prompting strategies to effectively fine-tune transformer-based models\nwithout succumbing to catastrophic forgetting. However, these methods struggle\nto specialize the model on domains significantly deviating from the\npre-training and preserving its zero-shot capabilities. In this work, we\npropose Continual Generative training for Incremental prompt-Learning, a novel\napproach to mitigate forgetting while adapting a VLM, which exploits generative\nreplay to align prompts to tasks. We also introduce a new metric to evaluate\nzero-shot capabilities within CL benchmarks. Through extensive experiments on\ndifferent domains, we demonstrate the effectiveness of our framework in\nadapting to new tasks while improving zero-shot capabilities. Further analysis\nreveals that our approach can bridge the gap with joint prompt tuning. The\ncodebase is available at https://github.com/aimagelab/mammoth.\n","authors":["Emanuele Frascaroli","Aniello Panariello","Pietro Buzzega","Lorenzo Bonicelli","Angelo Porrello","Simone Calderara"],"pdf_url":"https://arxiv.org/pdf/2407.15793v1.pdf","comment":"15 pages, 1 figure. Accepted at the The 35th British Machine Vision\n  Conference 2024 (BMVC 2024), Glasgow, UK"},{"id":"http://arxiv.org/abs/2407.15791v1","updated":"2024-07-22T16:49:58Z","published":"2024-07-22T16:49:58Z","title":"RADA: Robust and Accurate Feature Learning with Domain Adaptation","summary":"  Recent advancements in keypoint detection and descriptor extraction have\nshown impressive performance in local feature learning tasks. However, existing\nmethods generally exhibit suboptimal performance under extreme conditions such\nas significant appearance changes and domain shifts. In this study, we\nintroduce a multi-level feature aggregation network that incorporates two\npivotal components to facilitate the learning of robust and accurate features\nwith domain adaptation. First, we employ domain adaptation supervision to align\nhigh-level feature distributions across different domains to achieve invariant\ndomain representations. Second, we propose a Transformer-based booster that\nenhances descriptor robustness by integrating visual and geometric information\nthrough wave position encoding concepts, effectively handling complex\nconditions. To ensure the accuracy and robustness of features, we adopt a\nhierarchical architecture to capture comprehensive information and apply\nmeticulous targeted supervision to keypoint detection, descriptor extraction,\nand their coupled processing. Extensive experiments demonstrate that our\nmethod, RADA, achieves excellent results in image matching, camera pose\nestimation, and visual localization tasks.\n","authors":["Jingtai He","Gehao Zhang","Tingting Liu","Songlin Du"],"pdf_url":"https://arxiv.org/pdf/2407.15791v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15787v1","updated":"2024-07-22T16:47:29Z","published":"2024-07-22T16:47:29Z","title":"Unsupervised Mastoidectomy for Cochlear CT Mesh Reconstruction Using\n  Highly Noisy Data","summary":"  Cochlear Implant (CI) procedures involve inserting an array of electrodes\ninto the cochlea located inside the inner ear. Mastoidectomy is a surgical\nprocedure that uses a high-speed drill to remove part of the mastoid region of\nthe temporal bone, providing safe access to the cochlea through the middle and\ninner ear. We aim to develop an intraoperative navigation system that registers\nplans created using 3D preoperative Computerized Tomography (CT) volumes with\nthe 2D surgical microscope view. Herein, we propose a method to synthesize the\nmastoidectomy volume using only the preoperative CT scan, where the mastoid is\nintact. We introduce an unsupervised learning framework designed to synthesize\nmastoidectomy. For model training purposes, this method uses postoperative CT\nscans to avoid manual data cleaning or labeling, even when the region removed\nduring mastoidectomy is visible but affected by metal artifacts, low\nsignal-to-noise ratio, or electrode wiring. Our approach estimates\nmastoidectomy regions with a mean dice score of 70.0%. This approach represents\na major step forward for CI intraoperative navigation by predicting realistic\nmastoidectomy-removed regions in preoperative planning that can be used to\nregister the pre-surgery plan to intraoperative microscopy.\n","authors":["Yike Zhang","Dingjie Su","Eduardo Davalos","Jack H. Noble"],"pdf_url":"https://arxiv.org/pdf/2407.15787v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.09530v2","updated":"2024-07-22T16:42:25Z","published":"2024-02-14T19:10:40Z","title":"Reducing Texture Bias of Deep Neural Networks via Edge Enhancing\n  Diffusion","summary":"  Convolutional neural networks (CNNs) for image processing tend to focus on\nlocalized texture patterns, commonly referred to as texture bias. While most of\nthe previous works in the literature focus on the task of image classification,\nwe go beyond this and study the texture bias of CNNs in semantic segmentation.\nIn this work, we propose to train CNNs on pre-processed images with less\ntexture to reduce the texture bias. Therein, the challenge is to suppress image\ntexture while preserving shape information. To this end, we utilize edge\nenhancing diffusion (EED), an anisotropic image diffusion method initially\nintroduced for image compression, to create texture reduced duplicates of\nexisting datasets. Extensive numerical studies are performed with both CNNs and\nvision transformer models trained on original data and EED-processed data from\nthe Cityscapes dataset and the CARLA driving simulator. We observe strong\ntexture-dependence of CNNs and moderate texture-dependence of transformers.\nTraining CNNs on EED-processed images enables the models to become completely\nignorant with respect to texture, demonstrating resilience with respect to\ntexture re-introduction to any degree. Additionally we analyze the performance\nreduction in depth on a level of connected components in the semantic\nsegmentation and study the influence of EED pre-processing on domain\ngeneralization as well as adversarial robustness.\n","authors":["Edgar Heinert","Matthias Rottmann","Kira Maag","Karsten Kahl"],"pdf_url":"https://arxiv.org/pdf/2402.09530v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.17814v4","updated":"2024-07-22T16:38:07Z","published":"2024-05-28T04:18:00Z","title":"FAIntbench: A Holistic and Precise Benchmark for Bias Evaluation in\n  Text-to-Image Models","summary":"  The rapid development and reduced barriers to entry for Text-to-Image (T2I)\nmodels have raised concerns about the biases in their outputs, but existing\nresearch lacks a holistic definition and evaluation framework of biases,\nlimiting the enhancement of debiasing techniques. To address this issue, we\nintroduce FAIntbench, a holistic and precise benchmark for biases in T2I\nmodels. In contrast to existing benchmarks that evaluate bias in limited\naspects, FAIntbench evaluate biases from four dimensions: manifestation of\nbias, visibility of bias, acquired attributes, and protected attributes. We\napplied FAIntbench to evaluate seven recent large-scale T2I models and\nconducted human evaluation, whose results demonstrated the effectiveness of\nFAIntbench in identifying various biases. Our study also revealed new research\nquestions about biases, including the side-effect of distillation. The findings\npresented here are preliminary, highlighting the potential of FAIntbench to\nadvance future research aimed at mitigating the biases in T2I models. Our\nbenchmark is publicly available to ensure the reproducibility.\n","authors":["Hanjun Luo","Ziye Deng","Ruizhe Chen","Zuozhu Liu"],"pdf_url":"https://arxiv.org/pdf/2405.17814v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15773v1","updated":"2024-07-22T16:25:41Z","published":"2024-07-22T16:25:41Z","title":"STAMP: Outlier-Aware Test-Time Adaptation with Stable Memory Replay","summary":"  Test-time adaptation (TTA) aims to address the distribution shift between the\ntraining and test data with only unlabeled data at test time. Existing TTA\nmethods often focus on improving recognition performance specifically for test\ndata associated with classes in the training set. However, during the\nopen-world inference process, there are inevitably test data instances from\nunknown classes, commonly referred to as outliers. This paper pays attention to\nthe problem that conducts both sample recognition and outlier rejection during\ninference while outliers exist. To address this problem, we propose a new\napproach called STAble Memory rePlay (STAMP), which performs optimization over\na stable memory bank instead of the risky mini-batch. In particular, the memory\nbank is dynamically updated by selecting low-entropy and label-consistent\nsamples in a class-balanced manner. In addition, we develop a self-weighted\nentropy minimization strategy that assigns higher weight to low-entropy\nsamples. Extensive results demonstrate that STAMP outperforms existing TTA\nmethods in terms of both recognition and outlier detection performance. The\ncode is released at https://github.com/yuyongcan/STAMP.\n","authors":["Yongcan Yu","Lijun Sheng","Ran He","Jian Liang"],"pdf_url":"https://arxiv.org/pdf/2407.15773v1.pdf","comment":"Accepted by ECCV 2024"},{"id":"http://arxiv.org/abs/2407.15763v1","updated":"2024-07-22T16:16:38Z","published":"2024-07-22T16:16:38Z","title":"Towards Open-World Object-based Anomaly Detection via Self-Supervised\n  Outlier Synthesis","summary":"  Object detection is a pivotal task in computer vision that has received\nsignificant attention in previous years. Nonetheless, the capability of a\ndetector to localise objects out of the training distribution remains\nunexplored. Whilst recent approaches in object-level out-of-distribution (OoD)\ndetection heavily rely on class labels, such approaches contradict truly\nopen-world scenarios where the class distribution is often unknown. In this\ncontext, anomaly detection focuses on detecting unseen instances rather than\nclassifying detections as OoD. This work aims to bridge this gap by leveraging\nan open-world object detector and an OoD detector via virtual outlier\nsynthesis. This is achieved by using the detector backbone features to first\nlearn object pseudo-classes via self-supervision. These pseudo-classes serve as\nthe basis for class-conditional virtual outlier sampling of anomalous features\nthat are classified by an OoD head. Our approach empowers our overall object\ndetector architecture to learn anomaly-aware feature representations without\nrelying on class labels, hence enabling truly open-world object anomaly\ndetection. Empirical validation of our approach demonstrates its effectiveness\nacross diverse datasets encompassing various imaging modalities (visible,\ninfrared, and X-ray). Moreover, our method establishes state-of-the-art\nperformance on object-level anomaly detection, achieving an average recall\nscore improvement of over 5.4% for natural images and 23.5% for a security\nX-ray dataset compared to the current approaches. In addition, our method\ndetects anomalies in datasets where current approaches fail. Code available at\nhttps://github.com/KostadinovShalon/oln-ssos.\n","authors":["Brian K. S. Isaac-Medina","Yona Falinie A. Gaus","Neelanjan Bhowmik","Toby P. Breckon"],"pdf_url":"https://arxiv.org/pdf/2407.15763v1.pdf","comment":"35 pages, 21 figures, includes supplementary material, accepted at\n  ECCV 2024"},{"id":"http://arxiv.org/abs/2305.17349v3","updated":"2024-07-22T16:12:50Z","published":"2023-05-27T03:05:07Z","title":"Condition-Invariant Semantic Segmentation","summary":"  Adaptation of semantic segmentation networks to different visual conditions\nis vital for robust perception in autonomous cars and robots. However, previous\nwork has shown that most feature-level adaptation methods, which employ\nadversarial training and are validated on synthetic-to-real adaptation, provide\nmarginal gains in condition-level adaptation, being outperformed by simple\npixel-level adaptation via stylization. Motivated by these findings, we propose\nto leverage stylization in performing feature-level adaptation by aligning the\ninternal network features extracted by the encoder of the network from the\noriginal and the stylized view of each input image with a novel feature\ninvariance loss. In this way, we encourage the encoder to extract features that\nare already invariant to the style of the input, allowing the decoder to focus\non parsing these features and not on further abstracting from the specific\nstyle of the input. We implement our method, named Condition-Invariant Semantic\nSegmentation (CISS), on the current state-of-the-art domain adaptation\narchitecture and achieve outstanding results on condition-level adaptation. In\nparticular, CISS sets the new state of the art in the popular\ndaytime-to-nighttime Cityscapes$\\to$Dark Zurich benchmark. Furthermore, our\nmethod achieves the second-best performance on the normal-to-adverse\nCityscapes$\\to$ACDC benchmark. CISS is shown to generalize well to domains\nunseen during training, such as BDD100K-night and ACDC-night. Code is publicly\navailable at https://github.com/SysCV/CISS .\n","authors":["Christos Sakaridis","David Bruggemann","Fisher Yu","Luc Van Gool"],"pdf_url":"https://arxiv.org/pdf/2305.17349v3.pdf","comment":"Submitted for review to IEEE T-PAMI"},{"id":"http://arxiv.org/abs/2407.15754v1","updated":"2024-07-22T16:00:55Z","published":"2024-07-22T16:00:55Z","title":"LongVideoBench: A Benchmark for Long-context Interleaved Video-Language\n  Understanding","summary":"  Large multimodal models (LMMs) are processing increasingly longer and richer\ninputs. Albeit the progress, few public benchmark is available to measure such\ndevelopment. To mitigate this gap, we introduce LongVideoBench, a\nquestion-answering benchmark that features video-language interleaved inputs up\nto an hour long. Our benchmark includes 3,763 varying-length web-collected\nvideos with their subtitles across diverse themes, designed to comprehensively\nevaluate LMMs on long-term multimodal understanding. To achieve this, we\ninterpret the primary challenge as to accurately retrieve and reason over\ndetailed multimodal information from long inputs. As such, we formulate a novel\nvideo question-answering task termed referring reasoning. Specifically, as part\nof the question, it contains a referring query that references related video\ncontexts, called referred context. The model is then required to reason over\nrelevant video details from the referred context. Following the paradigm of\nreferring reasoning, we curate 6,678 human-annotated multiple-choice questions\nin 17 fine-grained categories, establishing one of the most comprehensive\nbenchmarks for long-form video understanding. Evaluations suggest that the\nLongVideoBench presents significant challenges even for the most advanced\nproprietary models (e.g. GPT-4o, Gemini-1.5-Pro, GPT-4-Turbo), while their\nopen-source counterparts show an even larger performance gap. In addition, our\nresults indicate that model performance on the benchmark improves only when\nthey are capable of processing more frames, positioning LongVideoBench as a\nvaluable benchmark for evaluating future-generation long-context LMMs.\n","authors":["Haoning Wu","Dongxu Li","Bei Chen","Junnan Li"],"pdf_url":"https://arxiv.org/pdf/2407.15754v1.pdf","comment":"29 pages"},{"id":"http://arxiv.org/abs/2407.15739v1","updated":"2024-07-22T15:41:37Z","published":"2024-07-22T15:41:37Z","title":"Diffusion for Out-of-Distribution Detection on Road Scenes and Beyond","summary":"  In recent years, research on out-of-distribution (OoD) detection for semantic\nsegmentation has mainly focused on road scenes -- a domain with a constrained\namount of semantic diversity. In this work, we challenge this constraint and\nextend the domain of this task to general natural images. To this end, we\nintroduce: 1. the ADE-OoD benchmark, which is based on the ADE20k dataset and\nincludes images from diverse domains with a high semantic diversity, and 2. a\nnovel approach that uses Diffusion score matching for OoD detection (DOoD) and\nis robust to the increased semantic diversity. ADE-OoD features indoor and\noutdoor images, defines 150 semantic categories as in-distribution, and\ncontains a variety of OoD objects. For DOoD, we train a diffusion model with an\nMLP architecture on semantic in-distribution embeddings and build on the score\nmatching interpretation to compute pixel-wise OoD scores at inference time. On\ncommon road scene OoD benchmarks, DOoD performs on par or better than the state\nof the art, without using outliers for training or making assumptions about the\ndata domain. On ADE-OoD, DOoD outperforms previous approaches, but leaves much\nroom for future improvements.\n","authors":["Silvio Galesso","Philipp Schröppel","Hssan Driss","Thomas Brox"],"pdf_url":"https://arxiv.org/pdf/2407.15739v1.pdf","comment":"ECCV 2024 - Benchmark page: https://ade-ood.github.io/"},{"id":"http://arxiv.org/abs/2407.15731v1","updated":"2024-07-22T15:35:09Z","published":"2024-07-22T15:35:09Z","title":"Zero-Shot Embeddings Inform Learning and Forgetting with Vision-Language\n  Encoders","summary":"  Despite the proliferation of large vision-language foundation models,\nestimation of the learning and forgetting outcomes following fine-tuning of\nthese models remains largely unexplored. Inspired by work highlighting the\nsignificance of the modality gap in contrastive dual-encoders, we propose the\nInter-Intra Modal Measure (IIMM). Combining terms quantifying the similarity\nbetween image embeddings and the similarity between incorrect image and label\nembedding pairs, the IIMM functions as a strong predictor of performance\nchanges with fine-tuning. Our extensive empirical analysis across four\nstate-of-the-art vision-language models (CLIP, SigLIP, CoCa, EVA-02-CLIP) and\nfive fine-tuning techniques (full fine-tuning, BitFit, attention-weight tuning,\nLoRA, CLIP-Adapter) demonstrates a strong, statistically significant linear\nrelationship: fine-tuning on tasks with higher IIMM scores produces greater\nin-domain performance gains but also induces more severe out-of-domain\nperformance degradation, with some parameter-efficient fine-tuning (PEFT)\nmethods showing extreme forgetting. We compare our measure against transfer\nscores from state-of-the-art model selection methods and show that the IIMM is\nsignificantly more predictive of accuracy gains. With only a single forward\npass of the target data, practitioners can leverage this key insight to\nheuristically evaluate the degree to which a model can be expected to improve\nfollowing fine-tuning. Given additional knowledge about the model's performance\non a few diverse tasks, this heuristic further evolves into a strong predictor\nof expected performance changes when training for new tasks.\n","authors":["Laura Niss","Kevin Vogt-Lowell","Theodoros Tsiligkaridis"],"pdf_url":"https://arxiv.org/pdf/2407.15731v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.07370v2","updated":"2024-07-22T15:32:36Z","published":"2024-02-12T02:01:53Z","title":"SelfSwapper: Self-Supervised Face Swapping via Shape Agnostic Masked\n  AutoEncoder","summary":"  Face swapping has gained significant attention for its varied applications.\nMost previous face swapping approaches have relied on the seesaw game training\nscheme, also known as the target-oriented approach. However, this often leads\nto instability in model training and results in undesired samples with blended\nidentities due to the target identity leakage problem. Source-oriented methods\nachieve more stable training with self-reconstruction objective but often fail\nto accurately reflect target image's skin color and illumination. This paper\nintroduces the Shape Agnostic Masked AutoEncoder (SAMAE) training scheme, a\nnovel self-supervised approach that combines the strengths of both\ntarget-oriented and source-oriented approaches. Our training scheme addresses\nthe limitations of traditional training methods by circumventing the\nconventional seesaw game and introducing clear ground truth through its\nself-reconstruction training regime. Our model effectively mitigates identity\nleakage and reflects target albedo and illumination through learned\ndisentangled identity and non-identity features. Additionally, we closely\ntackle the shape misalignment and volume discrepancy problems with new\ntechniques, including perforation confusion and random mesh scaling. SAMAE\nestablishes a new state-of-the-art, surpassing other baseline methods,\npreserving both identity and non-identity attributes without sacrificing on\neither aspect.\n","authors":["Jaeseong Lee","Junha Hyung","Sohyun Jeong","Jaegul Choo"],"pdf_url":"https://arxiv.org/pdf/2402.07370v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15728v1","updated":"2024-07-22T15:31:18Z","published":"2024-07-22T15:31:18Z","title":"SAM2CLIP2SAM: Vision Language Model for Segmentation of 3D CT Scans for\n  Covid-19 Detection","summary":"  This paper presents a new approach for effective segmentation of images that\ncan be integrated into any model and methodology; the paradigm that we choose\nis classification of medical images (3-D chest CT scans) for Covid-19\ndetection. Our approach includes a combination of vision-language models that\nsegment the CT scans, which are then fed to a deep neural architecture, named\nRACNet, for Covid-19 detection. In particular, a novel framework, named\nSAM2CLIP2SAM, is introduced for segmentation that leverages the strengths of\nboth Segment Anything Model (SAM) and Contrastive Language-Image Pre-Training\n(CLIP) to accurately segment the right and left lungs in CT scans, subsequently\nfeeding these segmented outputs into RACNet for classification of COVID-19 and\nnon-COVID-19 cases. At first, SAM produces multiple part-based segmentation\nmasks for each slice in the CT scan; then CLIP selects only the masks that are\nassociated with the regions of interest (ROIs), i.e., the right and left lungs;\nfinally SAM is given these ROIs as prompts and generates the final segmentation\nmask for the lungs. Experiments are presented across two Covid-19 annotated\ndatabases which illustrate the improved performance obtained when our method\nhas been used for segmentation of the CT scans.\n","authors":["Dimitrios Kollias","Anastasios Arsenos","James Wingate","Stefanos Kollias"],"pdf_url":"https://arxiv.org/pdf/2407.15728v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15724v1","updated":"2024-07-22T15:28:51Z","published":"2024-07-22T15:28:51Z","title":"Beyond Size and Class Balance: Alpha as a New Dataset Quality Metric for\n  Deep Learning","summary":"  In deep learning, achieving high performance on image classification tasks\nrequires diverse training sets. However, dataset diversity is incompletely\nunderstood. The current best practice is to try to maximize dataset size and\nclass balance. Yet large, class-balanced datasets are not guaranteed to be\ndiverse: images can still be arbitrarily similar. We hypothesized that, for a\ngiven model architecture, better model performance can be achieved by\nmaximizing dataset diversity more directly. This could open a path for\nperformance improvement without additional computational resources or\narchitectural advances. To test this hypothesis, we introduce a comprehensive\nframework of diversity measures, developed in ecology, that generalizes\nfamiliar quantities like Shannon entropy by accounting for similarities and\ndifferences among images. (Dataset size and class balance emerge from this\nframework as special cases.) By analyzing thousands of subsets from seven\nmedical datasets representing ultrasound, X-ray, CT, and pathology images, we\nfound that the best correlates of performance were not size or class balance\nbut $A$ -- ``big alpha'' -- a set of generalized entropy measures interpreted\nas the effective number of image-class pairs in the dataset, after accounting\nfor similarities among images. One of these, $A_0$, explained 67\\% of the\nvariance in balanced accuracy across all subsets, vs. 54\\% for class balance\nand just 39\\% for size. The best pair was size and $A_1$ (79\\%), which\noutperformed size and class balance (74\\%). $A$ performed best for subsets from\nindividual datasets as well as across datasets, supporting the generality of\nthese results. We propose maximizing $A$ as a potential new way to improve the\nperformance of deep learning in medical imaging.\n","authors":["Josiah Couch","Ramy Arnaout","Rima Arnaout"],"pdf_url":"https://arxiv.org/pdf/2407.15724v1.pdf","comment":"11 pages, 5 figures, 3 tables"},{"id":"http://arxiv.org/abs/2407.15719v1","updated":"2024-07-22T15:22:33Z","published":"2024-07-22T15:22:33Z","title":"GFE-Mamba: Mamba-based AD Multi-modal Progression Assessment via\n  Generative Feature Extraction from MCI","summary":"  Alzheimer's Disease (AD) is an irreversible neurodegenerative disorder that\noften progresses from Mild Cognitive Impairment (MCI), leading to memory loss\nand significantly impacting patients' lives. Clinical trials indicate that\nearly targeted interventions for MCI patients can potentially slow or halt the\ndevelopment and progression of AD. Previous research has shown that accurate\nmedical classification requires the inclusion of extensive multimodal data,\nsuch as assessment scales and various neuroimaging techniques like Magnetic\nResonance Imaging (MRI) and Positron Emission Tomography (PET). However,\nconsistently tracking the diagnosis of the same individual over time and\nsimultaneously collecting multimodal data poses significant challenges. To\naddress this issue, we introduce GFE-Mamba, a classifier based on Generative\nFeature Extraction (GFE). This classifier effectively integrates data from\nassessment scales, MRI, and PET, enabling deeper multimodal fusion. It\nefficiently extracts both long and short sequence information and incorporates\nadditional information beyond the pixel space. This approach not only improves\nclassification accuracy but also enhances the interpretability and stability of\nthe model. We constructed datasets of over 3000 samples based on the\nAlzheimer's Disease Neuroimaging Initiative (ADNI) for a two-step training\nprocess. Our experimental results demonstrate that the GFE-Mamba model is\neffective in predicting the conversion from MCI to AD and outperforms several\nstate-of-the-art methods. Our source code and ADNI dataset processing code are\navailable at https://github.com/Tinysqua/GFE-Mamba.\n","authors":["Zhaojie Fang","Shenghao Zhu","Yifei Chen","Binfeng Zou","Fan Jia","Linwei Qiu","Chang Liu","Yiyu Huang","Xiang Feng","Feiwei Qin","Changmiao Wang","Yeru Wang","Jin Fan","Changbiao Chu","Wan-Zhen Wu","Hu Zhao"],"pdf_url":"https://arxiv.org/pdf/2407.15719v1.pdf","comment":"35 pages, 4 figures"},{"id":"http://arxiv.org/abs/2407.15717v1","updated":"2024-07-22T15:22:08Z","published":"2024-07-22T15:22:08Z","title":"Harmonizing Flows: Leveraging normalizing flows for unsupervised and\n  source-free MRI harmonization","summary":"  Lack of standardization and various intrinsic parameters for magnetic\nresonance (MR) image acquisition results in heterogeneous images across\ndifferent sites and devices, which adversely affects the generalization of deep\nneural networks. To alleviate this issue, this work proposes a novel\nunsupervised harmonization framework that leverages normalizing flows to align\nMR images, thereby emulating the distribution of a source domain. The proposed\nstrategy comprises three key steps. Initially, a normalizing flow network is\ntrained to capture the distribution characteristics of the source domain. Then,\nwe train a shallow harmonizer network to reconstruct images from the source\ndomain via their augmented counterparts. Finally, during inference, the\nharmonizer network is updated to ensure that the output images conform to the\nlearned source domain distribution, as modeled by the normalizing flow network.\nOur approach, which is unsupervised, source-free, and task-agnostic is assessed\nin the context of both adults and neonatal cross-domain brain MRI segmentation,\nas well as neonatal brain age estimation, demonstrating its generalizability\nacross tasks and population demographics. The results underscore its superior\nperformance compared to existing methodologies. The code is available at\nhttps://github.com/farzad-bz/Harmonizing-Flows\n","authors":["Farzad Beizaee","Gregory A. Lodygensky","Chris L. Adamson","Deanne K. Thompso","Jeanie L. Y. Cheon","Alicia J. Spittl. Peter J. Anderso","Christian Desrosier","Jose Dolz"],"pdf_url":"https://arxiv.org/pdf/2407.15717v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15714v1","updated":"2024-07-22T15:21:35Z","published":"2024-07-22T15:21:35Z","title":"Mamba meets crack segmentation","summary":"  Cracks pose safety risks to infrastructure and cannot be overlooked. The\nprevailing structures in existing crack segmentation networks predominantly\nconsist of CNNs or Transformers. However, CNNs exhibit a deficiency in global\nmodeling capability, hindering the representation to entire crack features.\nTransformers can capture long-range dependencies but suffer from high and\nquadratic complexity. Recently, Mamba has garnered extensive attention due to\nits linear spatial and computational complexity and its powerful global\nperception. This study explores the representation capabilities of Mamba to\ncrack features. Specifically, this paper uncovers the connection between Mamba\nand the attention mechanism, providing a profound insight, an attention\nperspective, into interpreting Mamba and devising a novel Mamba module\nfollowing the principles of attention blocks, namely CrackMamba. We compare\nCrackMamba with the most prominent visual Mamba modules, Vim and Vmamba, on two\ndatasets comprising asphalt pavement and concrete pavement cracks, and steel\ncracks, respectively. The quantitative results show that CrackMamba stands out\nas the sole Mamba block consistently enhancing the baseline model's performance\nacross all evaluation measures, while reducing its parameters and computational\ncosts. Moreover, this paper substantiates that Mamba can achieve global\nreceptive fields through both theoretical analysis and visual interpretability.\nThe discoveries of this study offer a dual contribution. First, as a\nplug-and-play and simple yet effective Mamba module, CrackMamba exhibits\nimmense potential for integration into various crack segmentation models.\nSecond, the proposed innovative Mamba design concept, integrating Mamba with\nthe attention mechanism, holds significant reference value for all Mamba-based\ncomputer vision models, not limited to crack segmentation networks, as\ninvestigated in this study.\n","authors":["Zhili He","Yu-Hsing Wang"],"pdf_url":"https://arxiv.org/pdf/2407.15714v1.pdf","comment":"32 pages, 8 figures. Preprint submitted to Elsevier"},{"id":"http://arxiv.org/abs/2407.15708v1","updated":"2024-07-22T15:17:39Z","published":"2024-07-22T15:17:39Z","title":"SwinSF: Image Reconstruction from Spatial-Temporal Spike Streams","summary":"  The spike camera, with its high temporal resolution, low latency, and high\ndynamic range, addresses high-speed imaging challenges like motion blur. It\ncaptures photons at each pixel independently, creating binary spike streams\nrich in temporal information but challenging for image reconstruction. Current\nalgorithms, both traditional and deep learning-based, still need to be improved\nin the utilization of the rich temporal detail and the restoration of the\ndetails of the reconstructed image. To overcome this, we introduce Swin\nSpikeformer (SwinSF), a novel model for dynamic scene reconstruction from spike\nstreams. SwinSF is composed of Spike Feature Extraction, Spatial-Temporal\nFeature Extraction, and Final Reconstruction Module. It combines shifted window\nself-attention and proposed temporal spike attention, ensuring a comprehensive\nfeature extraction that encapsulates both spatial and temporal dynamics,\nleading to a more robust and accurate reconstruction of spike streams.\nFurthermore, we build a new synthesized dataset for spike image reconstruction\nwhich matches the resolution of the latest spike camera, ensuring its relevance\nand applicability to the latest developments in spike camera imaging.\nExperimental results demonstrate that the proposed network SwinSF sets a new\nbenchmark, achieving state-of-the-art performance across a series of datasets,\nincluding both real-world and synthesized data across various resolutions. Our\ncodes and proposed dataset will be available soon.\n","authors":["Liangyan Jiang","Chuang Zhu","Yanxu Chen"],"pdf_url":"https://arxiv.org/pdf/2407.15708v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15707v1","updated":"2024-07-22T15:17:09Z","published":"2024-07-22T15:17:09Z","title":"Predicting the Best of N Visual Trackers","summary":"  We observe that the performance of SOTA visual trackers surprisingly strongly\nvaries across different video attributes and datasets. No single tracker\nremains the best performer across all tracking attributes and datasets. To\nbridge this gap, for a given video sequence, we predict the \"Best of the N\nTrackers\", called the BofN meta-tracker. At its core, a Tracking Performance\nPrediction Network (TP2N) selects a predicted best performing visual tracker\nfor the given video sequence using only a few initial frames. We also introduce\na frame-level BofN meta-tracker which keeps predicting best performer after\nregular temporal intervals. The TP2N is based on self-supervised learning\narchitectures MocoV2, SwAv, BT, and DINO; experiments show that the DINO with\nViT-S as a backbone performs the best. The video-level BofN meta-tracker\noutperforms, by a large margin, existing SOTA trackers on nine standard\nbenchmarks - LaSOT, TrackingNet, GOT-10K, VOT2019, VOT2021, VOT2022, UAV123,\nOTB100, and WebUAV-3M. Further improvement is achieved by the frame-level BofN\nmeta-tracker effectively handling variations in the tracking scenarios within\nlong sequences. For instance, on GOT-10k, BofN meta-tracker average overlap is\n88.7% and 91.1% with video and frame-level settings respectively. The best\nperforming tracker, RTS, achieves 85.20% AO. On VOT2022, BofN expected average\noverlap is 67.88% and 70.98% with video and frame level settings, compared to\nthe best performing ARTrack, 64.12%. This work also presents an extensive\nevaluation of competitive tracking methods on all commonly used benchmarks,\nfollowing their protocols. The code, the trained models, and the results will\nsoon be made publicly available on\nhttps://github.com/BasitAlawode/Best_of_N_Trackers.\n","authors":["Basit Alawode","Sajid Javed","Arif Mahmood","Jiri Matas"],"pdf_url":"https://arxiv.org/pdf/2407.15707v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15706v1","updated":"2024-07-22T15:16:47Z","published":"2024-07-22T15:16:47Z","title":"Multi-Modality Co-Learning for Efficient Skeleton-based Action\n  Recognition","summary":"  Skeleton-based action recognition has garnered significant attention due to\nthe utilization of concise and resilient skeletons. Nevertheless, the absence\nof detailed body information in skeletons restricts performance, while other\nmultimodal methods require substantial inference resources and are inefficient\nwhen using multimodal data during both training and inference stages. To\naddress this and fully harness the complementary multimodal features, we\npropose a novel multi-modality co-learning (MMCL) framework by leveraging the\nmultimodal large language models (LLMs) as auxiliary networks for efficient\nskeleton-based action recognition, which engages in multi-modality co-learning\nduring the training stage and keeps efficiency by employing only concise\nskeletons in inference. Our MMCL framework primarily consists of two modules.\nFirst, the Feature Alignment Module (FAM) extracts rich RGB features from video\nframes and aligns them with global skeleton features via contrastive learning.\nSecond, the Feature Refinement Module (FRM) uses RGB images with temporal\ninformation and text instruction to generate instructive features based on the\npowerful generalization of multimodal LLMs. These instructive text features\nwill further refine the classification scores and the refined scores will\nenhance the model's robustness and generalization in a manner similar to soft\nlabels. Extensive experiments on NTU RGB+D, NTU RGB+D 120 and Northwestern-UCLA\nbenchmarks consistently verify the effectiveness of our MMCL, which outperforms\nthe existing skeleton-based action recognition methods. Meanwhile, experiments\non UTD-MHAD and SYSU-Action datasets demonstrate the commendable generalization\nof our MMCL in zero-shot and domain-adaptive action recognition. Our code is\npublicly available at: https://github.com/liujf69/MMCL-Action.\n","authors":["Jinfu Liu","Chen Chen","Mengyuan Liu"],"pdf_url":"https://arxiv.org/pdf/2407.15706v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.14671v3","updated":"2024-07-22T15:16:05Z","published":"2023-11-24T18:59:42Z","title":"SEGIC: Unleashing the Emergent Correspondence for In-Context\n  Segmentation","summary":"  In-context segmentation aims at segmenting novel images using a few labeled\nexample images, termed as \"in-context examples\", exploring content similarities\nbetween examples and the target. The resulting models can be generalized\nseamlessly to novel segmentation tasks, significantly reducing the labeling and\ntraining costs compared with conventional pipelines. However, in-context\nsegmentation is more challenging than classic ones requiring the model to learn\nsegmentation rules conditioned on a few samples. Unlike previous work with\nad-hoc or non-end-to-end designs, we propose SEGIC, an end-to-end\nsegment-in-context framework built upon a single vision foundation model (VFM).\nIn particular, SEGIC leverages the emergent correspondence within VFM to\ncapture dense relationships between target images and in-context samples. As\nsuch, information from in-context samples is then extracted into three types of\ninstructions, i.e. geometric, visual, and meta instructions, serving as\nexplicit conditions for the final mask prediction. SEGIC is a straightforward\nyet effective approach that yields state-of-the-art performance on one-shot\nsegmentation benchmarks. Notably, SEGIC can be easily generalized to diverse\ntasks, including video object segmentation and open-vocabulary segmentation.\nCode will be available at https://github.com/MengLcool/SEGIC.\n","authors":["Lingchen Meng","Shiyi Lan","Hengduo Li","Jose M. Alvarez","Zuxuan Wu","Yu-Gang Jiang"],"pdf_url":"https://arxiv.org/pdf/2311.14671v3.pdf","comment":"ECCV-24 camera-ready"},{"id":"http://arxiv.org/abs/2403.18328v3","updated":"2024-07-22T15:04:33Z","published":"2024-03-27T08:09:04Z","title":"PIPNet3D: Interpretable Detection of Alzheimer in MRI Scans","summary":"  Information from neuroimaging examinations is increasingly used to support\ndiagnoses of dementia, e.g., Alzheimer's disease. While current clinical\npractice is mainly based on visual inspection and feature engineering, Deep\nLearning approaches can be used to automate the analysis and to discover new\nimage-biomarkers. Part-prototype neural networks (PP-NN) are an alternative to\nstandard blackbox models, and have shown promising results in general computer\nvision. PP-NN's base their reasoning on prototypical image regions that are\nlearned fully unsupervised, and combined with a simple-to-understand decision\nlayer. We present PIPNet3D, a PP-NN for volumetric images. We apply PIPNet3D to\nthe clinical diagnosis of Alzheimer's Disease from structural Magnetic\nResonance Imaging (sMRI). We assess the quality of prototypes under a\nsystematic evaluation framework, propose new functionally grounded metrics to\nevaluate brain prototypes and develop an evaluation scheme to assess their\ncoherency with domain experts. Our results show that PIPNet3D is an\ninterpretable, compact model for Alzheimer's diagnosis with its reasoning well\naligned to medical domain knowledge. Notably, PIPNet3D achieves the same\naccuracy as its blackbox counterpart; and removing the remaining clinically\nirrelevant prototypes from its decision process does not decrease predictive\nperformance.\n","authors":["Lisa Anita De Santi","Jörg Schlötterer","Michael Scheschenja","Joel Wessendorf","Meike Nauta","Vincenzo Positano","Christin Seifert"],"pdf_url":"https://arxiv.org/pdf/2403.18328v3.pdf","comment":"Accepted at iMIMIC workshop @MICCAI 2024"},{"id":"http://arxiv.org/abs/2407.14277v2","updated":"2024-07-22T15:02:24Z","published":"2024-07-19T12:58:18Z","title":"Patch-based Intuitive Multimodal Prototypes Network (PIMPNet) for\n  Alzheimer's Disease classification","summary":"  Volumetric neuroimaging examinations like structural Magnetic Resonance\nImaging (sMRI) are routinely applied to support the clinical diagnosis of\ndementia like Alzheimer's Disease (AD). Neuroradiologists examine 3D sMRI to\ndetect and monitor abnormalities in brain morphology due to AD, like global\nand/or local brain atrophy and shape alteration of characteristic structures.\nThere is a strong research interest in developing diagnostic systems based on\nDeep Learning (DL) models to analyse sMRI for AD. However, anatomical\ninformation extracted from an sMRI examination needs to be interpreted together\nwith patient's age to distinguish AD patterns from the regular alteration due\nto a normal ageing process. In this context, part-prototype neural networks\nintegrate the computational advantages of DL in an interpretable-by-design\narchitecture and showed promising results in medical imaging applications. We\npresent PIMPNet, the first interpretable multimodal model for 3D images and\ndemographics applied to the binary classification of AD from 3D sMRI and\npatient's age. Despite age prototypes do not improve predictive performance\ncompared to the single modality model, this lays the foundation for future work\nin the direction of the model's design and multimodal prototype training\nprocess\n","authors":["Lisa Anita De Santi","Jörg Schlötterer","Meike Nauta","Vincenzo Positano","Christin Seifert"],"pdf_url":"https://arxiv.org/pdf/2407.14277v2.pdf","comment":"Accepted \"late-breaking work\" at XAI-2024"},{"id":"http://arxiv.org/abs/2407.15689v1","updated":"2024-07-22T14:54:51Z","published":"2024-07-22T14:54:51Z","title":"YOLOv10 for Automated Fracture Detection in Pediatric Wrist Trauma\n  X-rays","summary":"  Wrist fractures are highly prevalent among children and can significantly\nimpact their daily activities, such as attending school, participating in\nsports, and performing basic self-care tasks. If not treated properly, these\nfractures can result in chronic pain, reduced wrist functionality, and other\nlong-term complications. Recently, advancements in object detection have shown\npromise in enhancing fracture detection, with systems achieving accuracy\ncomparable to, or even surpassing, that of human radiologists. The YOLO series,\nin particular, has demonstrated notable success in this domain. This study is\nthe first to provide a thorough evaluation of various YOLOv10 variants to\nassess their performance in detecting pediatric wrist fractures using the\nGRAZPEDWRI-DX dataset. It investigates how changes in model complexity, scaling\nthe architecture, and implementing a dual-label assignment strategy can enhance\ndetection performance. Experimental results indicate that our trained model\nachieved mean average precision (mAP@50-95) of 51.9\\% surpassing the current\nYOLOv9 benchmark of 43.3\\% on this dataset. This represents an improvement of\n8.6\\%. The implementation code is publicly available at\nhttps://github.com/ammarlodhi255/YOLOv10-Fracture-Detection\n","authors":["Ammar Ahmed","Abdul Manaf"],"pdf_url":"https://arxiv.org/pdf/2407.15689v1.pdf","comment":"The code will soon be made publicly available"},{"id":"http://arxiv.org/abs/2407.15686v1","updated":"2024-07-22T14:53:29Z","published":"2024-07-22T14:53:29Z","title":"Differentiable Convex Polyhedra Optimization from Multi-view Images","summary":"  This paper presents a novel approach for the differentiable rendering of\nconvex polyhedra, addressing the limitations of recent methods that rely on\nimplicit field supervision. Our technique introduces a strategy that combines\nnon-differentiable computation of hyperplane intersection through duality\ntransform with differentiable optimization for vertex positioning with\nthree-plane intersection, enabling gradient-based optimization without the need\nfor 3D implicit fields. This allows for efficient shape representation across a\nrange of applications, from shape parsing to compact mesh reconstruction. This\nwork not only overcomes the challenges of previous approaches but also sets a\nnew standard for representing shapes with convex polyhedra.\n","authors":["Daxuan Ren","Haiyi Mei","Hezi Shi","Jianmin Zheng","Jianfei Cai","Lei Yang"],"pdf_url":"https://arxiv.org/pdf/2407.15686v1.pdf","comment":"ECCV2024 https://github.com/kimren227/DiffConvex"},{"id":"http://arxiv.org/abs/2311.16501v2","updated":"2024-07-22T14:52:46Z","published":"2023-11-26T06:40:16Z","title":"Context-Aware Indoor Point Cloud Object Generation through User\n  Instructions","summary":"  Indoor scene modification has emerged as a prominent area within computer\nvision, particularly for its applications in Augmented Reality (AR) and Virtual\nReality (VR). Traditional methods often rely on pre-existing object databases\nand predetermined object positions, limiting their flexibility and adaptability\nto new scenarios. In response to this challenge, we present a novel end-to-end\nmulti-modal deep neural network capable of generating point cloud objects\nseamlessly integrated with their surroundings, driven by textual instructions.\nOur model revolutionizes scene modification by enabling the creation of new\nenvironments with previously unseen object layouts, eliminating the need for\npre-stored CAD models. Leveraging Point-E as our generative model, we introduce\ninnovative techniques such as quantized position prediction and Top-K\nestimation to address the issue of false negatives resulting from ambiguous\nlanguage descriptions. Furthermore, we conduct comprehensive evaluations to\nshowcase the diversity of generated objects, the efficacy of textual\ninstructions, and the quantitative metrics, affirming the realism and\nversatility of our model in generating indoor objects. To provide a holistic\nassessment, we incorporate visual grounding as an additional metric, ensuring\nthe quality and coherence of the scenes produced by our model. Through these\nadvancements, our approach not only advances the state-of-the-art in indoor\nscene modification but also lays the foundation for future innovations in\nimmersive computing and digital environment creation.\n","authors":["Yiyang Luo","Ke Lin","Chao Gu"],"pdf_url":"https://arxiv.org/pdf/2311.16501v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15683v1","updated":"2024-07-22T14:51:28Z","published":"2024-07-22T14:51:28Z","title":"Enhancing Transferability of Targeted Adversarial Examples: A\n  Self-Universal Perspective","summary":"  Transfer-based targeted adversarial attacks against black-box deep neural\nnetworks (DNNs) have been proven to be significantly more challenging than\nuntargeted ones. The impressive transferability of current SOTA, the generative\nmethods, comes at the cost of requiring massive amounts of additional data and\ntime-consuming training for each targeted label. This results in limited\nefficiency and flexibility, significantly hindering their deployment in\npractical applications. In this paper, we offer a self-universal perspective\nthat unveils the great yet underexplored potential of input transformations in\npursuing this goal. Specifically, transformations universalize gradient-based\nattacks with intrinsic but overlooked semantics inherent within individual\nimages, exhibiting similar scalability and comparable results to time-consuming\nlearning over massive additional data from diverse classes. We also contribute\na surprising empirical insight that one of the most fundamental\ntransformations, simple image scaling, is highly effective, scalable,\nsufficient, and necessary in enhancing targeted transferability. We further\naugment simple scaling with orthogonal transformations and block-wise\napplicability, resulting in the Simple, faSt, Self-universal yet Strong Scale\nTransformation (S$^4$ST) for self-universal TTA. On the ImageNet-Compatible\nbenchmark dataset, our method achieves a 19.8% improvement in the average\ntargeted transfer success rate against various challenging victim models over\nexisting SOTA transformation methods while only consuming 36% time for\nattacking. It also outperforms resource-intensive attacks by a large margin in\nvarious challenging settings.\n","authors":["Bowen Peng","Li Liu","Tianpeng Liu","Zhen Liu","Yongxiang Liu"],"pdf_url":"https://arxiv.org/pdf/2407.15683v1.pdf","comment":"8 pages and 9 figures"},{"id":"http://arxiv.org/abs/2407.15680v1","updated":"2024-07-22T14:49:51Z","published":"2024-07-22T14:49:51Z","title":"HaloQuest: A Visual Hallucination Dataset for Advancing Multimodal\n  Reasoning","summary":"  Hallucination has been a major problem for large language models and remains\na critical challenge when it comes to multimodality in which vision-language\nmodels (VLMs) have to deal with not just textual but also visual inputs.\nDespite rapid progress in VLMs, resources for evaluating and addressing\nmultimodal hallucination are limited and mostly focused on evaluation. This\nwork introduces HaloQuest, a novel visual question answering dataset that\ncaptures various aspects of multimodal hallucination such as false premises,\ninsufficient contexts, and visual challenges. A novel idea from HaloQuest is to\nleverage synthetic images, apart from real ones, to enable dataset creation at\nscale. With over 7.7K examples spanning across a wide variety of categories,\nHaloQuest was designed to be both a challenging benchmark for VLMs and a\nfine-tuning dataset for advancing multimodal reasoning. Our experiments reveal\nthat current models struggle with HaloQuest, with all open-source VLMs\nachieving below 36% accuracy. On the other hand, fine-tuning on HaloQuest\nsignificantly reduces hallucination rates while preserving performance on\nstandard reasoning tasks. Our results discover that benchmarking with generated\nimages is highly correlated (r=0.97) with real images. Last but not least, we\npropose a novel Auto-Eval mechanism that is highly correlated with human raters\n(r=0.99) for evaluating VLMs. In sum, this work makes concrete strides towards\nunderstanding, evaluating, and mitigating hallucination in VLMs, serving as an\nimportant step towards more reliable multimodal AI systems in the future.\n","authors":["Zhecan Wang","Garrett Bingham","Adams Yu","Quoc Le","Thang Luong","Golnaz Ghiasi"],"pdf_url":"https://arxiv.org/pdf/2407.15680v1.pdf","comment":"Accepted as a main conference paper at ECCV 2024\n  (https://github.com/google/haloquest)"},{"id":"http://arxiv.org/abs/2407.15675v1","updated":"2024-07-22T14:42:34Z","published":"2024-07-22T14:42:34Z","title":"Flow-guided Motion Prediction with Semantics and Dynamic Occupancy Grid\n  Maps","summary":"  Accurate prediction of driving scenes is essential for road safety and\nautonomous driving. Occupancy Grid Maps (OGMs) are commonly employed for scene\nprediction due to their structured spatial representation, flexibility across\nsensor modalities and integration of uncertainty. Recent studies have\nsuccessfully combined OGMs with deep learning methods to predict the evolution\nof scene and learn complex behaviours. These methods, however, do not consider\nprediction of flow or velocity vectors in the scene. In this work, we propose a\nnovel multi-task framework that leverages dynamic OGMs and semantic information\nto predict both future vehicle semantic grids and the future flow of the scene.\nThis incorporation of semantic flow not only offers intermediate scene features\nbut also enables the generation of warped semantic grids. Evaluation on the\nreal-world NuScenes dataset demonstrates improved prediction capabilities and\nenhanced ability of the model to retain dynamic vehicles within the scene.\n","authors":["Rabbia Asghar","Wenqian Liu","Lukas Rummelhard","Anne Spalanzani","Christian Laugier"],"pdf_url":"https://arxiv.org/pdf/2407.15675v1.pdf","comment":"Accepted for publication at the 27th IEEE International Conference on\n  Intelligent Transportation Systems (ITSC) (ITSC 2024)"},{"id":"http://arxiv.org/abs/2407.15668v1","updated":"2024-07-22T14:29:36Z","published":"2024-07-22T14:29:36Z","title":"SLVideo: A Sign Language Video Moment Retrieval Framework","summary":"  Sign Language Recognition has been studied and developed throughout the years\nto help the deaf and hard-of-hearing people in their day-to-day lives. These\ntechnologies leverage manual sign recognition algorithms, however, most of them\nlack the recognition of facial expressions, which are also an essential part of\nSign Language as they allow the speaker to add expressiveness to their dialogue\nor even change the meaning of certain manual signs. SLVideo is a video moment\nretrieval software for Sign Language videos with a focus on both hands and\nfacial signs. The system extracts embedding representations for the hand and\nface signs from video frames to capture the language signs in full. This will\nthen allow the user to search for a specific sign language video segment with\ntext queries, or to search by similar sign language videos. To test this\nsystem, a collection of five hours of annotated Sign Language videos is used as\nthe dataset, and the initial results are promising in a zero-shot\nsetting.SLVideo is shown to not only address the problem of searching sign\nlanguage videos but also supports a Sign Language thesaurus with a search by\nsimilarity technique.\n  Project web page: https://novasearch.github.io/SLVideo/\n","authors":["Gonçalo Vinagre Martins","Afonso Quinaz","Carla Viegas","Sofia Cavaco","João Magalhães"],"pdf_url":"https://arxiv.org/pdf/2407.15668v1.pdf","comment":"5 pages, 3 figures, 1 table"},{"id":"http://arxiv.org/abs/2407.15663v1","updated":"2024-07-22T14:24:56Z","published":"2024-07-22T14:24:56Z","title":"MSSPlace: Multi-Sensor Place Recognition with Visual and Text Semantics","summary":"  Place recognition is a challenging task in computer vision, crucial for\nenabling autonomous vehicles and robots to navigate previously visited\nenvironments. While significant progress has been made in learnable multimodal\nmethods that combine onboard camera images and LiDAR point clouds, the full\npotential of these methods remains largely unexplored in localization\napplications. In this paper, we study the impact of leveraging a multi-camera\nsetup and integrating diverse data sources for multimodal place recognition,\nincorporating explicit visual semantics and text descriptions. Our proposed\nmethod named MSSPlace utilizes images from multiple cameras, LiDAR point\nclouds, semantic segmentation masks, and text annotations to generate\ncomprehensive place descriptors. We employ a late fusion approach to integrate\nthese modalities, providing a unified representation. Through extensive\nexperiments on the Oxford RobotCar and NCLT datasets, we systematically analyze\nthe impact of each data source on the overall quality of place descriptors. Our\nexperiments demonstrate that combining data from multiple sensors significantly\nimproves place recognition model performance compared to single modality\napproaches and leads to state-of-the-art quality. We also show that separate\nusage of visual or textual semantics (which are more compact representations of\nsensory data) can achieve promising results in place recognition. The code for\nour method is publicly available: https://github.com/alexmelekhin/MSSPlace\n","authors":["Alexander Melekhin","Dmitry Yudin","Ilia Petryashin","Vitaly Bezuglyj"],"pdf_url":"https://arxiv.org/pdf/2407.15663v1.pdf","comment":"This work has been submitted to the IEEE for possible publication"},{"id":"http://arxiv.org/abs/2309.16351v2","updated":"2024-07-22T14:21:31Z","published":"2023-09-28T11:20:24Z","title":"Dark Side Augmentation: Generating Diverse Night Examples for Metric\n  Learning","summary":"  Image retrieval methods based on CNN descriptors rely on metric learning from\na large number of diverse examples of positive and negative image pairs.\nDomains, such as night-time images, with limited availability and variability\nof training data suffer from poor retrieval performance even with methods\nperforming well on standard benchmarks. We propose to train a GAN-based\nsynthetic-image generator, translating available day-time image examples into\nnight images. Such a generator is used in metric learning as a form of\naugmentation, supplying training data to the scarce domain. Various types of\ngenerators are evaluated and analyzed. We contribute with a novel light-weight\nGAN architecture that enforces the consistency between the original and\ntranslated image through edge consistency. The proposed architecture also\nallows a simultaneous training of an edge detector that operates on both night\nand day images. To further increase the variability in the training examples\nand to maximize the generalization of the trained model, we propose a novel\nmethod of diverse anchor mining.\n  The proposed method improves over the state-of-the-art results on a standard\nTokyo 24/7 day-night retrieval benchmark while preserving the performance on\nOxford and Paris datasets. This is achieved without the need of training image\npairs of matching day and night images. The source code is available at\nhttps://github.com/mohwald/gandtr .\n","authors":["Albert Mohwald","Tomas Jenicek","Ondřej Chum"],"pdf_url":"https://arxiv.org/pdf/2309.16351v2.pdf","comment":"paper with supplementary, 26 pages, 8 figures, 13 tables"},{"id":"http://arxiv.org/abs/2407.15661v1","updated":"2024-07-22T14:18:52Z","published":"2024-07-22T14:18:52Z","title":"DriveDiTFit: Fine-tuning Diffusion Transformers for Autonomous Driving","summary":"  In autonomous driving, deep models have shown remarkable performance across\nvarious visual perception tasks with the demand of high-quality and\nhuge-diversity training datasets. Such datasets are expected to cover various\ndriving scenarios with adverse weather, lighting conditions and diverse moving\nobjects. However, manually collecting these data presents huge challenges and\nexpensive cost. With the rapid development of large generative models, we\npropose DriveDiTFit, a novel method for efficiently generating autonomous\nDriving data by Fine-tuning pre-trained Diffusion Transformers (DiTs).\nSpecifically, DriveDiTFit utilizes a gap-driven modulation technique to\ncarefully select and efficiently fine-tune a few parameters in DiTs according\nto the discrepancy between the pre-trained source data and the target driving\ndata. Additionally, DriveDiTFit develops an effective weather and lighting\ncondition embedding module to ensure diversity in the generated data, which is\ninitialized by a nearest-semantic-similarity initialization approach. Through\nprogressive tuning scheme to refined the process of detail generation in early\ndiffusion process and enlarging the weights corresponding to small objects in\ntraining loss, DriveDiTFit ensures high-quality generation of small moving\nobjects in the generated data. Extensive experiments conducted on driving\ndatasets confirm that our method could efficiently produce diverse real driving\ndata. The source codes will be available at\nhttps://github.com/TtuHamg/DriveDiTFit.\n","authors":["Jiahang Tu","Wei Ji","Hanbin Zhao","Chao Zhang","Roger Zimmermann","Hui Qian"],"pdf_url":"https://arxiv.org/pdf/2407.15661v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.18929v2","updated":"2024-07-22T14:11:07Z","published":"2024-04-29T17:59:30Z","title":"DGE: Direct Gaussian 3D Editing by Consistent Multi-view Editing","summary":"  We consider the problem of editing 3D objects and scenes based on open-ended\nlanguage instructions. A common approach to this problem is to use a 2D image\ngenerator or editor to guide the 3D editing process, obviating the need for 3D\ndata. However, this process is often inefficient due to the need for iterative\nupdates of costly 3D representations, such as neural radiance fields, either\nthrough individual view edits or score distillation sampling. A major\ndisadvantage of this approach is the slow convergence caused by aggregating\ninconsistent information across views, as the guidance from 2D models is not\nmulti-view consistent. We thus introduce the Direct Gaussian Editor (DGE), a\nmethod that addresses these issues in two stages. First, we modify a given\nhigh-quality image editor like InstructPix2Pix to be multi-view consistent. To\ndo so, we propose a training-free approach that integrates cues from the 3D\ngeometry of the underlying scene. Second, given a multi-view consistent edited\nsequence of images, we directly and efficiently optimize the 3D representation,\nwhich is based on 3D Gaussian Splatting. Because it avoids incremental and\niterative edits, DGE is significantly more accurate and efficient than existing\napproaches and offers additional benefits, such as enabling selective editing\nof parts of the scene.\n","authors":["Minghao Chen","Iro Laina","Andrea Vedaldi"],"pdf_url":"https://arxiv.org/pdf/2404.18929v2.pdf","comment":"ECCV 2024. Project Page: https://silent-chen.github.io/DGE/"},{"id":"http://arxiv.org/abs/2407.15648v1","updated":"2024-07-22T14:05:27Z","published":"2024-07-22T14:05:27Z","title":"TreeSBA: Tree-Transformer for Self-Supervised Sequential Brick Assembly","summary":"  Inferring step-wise actions to assemble 3D objects with primitive bricks from\nimages is a challenging task due to complex constraints and the vast number of\npossible combinations. Recent studies have demonstrated promising results on\nsequential LEGO brick assembly through the utilization of LEGO-Graph modeling\nto predict sequential actions. However, existing approaches are class-specific\nand require significant computational and 3D annotation resources. In this\nwork, we first propose a computationally efficient breadth-first search (BFS)\nLEGO-Tree structure to model the sequential assembly actions by considering\nconnections between consecutive layers. Based on the LEGO-Tree structure, we\nthen design a class-agnostic tree-transformer framework to predict the\nsequential assembly actions from the input multi-view images. A major challenge\nof the sequential brick assembly task is that the step-wise action labels are\ncostly and tedious to obtain in practice. We mitigate this problem by\nleveraging synthetic-to-real transfer learning. Specifically, our model is\nfirst pre-trained on synthetic data with full supervision from the available\naction labels. We then circumvent the requirement for action labels in the real\ndata by proposing an action-to-silhouette projection that replaces action\nlabels with input image silhouettes for self-supervision. Without any\nannotation on the real data, our model outperforms existing methods with 3D\nsupervision by 7.8% and 11.3% in mIoU on the MNIST and ModelNet Construction\ndatasets, respectively.\n","authors":["Mengqi Guo","Chen Li","Yuyang Zhao","Gim Hee Lee"],"pdf_url":"https://arxiv.org/pdf/2407.15648v1.pdf","comment":"ECCV 2024"},{"id":"http://arxiv.org/abs/2407.15646v1","updated":"2024-07-22T14:03:29Z","published":"2024-07-22T14:03:29Z","title":"SS-SFR: Synthetic Scenes Spatial Frequency Response on Virtual KITTI and\n  Degraded Automotive Simulations for Object Detection","summary":"  Automotive simulation can potentially compensate for a lack of training data\nin computer vision applications. However, there has been little to no image\nquality evaluation of automotive simulation and the impact of optical\ndegradations on simulation is little explored. In this work, we investigate\nVirtual KITTI and the impact of applying variations of Gaussian blur on image\nsharpness. Furthermore, we consider object detection, a common computer vision\napplication on three different state-of-the-art models, thus allowing us to\ncharacterize the relationship between object detection and sharpness. It was\nfound that while image sharpness (MTF50) degrades from an average of 0.245cy/px\nto approximately 0.119cy/px; object detection performance stays largely robust\nwithin 0.58\\%(Faster RCNN), 1.45\\%(YOLOF) and 1.93\\%(DETR) across all\nrespective held-out test sets.\n","authors":["Daniel Jakab","Alexander Braun","Cathaoir Agnew","Reenu Mohandas","Brian Michael Deegan","Dara Molloy","Enda Ward","Tony Scanlan","Ciarán Eising"],"pdf_url":"https://arxiv.org/pdf/2407.15646v1.pdf","comment":"8 pages, 2 figures, 2 tables"},{"id":"http://arxiv.org/abs/2407.15642v1","updated":"2024-07-22T14:00:03Z","published":"2024-07-22T14:00:03Z","title":"Cinemo: Consistent and Controllable Image Animation with Motion\n  Diffusion Models","summary":"  Diffusion models have achieved great progress in image animation due to\npowerful generative capabilities. However, maintaining spatio-temporal\nconsistency with detailed information from the input static image over time\n(e.g., style, background, and object of the input static image) and ensuring\nsmoothness in animated video narratives guided by textual prompts still remains\nchallenging. In this paper, we introduce Cinemo, a novel image animation\napproach towards achieving better motion controllability, as well as stronger\ntemporal consistency and smoothness. In general, we propose three effective\nstrategies at the training and inference stages of Cinemo to accomplish our\ngoal. At the training stage, Cinemo focuses on learning the distribution of\nmotion residuals, rather than directly predicting subsequent via a motion\ndiffusion model. Additionally, a structural similarity index-based strategy is\nproposed to enable Cinemo to have better controllability of motion intensity.\nAt the inference stage, a noise refinement technique based on discrete cosine\ntransformation is introduced to mitigate sudden motion changes. Such three\nstrategies enable Cinemo to produce highly consistent, smooth, and\nmotion-controllable results. Compared to previous methods, Cinemo offers\nsimpler and more precise user controllability. Extensive experiments against\nseveral state-of-the-art methods, including both commercial tools and research\napproaches, across multiple metrics, demonstrate the effectiveness and\nsuperiority of our proposed approach.\n","authors":["Xin Ma","Yaohui Wang","Gengyu Jia","Xinyuan Chen","Yuan-Fang Li","Cunjian Chen","Yu Qiao"],"pdf_url":"https://arxiv.org/pdf/2407.15642v1.pdf","comment":"Project webpage: https://maxin-cn.github.io/cinemo_project/"},{"id":"http://arxiv.org/abs/2407.14087v2","updated":"2024-07-22T13:59:10Z","published":"2024-07-19T07:51:51Z","title":"Score Normalization for Demographic Fairness in Face Recognition","summary":"  Fair biometric algorithms have similar verification performance across\ndifferent demographic groups given a single decision threshold. Unfortunately,\nfor state-of-the-art face recognition networks, score distributions differ\nbetween demographics. Contrary to work that tries to align those distributions\nby extra training or fine-tuning, we solely focus on score post-processing\nmethods. As proved, well-known sample-centered score normalization techniques,\nZ-norm and T-norm, do not improve fairness for high-security operating points.\nThus, we extend the standard Z/T-norm to integrate demographic information in\nnormalization. Additionally, we investigate several possibilities to\nincorporate cohort similarities for both genuine and impostor pairs per\ndemographic to improve fairness across different operating points. We run\nexperiments on two datasets with different demographics (gender and ethnicity)\nand show that our techniques generally improve the overall fairness of five\nstate-of-the-art pre-trained face recognition networks, without downgrading\nverification performance. We also indicate that an equal contribution of False\nMatch Rate (FMR) and False Non-Match Rate (FNMR) in fairness evaluation is\nrequired for the highest gains. Code and protocols are available.\n","authors":["Yu Linghu","Tiago de Freitas Pereira","Christophe Ecabert","Sébastien Marcel","Manuel Günther"],"pdf_url":"https://arxiv.org/pdf/2407.14087v2.pdf","comment":"Accepted for presentation at IJCB 2024"},{"id":"http://arxiv.org/abs/2407.14066v2","updated":"2024-07-22T13:50:55Z","published":"2024-07-19T06:50:24Z","title":"360VFI: A Dataset and Benchmark for Omnidirectional Video Frame\n  Interpolation","summary":"  With the development of VR-related techniques, viewers can enjoy a realistic\nand immersive experience through a head-mounted display, while omnidirectional\nvideo with a low frame rate can lead to user dizziness. However, the prevailing\nplane frame interpolation methodologies are unsuitable for Omnidirectional\nVideo Interpolation, chiefly due to the lack of models tailored to such videos\nwith strong distortion, compounded by the scarcity of valuable datasets for\nOmnidirectional Video Frame Interpolation. In this paper, we introduce the\nbenchmark dataset, 360VFI, for Omnidirectional Video Frame Interpolation. We\npresent a practical implementation that introduces a distortion prior from\nomnidirectional video into the network to modulate distortions. We especially\npropose a pyramid distortion-sensitive feature extractor that uses the unique\ncharacteristics of equirectangular projection (ERP) format as prior\ninformation. Moreover, we devise a decoder that uses an affine transformation\nto facilitate the synthesis of intermediate frames further. 360VFI is the first\ndataset and benchmark that explores the challenge of Omnidirectional Video\nFrame Interpolation. Through our benchmark analysis, we presented four\ndifferent distortion conditions scenes in the proposed 360VFI dataset to\nevaluate the challenge triggered by distortion during interpolation. Besides,\nexperimental results demonstrate that Omnidirectional Video Interpolation can\nbe effectively improved by modeling for omnidirectional distortion.\n","authors":["Wenxuan Lu","Mengshun Hu","Yansheng Qiu","Liang Liao","Zheng Wang"],"pdf_url":"https://arxiv.org/pdf/2407.14066v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.11587v3","updated":"2024-07-22T13:46:46Z","published":"2023-11-20T07:54:54Z","title":"LDConv: Linear deformable convolution for improving convolutional neural\n  networks","summary":"  Neural networks based on convolutional operations have achieved remarkable\nresults in the field of deep learning, but there are two inherent flaws in\nstandard convolutional operations. On the one hand, the convolution operation\nis confined to a local window, so it cannot capture information from other\nlocations, and its sampled shapes is fixed. On the other hand, the size of the\nconvolutional kernel are fixed to k $\\times$ k, which is a fixed square shape,\nand the number of parameters tends to grow squarely with size. Although\nDeformable Convolution (Deformable Conv) address the problem of fixed sampling\nof standard convolutions, the number of parameters also tends to grow in a\nsquared manner. In response to the above questions, the Linear Deformable\nConvolution (LDConv) is explored in this work, which gives the convolution\nkernel an arbitrary number of parameters and arbitrary sampled shapes to\nprovide richer options for the trade-off between network overhead and\nperformance. In LDConv, a novel coordinate generation algorithm is defined to\ngenerate different initial sampled positions for convolutional kernels of\narbitrary size. To adapt to changing targets, offsets are introduced to adjust\nthe shape of the samples at each position. LDConv corrects the growth trend of\nthe number of parameters for standard convolution and Deformable Conv to a\nlinear growth. Moreover, it completes the process of efficient feature\nextraction by irregular convolutional operations and brings more exploration\noptions for convolutional sampled shapes. Object detection experiments on\nrepresentative datasets COCO2017, VOC 7+12, and VisDrone-DET2021 fully\ndemonstrate the advantages of LDConv. LDConv is a plug-and-play convolutional\noperation that can replace the convolutional operation to improve network\nperformance. The code for the relevant tasks can be found at\nhttps://github.com/CV-ZhangXin/LDConv.\n","authors":["Xin Zhang","Yingze Song","Tingting Song","Degang Yang","Yichen Ye","Jie Zhou","Liming Zhang"],"pdf_url":"https://arxiv.org/pdf/2311.11587v3.pdf","comment":"14 pages, 9 figures"},{"id":"http://arxiv.org/abs/2407.15631v1","updated":"2024-07-22T13:44:06Z","published":"2024-07-22T13:44:06Z","title":"A Diffusion Model for Simulation Ready Coronary Anatomy with\n  Morpho-skeletal Control","summary":"  Virtual interventions enable the physics-based simulation of device\ndeployment within coronary arteries. This framework allows for counterfactual\nreasoning by deploying the same device in different arterial anatomies.\nHowever, current methods to create such counterfactual arteries face a\ntrade-off between controllability and realism. In this study, we investigate\nhow Latent Diffusion Models (LDMs) can custom synthesize coronary anatomy for\nvirtual intervention studies based on mid-level anatomic constraints such as\ntopological validity, local morphological shape, and global skeletal structure.\nWe also extend diffusion model guidance strategies to the context of\nmorpho-skeletal conditioning and propose a novel guidance method for continuous\nattributes that adaptively updates the negative guiding condition throughout\nsampling. Our framework enables the generation and editing of coronary anatomy\nin a controllable manner, allowing device designers to derive mechanistic\ninsights regarding anatomic variation and simulated device deployment.\n","authors":["Karim Kadry","Shreya Gupta","Jonas Sogbadji","Michiel Schaap","Kersten Petersen","Takuya Mizukami","Carlos Collet","Farhad R. Nezami","Elazer R. Edelman"],"pdf_url":"https://arxiv.org/pdf/2407.15631v1.pdf","comment":"Accepted to ECCV 2024"},{"id":"http://arxiv.org/abs/2407.15626v1","updated":"2024-07-22T13:37:55Z","published":"2024-07-22T13:37:55Z","title":"Reinforcement Learning Meets Visual Odometry","summary":"  Visual Odometry (VO) is essential to downstream mobile robotics and\naugmented/virtual reality tasks. Despite recent advances, existing VO methods\nstill rely on heuristic design choices that require several weeks of\nhyperparameter tuning by human experts, hindering generalizability and\nrobustness. We address these challenges by reframing VO as a sequential\ndecision-making task and applying Reinforcement Learning (RL) to adapt the VO\nprocess dynamically. Our approach introduces a neural network, operating as an\nagent within the VO pipeline, to make decisions such as keyframe and grid-size\nselection based on real-time conditions. Our method minimizes reliance on\nheuristic choices using a reward function based on pose error, runtime, and\nother metrics to guide the system. Our RL framework treats the VO system and\nthe image sequence as an environment, with the agent receiving observations\nfrom keypoints, map statistics, and prior poses. Experimental results using\nclassical VO methods and public benchmarks demonstrate improvements in accuracy\nand robustness, validating the generalizability of our RL-enhanced VO approach\nto different scenarios. We believe this paradigm shift advances VO technology\nby eliminating the need for time-intensive parameter tuning of heuristics.\n","authors":["Nico Messikommer","Giovanni Cioffi","Mathias Gehrig","Davide Scaramuzza"],"pdf_url":"https://arxiv.org/pdf/2407.15626v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15617v1","updated":"2024-07-22T13:24:32Z","published":"2024-07-22T13:24:32Z","title":"Norface: Improving Facial Expression Analysis by Identity Normalization","summary":"  Facial Expression Analysis remains a challenging task due to unexpected\ntask-irrelevant noise, such as identity, head pose, and background. To address\nthis issue, this paper proposes a novel framework, called Norface, that is\nunified for both Action Unit (AU) analysis and Facial Emotion Recognition (FER)\ntasks. Norface consists of a normalization network and a classification\nnetwork. First, the carefully designed normalization network struggles to\ndirectly remove the above task-irrelevant noise, by maintaining facial\nexpression consistency but normalizing all original images to a common identity\nwith consistent pose, and background. Then, these additional normalized images\nare fed into the classification network. Due to consistent identity and other\nfactors (e.g. head pose, background, etc.), the normalized images enable the\nclassification network to extract useful expression information more\neffectively. Additionally, the classification network incorporates a Mixture of\nExperts to refine the latent representation, including handling the input of\nfacial representations and the output of multiple (AU or emotion) labels.\nExtensive experiments validate the carefully designed framework with the\ninsight of identity normalization. The proposed method outperforms existing\nSOTA methods in multiple facial expression analysis tasks, including AU\ndetection, AU intensity estimation, and FER tasks, as well as their\ncross-dataset tasks. For the normalized datasets and code please visit\n{https://norface-fea.github.io/}.\n","authors":["Hanwei Liu","Rudong An","Zhimeng Zhang","Bowen Ma","Wei Zhang","Yan Song","Yujing Hu","Wei Chen","Yu Ding"],"pdf_url":"https://arxiv.org/pdf/2407.15617v1.pdf","comment":"Accepted by ECCV2024"},{"id":"http://arxiv.org/abs/2407.15613v1","updated":"2024-07-22T13:15:04Z","published":"2024-07-22T13:15:04Z","title":"Visual-Semantic Decomposition and Partial Alignment for Document-based\n  Zero-Shot Learning","summary":"  Recent work shows that documents from encyclopedias serve as helpful\nauxiliary information for zero-shot learning. Existing methods align the entire\nsemantics of a document with corresponding images to transfer knowledge.\nHowever, they disregard that semantic information is not equivalent between\nthem, resulting in a suboptimal alignment. In this work, we propose a novel\nnetwork to extract multi-view semantic concepts from documents and images and\nalign the matching rather than entire concepts. Specifically, we propose a\nsemantic decomposition module to generate multi-view semantic embeddings from\nvisual and textual sides, providing the basic concepts for partial alignment.\nTo alleviate the issue of information redundancy among embeddings, we propose\nthe local-to-semantic variance loss to capture distinct local details and\nmultiple semantic diversity loss to enforce orthogonality among embeddings.\nSubsequently, two losses are introduced to partially align visual-semantic\nembedding pairs according to their semantic relevance at the view and\nword-to-patch levels. Consequently, we consistently outperform state-of-the-art\nmethods under two document sources in three standard benchmarks for\ndocument-based zero-shot learning. Qualitatively, we show that our model learns\nthe interpretable partial association.\n","authors":["Xiangyan Qu","Jing Yu","Keke Gai","Jiamin Zhuang","Yuanmin Tang","Gang Xiong","Gaopeng Gou","Qi Wu"],"pdf_url":"https://arxiv.org/pdf/2407.15613v1.pdf","comment":"Accepted to ACM International Conference on Multimedia (MM) 2024"},{"id":"http://arxiv.org/abs/2407.15605v1","updated":"2024-07-22T12:59:57Z","published":"2024-07-22T12:59:57Z","title":"Probing Fine-Grained Action Understanding and Cross-View Generalization\n  of Foundation Models","summary":"  Foundation models (FMs) are large neural networks trained on broad datasets,\nexcelling in downstream tasks with minimal fine-tuning. Human activity\nrecognition in video has advanced with FMs, driven by competition among\ndifferent architectures. However, high accuracies on standard benchmarks can\ndraw an artificially rosy picture, as they often overlook real-world factors\nlike changing camera perspectives. Popular benchmarks, mostly from YouTube or\nmovies, offer diverse views but only coarse actions, which are insufficient for\nuse-cases needing fine-grained, domain-specific actions. Domain-specific\ndatasets (e.g., for industrial assembly) typically use data from limited static\nperspectives. This paper empirically evaluates how perspective changes affect\ndifferent FMs in fine-grained human activity recognition. We compare multiple\nbackbone architectures and design choices, including image- and video- based\nmodels, and various strategies for temporal information fusion, including\ncommonly used score averaging and more novel attention-based temporal\naggregation mechanisms. This is the first systematic study of different\nfoundation models and specific design choices for human activity recognition\nfrom unknown views, conducted with the goal to provide guidance for backbone-\nand temporal- fusion scheme selection. Code and models will be made publicly\navailable to the community.\n","authors":["Thinesh Thiyakesan Ponbagavathi","Kunyu Peng","Alina Roitberg"],"pdf_url":"https://arxiv.org/pdf/2407.15605v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.02392v2","updated":"2024-07-22T12:55:46Z","published":"2024-07-02T16:10:55Z","title":"TokenPacker: Efficient Visual Projector for Multimodal LLM","summary":"  The visual projector serves as an essential bridge between the visual encoder\nand the Large Language Model (LLM) in a Multimodal LLM (MLLM). Typically, MLLMs\nadopt a simple MLP to preserve all visual contexts via one-to-one\ntransformation. However, the visual tokens are redundant and can be\nconsiderably increased when dealing with high-resolution images, impairing the\nefficiency of MLLMs significantly. Some recent works have introduced resampler\nor abstractor to reduce the number of resulting visual tokens. Unfortunately,\nthey fail to capture finer details and undermine the visual reasoning\ncapabilities of MLLMs. In this work, we propose a novel visual projector, which\nadopts a coarse-to-fine scheme to inject the enriched characteristics to\ngenerate the condensed visual tokens. In specific, we first interpolate the\nvisual features as a low-resolution point query, providing the overall visual\nrepresentation as the foundation. Then, we introduce a region-to-point\ninjection module that utilizes high-resolution, multi-level region-based cues\nas fine-grained reference keys and values, allowing them to be fully absorbed\nwithin the corresponding local context region. This step effectively updates\nthe coarse point query, transforming it into an enriched one for the subsequent\nLLM reasoning. Extensive experiments demonstrate that our approach compresses\nthe visual tokens by 75%~89%, while achieves comparable or even better\nperformance across diverse benchmarks with significantly higher efficiency. The\nsource codes can be found at https://github.com/CircleRadon/TokenPacker.\n","authors":["Wentong Li","Yuqian Yuan","Jian Liu","Dongqi Tang","Song Wang","Jianke Zhu","Lei Zhang"],"pdf_url":"https://arxiv.org/pdf/2407.02392v2.pdf","comment":"16 pages, Codes:https://github.com/CircleRadon/TokenPacker"},{"id":"http://arxiv.org/abs/2407.15593v1","updated":"2024-07-22T12:32:09Z","published":"2024-07-22T12:32:09Z","title":"Learning Where to Look: Self-supervised Viewpoint Selection for Active\n  Localization using Geometrical Information","summary":"  Accurate localization in diverse environments is a fundamental challenge in\ncomputer vision and robotics. The task involves determining a sensor's precise\nposition and orientation, typically a camera, within a given space. Traditional\nlocalization methods often rely on passive sensing, which may struggle in\nscenarios with limited features or dynamic environments. In response, this\npaper explores the domain of active localization, emphasizing the importance of\nviewpoint selection to enhance localization accuracy. Our contributions involve\nusing a data-driven approach with a simple architecture designed for real-time\noperation, a self-supervised data training method, and the capability to\nconsistently integrate our map into a planning framework tailored for\nreal-world robotics applications. Our results demonstrate that our method\nperforms better than the existing one, targeting similar problems and\ngeneralizing on synthetic and real data. We also release an open-source\nimplementation to benefit the community.\n","authors":["Luca Di Giammarino","Boyang Sun","Giorgio Grisetti","Marc Pollefeys","Hermann Blum","Daniel Barath"],"pdf_url":"https://arxiv.org/pdf/2407.15593v1.pdf","comment":"www.github.com/rvp-group/learning-where-to-look"},{"id":"http://arxiv.org/abs/2407.15590v1","updated":"2024-07-22T12:26:31Z","published":"2024-07-22T12:26:31Z","title":"All rivers run into the sea: Unified Modality Brain-like Emotional\n  Central Mechanism","summary":"  In the field of affective computing, fully leveraging information from a\nvariety of sensory modalities is essential for the comprehensive understanding\nand processing of human emotions. Inspired by the process through which the\nhuman brain handles emotions and the theory of cross-modal plasticity, we\npropose UMBEnet, a brain-like unified modal affective processing network. The\nprimary design of UMBEnet includes a Dual-Stream (DS) structure that fuses\ninherent prompts with a Prompt Pool and a Sparse Feature Fusion (SFF) module.\nThe design of the Prompt Pool is aimed at integrating information from\ndifferent modalities, while inherent prompts are intended to enhance the\nsystem's predictive guidance capabilities and effectively manage knowledge\nrelated to emotion classification. Moreover, considering the sparsity of\neffective information across different modalities, the SSF module aims to make\nfull use of all available sensory data through the sparse integration of\nmodality fusion prompts and inherent prompts, maintaining high adaptability and\nsensitivity to complex emotional states. Extensive experiments on the largest\nbenchmark datasets in the Dynamic Facial Expression Recognition (DFER) field,\nincluding DFEW, FERV39k, and MAFW, have proven that UMBEnet consistently\noutperforms the current state-of-the-art methods. Notably, in scenarios of\nModality Missingness and multimodal contexts, UMBEnet significantly surpasses\nthe leading current methods, demonstrating outstanding performance and\nadaptability in tasks that involve complex emotional understanding with rich\nmultimodal information.\n","authors":["Xinji Mai","Junxiong Lin","Haoran Wang","Zeng Tao","Yan Wang","Shaoqi Yan","Xuan Tong","Jiawen Yu","Boyang Wang","Ziheng Zhou","Qing Zhao","Shuyong Gao","Wenqiang Zhang"],"pdf_url":"https://arxiv.org/pdf/2407.15590v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15589v1","updated":"2024-07-22T12:26:08Z","published":"2024-07-22T12:26:08Z","title":"Exploring the Effectiveness of Object-Centric Representations in Visual\n  Question Answering: Comparative Insights with Foundation Models","summary":"  Object-centric (OC) representations, which represent the state of a visual\nscene by modeling it as a composition of objects, have the potential to be used\nin various downstream tasks to achieve systematic compositional generalization\nand facilitate reasoning. However, these claims have not been thoroughly\nanalyzed yet. Recently, foundation models have demonstrated unparalleled\ncapabilities across diverse domains from language to computer vision, marking\nthem as a potential cornerstone of future research for a multitude of\ncomputational tasks. In this paper, we conduct an extensive empirical study on\nrepresentation learning for downstream Visual Question Answering (VQA), which\nrequires an accurate compositional understanding of the scene. We thoroughly\ninvestigate the benefits and trade-offs of OC models and alternative approaches\nincluding large pre-trained foundation models on both synthetic and real-world\ndata, and demonstrate a viable way to achieve the best of both worlds. The\nextensiveness of our study, encompassing over 800 downstream VQA models and 15\ndifferent types of upstream representations, also provides several additional\ninsights that we believe will be of interest to the community at large.\n","authors":["Amir Mohammad Karimi Mamaghan","Samuele Papa","Karl Henrik Johansson","Stefan Bauer","Andrea Dittadi"],"pdf_url":"https://arxiv.org/pdf/2407.15589v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11586v2","updated":"2024-07-22T12:16:22Z","published":"2024-03-18T08:58:48Z","title":"DynoSurf: Neural Deformation-based Temporally Consistent Dynamic Surface\n  Reconstruction","summary":"  This paper explores the problem of reconstructing temporally consistent\nsurfaces from a 3D point cloud sequence without correspondence. To address this\nchallenging task, we propose DynoSurf, an unsupervised learning framework\nintegrating a template surface representation with a learnable deformation\nfield. Specifically, we design a coarse-to-fine strategy for learning the\ntemplate surface based on the deformable tetrahedron representation.\nFurthermore, we propose a learnable deformation representation based on the\nlearnable control points and blending weights, which can deform the template\nsurface non-rigidly while maintaining the consistency of the local shape.\nExperimental results demonstrate the significant superiority of DynoSurf over\ncurrent state-of-the-art approaches, showcasing its potential as a powerful\ntool for dynamic mesh reconstruction. The code is publicly available at\nhttps://github.com/yaoyx689/DynoSurf.\n","authors":["Yuxin Yao","Siyu Ren","Junhui Hou","Zhi Deng","Juyong Zhang","Wenping Wang"],"pdf_url":"https://arxiv.org/pdf/2403.11586v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15566v1","updated":"2024-07-22T11:52:04Z","published":"2024-07-22T11:52:04Z","title":"Not All Pairs are Equal: Hierarchical Learning for\n  Average-Precision-Oriented Video Retrieval","summary":"  The rapid growth of online video resources has significantly promoted the\ndevelopment of video retrieval methods. As a standard evaluation metric for\nvideo retrieval, Average Precision (AP) assesses the overall rankings of\nrelevant videos at the top list, making the predicted scores a reliable\nreference for users. However, recent video retrieval methods utilize pair-wise\nlosses that treat all sample pairs equally, leading to an evident gap between\nthe training objective and evaluation metric. To effectively bridge this gap,\nin this work, we aim to address two primary challenges: a) The current\nsimilarity measure and AP-based loss are suboptimal for video retrieval; b) The\nnoticeable noise from frame-to-frame matching introduces ambiguity in\nestimating the AP loss. In response to these challenges, we propose the\nHierarchical learning framework for Average-Precision-oriented Video Retrieval\n(HAP-VR). For the former challenge, we develop the TopK-Chamfer Similarity and\nQuadLinear-AP loss to measure and optimize video-level similarities in terms of\nAP. For the latter challenge, we suggest constraining the frame-level\nsimilarities to achieve an accurate AP loss estimation. Experimental results\npresent that HAP-VR outperforms existing methods on several benchmark datasets,\nproviding a feasible solution for video retrieval tasks and thus offering\npotential benefits for the multi-media application.\n","authors":["Yang Liu","Qianqian Xu","Peisong Wen","Siran Dai","Qingming Huang"],"pdf_url":"https://arxiv.org/pdf/2407.15566v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15554v1","updated":"2024-07-22T11:32:33Z","published":"2024-07-22T11:32:33Z","title":"Decomposition of Neural Discrete Representations for Large-Scale 3D\n  Mapping","summary":"  Learning efficient representations of local features is a key challenge in\nfeature volume-based 3D neural mapping, especially in large-scale environments.\nIn this paper, we introduce Decomposition-based Neural Mapping (DNMap), a\nstorage-efficient large-scale 3D mapping method that employs a discrete\nrepresentation based on a decomposition strategy. This decomposition strategy\naims to efficiently capture repetitive and representative patterns of shapes by\ndecomposing each discrete embedding into component vectors that are shared\nacross the embedding space. Our DNMap optimizes a set of component vectors,\nrather than entire discrete embeddings, and learns composition rather than\nindexing the discrete embeddings. Furthermore, to complement the mapping\nquality, we additionally learn low-resolution continuous embeddings that\nrequire tiny storage space. By combining these representations with a shallow\nneural network and an efficient octree-based feature volume, our DNMap\nsuccessfully approximates signed distance functions and compresses the feature\nvolume while preserving mapping quality. Our source code is available at\nhttps://github.com/minseong-p/dnmap.\n","authors":["Minseong Park","Suhan Woo","Euntai Kim"],"pdf_url":"https://arxiv.org/pdf/2407.15554v1.pdf","comment":"ECCV 2024"},{"id":"http://arxiv.org/abs/2404.10177v2","updated":"2024-07-22T11:31:08Z","published":"2024-03-20T14:22:12Z","title":"Consistent Diffusion Meets Tweedie: Training Exact Ambient Diffusion\n  Models with Noisy Data","summary":"  Ambient diffusion is a recently proposed framework for training diffusion\nmodels using corrupted data. Both Ambient Diffusion and alternative SURE-based\napproaches for learning diffusion models from corrupted data resort to\napproximations which deteriorate performance. We present the first framework\nfor training diffusion models that provably sample from the uncorrupted\ndistribution given only noisy training data, solving an open problem in this\nspace. Our key technical contribution is a method that uses a double\napplication of Tweedie's formula and a consistency loss function that allows us\nto extend sampling at noise levels below the observed data noise. We also\nprovide further evidence that diffusion models memorize from their training\nsets by identifying extremely corrupted images that are almost perfectly\nreconstructed, raising copyright and privacy concerns. Our method for training\nusing corrupted samples can be used to mitigate this problem. We demonstrate\nthis by fine-tuning Stable Diffusion XL to generate samples from a distribution\nusing only noisy samples. Our framework reduces the amount of memorization of\nthe fine-tuning dataset, while maintaining competitive performance.\n","authors":["Giannis Daras","Alexandros G. Dimakis","Constantinos Daskalakis"],"pdf_url":"https://arxiv.org/pdf/2404.10177v2.pdf","comment":"Accepted to ICML 2024"},{"id":"http://arxiv.org/abs/2404.10527v2","updated":"2024-07-22T11:26:57Z","published":"2024-04-16T12:55:15Z","title":"SPVLoc: Semantic Panoramic Viewport Matching for 6D Camera Localization\n  in Unseen Environments","summary":"  In this paper, we present SPVLoc, a global indoor localization method that\naccurately determines the six-dimensional (6D) camera pose of a query image and\nrequires minimal scene-specific prior knowledge and no scene-specific training.\nOur approach employs a novel matching procedure to localize the perspective\ncamera's viewport, given as an RGB image, within a set of panoramic semantic\nlayout representations of the indoor environment. The panoramas are rendered\nfrom an untextured 3D reference model, which only comprises approximate\nstructural information about room shapes, along with door and window\nannotations. We demonstrate that a straightforward convolutional network\nstructure can successfully achieve image-to-panorama and ultimately\nimage-to-model matching. Through a viewport classification score, we rank\nreference panoramas and select the best match for the query image. Then, a 6D\nrelative pose is estimated between the chosen panorama and query image. Our\nexperiments demonstrate that this approach not only efficiently bridges the\ndomain gap but also generalizes well to previously unseen scenes that are not\npart of the training data. Moreover, it achieves superior localization accuracy\ncompared to the state of the art methods and also estimates more degrees of\nfreedom of the camera pose. Our source code is publicly available at\nhttps://fraunhoferhhi.github.io/spvloc .\n","authors":["Niklas Gard","Anna Hilsmann","Peter Eisert"],"pdf_url":"https://arxiv.org/pdf/2404.10527v2.pdf","comment":"ECCV 2024. 24 pages, 11 figures, 8 tables. Includes paper and\n  supplementary material"},{"id":"http://arxiv.org/abs/2407.14352v2","updated":"2024-07-22T11:20:59Z","published":"2024-07-19T14:34:25Z","title":"Vision-Based Power Line Cables and Pylons Detection for Low Flying\n  Aircrafts","summary":"  Power lines are dangerous for low-flying aircrafts, especially in\nlow-visibility conditions. Thus, a vision-based system able to analyze the\naircraft's surroundings and to provide the pilots with a \"second pair of eyes\"\ncan contribute to enhancing their safety. To this end, we have developed a deep\nlearning approach to jointly detect power line cables and pylons from images\ncaptured at distances of several hundred meters by aircraft-mounted cameras. In\ndoing so, we have combined a modern convolutional architecture with transfer\nlearning and a loss function adapted to curvilinear structure delineation. We\nuse a single network for both detection tasks and demonstrated its performance\non two benchmarking datasets. We have integrated it within an onboard system\nand run it in flight, and have demonstrated with our experiments that it\noutperforms the prior distant cable detection method on both datasets, while\nalso successfully detecting pylons, given their annotations are available for\nthe data.\n","authors":["Jakub Gwizdała","Doruk Oner","Soumava Kumar Roy","Mian Akbar Shah","Ad Eberhard","Ivan Egorov","Philipp Krüsi","Grigory Yakushev","Pascal Fua"],"pdf_url":"https://arxiv.org/pdf/2407.14352v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15540v1","updated":"2024-07-22T11:05:58Z","published":"2024-07-22T11:05:58Z","title":"Differentiable Product Quantization for Memory Efficient Camera\n  Relocalization","summary":"  Camera relocalization relies on 3D models of the scene with a large memory\nfootprint that is incompatible with the memory budget of several applications.\nOne solution to reduce the scene memory size is map compression by removing\ncertain 3D points and descriptor quantization. This achieves high compression\nbut leads to performance drop due to information loss. To address the memory\nperformance trade-off, we train a light-weight scene-specific auto-encoder\nnetwork that performs descriptor quantization-dequantization in an end-to-end\ndifferentiable manner updating both product quantization centroids and network\nparameters through back-propagation. In addition to optimizing the network for\ndescriptor reconstruction, we encourage it to preserve the descriptor-matching\nperformance with margin-based metric loss functions. Results show that for a\nlocal descriptor memory of only 1MB, the synergistic combination of the\nproposed network and map compression achieves the best performance on the\nAachen Day-Night compared to existing compression methods.\n","authors":["Zakaria Laskar","Iaroslav Melekhov","Assia Benbihi","Shuzhe Wang","Juho Kannala"],"pdf_url":"https://arxiv.org/pdf/2407.15540v1.pdf","comment":"Accepted to the European Conference on Computer Vision (ECCV) 2024"},{"id":"http://arxiv.org/abs/2407.11379v2","updated":"2024-07-22T11:04:25Z","published":"2024-07-16T04:49:48Z","title":"Exploring connections of spectral analysis and transfer learning in\n  medical imaging","summary":"  In this paper, we use spectral analysis to investigate transfer learning and\nstudy model sensitivity to frequency shortcuts in medical imaging. By analyzing\nthe power spectrum density of both pre-trained and fine-tuned model gradients,\nas well as artificially generated frequency shortcuts, we observe notable\ndifferences in learning priorities between models pre-trained on natural vs\nmedical images, which generally persist during fine-tuning. We find that when a\nmodel's learning priority aligns with the power spectrum density of an\nartifact, it results in overfitting to that artifact. Based on these\nobservations, we show that source data editing can alter the model's resistance\nto shortcut learning.\n","authors":["Yucheng Lu","Dovile Juodelyte","Jonathan D. Victor","Veronika Cheplygina"],"pdf_url":"https://arxiv.org/pdf/2407.11379v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15531v1","updated":"2024-07-22T10:45:55Z","published":"2024-07-22T10:45:55Z","title":"Double Deep Learning-based Event Data Coding and Classification","summary":"  Event cameras have the ability to capture asynchronous per-pixel brightness\nchanges, called \"events\", offering advantages over traditional frame-based\ncameras for computer vision applications. Efficiently coding event data is\ncritical for transmission and storage, given the significant volume of events.\nThis paper proposes a novel double deep learning-based architecture for both\nevent data coding and classification, using a point cloud-based representation\nfor events. In this context, the conversions from events to point clouds and\nback to events are key steps in the proposed solution, and therefore its impact\nis evaluated in terms of compression and classification performance.\nExperimental results show that it is possible to achieve a classification\nperformance of compressed events which is similar to one of the original\nevents, even after applying a lossy point cloud codec, notably the recent\nlearning-based JPEG Pleno Point Cloud Coding standard, with a clear rate\nreduction. Experimental results also demonstrate that events coded using JPEG\nPCC achieve better classification performance than those coded using the\nconventional lossy MPEG Geometry-based Point Cloud Coding standard.\nFurthermore, the adoption of learning-based coding offers high potential for\nperforming computer vision tasks in the compressed domain, which allows\nskipping the decoding stage while mitigating the impact of coding artifacts.\n","authors":["Abdelrahman Seleem","André F. R. Guarda","Nuno M. M. Rodrigues","Fernando Pereira"],"pdf_url":"https://arxiv.org/pdf/2407.15531v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15526v1","updated":"2024-07-22T10:31:07Z","published":"2024-07-22T10:31:07Z","title":"Synthetic Image Learning: Preserving Performance and Preventing\n  Membership Inference Attacks","summary":"  Generative artificial intelligence has transformed the generation of\nsynthetic data, providing innovative solutions to challenges like data scarcity\nand privacy, which are particularly critical in fields such as medicine.\nHowever, the effective use of this synthetic data to train high-performance\nmodels remains a significant challenge. This paper addresses this issue by\nintroducing Knowledge Recycling (KR), a pipeline designed to optimise the\ngeneration and use of synthetic data for training downstream classifiers. At\nthe heart of this pipeline is Generative Knowledge Distillation (GKD), the\nproposed technique that significantly improves the quality and usefulness of\nthe information provided to classifiers through a synthetic dataset\nregeneration and soft labelling mechanism. The KR pipeline has been tested on a\nvariety of datasets, with a focus on six highly heterogeneous medical image\ndatasets, ranging from retinal images to organ scans. The results show a\nsignificant reduction in the performance gap between models trained on real and\nsynthetic data, with models based on synthetic data outperforming those trained\non real data in some cases. Furthermore, the resulting models show almost\ncomplete immunity to Membership Inference Attacks, manifesting privacy\nproperties missing in models trained with conventional techniques.\n","authors":["Eugenio Lomurno","Matteo Matteucci"],"pdf_url":"https://arxiv.org/pdf/2407.15526v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.14718v3","updated":"2024-07-22T10:18:26Z","published":"2024-01-26T08:59:38Z","title":"A Survey on Video Prediction: From Deterministic to Generative\n  Approaches","summary":"  Video prediction, a fundamental task in computer vision, aims to enable\nmodels to generate sequences of future frames based on existing video content.\nThis task has garnered widespread application across various domains. In this\npaper, we comprehensively survey both historical and contemporary works in this\nfield, encompassing the most widely used datasets and algorithms. Our survey\nscrutinizes the challenges and evolving landscape of video prediction within\nthe realm of computer vision. We propose a novel taxonomy centered on the\nstochastic nature of video prediction algorithms. This taxonomy accentuates the\ngradual transition from deterministic to generative prediction methodologies,\nunderlining significant advancements and shifts in approach.\n","authors":["Ruibo Ming","Zhewei Huang","Zhuoxuan Ju","Jianming Hu","Lihui Peng","Shuchang Zhou"],"pdf_url":"https://arxiv.org/pdf/2401.14718v3.pdf","comment":"under review"},{"id":"http://arxiv.org/abs/2404.07713v2","updated":"2024-07-22T10:09:39Z","published":"2024-04-11T12:59:38Z","title":"Progressive Semantic-Guided Vision Transformer for Zero-Shot Learning","summary":"  Zero-shot learning (ZSL) recognizes the unseen classes by conducting\nvisual-semantic interactions to transfer semantic knowledge from seen classes\nto unseen ones, supported by semantic information (e.g., attributes). However,\nexisting ZSL methods simply extract visual features using a pre-trained network\nbackbone (i.e., CNN or ViT), which fail to learn matched visual-semantic\ncorrespondences for representing semantic-related visual features as lacking of\nthe guidance of semantic information, resulting in undesirable visual-semantic\ninteractions. To tackle this issue, we propose a progressive semantic-guided\nvision transformer for zero-shot learning (dubbed ZSLViT). ZSLViT mainly\nconsiders two properties in the whole network: i) discover the semantic-related\nvisual representations explicitly, and ii) discard the semantic-unrelated\nvisual information. Specifically, we first introduce semantic-embedded token\nlearning to improve the visual-semantic correspondences via semantic\nenhancement and discover the semantic-related visual tokens explicitly with\nsemantic-guided token attention. Then, we fuse low semantic-visual\ncorrespondence visual tokens to discard the semantic-unrelated visual\ninformation for visual enhancement. These two operations are integrated into\nvarious encoders to progressively learn semantic-related visual representations\nfor accurate visual-semantic interactions in ZSL. The extensive experiments\nshow that our ZSLViT achieves significant performance gains on three popular\nbenchmark datasets, i.e., CUB, SUN, and AWA2. Codes are available at:\nhttps://github.com/shiming-chen/ZSLViT .\n","authors":["Shiming Chen","Wenjin Hou","Salman Khan","Fahad Shahbaz Khan"],"pdf_url":"https://arxiv.org/pdf/2404.07713v2.pdf","comment":"Accepted to CVPR'24"},{"id":"http://arxiv.org/abs/2407.15512v1","updated":"2024-07-22T09:58:29Z","published":"2024-07-22T09:58:29Z","title":"Increasing the Robustness of Model Predictions to Missing Sensors in\n  Earth Observation","summary":"  Multi-sensor ML models for EO aim to enhance prediction accuracy by\nintegrating data from various sources. However, the presence of missing data\nposes a significant challenge, particularly in non-persistent sensors that can\nbe affected by external factors. Existing literature has explored strategies\nlike temporal dropout and sensor-invariant models to address the generalization\nto missing data issues. Inspired by these works, we study two novel methods\ntailored for multi-sensor scenarios, namely Input Sensor Dropout (ISensD) and\nEnsemble Sensor Invariant (ESensI). Through experimentation on three\nmulti-sensor temporal EO datasets, we demonstrate that these methods\neffectively increase the robustness of model predictions to missing sensors.\nParticularly, we focus on how the predictive performance of models drops when\nsensors are missing at different levels. We observe that ensemble multi-sensor\nmodels are the most robust to the lack of sensors. In addition, the sensor\ndropout component in ISensD shows promising robustness results.\n","authors":["Francisco Mena","Diego Arenas","Andreas Dengel"],"pdf_url":"https://arxiv.org/pdf/2407.15512v1.pdf","comment":"Accepted at the MACLEAN workshop in the ECML/PKDD 2024"},{"id":"http://arxiv.org/abs/2407.13863v2","updated":"2024-07-22T09:58:19Z","published":"2024-07-18T19:16:22Z","title":"A Closer Look at GAN Priors: Exploiting Intermediate Features for\n  Enhanced Model Inversion Attacks","summary":"  Model Inversion (MI) attacks aim to reconstruct privacy-sensitive training\ndata from released models by utilizing output information, raising extensive\nconcerns about the security of Deep Neural Networks (DNNs). Recent advances in\ngenerative adversarial networks (GANs) have contributed significantly to the\nimproved performance of MI attacks due to their powerful ability to generate\nrealistic images with high fidelity and appropriate semantics. However,\nprevious MI attacks have solely disclosed private information in the latent\nspace of GAN priors, limiting their semantic extraction and transferability\nacross multiple target models and datasets. To address this challenge, we\npropose a novel method, Intermediate Features enhanced Generative Model\nInversion (IF-GMI), which disassembles the GAN structure and exploits features\nbetween intermediate blocks. This allows us to extend the optimization space\nfrom latent code to intermediate features with enhanced expressive\ncapabilities. To prevent GAN priors from generating unrealistic images, we\napply a L1 ball constraint to the optimization process. Experiments on multiple\nbenchmarks demonstrate that our method significantly outperforms previous\napproaches and achieves state-of-the-art results under various settings,\nespecially in the out-of-distribution (OOD) scenario. Our code is available at:\nhttps://github.com/final-solution/IF-GMI\n","authors":["Yixiang Qiu","Hao Fang","Hongyao Yu","Bin Chen","MeiKang Qiu","Shu-Tao Xia"],"pdf_url":"https://arxiv.org/pdf/2407.13863v2.pdf","comment":"ECCV 2024"},{"id":"http://arxiv.org/abs/2402.11530v3","updated":"2024-07-22T09:54:40Z","published":"2024-02-18T10:09:10Z","title":"Efficient Multimodal Learning from Data-centric Perspective","summary":"  Multimodal Large Language Models (MLLMs) have demonstrated notable\ncapabilities in general visual understanding and reasoning tasks. However,\ntheir deployment is hindered by substantial computational costs in both\ntraining and inference, limiting accessibility to the broader research and user\ncommunities. A straightforward solution is to leverage smaller pre-trained\nvision and language models, which inevitably cause significant performance\ndrops. In this paper, we demonstrate the possibility of training a smaller but\nbetter MLLM with high-quality training data. Specifically, we introduce Bunny,\na family of lightweight MLLMs with flexible vision and language backbones for\nefficient multimodal learning from selected training data. Experiments show\nthat our Bunny-4B/8B outperforms the state-of-the-art large MLLMs on multiple\nbenchmarks. We expect that this work can provide the community with a clean and\nflexible open-source tool for further research and development. The code,\nmodels, and data can be found in https://github.com/BAAI-DCAI/Bunny.\n","authors":["Muyang He","Yexin Liu","Boya Wu","Jianhao Yuan","Yueze Wang","Tiejun Huang","Bo Zhao"],"pdf_url":"https://arxiv.org/pdf/2402.11530v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15507v1","updated":"2024-07-22T09:44:35Z","published":"2024-07-22T09:44:35Z","title":"SpotDiffusion: A Fast Approach For Seamless Panorama Generation Over\n  Time","summary":"  Generating high-resolution images with generative models has recently been\nmade widely accessible by leveraging diffusion models pre-trained on\nlarge-scale datasets. Various techniques, such as MultiDiffusion and\nSyncDiffusion, have further pushed image generation beyond training\nresolutions, i.e., from square images to panorama, by merging multiple\noverlapping diffusion paths or employing gradient descent to maintain\nperceptual coherence. However, these methods suffer from significant\ncomputational inefficiencies due to generating and averaging numerous\npredictions, which is required in practice to produce high-quality and seamless\nimages. This work addresses this limitation and presents a novel approach that\neliminates the need to generate and average numerous overlapping denoising\npredictions. Our method shifts non-overlapping denoising windows over time,\nensuring that seams in one timestep are corrected in the next. This results in\ncoherent, high-resolution images with fewer overall steps. We demonstrate the\neffectiveness of our approach through qualitative and quantitative evaluations,\ncomparing it with MultiDiffusion, SyncDiffusion, and StitchDiffusion. Our\nmethod offers several key benefits, including improved computational efficiency\nand faster inference times while producing comparable or better image quality.\n","authors":["Stanislav Frolov","Brian B. Moser","Andreas Dengel"],"pdf_url":"https://arxiv.org/pdf/2407.15507v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10179v2","updated":"2024-07-22T09:41:57Z","published":"2024-07-14T12:30:32Z","title":"CLIP-Guided Networks for Transferable Targeted Attacks","summary":"  Transferable targeted adversarial attacks aim to mislead models into\noutputting adversary-specified predictions in black-box scenarios. Recent\nstudies have introduced \\textit{single-target} generative attacks that train a\ngenerator for each target class to generate highly transferable perturbations,\nresulting in substantial computational overhead when handling multiple classes.\n\\textit{Multi-target} attacks address this by training only one\nclass-conditional generator for multiple classes. However, the generator simply\nuses class labels as conditions, failing to leverage the rich semantic\ninformation of the target class. To this end, we design a \\textbf{C}LIP-guided\n\\textbf{G}enerative \\textbf{N}etwork with \\textbf{C}ross-attention modules\n(CGNC) to enhance multi-target attacks by incorporating textual knowledge of\nCLIP into the generator. Extensive experiments demonstrate that CGNC yields\nsignificant improvements over previous multi-target generative attacks, e.g., a\n21.46\\% improvement in success rate from ResNet-152 to DenseNet-121. Moreover,\nwe propose a masked fine-tuning mechanism to further strengthen our method in\nattacking a single class, which surpasses existing single-target methods.\n","authors":["Hao Fang","Jiawei Kong","Bin Chen","Tao Dai","Hao Wu","Shu-Tao Xia"],"pdf_url":"https://arxiv.org/pdf/2407.10179v2.pdf","comment":"ECCV 2024"},{"id":"http://arxiv.org/abs/2407.04538v3","updated":"2024-07-22T09:41:39Z","published":"2024-07-05T14:24:37Z","title":"PDiscoFormer: Relaxing Part Discovery Constraints with Vision\n  Transformers","summary":"  Computer vision methods that explicitly detect object parts and reason on\nthem are a step towards inherently interpretable models. Existing approaches\nthat perform part discovery driven by a fine-grained classification task make\nvery restrictive assumptions on the geometric properties of the discovered\nparts; they should be small and compact. Although this prior is useful in some\ncases, in this paper we show that pre-trained transformer-based vision models,\nsuch as self-supervised DINOv2 ViT, enable the relaxation of these constraints.\nIn particular, we find that a total variation (TV) prior, which allows for\nmultiple connected components of any size, substantially outperforms previous\nwork. We test our approach on three fine-grained classification benchmarks:\nCUB, PartImageNet and Oxford Flowers, and compare our results to previously\npublished methods as well as a re-implementation of the state-of-the-art method\nPDiscoNet with a transformer-based backbone. We consistently obtain substantial\nimprovements across the board, both on part discovery metrics and the\ndownstream classification task, showing that the strong inductive biases in\nself-supervised ViT models require to rethink the geometric priors that can be\nused for unsupervised part discovery.\n","authors":["Ananthu Aniraj","Cassio F. Dantas","Dino Ienco","Diego Marcos"],"pdf_url":"https://arxiv.org/pdf/2407.04538v3.pdf","comment":"Accepted as a main conference paper at the European Conference of\n  Computer Vision (ECCV) 2024"},{"id":"http://arxiv.org/abs/2405.15434v2","updated":"2024-07-22T09:37:36Z","published":"2024-05-24T11:02:55Z","title":"Biometrics and Behavior Analysis for Detecting Distractions in\n  e-Learning","summary":"  In this article, we explore computer vision approaches to detect abnormal\nhead pose during e-learning sessions and we introduce a study on the effects of\nmobile phone usage during these sessions. We utilize behavioral data collected\nfrom 120 learners monitored while participating in a MOOC learning sessions.\nOur study focuses on the influence of phone-usage events on behavior and\nphysiological responses, specifically attention, heart rate, and meditation,\nbefore, during, and after phone usage. Additionally, we propose an approach for\nestimating head pose events using images taken by the webcam during the MOOC\nlearning sessions to detect phone-usage events. Our hypothesis suggests that\nhead posture undergoes significant changes when learners interact with a mobile\nphone, contrasting with the typical behavior seen when learners face a computer\nduring e-learning sessions. We propose an approach designed to detect\ndeviations in head posture from the average observed during a learner's\nsession, operating as a semi-supervised method. This system flags events\nindicating alterations in head posture for subsequent human review and\nselection of mobile phone usage occurrences with a sensitivity over 90%.\n","authors":["Álvaro Becerra","Javier Irigoyen","Roberto Daza","Ruth Cobos","Aythami Morales","Julian Fierrez","Mutlu Cukurova"],"pdf_url":"https://arxiv.org/pdf/2405.15434v2.pdf","comment":"Accepted in CEDI 2024 (VII Congreso Espa\\~nol de Inform\\'atica), A\n  Coru\\~na, Spain"},{"id":"http://arxiv.org/abs/2407.15502v1","updated":"2024-07-22T09:35:43Z","published":"2024-07-22T09:35:43Z","title":"WebRPG: Automatic Web Rendering Parameters Generation for Visual\n  Presentation","summary":"  In the era of content creation revolution propelled by advancements in\ngenerative models, the field of web design remains unexplored despite its\ncritical role in modern digital communication. The web design process is\ncomplex and often time-consuming, especially for those with limited expertise.\nIn this paper, we introduce Web Rendering Parameters Generation (WebRPG), a new\ntask that aims at automating the generation for visual presentation of web\npages based on their HTML code. WebRPG would contribute to a faster web\ndevelopment workflow. Since there is no existing benchmark available, we\ndevelop a new dataset for WebRPG through an automated pipeline. Moreover, we\npresent baseline models, utilizing VAE to manage numerous elements and\nrendering parameters, along with custom HTML embedding for capturing essential\nsemantic and hierarchical information from HTML. Extensive experiments,\nincluding customized quantitative evaluations for this specific task, are\nconducted to evaluate the quality of the generated results.\n","authors":["Zirui Shao","Feiyu Gao","Hangdi Xing","Zepeng Zhu","Zhi Yu","Jiajun Bu","Qi Zheng","Cong Yao"],"pdf_url":"https://arxiv.org/pdf/2407.15502v1.pdf","comment":"Accepted at ECCV 2024. The dataset and code can be accessed at\n  https://github.com/AlibabaResearch/AdvancedLiterateMachinery/tree/main/DocumentUnderstanding/WebRPG"},{"id":"http://arxiv.org/abs/2407.15500v1","updated":"2024-07-22T09:31:30Z","published":"2024-07-22T09:31:30Z","title":"TextureCrop: Enhancing Synthetic Image Detection through Texture-based\n  Cropping","summary":"  Generative AI technologies produce hyper-realistic imagery that can be used\nfor nefarious purposes such as producing misleading or harmful content, among\nothers. This makes Synthetic Image Detection (SID) an essential tool for\ndefending against AI-generated harmful content. Current SID methods typically\nresize input images to a fixed resolution or perform center-cropping due to\ncomputational concerns, leading to challenges in effectively detecting\nartifacts in high-resolution images. To this end, we propose TextureCrop, a\nnovel image pre-processing technique. By focusing on high-frequency image parts\nwhere generation artifacts are prevalent, TextureCrop effectively enhances SID\naccuracy while maintaining manageable memory requirements. Experimental results\ndemonstrate a consistent improvement in AUC across various detectors by 5.7%\ncompared to center cropping and by 14% compared to resizing, across\nhigh-resolution images from the Forensynths and Synthbuster datasets.\n","authors":["Despina Konstantinidou","Christos Koutlis","Symeon Papadopoulos"],"pdf_url":"https://arxiv.org/pdf/2407.15500v1.pdf","comment":"17 pages, 6 images"},{"id":"http://arxiv.org/abs/2405.12713v2","updated":"2024-07-22T09:23:26Z","published":"2024-05-21T12:04:56Z","title":"Dynamic Identity-Guided Attention Network for Visible-Infrared Person\n  Re-identification","summary":"  Visible-infrared person re-identification (VI-ReID) aims to match people with\nthe same identity between visible and infrared modalities. VI-ReID is a\nchallenging task due to the large differences in individual appearance under\ndifferent modalities. Existing methods generally try to bridge the cross-modal\ndifferences at image or feature level, which lacks exploring the discriminative\nembeddings. Effectively minimizing these cross-modal discrepancies relies on\nobtaining representations that are guided by identity and consistent across\nmodalities, while also filtering out representations that are irrelevant to\nidentity. To address these challenges, we introduce a dynamic identity-guided\nattention network (DIAN) to mine identity-guided and modality-consistent\nembeddings, facilitating effective bridging the gap between different\nmodalities. Specifically, in DIAN, to pursue a semantically richer\nrepresentation, we first use orthogonal projection to fuse the features from\ntwo connected coarse and fine layers. Furthermore, we first use dynamic\nconvolution kernels to mine identity-guided and modality-consistent\nrepresentations. More notably, a cross embedding balancing loss is introduced\nto effectively bridge cross-modal discrepancies by above embeddings.\nExperimental results on SYSU-MM01 and RegDB datasets show that DIAN achieves\nstate-of-the-art performance. Specifically, for indoor search on SYSU-MM01, our\nmethod achieves 86.28% rank-1 accuracy and 87.41% mAP, respectively. Our code\nwill be available soon.\n","authors":["Peng Gao","Yujian Lee","Hui Zhang","Xubo Liu","Yiyang Hu","Guquan Jing"],"pdf_url":"https://arxiv.org/pdf/2405.12713v2.pdf","comment":"I need to further debug my code to improve accuracy"},{"id":"http://arxiv.org/abs/2305.14668v3","updated":"2024-07-22T09:21:12Z","published":"2023-05-24T03:20:09Z","title":"NOVUM: Neural Object Volumes for Robust Object Classification","summary":"  Discriminative models for object classification typically learn image-based\nrepresentations that do not capture the compositional and 3D nature of objects.\nIn this work, we show that explicitly integrating 3D compositional object\nrepresentations into deep networks for image classification leads to a largely\nenhanced generalization in out-of-distribution scenarios. In particular, we\nintroduce a novel architecture, referred to as \\OURS, that consists of a\nfeature extractor and a \\textit{neural object volume} for every target object\nclass. Each neural object volume is a composition of 3D Gaussians that emit\nfeature vectors. This compositional object representation allows for a highly\nrobust and fast estimation of the object class by independently matching the\nfeatures of the 3D Gaussians of each category to features extracted from an\ninput image. Additionally, the object pose can be estimated via inverse\nrendering of the corresponding neural object volume. To enable the\nclassification of objects, the neural features at each 3D Gaussian are trained\ndiscriminatively to be distinct from (i) the features of 3D Gaussians in other\ncategories, (ii) features of other 3D Gaussians of the same object, and (iii)\nthe background features. Our experiments show that \\OURS offers intriguing\nadvantages over standard architectures due to the 3D compositional structure of\nthe object representation, namely: (1) An exceptional robustness across a\nspectrum of real-world and synthetic out-of-distribution shifts and (2) an\nenhanced human interpretability compared to standard models, all while\nmaintaining real-time inference and a competitive accuracy on in-distribution\ndata.\n","authors":["Artur Jesslen","Guofeng Zhang","Angtian Wang","Wufei Ma","Alan Yuille","Adam Kortylewski"],"pdf_url":"https://arxiv.org/pdf/2305.14668v3.pdf","comment":"14 pages, 4 figures, accepted at ECCV 2024, code is accessible at\n  https://github.com/GenIntel/NOVUM"},{"id":"http://arxiv.org/abs/2401.02957v2","updated":"2024-07-22T09:07:27Z","published":"2024-01-05T18:59:52Z","title":"Denoising Vision Transformers","summary":"  We study a crucial yet often overlooked issue inherent to Vision Transformers\n(ViTs): feature maps of these models exhibit grid-like artifacts, which hurt\nthe performance of ViTs in downstream dense prediction tasks such as semantic\nsegmentation, depth prediction, and object discovery. We trace this issue down\nto the positional embeddings at the input stage. To mitigate this, we propose a\ntwo-stage denoising approach, termed Denoising Vision Transformers (DVT). In\nthe first stage, we separate the clean features from those contaminated by\npositional artifacts by enforcing cross-view feature consistency with neural\nfields on a per-image basis. This per-image optimization process extracts\nartifact-free features from raw ViT outputs, providing clean feature estimates\nfor offline applications. In the second stage, we train a lightweight\ntransformer block to predict clean features from raw ViT outputs, leveraging\nthe derived estimates of the clean features as supervision. Our method, DVT,\ndoes not require re-training the existing pre-trained ViTs, and is immediately\napplicable to any Vision Transformer architecture. We evaluate our method on a\nvariety of representative ViTs (DINO, DeiT-III, EVA02, CLIP, DINOv2,\nDINOv2-reg) and demonstrate that DVT consistently improves existing\nstate-of-the-art general-purpose models in semantic and geometric tasks across\nmultiple datasets. We hope our study will encourage a re-evaluation of ViT\ndesign, especially regarding the naive use of positional embeddings. Our code\nand checkpoints are publicly available.\n","authors":["Jiawei Yang","Katie Z Luo","Jiefeng Li","Congyue Deng","Leonidas Guibas","Dilip Krishnan","Kilian Q Weinberger","Yonglong Tian","Yue Wang"],"pdf_url":"https://arxiv.org/pdf/2401.02957v2.pdf","comment":"Accepted to ECCV2024. Project website:\n  https://jiawei-yang.github.io/DenoisingViT/"},{"id":"http://arxiv.org/abs/2407.15488v1","updated":"2024-07-22T09:05:16Z","published":"2024-07-22T09:05:16Z","title":"DiffX: Guide Your Layout to Cross-Modal Generative Modeling","summary":"  Diffusion models have made significant strides in text-driven and\nlayout-driven image generation. However, most diffusion models are limited to\nvisible RGB image generation. In fact, human perception of the world is\nenriched by diverse viewpoints, including chromatic contrast, thermal\nillumination, and depth information. In this paper, we introduce a novel\ndiffusion model for general layout-guided cross-modal \"RGB+X\" generation,\ncalled DiffX. We firstly construct the cross-modal image datasets with text\ndescriptions using the LLaVA model for image captioning, supplemented by manual\ncorrections. Notably, DiffX presents a simple yet effective cross-modal\ngenerative modeling pipeline, which conducts diffusion and denoising processes\nin the modality-shared latent space, facilitated by our Dual-Path Variational\nAutoEncoder (DP-VAE). Furthermore, we incorporate the gated cross-attention\nmechanism to connect the layout and text conditions, leveraging Long-CLIP for\nembedding long captions to enhance user guidance. Through extensive\nexperiments, DiffX demonstrates robustness and flexibility in cross-modal\ngeneration across three RGB+X datasets: FLIR, MFNet, and COME15K, guided by\nvarious layout types. It also shows the potential for adaptive generation of\n\"RGB+X+Y\" or more diverse modalities. Our code and processed image captions are\navailable at https://github.com/zeyuwang-zju/DiffX.\n","authors":["Zeyu Wang","Jingyu Lin","Yifei Qian","Yi Huang","Shicen Tian","Bosong Chai","Juncan Deng","Lan Du","Cunjian Chen","Yufei Guo","Kejie Huang"],"pdf_url":"https://arxiv.org/pdf/2407.15488v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15487v1","updated":"2024-07-22T09:03:29Z","published":"2024-07-22T09:03:29Z","title":"In-Context Learning Improves Compositional Understanding of\n  Vision-Language Models","summary":"  Vision-Language Models (VLMs) have shown remarkable capabilities in a large\nnumber of downstream tasks. Nonetheless, compositional image understanding\nremains a rather difficult task due to the object bias present in training\ndata. In this work, we investigate the reasons for such a lack of capability by\nperforming an extensive bench-marking of compositional understanding in VLMs.\nWe compare contrastive models with generative ones and analyze their\ndifferences in architecture, pre-training data, and training tasks and losses.\nFurthermore, we leverage In-Context Learning (ICL) as a way to improve the\nability of VLMs to perform more complex reasoning and understanding given an\nimage. Our extensive experiments demonstrate that our proposed approach\noutperforms baseline models across multiple compositional understanding\ndatasets.\n","authors":["Matteo Nulli","Anesa Ibrahimi","Avik Pal","Hoshe Lee","Ivona Najdenkoska"],"pdf_url":"https://arxiv.org/pdf/2407.15487v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15485v1","updated":"2024-07-22T08:57:19Z","published":"2024-07-22T08:57:19Z","title":"Subthalamic Nucleus segmentation in high-field Magnetic Resonance data.\n  Is space normalization by template co-registration necessary?","summary":"  Deep Brain Stimulation (DBS) is one of the most successful methods to\ndiminish late-stage Parkinson's Disease (PD) symptoms. It is a delicate\nsurgical procedure which requires detailed pre-surgical patient's study.\nHigh-field Magnetic Resonance Imaging (MRI) has proven its improved capacity of\ncapturing the Subthalamic Nucleus (STN) - the main target of DBS in PD - in\ngreater detail than lower field images. Here, we present a comparison between\nthe performance of two different Deep Learning (DL) automatic segmentation\narchitectures, one based in the registration to a brain template and the other\nperforming the segmentation in in the MRI acquisition native space. The study\nwas based on publicly available high-field 7 Tesla (T) brain MRI datasets of\nT1-weighted and T2-weighted sequences. nnUNet was used on the segmentation step\nof both architectures, while the data pre and post-processing pipelines\ndiverged. The evaluation metrics showed that the performance of the\nsegmentation directly in the native space yielded better results for the STN\nsegmentation, despite not showing any advantage over the template-based method\nfor the to other analysed structures: the Red Nucleus (RN) and the Substantia\nNigra (SN).\n","authors":["Tomás Lima","Igor Varga","Eduard Bakštein","Daniel Novák","Victor Alves"],"pdf_url":"https://arxiv.org/pdf/2407.15485v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15484v1","updated":"2024-07-22T08:55:46Z","published":"2024-07-22T08:55:46Z","title":"6DGS: 6D Pose Estimation from a Single Image and a 3D Gaussian Splatting\n  Model","summary":"  We propose 6DGS to estimate the camera pose of a target RGB image given a 3D\nGaussian Splatting (3DGS) model representing the scene. 6DGS avoids the\niterative process typical of analysis-by-synthesis methods (e.g. iNeRF) that\nalso require an initialization of the camera pose in order to converge.\nInstead, our method estimates a 6DoF pose by inverting the 3DGS rendering\nprocess. Starting from the object surface, we define a radiant Ellicell that\nuniformly generates rays departing from each ellipsoid that parameterize the\n3DGS model. Each Ellicell ray is associated with the rendering parameters of\neach ellipsoid, which in turn is used to obtain the best bindings between the\ntarget image pixels and the cast rays. These pixel-ray bindings are then ranked\nto select the best scoring bundle of rays, which their intersection provides\nthe camera center and, in turn, the camera rotation. The proposed solution\nobviates the necessity of an \"a priori\" pose for initialization, and it solves\n6DoF pose estimation in closed form, without the need for iterations. Moreover,\ncompared to the existing Novel View Synthesis (NVS) baselines for pose\nestimation, 6DGS can improve the overall average rotational accuracy by 12% and\ntranslation accuracy by 22% on real scenes, despite not requiring any\ninitialization pose. At the same time, our method operates near real-time,\nreaching 15fps on consumer hardware.\n","authors":["Matteo Bortolon","Theodore Tsesmelis","Stuart James","Fabio Poiesi","Alessio Del Bue"],"pdf_url":"https://arxiv.org/pdf/2407.15484v1.pdf","comment":"Project page: https://mbortolon97.github.io/6dgs/ Accepted to ECCV\n  2024"},{"id":"http://arxiv.org/abs/2407.15481v1","updated":"2024-07-22T08:51:11Z","published":"2024-07-22T08:51:11Z","title":"Diverse Image Harmonization","summary":"  Image harmonization aims to adjust the foreground illumination in a composite\nimage to make it harmonious. The existing harmonization methods can only\nproduce one deterministic result for a composite image, ignoring that a\ncomposite image could have multiple plausible harmonization results due to\nmultiple plausible reflectances. In this work, we first propose a\nreflectance-guided harmonization network, which can achieve better performance\nwith the guidance of ground-truth foreground reflectance. Then, we also design\na diverse reflectance generation network to predict multiple plausible\nforeground reflectances, leading to multiple plausible harmonization results.\nThe extensive experiments on the benchmark datasets demonstrate the\neffectiveness of our method.\n","authors":["Xinhao Tao","Tianyuan Qiu","Junyan Cao","Li Niu"],"pdf_url":"https://arxiv.org/pdf/2407.15481v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15479v1","updated":"2024-07-22T08:46:20Z","published":"2024-07-22T08:46:20Z","title":"Affordance Labeling and Exploration: A Manifold-Based Approach","summary":"  The advancement in computing power has significantly reduced the training\ntimes for deep learning, fostering the rapid development of networks designed\nfor object recognition. However, the exploration of object utility, which is\nthe affordance of the object, as opposed to object recognition, has received\ncomparatively less attention. This work focuses on the problem of exploration\nof object affordances using existing networks trained on the object\nclassification dataset. While pre-trained networks have proven to be\ninstrumental in transfer learning for classification tasks, this work diverges\nfrom conventional object classification methods. Instead, it employs\npre-trained networks to discern affordance labels without the need for\nspecialized layers, abstaining from modifying the final layers through the\naddition of classification layers. To facilitate the determination of\naffordance labels without such modifications, two approaches, i.e. subspace\nclustering and manifold curvature methods are tested. These methods offer a\ndistinct perspective on affordance label recognition. Especially, manifold\ncurvature method has been successfully tested with nine distinct pre-trained\nnetworks, each achieving an accuracy exceeding 95%. Moreover, it is observed\nthat manifold curvature and subspace clustering methods explore affordance\nlabels that are not marked in the ground truth, but object affords in various\ncases.\n","authors":["İsmail Özçil","A. Buğra Koku"],"pdf_url":"https://arxiv.org/pdf/2407.15479v1.pdf","comment":"17 Pages, 3 Figures, 3 Tables"},{"id":"http://arxiv.org/abs/2404.09342v3","updated":"2024-07-22T08:43:12Z","published":"2024-04-14T19:51:32Z","title":"Face-voice Association in Multilingual Environments (FAME) Challenge\n  2024 Evaluation Plan","summary":"  The advancements of technology have led to the use of multimodal systems in\nvarious real-world applications. Among them, the audio-visual systems are one\nof the widely used multimodal systems. In the recent years, associating face\nand voice of a person has gained attention due to presence of unique\ncorrelation between them. The Face-voice Association in Multilingual\nEnvironments (FAME) Challenge 2024 focuses on exploring face-voice association\nunder a unique condition of multilingual scenario. This condition is inspired\nfrom the fact that half of the world's population is bilingual and most often\npeople communicate under multilingual scenario. The challenge uses a dataset\nnamely, Multilingual Audio-Visual (MAV-Celeb) for exploring face-voice\nassociation in multilingual environments. This report provides the details of\nthe challenge, dataset, baselines and task details for the FAME Challenge.\n","authors":["Muhammad Saad Saeed","Shah Nawaz","Muhammad Salman Tahir","Rohan Kumar Das","Muhammad Zaigham Zaheer","Marta Moscati","Markus Schedl","Muhammad Haris Khan","Karthik Nandakumar","Muhammad Haroon Yousaf"],"pdf_url":"https://arxiv.org/pdf/2404.09342v3.pdf","comment":"ACM Multimedia Conference - Grand Challenge"},{"id":"http://arxiv.org/abs/2407.15472v1","updated":"2024-07-22T08:35:41Z","published":"2024-07-22T08:35:41Z","title":"Learning deep illumination-robust features from multispectral filter\n  array images","summary":"  Multispectral (MS) snapshot cameras equipped with a MS filter array (MSFA),\ncapture multiple spectral bands in a single shot, resulting in a raw mosaic\nimage where each pixel holds only one channel value. The fully-defined MS image\nis estimated from the raw one through $\\textit{demosaicing}$, which inevitably\nintroduces spatio-spectral artifacts. Moreover, training on fully-defined MS\nimages can be computationally intensive, particularly with deep neural networks\n(DNNs), and may result in features lacking discrimination power due to\nsuboptimal learning of spatio-spectral interactions. Furthermore, outdoor MS\nimage acquisition occurs under varying lighting conditions, leading to\nillumination-dependent features. This paper presents an original approach to\nlearn discriminant and illumination-robust features directly from raw images.\nIt involves: $\\textit{raw spectral constancy}$ to mitigate the impact of\nillumination, $\\textit{MSFA-preserving}$ transformations suited for raw image\naugmentation to train DNNs on diverse raw textures, and $\\textit{raw-mixing}$\nto capture discriminant spatio-spectral interactions in raw images. Experiments\non MS image classification show that our approach outperforms both handcrafted\nand recent deep learning-based methods, while also requiring significantly less\ncomputational effort.~The source code will be available.\n","authors":["Anis Amziane"],"pdf_url":"https://arxiv.org/pdf/2407.15472v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10217v3","updated":"2024-07-22T08:30:53Z","published":"2023-12-15T21:30:49Z","title":"T-MAE: Temporal Masked Autoencoders for Point Cloud Representation\n  Learning","summary":"  The scarcity of annotated data in LiDAR point cloud understanding hinders\neffective representation learning. Consequently, scholars have been actively\ninvestigating efficacious self-supervised pre-training paradigms. Nevertheless,\ntemporal information, which is inherent in the LiDAR point cloud sequence, is\nconsistently disregarded. To better utilize this property, we propose an\neffective pre-training strategy, namely Temporal Masked Auto-Encoders (T-MAE),\nwhich takes as input temporally adjacent frames and learns temporal dependency.\nA SiamWCA backbone, containing a Siamese encoder and a windowed cross-attention\n(WCA) module, is established for the two-frame input. Considering that the\nmovement of an ego-vehicle alters the view of the same instance, temporal\nmodeling also serves as a robust and natural data augmentation, enhancing the\ncomprehension of target objects. SiamWCA is a powerful architecture but heavily\nrelies on annotated data. Our T-MAE pre-training strategy alleviates its demand\nfor annotated data. Comprehensive experiments demonstrate that T-MAE achieves\nthe best performance on both Waymo and ONCE datasets among competitive\nself-supervised approaches. Codes will be released at\nhttps://github.com/codename1995/T-MAE\n","authors":["Weijie Wei","Fatemeh Karimi Nejadasl","Theo Gevers","Martin R. Oswald"],"pdf_url":"https://arxiv.org/pdf/2312.10217v3.pdf","comment":"Accepted to ECCV 2024"},{"id":"http://arxiv.org/abs/2301.04494v5","updated":"2024-07-22T08:16:26Z","published":"2023-01-11T14:42:47Z","title":"Multi-label Image Classification using Adaptive Graph Convolutional\n  Networks: from a Single Domain to Multiple Domains","summary":"  This paper proposes an adaptive graph-based approach for multi-label image\nclassification. Graph-based methods have been largely exploited in the field of\nmulti-label classification, given their ability to model label correlations.\nSpecifically, their effectiveness has been proven not only when considering a\nsingle domain but also when taking into account multiple domains. However, the\ntopology of the used graph is not optimal as it is pre-defined heuristically.\nIn addition, consecutive Graph Convolutional Network (GCN) aggregations tend to\ndestroy the feature similarity. To overcome these issues, an architecture for\nlearning the graph connectivity in an end-to-end fashion is introduced. This is\ndone by integrating an attention-based mechanism and a similarity-preserving\nstrategy. The proposed framework is then extended to multiple domains using an\nadversarial training scheme. Numerous experiments are reported on well-known\nsingle-domain and multi-domain benchmarks. The results demonstrate that our\napproach achieves competitive results in terms of mean Average Precision (mAP)\nand model size as compared to the state-of-the-art. The code will be made\npublicly available.\n","authors":["Indel Pal Singh","Enjie Ghorbel","Oyebade Oyedotun","Djamila Aouada"],"pdf_url":"https://arxiv.org/pdf/2301.04494v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15451v1","updated":"2024-07-22T08:09:14Z","published":"2024-07-22T08:09:14Z","title":"Domain-Adaptive 2D Human Pose Estimation via Dual Teachers in Extremely\n  Low-Light Conditions","summary":"  Existing 2D human pose estimation research predominantly concentrates on\nwell-lit scenarios, with limited exploration of poor lighting conditions, which\nare a prevalent aspect of daily life. Recent studies on low-light pose\nestimation require the use of paired well-lit and low-light images with ground\ntruths for training, which are impractical due to the inherent challenges\nassociated with annotation on low-light images. To this end, we introduce a\nnovel approach that eliminates the need for low-light ground truths. Our\nprimary novelty lies in leveraging two complementary-teacher networks to\ngenerate more reliable pseudo labels, enabling our model achieves competitive\nperformance on extremely low-light images without the need for training with\nlow-light ground truths. Our framework consists of two stages. In the first\nstage, our model is trained on well-lit data with low-light augmentations. In\nthe second stage, we propose a dual-teacher framework to utilize the unlabeled\nlow-light data, where a center-based main teacher produces the pseudo labels\nfor relatively visible cases, while a keypoints-based complementary teacher\nfocuses on producing the pseudo labels for the missed persons of the main\nteacher. With the pseudo labels from both teachers, we propose a\nperson-specific low-light augmentation to challenge a student model in training\nto outperform the teachers. Experimental results on real low-light dataset\n(ExLPose-OCN) show, our method achieves 6.8% (2.4 AP) improvement over the\nstate-of-the-art (SOTA) method, despite no low-light ground-truth data is used\nin our approach, in contrast to the SOTA method. Our code will be available\nat:https://github.com/ayh015-dev/DA-LLPose.\n","authors":["Yihao Ai","Yifei Qi","Bo Wang","Yu Cheng","Xinchao Wang","Robby T. Tan"],"pdf_url":"https://arxiv.org/pdf/2407.15451v1.pdf","comment":"18 pages, 3 figure. Accepted by ECCV24"},{"id":"http://arxiv.org/abs/2407.15447v1","updated":"2024-07-22T08:04:09Z","published":"2024-07-22T08:04:09Z","title":"SIGMA:Sinkhorn-Guided Masked Video Modeling","summary":"  Video-based pretraining offers immense potential for learning strong visual\nrepresentations on an unprecedented scale. Recently, masked video modeling\nmethods have shown promising scalability, yet fall short in capturing\nhigher-level semantics due to reconstructing predefined low-level targets such\nas pixels. To tackle this, we present Sinkhorn-guided Masked Video Modelling\n(SIGMA), a novel video pretraining method that jointly learns the video model\nin addition to a target feature space using a projection network. However, this\nsimple modification means that the regular L2 reconstruction loss will lead to\ntrivial solutions as both networks are jointly optimized. As a solution, we\ndistribute features of space-time tubes evenly across a limited number of\nlearnable clusters. By posing this as an optimal transport problem, we enforce\nhigh entropy in the generated features across the batch, infusing semantic and\ntemporal meaning into the feature space. The resulting cluster assignments are\nused as targets for a symmetric prediction task where the video model predicts\ncluster assignment of the projection network and vice versa. Experimental\nresults on ten datasets across three benchmarks validate the effectiveness of\nSIGMA in learning more performant, temporally-aware, and robust video\nrepresentations improving upon state-of-the-art methods. Our project website\nwith code is available at: https://quva-lab.github.io/SIGMA.\n","authors":["Mohammadreza Salehi","Michael Dorkenwald","Fida Mohammad Thoker","Efstratios Gavves","Cees G. M. Snoek","Yuki M. Asano"],"pdf_url":"https://arxiv.org/pdf/2407.15447v1.pdf","comment":"Accepted at ECCV 24"},{"id":"http://arxiv.org/abs/2407.15446v1","updated":"2024-07-22T08:00:06Z","published":"2024-07-22T08:00:06Z","title":"Text2Place: Affordance-aware Text Guided Human Placement","summary":"  For a given scene, humans can easily reason for the locations and pose to\nplace objects. Designing a computational model to reason about these\naffordances poses a significant challenge, mirroring the intuitive reasoning\nabilities of humans. This work tackles the problem of realistic human insertion\nin a given background scene termed as \\textbf{Semantic Human Placement}. This\ntask is extremely challenging given the diverse backgrounds, scale, and pose of\nthe generated person and, finally, the identity preservation of the person. We\ndivide the problem into the following two stages \\textbf{i)} learning\n\\textit{semantic masks} using text guidance for localizing regions in the image\nto place humans and \\textbf{ii)} subject-conditioned inpainting to place a\ngiven subject adhering to the scene affordance within the \\textit{semantic\nmasks}. For learning semantic masks, we leverage rich object-scene priors\nlearned from the text-to-image generative models and optimize a novel\nparameterization of the semantic mask, eliminating the need for large-scale\ntraining. To the best of our knowledge, we are the first ones to provide an\neffective solution for realistic human placements in diverse real-world scenes.\nThe proposed method can generate highly realistic scene compositions while\npreserving the background and subject identity. Further, we present results for\nseveral downstream tasks - scene hallucination from a single or multiple\ngenerated persons and text-based attribute editing. With extensive comparisons\nagainst strong baselines, we show the superiority of our method in realistic\nhuman placement.\n","authors":["Rishubh Parihar","Harsh Gupta","Sachidanand VS","R. Venkatesh Babu"],"pdf_url":"https://arxiv.org/pdf/2407.15446v1.pdf","comment":"ECCV 2024, Project Page: https://rishubhpar.github.io/Text2Place/"},{"id":"http://arxiv.org/abs/2312.06721v3","updated":"2024-07-22T07:51:25Z","published":"2023-12-11T03:07:25Z","title":"Understanding Physical Dynamics with Counterfactual World Modeling","summary":"  The ability to understand physical dynamics is critical for agents to act in\nthe world. Here, we use Counterfactual World Modeling (CWM) to extract vision\nstructures for dynamics understanding. CWM uses a temporally-factored masking\npolicy for masked prediction of video data without annotations. This policy\nenables highly effective \"counterfactual prompting\" of the predictor, allowing\na spectrum of visual structures to be extracted from a single pre-trained\npredictor without finetuning on annotated datasets. We demonstrate that these\nstructures are useful for physical dynamics understanding, allowing CWM to\nachieve the state-of-the-art performance on the Physion benchmark.\n","authors":["Rahul Venkatesh","Honglin Chen","Kevin Feigelis","Daniel M. Bear","Khaled Jedoui","Klemen Kotar","Felix Binder","Wanhee Lee","Sherry Liu","Kevin A. Smith","Judith E. Fan","Daniel L. K. Yamins"],"pdf_url":"https://arxiv.org/pdf/2312.06721v3.pdf","comment":"ECCV 2024. Project page at: https://neuroailab.github.io/cwm-physics/"},{"id":"http://arxiv.org/abs/2407.03919v2","updated":"2024-07-22T07:49:34Z","published":"2024-07-04T13:31:47Z","title":"MedRAT: Unpaired Medical Report Generation via Auxiliary Tasks","summary":"  Medical report generation from X-ray images is a challenging task,\nparticularly in an unpaired setting where paired image-report data is\nunavailable for training. To address this challenge, we propose a novel model\nthat leverages the available information in two distinct datasets, one\ncomprising reports and the other consisting of images. The core idea of our\nmodel revolves around the notion that combining auto-encoding report generation\nwith multi-modal (report-image) alignment can offer a solution. However, the\nchallenge persists regarding how to achieve this alignment when pair\ncorrespondence is absent. Our proposed solution involves the use of auxiliary\ntasks, particularly contrastive learning and classification, to position\nrelated images and reports in close proximity to each other. This approach\ndiffers from previous methods that rely on pre-processing steps, such as using\nexternal information stored in a knowledge graph. Our model, named MedRAT,\nsurpasses previous state-of-the-art methods, demonstrating the feasibility of\ngenerating comprehensive medical reports without the need for paired data or\nexternal tools.\n","authors":["Elad Hirsch","Gefen Dawidowicz","Ayellet Tal"],"pdf_url":"https://arxiv.org/pdf/2407.03919v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.14111v3","updated":"2024-07-22T07:38:19Z","published":"2024-01-25T11:46:31Z","title":"Image Synthesis with Graph Conditioning: CLIP-Guided Diffusion Models\n  for Scene Graphs","summary":"  Advancements in generative models have sparked significant interest in\ngenerating images while adhering to specific structural guidelines. Scene graph\nto image generation is one such task of generating images which are consistent\nwith the given scene graph. However, the complexity of visual scenes poses a\nchallenge in accurately aligning objects based on specified relations within\nthe scene graph. Existing methods approach this task by first predicting a\nscene layout and generating images from these layouts using adversarial\ntraining. In this work, we introduce a novel approach to generate images from\nscene graphs which eliminates the need of predicting intermediate layouts. We\nleverage pre-trained text-to-image diffusion models and CLIP guidance to\ntranslate graph knowledge into images. Towards this, we first pre-train our\ngraph encoder to align graph features with CLIP features of corresponding\nimages using a GAN based training. Further, we fuse the graph features with\nCLIP embedding of object labels present in the given scene graph to create a\ngraph consistent CLIP guided conditioning signal. In the conditioning input,\nobject embeddings provide coarse structure of the image and graph features\nprovide structural alignment based on relationships among objects. Finally, we\nfine tune a pre-trained diffusion model with the graph consistent conditioning\nsignal with reconstruction and CLIP alignment loss. Elaborate experiments\nreveal that our method outperforms existing methods on standard benchmarks of\nCOCO-stuff and Visual Genome dataset.\n","authors":["Rameshwar Mishra","A V Subramanyam"],"pdf_url":"https://arxiv.org/pdf/2401.14111v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15435v1","updated":"2024-07-22T07:29:38Z","published":"2024-07-22T07:29:38Z","title":"Enhancement of 3D Gaussian Splatting using Raw Mesh for Photorealistic\n  Recreation of Architectures","summary":"  The photorealistic reconstruction and rendering of architectural scenes have\nextensive applications in industries such as film, games, and transportation.\nIt also plays an important role in urban planning, architectural design, and\nthe city's promotion, especially in protecting historical and cultural relics.\nThe 3D Gaussian Splatting, due to better performance over NeRF, has become a\nmainstream technology in 3D reconstruction. Its only input is a set of images\nbut it relies heavily on geometric parameters computed by the SfM process. At\nthe same time, there is an existing abundance of raw 3D models, that could\ninform the structural perception of certain buildings but cannot be applied. In\nthis paper, we propose a straightforward method to harness these raw 3D models\nto guide 3D Gaussians in capturing the basic shape of the building and improve\nthe visual quality of textures and details when photos are captured\nnon-systematically. This exploration opens up new possibilities for improving\nthe effectiveness of 3D reconstruction techniques in the field of architectural\ndesign.\n","authors":["Ruizhe Wang","Chunliang Hua","Tomakayev Shingys","Mengyuan Niu","Qingxin Yang","Lizhong Gao","Yi Zheng","Junyan Yang","Qiao Wang"],"pdf_url":"https://arxiv.org/pdf/2407.15435v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15429v1","updated":"2024-07-22T07:17:52Z","published":"2024-07-22T07:17:52Z","title":"Learning at a Glance: Towards Interpretable Data-limited Continual\n  Semantic Segmentation via Semantic-Invariance Modelling","summary":"  Continual semantic segmentation (CSS) based on incremental learning (IL) is a\ngreat endeavour in developing human-like segmentation models. However, current\nCSS approaches encounter challenges in the trade-off between preserving old\nknowledge and learning new ones, where they still need large-scale annotated\ndata for incremental training and lack interpretability. In this paper, we\npresent Learning at a Glance (LAG), an efficient, robust, human-like and\ninterpretable approach for CSS. Specifically, LAG is a simple and\nmodel-agnostic architecture, yet it achieves competitive CSS efficiency with\nlimited incremental data. Inspired by human-like recognition patterns, we\npropose a semantic-invariance modelling approach via semantic features\ndecoupling that simultaneously reconciles solid knowledge inheritance and\nnew-term learning. Concretely, the proposed decoupling manner includes two\nways, i.e., channel-wise decoupling and spatial-level neuron-relevant semantic\nconsistency. Our approach preserves semantic-invariant knowledge as solid\nprototypes to alleviate catastrophic forgetting, while also constraining\nsample-specific contents through an asymmetric contrastive learning method to\nenhance model robustness during IL steps. Experimental results in multiple\ndatasets validate the effectiveness of the proposed method. Furthermore, we\nintroduce a novel CSS protocol that better reflects realistic data-limited CSS\nsettings, and LAG achieves superior performance under multiple data-limited\nconditions.\n","authors":["Bo Yuan","Danpei Zhao","Zhenwei Shi"],"pdf_url":"https://arxiv.org/pdf/2407.15429v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15427v1","updated":"2024-07-22T07:08:22Z","published":"2024-07-22T07:08:22Z","title":"YOLO-pdd: A Novel Multi-scale PCB Defect Detection Method Using Deep\n  Representations with Sequential Images","summary":"  With the rapid growth of the PCB manufacturing industry, there is an\nincreasing demand for computer vision inspection to detect defects during\nproduction. Improving the accuracy and generalization of PCB defect detection\nmodels remains a significant challenge. This paper proposes a high-precision,\nrobust, and real-time end-to-end method for PCB defect detection based on deep\nConvolutional Neural Networks (CNN). Traditional methods often suffer from low\naccuracy and limited applicability. We propose a novel approach combining\nYOLOv5 and multiscale modules for hierarchical residual-like connections. In\nPCB defect detection, noise can confuse the background and small targets. The\nYOLOv5 model provides a strong foundation with its real-time processing and\naccurate object detection capabilities. The multi-scale module extends\ntraditional approaches by incorporating hierarchical residual-like connections\nwithin a single block, enabling multiscale feature extraction. This\nplug-and-play module significantly enhances performance by extracting features\nat multiple scales and levels, which are useful for identifying defects of\nvarying sizes and complexities. Our multi-scale architecture integrates feature\nextraction, defect localization, and classification into a unified network.\nExperiments on a large-scale PCB dataset demonstrate significant improvements\nin precision, recall, and F1-score compared to existing methods. This work\nadvances computer vision inspection for PCB defect detection, providing a\nreliable solution for high-precision, robust, real-time, and domain-adaptive\ndefect detection in the PCB manufacturing industry.\n","authors":["Bowen Liu","Dongjie Chen","Xiao Qi"],"pdf_url":"https://arxiv.org/pdf/2407.15427v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.14277v2","updated":"2024-07-22T07:04:04Z","published":"2023-10-22T11:53:56Z","title":"A Survey on Continual Semantic Segmentation: Theory, Challenge, Method\n  and Application","summary":"  Continual learning, also known as incremental learning or life-long learning,\nstands at the forefront of deep learning and AI systems. It breaks through the\nobstacle of one-way training on close sets and enables continuous adaptive\nlearning on open-set conditions. In the recent decade, continual learning has\nbeen explored and applied in multiple fields especially in computer vision\ncovering classification, detection and segmentation tasks. Continual semantic\nsegmentation (CSS), of which the dense prediction peculiarity makes it a\nchallenging, intricate and burgeoning task. In this paper, we present a review\nof CSS, committing to building a comprehensive survey on problem formulations,\nprimary challenges, universal datasets, neoteric theories and multifarious\napplications. Concretely, we begin by elucidating the problem definitions and\nprimary challenges. Based on an in-depth investigation of relevant approaches,\nwe sort out and categorize current CSS models into two main branches including\ndata-replay and data-free sets. In each branch, the corresponding approaches\nare similarity-based clustered and thoroughly analyzed, following qualitative\ncomparison and quantitative reproductions on relevant datasets. Besides, we\nalso introduce four CSS specialities with diverse application scenarios and\ndevelopment tendencies. Furthermore, we develop a benchmark for CSS\nencompassing representative references, evaluation results and reproductions,\nwhich is available at~\\url{https://github.com/YBIO/SurveyCSS}. We hope this\nsurvey can serve as a reference-worthy and stimulating contribution to the\nadvancement of the life-long learning field, while also providing valuable\nperspectives for related fields.\n","authors":["Bo Yuan","Danpei Zhao"],"pdf_url":"https://arxiv.org/pdf/2310.14277v2.pdf","comment":"20 pages, 11 figures"},{"id":"http://arxiv.org/abs/2407.15424v1","updated":"2024-07-22T07:01:50Z","published":"2024-07-22T07:01:50Z","title":"Bidirectional skip-frame prediction for video anomaly detection with\n  intra-domain disparity-driven attention","summary":"  With the widespread deployment of video surveillance devices and the demand\nfor intelligent system development, video anomaly detection (VAD) has become an\nimportant part of constructing intelligent surveillance systems. Expanding the\ndiscriminative boundary between normal and abnormal events to enhance\nperformance is the common goal and challenge of VAD. To address this problem,\nwe propose a Bidirectional Skip-frame Prediction (BiSP) network based on a\ndual-stream autoencoder, from the perspective of learning the intra-domain\ndisparity between different features. The BiSP skips frames in the training\nphase to achieve the forward and backward frame prediction respectively, and in\nthe testing phase, it utilizes bidirectional consecutive frames to co-predict\nthe same intermediate frames, thus expanding the degree of disparity between\nnormal and abnormal events. The BiSP designs the variance channel attention and\ncontext spatial attention from the perspectives of movement patterns and object\nscales, respectively, thus ensuring the maximization of the disparity between\nnormal and abnormal in the feature extraction and delivery with different\ndimensions. Extensive experiments from four benchmark datasets demonstrate the\neffectiveness of the proposed BiSP, which substantially outperforms\nstate-of-the-art competing methods.\n","authors":["Jiahao Lyu","Minghua Zhao","Jing Hu","Runtao Xi","Xuewen Huang","Shuangli Du","Cheng Shi","Tian Ma"],"pdf_url":"https://arxiv.org/pdf/2407.15424v1.pdf","comment":"7 pages"},{"id":"http://arxiv.org/abs/2407.15420v1","updated":"2024-07-22T06:49:56Z","published":"2024-07-22T06:49:56Z","title":"Local All-Pair Correspondence for Point Tracking","summary":"  We introduce LocoTrack, a highly accurate and efficient model designed for\nthe task of tracking any point (TAP) across video sequences. Previous\napproaches in this task often rely on local 2D correlation maps to establish\ncorrespondences from a point in the query image to a local region in the target\nimage, which often struggle with homogeneous regions or repetitive features,\nleading to matching ambiguities. LocoTrack overcomes this challenge with a\nnovel approach that utilizes all-pair correspondences across regions, i.e.,\nlocal 4D correlation, to establish precise correspondences, with bidirectional\ncorrespondence and matching smoothness significantly enhancing robustness\nagainst ambiguities. We also incorporate a lightweight correlation encoder to\nenhance computational efficiency, and a compact Transformer architecture to\nintegrate long-term temporal information. LocoTrack achieves unmatched accuracy\non all TAP-Vid benchmarks and operates at a speed almost 6 times faster than\nthe current state-of-the-art.\n","authors":["Seokju Cho","Jiahui Huang","Jisu Nam","Honggyu An","Seungryong Kim","Joon-Young Lee"],"pdf_url":"https://arxiv.org/pdf/2407.15420v1.pdf","comment":"ECCV 2024. Project page: https://ku-cvlab.github.io/locotrack Code:\n  https://github.com/KU-CVLAB/locotrack"},{"id":"http://arxiv.org/abs/2407.15408v1","updated":"2024-07-22T06:25:21Z","published":"2024-07-22T06:25:21Z","title":"Chronologically Accurate Retrieval for Temporal Grounding of\n  Motion-Language Models","summary":"  With the release of large-scale motion datasets with textual annotations, the\ntask of establishing a robust latent space for language and 3D human motion has\nrecently witnessed a surge of interest. Methods have been proposed to convert\nhuman motion and texts into features to achieve accurate correspondence between\nthem. Despite these efforts to align language and motion representations, we\nclaim that the temporal element is often overlooked, especially for compound\nactions, resulting in chronological inaccuracies. To shed light on the temporal\nalignment in motion-language latent spaces, we propose Chronologically Accurate\nRetrieval (CAR) to evaluate the chronological understanding of the models. We\ndecompose textual descriptions into events, and prepare negative text samples\nby shuffling the order of events in compound action descriptions. We then\ndesign a simple task for motion-language models to retrieve the more likely\ntext from the ground truth and its chronologically shuffled version. CAR\nreveals many cases where current motion-language models fail to distinguish the\nevent chronology of human motion, despite their impressive performance in terms\nof conventional evaluation metrics. To achieve better temporal alignment\nbetween text and motion, we further propose to use these texts with shuffled\nsequence of events as negative samples during training to reinforce the\nmotion-language models. We conduct experiments on text-motion retrieval and\ntext-to-motion generation using the reinforced motion-language models, which\ndemonstrate improved performance over conventional approaches, indicating the\nnecessity to consider temporal elements in motion-language alignment.\n","authors":["Kent Fujiwara","Mikihiro Tanaka","Qing Yu"],"pdf_url":"https://arxiv.org/pdf/2407.15408v1.pdf","comment":"To appear at ECCV 2024. Project page: https://kfworks.com/CAR-WP/"},{"id":"http://arxiv.org/abs/2312.17432v3","updated":"2024-07-22T06:25:20Z","published":"2023-12-29T01:56:17Z","title":"Video Understanding with Large Language Models: A Survey","summary":"  With the burgeoning growth of online video platforms and the escalating\nvolume of video content, the demand for proficient video understanding tools\nhas intensified markedly. Given the remarkable capabilities of large language\nmodels (LLMs) in language and multimodal tasks, this survey provides a detailed\noverview of recent advancements in video understanding that harness the power\nof LLMs (Vid-LLMs). The emergent capabilities of Vid-LLMs are surprisingly\nadvanced, particularly their ability for open-ended multi-granularity (general,\ntemporal, and spatiotemporal) reasoning combined with commonsense knowledge,\nsuggesting a promising path for future video understanding. We examine the\nunique characteristics and capabilities of Vid-LLMs, categorizing the\napproaches into three main types: Video Analyzer x LLM, Video Embedder x LLM,\nand (Analyzer + Embedder) x LLM. Furthermore, we identify five sub-types based\non the functions of LLMs in Vid-LLMs: LLM as Summarizer, LLM as Manager, LLM as\nText Decoder, LLM as Regressor, and LLM as Hidden Layer. Furthermore, this\nsurvey presents a comprehensive study of the tasks, datasets, benchmarks, and\nevaluation methodologies for Vid-LLMs. Additionally, it explores the expansive\napplications of Vid-LLMs across various domains, highlighting their remarkable\nscalability and versatility in real-world video understanding challenges.\nFinally, it summarizes the limitations of existing Vid-LLMs and outlines\ndirections for future research. For more information, readers are recommended\nto visit the repository at\nhttps://github.com/yunlong10/Awesome-LLMs-for-Video-Understanding.\n","authors":["Yunlong Tang","Jing Bi","Siting Xu","Luchuan Song","Susan Liang","Teng Wang","Daoan Zhang","Jie An","Jingyang Lin","Rongyi Zhu","Ali Vosoughi","Chao Huang","Zeliang Zhang","Pinxin Liu","Mingqian Feng","Feng Zheng","Jianguo Zhang","Ping Luo","Jiebo Luo","Chenliang Xu"],"pdf_url":"https://arxiv.org/pdf/2312.17432v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15017v1","updated":"2024-07-22T06:15:59Z","published":"2024-07-22T06:15:59Z","title":"Knowledge Mechanisms in Large Language Models: A Survey and Perspective","summary":"  Understanding knowledge mechanisms in Large Language Models (LLMs) is crucial\nfor advancing towards trustworthy AGI. This paper reviews knowledge mechanism\nanalysis from a novel taxonomy including knowledge utilization and evolution.\nKnowledge utilization delves into the mechanism of memorization, comprehension\nand application, and creation. Knowledge evolution focuses on the dynamic\nprogression of knowledge within individual and group LLMs. Moreover, we discuss\nwhat knowledge LLMs have learned, the reasons for the fragility of parametric\nknowledge, and the potential dark knowledge (hypothesis) that will be\nchallenging to address. We hope this work can help understand knowledge in LLMs\nand provide insights for future research.\n","authors":["Mengru Wang","Yunzhi Yao","Ziwen Xu","Shuofei Qiao","Shumin Deng","Peng Wang","Xiang Chen","Jia-Chen Gu","Yong Jiang","Pengjun Xie","Fei Huang","Huajun Chen","Ningyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2407.15017v1.pdf","comment":"Ongoing work (v1); 34 pages, 5 figures"},{"id":"http://arxiv.org/abs/2404.09857v2","updated":"2024-07-22T06:13:32Z","published":"2024-04-15T15:12:53Z","title":"Empowering Embodied Visual Tracking with Visual Foundation Models and\n  Offline RL","summary":"  Embodied visual tracking is to follow a target object in dynamic 3D\nenvironments using an agent's egocentric vision. This is a vital and\nchallenging skill for embodied agents. However, existing methods suffer from\ninefficient training and poor generalization. In this paper, we propose a novel\nframework that combines visual foundation models(VFM) and offline reinforcement\nlearning(offline RL) to empower embodied visual tracking. We use a pre-trained\nVFM, such as \"Tracking Anything\", to extract semantic segmentation masks with\ntext prompts. We then train a recurrent policy network with offline RL, e.g.,\nConservative Q-Learning, to learn from the collected demonstrations without\nonline interactions. To further improve the robustness and generalization of\nthe policy network, we also introduce a mask re-targeting mechanism and a\nmulti-level data collection strategy. In this way, we can train a robust policy\nwithin an hour on a consumer-level GPU, e.g., Nvidia RTX 3090. We evaluate our\nagent on several high-fidelity environments with challenging situations, such\nas distraction and occlusion. The results show that our agent outperforms\nstate-of-the-art methods in terms of sample efficiency, robustness to\ndistractors, and generalization to unseen scenarios and targets. We also\ndemonstrate the transferability of the learned agent from virtual environments\nto a real-world robot.\n","authors":["Fangwei Zhong","Kui Wu","Hai Ci","Churan Wang","Hao Chen"],"pdf_url":"https://arxiv.org/pdf/2404.09857v2.pdf","comment":"ECCV 2024"},{"id":"http://arxiv.org/abs/2403.15378v3","updated":"2024-07-22T06:10:41Z","published":"2024-03-22T17:58:16Z","title":"Long-CLIP: Unlocking the Long-Text Capability of CLIP","summary":"  Contrastive Language-Image Pre-training (CLIP) has been the cornerstone for\nzero-shot classification, text-image retrieval, and text-image generation by\naligning image and text modalities. Despite its widespread adoption, a\nsignificant limitation of CLIP lies in the inadequate length of text input. The\nlength of the text token is restricted to 77, and an empirical study shows the\nactual effective length is even less than 20. This prevents CLIP from handling\ndetailed descriptions, limiting its applications for image retrieval and\ntext-to-image generation with extensive prerequisites. To this end, we propose\nLong-CLIP as a plug-and-play alternative to CLIP that supports long-text input,\nretains or even surpasses its zero-shot generalizability, and aligns the CLIP\nlatent space, making it readily replace CLIP without any further adaptation in\ndownstream frameworks. Nevertheless, achieving this goal is far from\nstraightforward, as simplistic fine-tuning can result in a significant\ndegradation of CLIP's performance. Moreover, substituting the text encoder with\na language model supporting longer contexts necessitates pretraining with vast\namounts of data, incurring significant expenses. Accordingly, Long-CLIP\nintroduces an efficient fine-tuning solution on CLIP with two novel strategies\ndesigned to maintain the original capabilities, including (1) a\nknowledge-preserved stretching of positional embedding and (2) a primary\ncomponent matching of CLIP features. With leveraging just one million extra\nlong text-image pairs, Long-CLIP has shown the superiority to CLIP for about\n20% in long caption text-image retrieval and 6% in traditional text-image\nretrieval tasks, e.g., COCO and Flickr30k. Furthermore, Long-CLIP offers\nenhanced capabilities for generating images from detailed text descriptions by\nreplacing CLIP in a plug-and-play manner.\n","authors":["Beichen Zhang","Pan Zhang","Xiaoyi Dong","Yuhang Zang","Jiaqi Wang"],"pdf_url":"https://arxiv.org/pdf/2403.15378v3.pdf","comment":"ECCV 2024. All codes and models are publicly available at\n  https://github.com/beichenzbc/Long-CLIP"},{"id":"http://arxiv.org/abs/2407.15396v1","updated":"2024-07-22T05:53:46Z","published":"2024-07-22T05:53:46Z","title":"Semantic Diversity-aware Prototype-based Learning for Unbiased Scene\n  Graph Generation","summary":"  The scene graph generation (SGG) task involves detecting objects within an\nimage and predicting predicates that represent the relationships between the\nobjects. However, in SGG benchmark datasets, each subject-object pair is\nannotated with a single predicate even though a single predicate may exhibit\ndiverse semantics (i.e., semantic diversity), existing SGG models are trained\nto predict the one and only predicate for each pair. This in turn results in\nthe SGG models to overlook the semantic diversity that may exist in a\npredicate, thus leading to biased predictions. In this paper, we propose a\nnovel model-agnostic Semantic Diversity-aware Prototype-based Learning (DPL)\nframework that enables unbiased predictions based on the understanding of the\nsemantic diversity of predicates. Specifically, DPL learns the regions in the\nsemantic space covered by each predicate to distinguish among the various\ndifferent semantics that a single predicate can represent. Extensive\nexperiments demonstrate that our proposed model-agnostic DPL framework brings\nsignificant performance improvement on existing SGG models, and also\neffectively understands the semantic diversity of predicates.\n","authors":["Jaehyeong Jeon","Kibum Kim","Kanghoon Yoon","Chanyoung Park"],"pdf_url":"https://arxiv.org/pdf/2407.15396v1.pdf","comment":"14 pages, ECCV 2024"},{"id":"http://arxiv.org/abs/2407.13322v2","updated":"2024-07-22T05:47:36Z","published":"2024-07-18T09:22:40Z","title":"Fully Test-Time rPPG Estimation via Synthetic Signal-Guided Feature\n  Learning","summary":"  Many remote photoplethysmography (rPPG) estimation models have achieved\npromising performance on the training domain but often fail to measure the\nphysiological signals or heart rates (HR) on test domains. Domain\ngeneralization (DG) or domain adaptation (DA) techniques are therefore adopted\nin the offline training stage to adapt the model to the unobserved or observed\ntest domain by referring to all the available source domain data. However, in\nrPPG estimation problems, the adapted model usually confronts challenges of\nestimating target data with various domain information, such as different video\ncapturing settings, individuals of different age ranges, or of different HR\ndistributions. In contrast, Test-Time Adaptation (TTA), by online adapting to\nunlabeled target data without referring to any source data, enables the model\nto adaptively estimate rPPG signals of various unseen domains. In this paper,\nwe first propose a novel TTA-rPPG benchmark, which encompasses various domain\ninformation and HR distributions, to simulate the challenges encountered in\nrPPG estimation. Next, we propose a novel synthetic signal-guided rPPG\nestimation framework with a two-fold purpose. First, we design an effective\nspectral-based entropy minimization to enforce the rPPG model to learn new\ntarget domain information. Second, we develop a synthetic signal-guided feature\nlearning, by synthesizing pseudo rPPG signals as pseudo ground-truths to guide\na conditional generator to generate latent rPPG features. The synthesized rPPG\nsignals and the generated rPPG features are used to guide the rPPG model to\nbroadly cover various HR distributions. Our extensive experiments on the\nTTA-rPPG benchmark show that the proposed method achieves superior performance\nand outperforms previous DG and DA methods across most protocols of the\nproposed TTA-rPPG benchmark.\n","authors":["Pei-Kai Huang","Tzu-Hsien Chen","Ya-Ting Chan","Kuan-Wen Chen","Chiou-Ting Hsu"],"pdf_url":"https://arxiv.org/pdf/2407.13322v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08236v2","updated":"2024-07-22T05:39:53Z","published":"2023-11-14T15:18:54Z","title":"MeLo: Low-rank Adaptation is Better than Fine-tuning for Medical Image\n  Diagnosis","summary":"  The common practice in developing computer-aided diagnosis (CAD) models based\non transformer architectures usually involves fine-tuning from ImageNet\npre-trained weights. However, with recent advances in large-scale pre-training\nand the practice of scaling laws, Vision Transformers (ViT) have become much\nlarger and less accessible to medical imaging communities. Additionally, in\nreal-world scenarios, the deployments of multiple CAD models can be troublesome\ndue to problems such as limited storage space and time-consuming model\nswitching. To address these challenges, we propose a new method MeLo (Medical\nimage Low-rank adaptation), which enables the development of a single CAD model\nfor multiple clinical tasks in a lightweight manner. It adopts low-rank\nadaptation instead of resource-demanding fine-tuning. By fixing the weight of\nViT models and only adding small low-rank plug-ins, we achieve competitive\nresults on various diagnosis tasks across different imaging modalities using\nonly a few trainable parameters. Specifically, our proposed method achieves\ncomparable performance to fully fine-tuned ViT models on four distinct medical\nimaging datasets using about 0.17% trainable parameters. Moreover, MeLo adds\nonly about 0.5MB of storage space and allows for extremely fast model switching\nin deployment and inference. Our source code and pre-trained weights are\navailable on our website (https://absterzhu.github.io/melo.github.io/).\n","authors":["Yitao Zhu","Zhenrong Shen","Zihao Zhao","Sheng Wang","Xin Wang","Xiangyu Zhao","Dinggang Shen","Qian Wang"],"pdf_url":"https://arxiv.org/pdf/2311.08236v2.pdf","comment":"IEEE International Symposium on Biomedical Imaging (ISBI 2024)"},{"id":"http://arxiv.org/abs/2406.07176v2","updated":"2024-07-22T05:38:36Z","published":"2024-06-11T11:39:44Z","title":"RAD: A Comprehensive Dataset for Benchmarking the Robustness of Image\n  Anomaly Detection","summary":"  Robustness against noisy imaging is crucial for practical image anomaly\ndetection systems. This study introduces a Robust Anomaly Detection (RAD)\ndataset with free views, uneven illuminations, and blurry collections to\nsystematically evaluate the robustness of current anomaly detection methods.\nSpecifically, RAD aims to identify foreign objects on working platforms as\nanomalies. The collection process incorporates various sources of imaging\nnoise, such as viewpoint changes, uneven illuminations, and blurry collections,\nto replicate real-world inspection scenarios. Subsequently, we assess and\nanalyze 11 state-of-the-art unsupervised and zero-shot methods on RAD. Our\nfindings indicate that: 1) Variations in viewpoint, illumination, and blurring\naffect anomaly detection methods to varying degrees; 2) Methods relying on\nmemory banks and assisted by synthetic anomalies demonstrate stronger\nrobustness; 3) Effectively leveraging the general knowledge of foundational\nmodels is a promising avenue for enhancing the robustness of anomaly detection\nmethods. The dataset is available at https://github.com/hustCYQ/RAD-dataset.\n","authors":["Yuqi Cheng","Yunkang Cao","Rui Chen","Weiming Shen"],"pdf_url":"https://arxiv.org/pdf/2406.07176v2.pdf","comment":"6 pages, 5 figures"},{"id":"http://arxiv.org/abs/2407.15385v1","updated":"2024-07-22T05:28:29Z","published":"2024-07-22T05:28:29Z","title":"Towards Robust Vision Transformer via Masked Adaptive Ensemble","summary":"  Adversarial training (AT) can help improve the robustness of Vision\nTransformers (ViT) against adversarial attacks by intentionally injecting\nadversarial examples into the training data. However, this way of adversarial\ninjection inevitably incurs standard accuracy degradation to some extent,\nthereby calling for a trade-off between standard accuracy and robustness.\nBesides, the prominent AT solutions are still vulnerable to adaptive attacks.\nTo tackle such shortcomings, this paper proposes a novel ViT architecture,\nincluding a detector and a classifier bridged by our newly developed adaptive\nensemble. Specifically, we empirically discover that detecting adversarial\nexamples can benefit from the Guided Backpropagation technique. Driven by this\ndiscovery, a novel Multi-head Self-Attention (MSA) mechanism is introduced to\nenhance our detector to sniff adversarial examples. Then, a classifier with two\nencoders is employed for extracting visual representations respectively from\nclean images and adversarial examples, with our adaptive ensemble to adaptively\nadjust the proportion of visual representations from the two encoders for\naccurate classification. This design enables our ViT architecture to achieve a\nbetter trade-off between standard accuracy and robustness. Besides, our\nadaptive ensemble technique allows us to mask off a random subset of image\npatches within input data, boosting our ViT's robustness against adaptive\nattacks, while maintaining high standard accuracy. Experimental results exhibit\nthat our ViT architecture, on CIFAR-10, achieves the best standard accuracy and\nadversarial robustness of 90.3% and 49.8%, respectively.\n","authors":["Fudong Lin","Jiadong Lou","Xu Yuan","Nian-Feng Tzeng"],"pdf_url":"https://arxiv.org/pdf/2407.15385v1.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2312.02216v3","updated":"2024-07-22T05:25:28Z","published":"2023-12-03T10:41:06Z","title":"DragVideo: Interactive Drag-style Video Editing","summary":"  Video generation models have shown their superior ability to generate\nphoto-realistic video. However, how to accurately control (or edit) the video\nremains a formidable challenge. The main issues are: 1) how to perform direct\nand accurate user control in editing; 2) how to execute editings like changing\nshape, expression, and layout without unsightly distortion and artifacts to the\nedited content; and 3) how to maintain spatio-temporal consistency of video\nafter editing. To address the above issues, we propose DragVideo, a general\ndrag-style video editing framework. Inspired by DragGAN, DragVideo addresses\nissues 1) and 2) by proposing the drag-style video latent optimization method\nwhich gives desired control by updating noisy video latent according to drag\ninstructions through video-level drag objective function. We amend issue 3) by\nintegrating the video diffusion model with sample-specific LoRA and Mutual\nSelf-Attention in DragVideo to ensure the edited result is spatio-temporally\nconsistent. We also present a series of testing examples for drag-style video\nediting and conduct extensive experiments across a wide array of challenging\nediting tasks, such as motion, skeleton editing, etc, underscoring DragVideo\ncan edit video in an intuitive, faithful to the user's intention manner, with\nnearly unnoticeable distortion and artifacts, while maintaining spatio-temporal\nconsistency. While traditional prompt-based video editing fails to do the\nformer two and directly applying image drag editing fails in the last,\nDragVideo's versatility and generality are emphasized. Github link:\nhttps://github.com/RickySkywalker/DragVideo-Official.\n","authors":["Yufan Deng","Ruida Wang","Yuhao Zhang","Yu-Wing Tai","Chi-Keung Tang"],"pdf_url":"https://arxiv.org/pdf/2312.02216v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.04518v2","updated":"2024-07-22T05:24:52Z","published":"2024-04-06T06:18:11Z","title":"MedIAnomaly: A comparative study of anomaly detection in medical images","summary":"  Anomaly detection (AD) aims at detecting abnormal samples that deviate from\nthe expected normal patterns. Generally, it can be trained merely on normal\ndata, without a requirement for abnormal samples, and thereby plays an\nimportant role in the recognition of rare diseases and health screening in the\nmedical domain. Despite the emergence of numerous methods for medical AD, we\nobserve a lack of a fair and comprehensive evaluation, which causes ambiguous\nconclusions and hinders the development of this field. To address this problem,\nthis paper builds a benchmark with unified comparison. Seven medical datasets\nwith five image modalities, including chest X-rays, brain MRIs, retinal fundus\nimages, dermatoscopic images, and histopathology whole slide images, are\ncurated for extensive evaluation. Thirty typical AD methods, including\nreconstruction and self-supervised learning-based methods, are involved in\ncomparison of image-level anomaly classification and pixel-level anomaly\nsegmentation. Furthermore, for the first time, we formally explore the effect\nof key components in existing methods, clearly revealing unresolved challenges\nand potential future directions. The datasets and code are available at\n\\url{https://github.com/caiyu6666/MedIAnomaly}.\n","authors":["Yu Cai","Weiwen Zhang","Hao Chen","Kwang-Ting Cheng"],"pdf_url":"https://arxiv.org/pdf/2404.04518v2.pdf","comment":"Under submission"},{"id":"http://arxiv.org/abs/2407.12736v2","updated":"2024-07-22T05:19:15Z","published":"2024-07-17T16:56:06Z","title":"CHOSEN: Compilation to Hardware Optimization Stack for Efficient Vision\n  Transformer Inference","summary":"  Vision Transformers (ViTs) represent a groundbreaking shift in machine\nlearning approaches to computer vision. Unlike traditional approaches, ViTs\nemploy the self-attention mechanism, which has been widely used in natural\nlanguage processing, to analyze image patches. Despite their advantages in\nmodeling visual tasks, deploying ViTs on hardware platforms, notably\nField-Programmable Gate Arrays (FPGAs), introduces considerable challenges.\nThese challenges stem primarily from the non-linear calculations and high\ncomputational and memory demands of ViTs. This paper introduces CHOSEN, a\nsoftware-hardware co-design framework to address these challenges and offer an\nautomated framework for ViT deployment on the FPGAs in order to maximize\nperformance. Our framework is built upon three fundamental contributions:\nmulti-kernel design to maximize the bandwidth, mainly targeting benefits of\nmulti DDR memory banks, approximate non-linear functions that exhibit minimal\naccuracy degradation, and efficient use of available logic blocks on the FPGA,\nand efficient compiler to maximize the performance and memory-efficiency of the\ncomputing kernels by presenting a novel algorithm for design space exploration\nto find optimal hardware configuration that achieves optimal throughput and\nlatency. Compared to the state-of-the-art ViT accelerators, CHOSEN achieves a\n1.5x and 1.42x improvement in the throughput on the DeiT-S and DeiT-B models.\n","authors":["Mohammad Erfan Sadeghi","Arash Fayyazi","Suhas Somashekar","Massoud Pedram"],"pdf_url":"https://arxiv.org/pdf/2407.12736v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15383v1","updated":"2024-07-22T05:15:41Z","published":"2024-07-22T05:15:41Z","title":"Is user feedback always informative? Retrieval Latent Defending for\n  Semi-Supervised Domain Adaptation without Source Data","summary":"  This paper aims to adapt the source model to the target environment,\nleveraging small user feedback (i.e., labeled target data) readily available in\nreal-world applications. We find that existing semi-supervised domain\nadaptation (SemiSDA) methods often suffer from poorly improved adaptation\nperformance when directly utilizing such feedback data, as shown in Figure 1.\nWe analyze this phenomenon via a novel concept called Negatively Biased\nFeedback (NBF), which stems from the observation that user feedback is more\nlikely for data points where the model produces incorrect predictions. To\nleverage this feedback while avoiding the issue, we propose a scalable adapting\napproach, Retrieval Latent Defending. This approach helps existing SemiSDA\nmethods to adapt the model with a balanced supervised signal by utilizing\nlatent defending samples throughout the adaptation process. We demonstrate the\nproblem caused by NBF and the efficacy of our approach across various\nbenchmarks, including image classification, semantic segmentation, and a\nreal-world medical imaging application. Our extensive experiments reveal that\nintegrating our approach with multiple state-of-the-art SemiSDA methods leads\nto significant performance improvements.\n","authors":["Junha Song","Tae Soo Kim","Junha Kim","Gunhee Nam","Thijs Kooi","Jaegul Choo"],"pdf_url":"https://arxiv.org/pdf/2407.15383v1.pdf","comment":"Accepted to ECCV 2024, Project page:\n  https://sites.google.com/view/junha/nbf-rld"},{"id":"http://arxiv.org/abs/2401.03890v4","updated":"2024-07-22T05:13:49Z","published":"2024-01-08T13:42:59Z","title":"A Survey on 3D Gaussian Splatting","summary":"  3D Gaussian splatting (GS) has recently emerged as a transformative technique\nin the realm of explicit radiance field and computer graphics. This innovative\napproach, characterized by the utilization of millions of learnable 3D\nGaussians, represents a significant departure from mainstream neural radiance\nfield approaches, which predominantly use implicit, coordinate-based models to\nmap spatial coordinates to pixel values. 3D GS, with its explicit scene\nrepresentation and differentiable rendering algorithm, not only promises\nreal-time rendering capability but also introduces unprecedented levels of\neditability. This positions 3D GS as a potential game-changer for the next\ngeneration of 3D reconstruction and representation. In the present paper, we\nprovide the first systematic overview of the recent developments and critical\ncontributions in the domain of 3D GS. We begin with a detailed exploration of\nthe underlying principles and the driving forces behind the emergence of 3D GS,\nlaying the groundwork for understanding its significance. A focal point of our\ndiscussion is the practical applicability of 3D GS. By enabling unprecedented\nrendering speed, 3D GS opens up a plethora of applications, ranging from\nvirtual reality to interactive media and beyond. This is complemented by a\ncomparative analysis of leading 3D GS models, evaluated across various\nbenchmark tasks to highlight their performance and practical utility. The\nsurvey concludes by identifying current challenges and suggesting potential\navenues for future research in this domain. Through this survey, we aim to\nprovide a valuable resource for both newcomers and seasoned researchers,\nfostering further exploration and advancement in applicable and explicit\nradiance field representation.\n","authors":["Guikun Chen","Wenguan Wang"],"pdf_url":"https://arxiv.org/pdf/2401.03890v4.pdf","comment":"Ongoing project"},{"id":"http://arxiv.org/abs/2407.15380v1","updated":"2024-07-22T05:06:06Z","published":"2024-07-22T05:06:06Z","title":"Iterative approach to reconstructing neural disparity fields from\n  light-field data","summary":"  This study proposes a neural disparity field (NDF) that establishes an\nimplicit, continuous representation of scene disparity based on a neural field\nand an iterative approach to address the inverse problem of NDF reconstruction\nfrom light-field data. NDF enables seamless and precise characterization of\ndisparity variations in three-dimensional scenes and can discretize disparity\nat any arbitrary resolution, overcoming the limitations of traditional\ndisparity maps that are prone to sampling errors and interpolation\ninaccuracies. The proposed NDF network architecture utilizes hash encoding\ncombined with multilayer perceptrons to capture detailed disparities in texture\nlevels, thereby enhancing its ability to represent the geometric information of\ncomplex scenes. By leveraging the spatial-angular consistency inherent in\nlight-field data, a differentiable forward model to generate a central view\nimage from the light-field data is developed. Based on the forward model, an\noptimization scheme for the inverse problem of NDF reconstruction using\ndifferentiable propagation operators is established. Furthermore, an iterative\nsolution method is adopted to reconstruct the NDF in the optimization scheme,\nwhich does not require training datasets and applies to light-field data\ncaptured by various acquisition methods. Experimental results demonstrate that\nhigh-quality NDF can be reconstructed from light-field data using the proposed\nmethod. High-resolution disparity can be effectively recovered by NDF,\ndemonstrating its capability for the implicit, continuous representation of\nscene disparities.\n","authors":["Ligen Shi","Chang Liu","Xing Zhao","Jun Qiu"],"pdf_url":"https://arxiv.org/pdf/2407.15380v1.pdf","comment":"12 pages, 7 figures"},{"id":"http://arxiv.org/abs/2407.15369v1","updated":"2024-07-22T04:32:43Z","published":"2024-07-22T04:32:43Z","title":"Sparse Prior Is Not All You Need: When Differential Directionality Meets\n  Saliency Coherence for Infrared Small Target Detection","summary":"  Infrared small target detection is crucial for the efficacy of infrared\nsearch and tracking systems. Current tensor decomposition methods emphasize\nrepresenting small targets with sparsity but struggle to separate targets from\ncomplex backgrounds due to insufficient use of intrinsic directional\ninformation and reduced target visibility during decomposition. To address\nthese challenges, this study introduces a Sparse Differential Directionality\nprior (SDD) framework. SDD leverages the distinct directional characteristics\nof targets to differentiate them from the background, applying mixed sparse\nconstraints on the differential directional images and continuity difference\nmatrix of the temporal component, both derived from Tucker decomposition. We\nfurther enhance target detectability with a saliency coherence strategy that\nintensifies target contrast against the background during hierarchical\ndecomposition. A Proximal Alternating Minimization-based (PAM) algorithm\nefficiently solves our proposed model. Experimental results on several\nreal-world datasets validate our method's effectiveness, outperforming ten\nstate-of-the-art methods in target detection and clutter suppression. Our code\nis available at https://github.com/GrokCV/SDD.\n","authors":["Fei Zhou","Maixia Fu","Yulei Qian","Jian Yang","Yimian Dai"],"pdf_url":"https://arxiv.org/pdf/2407.15369v1.pdf","comment":"Submitted to IEEE TIM, Minor Revision"},{"id":"http://arxiv.org/abs/2401.08742v3","updated":"2024-07-22T04:14:11Z","published":"2024-01-16T18:58:36Z","title":"Efficient4D: Fast Dynamic 3D Object Generation from a Single-view Video","summary":"  Generating dynamic 3D object from a single-view video is challenging due to\nthe lack of 4D labeled data. An intuitive approach is to extend previous\nimage-to-3D pipelines by transferring off-the-shelf image generation models\nsuch as score distillation sampling.However, this approach would be slow and\nexpensive to scale due to the need for back-propagating the information-limited\nsupervision signals through a large pretrained model. To address this, we\npropose an efficient video-to-4D object generation framework called\nEfficient4D. It generates high-quality spacetime-consistent images under\ndifferent camera views, and then uses them as labeled data to directly\nreconstruct the 4D content through a 4D Gaussian splatting model. Importantly,\nour method can achieve real-time rendering under continuous camera\ntrajectories. To enable robust reconstruction under sparse views, we introduce\ninconsistency-aware confidence-weighted loss design, along with a lightly\nweighted score distillation loss. Extensive experiments on both synthetic and\nreal videos show that Efficient4D offers a remarkable 10-fold increase in speed\nwhen compared to prior art alternatives while preserving the quality of novel\nview synthesis. For example, Efficient4D takes only 10 minutes to model a\ndynamic object, vs 120 minutes by the previous art model Consistent4D.\n","authors":["Zijie Pan","Zeyu Yang","Xiatian Zhu","Li Zhang"],"pdf_url":"https://arxiv.org/pdf/2401.08742v3.pdf","comment":"Technical report"},{"id":"http://arxiv.org/abs/2407.15362v1","updated":"2024-07-22T04:09:27Z","published":"2024-07-22T04:09:27Z","title":"A Multimodal Knowledge-enhanced Whole-slide Pathology Foundation Model","summary":"  Remarkable strides in computational pathology have been made in the\ntask-agnostic foundation model that advances the performance of a wide array of\ndownstream clinical tasks. Despite the promising performance, there are still\nseveral challenges. First, prior works have resorted to either vision-only or\nvision-captions data, disregarding invaluable pathology reports and gene\nexpression profiles which respectively offer distinct knowledge for versatile\nclinical applications. Second, the current progress in pathology FMs\npredominantly concentrates on the patch level, where the restricted context of\npatch-level pretraining fails to capture whole-slide patterns. Here we curated\nthe largest multimodal dataset consisting of H\\&E diagnostic whole slide images\nand their associated pathology reports and RNA-Seq data, resulting in 26,169\nslide-level modality pairs from 10,275 patients across 32 cancer types. To\nleverage these data for CPath, we propose a novel whole-slide pretraining\nparadigm which injects multimodal knowledge at the whole-slide context into the\npathology FM, called Multimodal Self-TAught PRetraining (mSTAR). The proposed\nparadigm revolutionizes the workflow of pretraining for CPath, which enables\nthe pathology FM to acquire the whole-slide context. To our knowledge, this is\nthe first attempt to incorporate multimodal knowledge at the slide level for\nenhancing pathology FMs, expanding the modelling context from unimodal to\nmultimodal knowledge and from patch-level to slide-level. To systematically\nevaluate the capabilities of mSTAR, extensive experiments including slide-level\nunimodal and multimodal applications, are conducted across 7 diverse types of\ntasks on 43 subtasks, resulting in the largest spectrum of downstream tasks.\nThe average performance in various slide-level applications consistently\ndemonstrates significant performance enhancements for mSTAR compared to SOTA\nFMs.\n","authors":["Yingxue Xu","Yihui Wang","Fengtao Zhou","Jiabo Ma","Shu Yang","Huangjing Lin","Xin Wang","Jiguang Wang","Li Liang","Anjia Han","Ronald Cheong Kin Chan","Hao Chen"],"pdf_url":"https://arxiv.org/pdf/2407.15362v1.pdf","comment":"44 pages, 9 figures"},{"id":"http://arxiv.org/abs/2311.08100v4","updated":"2024-07-22T03:57:03Z","published":"2023-11-14T11:53:24Z","title":"PPAD: Iterative Interactions of Prediction and Planning for End-to-end\n  Autonomous Driving","summary":"  We present a new interaction mechanism of prediction and planning for\nend-to-end autonomous driving, called PPAD (Iterative Interaction of Prediction\nand Planning Autonomous Driving), which considers the timestep-wise interaction\nto better integrate prediction and planning. An ego vehicle performs motion\nplanning at each timestep based on the trajectory prediction of surrounding\nagents (e.g., vehicles and pedestrians) and its local road conditions. Unlike\nexisting end-to-end autonomous driving frameworks, PPAD models the interactions\namong ego, agents, and the dynamic environment in an autoregressive manner by\ninterleaving the Prediction and Planning processes at every timestep, instead\nof a single sequential process of prediction followed by planning.\nSpecifically, we design ego-to-agent, ego-to-map, and ego-to-BEV interaction\nmechanisms with hierarchical dynamic key objects attention to better model the\ninteractions. The experiments on the nuScenes benchmark show that our approach\noutperforms state-of-the-art methods.\n","authors":["Zhili Chen","Maosheng Ye","Shuangjie Xu","Tongyi Cao","Qifeng Chen"],"pdf_url":"https://arxiv.org/pdf/2311.08100v4.pdf","comment":"Accepted to ECCV 2024. Project page: https://github.com/zlichen/PPAD"},{"id":"http://arxiv.org/abs/2407.15356v1","updated":"2024-07-22T03:55:36Z","published":"2024-07-22T03:55:36Z","title":"X-Recon: Learning-based Patient-specific High-Resolution CT\n  Reconstruction from Orthogonal X-Ray Images","summary":"  Rapid and accurate diagnosis of pneumothorax, utilizing chest X-ray and\ncomputed tomography (CT), is crucial for assisted diagnosis. Chest X-ray is\ncommonly used for initial localization of pneumothorax, while CT ensures\naccurate quantification. However, CT scans involve high radiation doses and can\nbe costly. To achieve precise quantitative diagnosis while minimizing radiation\nexposure, we proposed X-Recon, a CT ultra-sparse reconstruction network based\non ortho-lateral chest X-ray images. X-Recon integrates generative adversarial\nnetworks (GANs), including a generator with a multi-scale fusion rendering\nmodule and a discriminator enhanced by 3D coordinate convolutional layers,\ndesigned to facilitate CT reconstruction. To improve precision, a projective\nspatial transformer is utilized to incorporate multi-angle projection loss.\nAdditionally, we proposed PTX-Seg, a zero-shot pneumothorax segmentation\nalgorithm, combining image processing techniques with deep-learning models for\nthe segmentation of air-accumulated regions and lung structures. Experiments on\na large-scale dataset demonstrate its superiority over existing approaches.\nX-Recon achieved a significantly higher reconstruction resolution with a higher\naverage spatial resolution and a lower average slice thickness. The\nreconstruction metrics achieved state-of-the-art performance in terms of\nseveral metrics including peak signal-to-noise ratio. The zero-shot\nsegmentation algorithm, PTX-Seg, also demonstrated high segmentation precision\nfor the air-accumulated region, the left lung, and the right lung. Moreover,\nthe consistency analysis for the pneumothorax chest occupancy ratio between\nreconstructed CT and original CT obtained a high correlation coefficient. Code\nwill be available at: https://github.com/wangyunpengbio/X-Recon\n","authors":["Yunpeng Wang","Kang Wang","Yaoyao Zhuo","Weiya Shi","Fei Shan","Lei Liu"],"pdf_url":"https://arxiv.org/pdf/2407.15356v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15355v1","updated":"2024-07-22T03:52:18Z","published":"2024-07-22T03:52:18Z","title":"Attention Beats Linear for Fast Implicit Neural Representation\n  Generation","summary":"  Implicit Neural Representation (INR) has gained increasing popularity as a\ndata representation method, serving as a prerequisite for innovative generation\nmodels. Unlike gradient-based methods, which exhibit lower efficiency in\ninference, the adoption of hyper-network for generating parameters in\nMulti-Layer Perceptrons (MLP), responsible for executing INR functions, has\nsurfaced as a promising and efficient alternative. However, as a global\ncontinuous function, MLP is challenging in modeling highly discontinuous\nsignals, resulting in slow convergence during the training phase and inaccurate\nreconstruction performance. Moreover, MLP requires massive representation\nparameters, which implies inefficiencies in data representation. In this paper,\nwe propose a novel Attention-based Localized INR (ANR) composed of a localized\nattention layer (LAL) and a global MLP that integrates coordinate features with\ndata features and converts them to meaningful outputs. Subsequently, we design\nan instance representation framework that delivers a transformer-like\nhyper-network to represent data instances as a compact representation vector.\nWith instance-specific representation vector and instance-agnostic ANR\nparameters, the target signals are well reconstructed as a continuous function.\nWe further address aliasing artifacts with variational coordinates when\nobtaining the super-resolution inference results. Extensive experimentation\nacross four datasets showcases the notable efficacy of our ANR method, e.g.\nenhancing the PSNR value from 37.95dB to 47.25dB on the CelebA dataset. Code is\nreleased at https://github.com/Roninton/ANR.\n","authors":["Shuyi Zhang","Ke Liu","Jingjun Gu","Xiaoxu Cai","Zhihua Wang","Jiajun Bu","Haishuai Wang"],"pdf_url":"https://arxiv.org/pdf/2407.15355v1.pdf","comment":"Accept by ECCV 2024"},{"id":"http://arxiv.org/abs/2407.15354v1","updated":"2024-07-22T03:51:18Z","published":"2024-07-22T03:51:18Z","title":"Learning High-resolution Vector Representation from Multi-Camera Images\n  for 3D Object Detection","summary":"  The Bird's-Eye-View (BEV) representation is a critical factor that directly\nimpacts the 3D object detection performance, but the traditional BEV grid\nrepresentation induces quadratic computational cost as the spatial resolution\ngrows. To address this limitation, we present a new camera-based 3D object\ndetector with high-resolution vector representation: VectorFormer. The\npresented high-resolution vector representation is combined with the\nlower-resolution BEV representation to efficiently exploit 3D geometry from\nmulti-camera images at a high resolution through our two novel modules: vector\nscattering and gathering. To this end, the learned vector representation with\nricher scene contexts can serve as the decoding query for final predictions. We\nconduct extensive experiments on the nuScenes dataset and demonstrate\nstate-of-the-art performance in NDS and inference time. Furthermore, we\ninvestigate query-BEV-based methods incorporated with our proposed vector\nrepresentation and observe a consistent performance improvement.\n","authors":["Zhili Chen","Shuangjie Xu","Maosheng Ye","Zian Qian","Xiaoyi Zou","Dit-Yan Yeung","Qifeng Chen"],"pdf_url":"https://arxiv.org/pdf/2407.15354v1.pdf","comment":"Accepted to ECCV 2024. Project page:\n  https://github.com/zlichen/VectorFormer"},{"id":"http://arxiv.org/abs/2301.12554v5","updated":"2024-07-22T03:41:03Z","published":"2023-01-29T22:05:28Z","title":"Improving the Accuracy-Robustness Trade-Off of Classifiers via Adaptive\n  Smoothing","summary":"  While prior research has proposed a plethora of methods that build neural\nclassifiers robust against adversarial robustness, practitioners are still\nreluctant to adopt them due to their unacceptably severe clean accuracy\npenalties. This paper significantly alleviates this accuracy-robustness\ntrade-off by mixing the output probabilities of a standard classifier and a\nrobust classifier, where the standard network is optimized for clean accuracy\nand is not robust in general. We show that the robust base classifier's\nconfidence difference for correct and incorrect examples is the key to this\nimprovement. In addition to providing intuitions and empirical evidence, we\ntheoretically certify the robustness of the mixed classifier under realistic\nassumptions. Furthermore, we adapt an adversarial input detector into a mixing\nnetwork that adaptively adjusts the mixture of the two base models, further\nreducing the accuracy penalty of achieving robustness. The proposed flexible\nmethod, termed \"adaptive smoothing\", can work in conjunction with existing or\neven future methods that improve clean accuracy, robustness, or adversary\ndetection. Our empirical evaluation considers strong attack methods, including\nAutoAttack and adaptive attack. On the CIFAR-100 dataset, our method achieves\nan 85.21% clean accuracy while maintaining a 38.72% $\\ell_\\infty$-AutoAttacked\n($\\epsilon = 8/255$) accuracy, becoming the second most robust method on the\nRobustBench CIFAR-100 benchmark as of submission, while improving the clean\naccuracy by ten percentage points compared with all listed models. The code\nthat implements our method is available at\nhttps://github.com/Bai-YT/AdaptiveSmoothing.\n","authors":["Yatong Bai","Brendon G. Anderson","Aerin Kim","Somayeh Sojoudi"],"pdf_url":"https://arxiv.org/pdf/2301.12554v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15350v1","updated":"2024-07-22T03:29:22Z","published":"2024-07-22T03:29:22Z","title":"WTS: A Pedestrian-Centric Traffic Video Dataset for Fine-grained\n  Spatial-Temporal Understanding","summary":"  In this paper, we address the challenge of fine-grained video event\nunderstanding in traffic scenarios, vital for autonomous driving and safety.\nTraditional datasets focus on driver or vehicle behavior, often neglecting\npedestrian perspectives. To fill this gap, we introduce the WTS dataset,\nhighlighting detailed behaviors of both vehicles and pedestrians across over\n1.2k video events in hundreds of traffic scenarios. WTS integrates diverse\nperspectives from vehicle ego and fixed overhead cameras in a\nvehicle-infrastructure cooperative environment, enriched with comprehensive\ntextual descriptions and unique 3D Gaze data for a synchronized 2D/3D view,\nfocusing on pedestrian analysis. We also pro-vide annotations for 5k publicly\nsourced pedestrian-related traffic videos. Additionally, we introduce\nLLMScorer, an LLM-based evaluation metric to align inference captions with\nground truth. Using WTS, we establish a benchmark for dense video-to-text\ntasks, exploring state-of-the-art Vision-Language Models with an instance-aware\nVideoLLM method as a baseline. WTS aims to advance fine-grained video event\nunderstanding, enhancing traffic safety and autonomous driving development.\n","authors":["Quan Kong","Yuki Kawana","Rajat Saini","Ashutosh Kumar","Jingjing Pan","Ta Gu","Yohei Ozao","Balazs Opra","David C. Anastasiu","Yoichi Sato","Norimasa Kobori"],"pdf_url":"https://arxiv.org/pdf/2407.15350v1.pdf","comment":"ECCV24. Website:\n  https://woven-visionai.github.io/wts-dataset-homepage/"},{"id":"http://arxiv.org/abs/2403.16198v2","updated":"2024-07-22T03:27:30Z","published":"2024-03-24T15:39:52Z","title":"Diffusion Model is a Good Pose Estimator from 3D RF-Vision","summary":"  Human pose estimation (HPE) from Radio Frequency vision (RF-vision) performs\nhuman sensing using RF signals that penetrate obstacles without revealing\nprivacy (e.g., facial information). Recently, mmWave radar has emerged as a\npromising RF-vision sensor, providing radar point clouds by processing RF\nsignals. However, the mmWave radar has a limited resolution with severe noise,\nleading to inaccurate and inconsistent human pose estimation. This work\nproposes mmDiff, a novel diffusion-based pose estimator tailored for noisy\nradar data. Our approach aims to provide reliable guidance as conditions to\ndiffusion models. Two key challenges are addressed by mmDiff: (1)\nmiss-detection of parts of human bodies, which is addressed by a module that\nisolates feature extraction from different body parts, and (2) signal\ninconsistency due to environmental interference, which is tackled by\nincorporating prior knowledge of body structure and motion. Several modules are\ndesigned to achieve these goals, whose features work as the conditions for the\nsubsequent diffusion model, eliminating the miss-detection and instability of\nHPE based on RF-vision. Extensive experiments demonstrate that mmDiff\noutperforms existing methods significantly, achieving state-of-the-art\nperformances on public datasets.\n","authors":["Junqiao Fan","Jianfei Yang","Yuecong Xu","Lihua Xie"],"pdf_url":"https://arxiv.org/pdf/2403.16198v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07844v2","updated":"2024-07-22T03:26:21Z","published":"2024-07-10T17:05:49Z","title":"OV-DINO: Unified Open-Vocabulary Detection with Language-Aware Selective\n  Fusion","summary":"  Open-vocabulary detection is a challenging task due to the requirement of\ndetecting objects based on class names, including those not encountered during\ntraining. Existing methods have shown strong zero-shot detection capabilities\nthrough pre-training and pseudo-labeling on diverse large-scale datasets.\nHowever, these approaches encounter two main challenges: (i) how to effectively\neliminate data noise from pseudo-labeling, and (ii) how to efficiently leverage\nthe language-aware capability for region-level cross-modality fusion and\nalignment. To address these challenges, we propose a novel unified\nopen-vocabulary detection method called OV-DINO, which is pre-trained on\ndiverse large-scale datasets with language-aware selective fusion in a unified\nframework. Specifically, we introduce a Unified Data Integration (UniDI)\npipeline to enable end-to-end training and eliminate noise from pseudo-label\ngeneration by unifying different data sources into detection-centric data\nformat. In addition, we propose a Language-Aware Selective Fusion (LASF) module\nto enhance the cross-modality alignment through a language-aware query\nselection and fusion process. We evaluate the performance of the proposed\nOV-DINO on popular open-vocabulary detection benchmarks, achieving\nstate-of-the-art results with an AP of 50.6% on the COCO benchmark and 40.1% on\nthe LVIS benchmark in a zero-shot manner, demonstrating its strong\ngeneralization ability. Furthermore, the fine-tuned OV-DINO on COCO achieves\n58.4% AP, outperforming many existing methods with the same backbone. The code\nfor OV-DINO is available at https://github.com/wanghao9610/OV-DINO.\n","authors":["Hao Wang","Pengzhen Ren","Zequn Jie","Xiao Dong","Chengjian Feng","Yinlong Qian","Lin Ma","Dongmei Jiang","Yaowei Wang","Xiangyuan Lan","Xiaodan Liang"],"pdf_url":"https://arxiv.org/pdf/2407.07844v2.pdf","comment":"Technical Report"},{"id":"http://arxiv.org/abs/2407.15349v1","updated":"2024-07-22T03:23:35Z","published":"2024-07-22T03:23:35Z","title":"RoadPainter: Points Are Ideal Navigators for Topology transformER","summary":"  Topology reasoning aims to provide a precise understanding of road scenes,\nenabling autonomous systems to identify safe and efficient routes. In this\npaper, we present RoadPainter, an innovative approach for detecting and\nreasoning the topology of lane centerlines using multi-view images. The core\nconcept behind RoadPainter is to extract a set of points from each centerline\nmask to improve the accuracy of centerline prediction. We start by implementing\na transformer decoder that integrates a hybrid attention mechanism and a\nreal-virtual separation strategy to predict coarse lane centerlines and\nestablish topological associations. Then, we generate centerline instance masks\nguided by the centerline points from the transformer decoder. Moreover, we\nderive an additional set of points from each mask and combine them with\npreviously detected centerline points for further refinement. Additionally, we\nintroduce an optional module that incorporates a Standard Definition (SD) map\nto further optimize centerline detection and enhance topological reasoning\nperformance. Experimental evaluations on the OpenLane-V2 dataset demonstrate\nthe state-of-the-art performance of RoadPainter.\n","authors":["Zhongxing Ma","Shuang Liang","Yongkun Wen","Weixin Lu","Guowei Wan"],"pdf_url":"https://arxiv.org/pdf/2407.15349v1.pdf","comment":"17 pages, 5 figures, Accepted by ECCV 2024"},{"id":"http://arxiv.org/abs/2406.05785v2","updated":"2024-07-22T03:21:27Z","published":"2024-06-09T13:52:12Z","title":"A Survey on Text-guided 3D Visual Grounding: Elements, Recent Advances,\n  and Future Directions","summary":"  Text-guided 3D visual grounding (T-3DVG), which aims to locate a specific\nobject that semantically corresponds to a language query from a complicated 3D\nscene, has drawn increasing attention in the 3D research community over the\npast few years. Compared to 2D visual grounding, this task presents great\npotential and challenges due to its closer proximity to the real world and the\ncomplexity of data collection and 3D point cloud source processing. In this\nsurvey, we attempt to provide a comprehensive overview of the T-3DVG progress,\nincluding its fundamental elements, recent research advances, and future\nresearch directions. To the best of our knowledge, this is the first systematic\nsurvey on the T-3DVG task. Specifically, we first provide a general structure\nof the T-3DVG pipeline with detailed components in a tutorial style, presenting\na complete background overview. Then, we summarize the existing T-3DVG\napproaches into different categories and analyze their strengths and\nweaknesses. We also present the benchmark datasets and evaluation metrics to\nassess their performances. Finally, we discuss the potential limitations of\nexisting T-3DVG and share some insights on several promising research\ndirections. The latest papers are continually collected at\nhttps://github.com/liudaizong/Awesome-3D-Visual-Grounding.\n","authors":["Daizong Liu","Yang Liu","Wencan Huang","Wei Hu"],"pdf_url":"https://arxiv.org/pdf/2406.05785v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.14055v2","updated":"2024-07-22T03:17:29Z","published":"2023-12-21T17:28:09Z","title":"Multi-Sentence Grounding for Long-term Instructional Video","summary":"  In this paper, we aim to establish an automatic, scalable pipeline for\ndenoising the large-scale instructional dataset and construct a high-quality\nvideo-text dataset with multiple descriptive steps supervision, named\nHowToStep. We make the following contributions: (i) improving the quality of\nsentences in dataset by upgrading ASR systems to reduce errors from speech\nrecognition and prompting a large language model to transform noisy ASR\ntranscripts into descriptive steps; (ii) proposing a Transformer-based\narchitecture with all texts as queries, iteratively attending to the visual\nfeatures, to temporally align the generated steps to corresponding video\nsegments. To measure the quality of our curated datasets, we train models for\nthe task of multi-sentence grounding on it, i.e., given a long-form video, and\nassociated multiple sentences, to determine their corresponding timestamps in\nthe video simultaneously, as a result, the model shows superior performance on\na series of multi-sentence grounding tasks, surpassing existing\nstate-of-the-art methods by a significant margin on three public benchmarks,\nnamely, 9.0% on HT-Step, 5.1% on HTM-Align and 1.9% on CrossTask. All codes,\nmodels, and the resulting dataset have been publicly released.\n","authors":["Zeqian Li","Qirui Chen","Tengda Han","Ya Zhang","Yanfeng Wang","Weidi Xie"],"pdf_url":"https://arxiv.org/pdf/2312.14055v2.pdf","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2407.15831v1","updated":"2024-07-22T17:50:31Z","published":"2024-07-22T17:50:31Z","title":"NV-Retriever: Improving text embedding models with effective\n  hard-negative mining","summary":"  Text embedding models have been popular for information retrieval\napplications such as semantic search and Question-Answering systems based on\nRetrieval-Augmented Generation (RAG). Those models are typically Transformer\nmodels that are fine-tuned with contrastive learning objectives. Many papers\nintroduced new embedding model architectures and training approaches, however,\none of the key ingredients, the process of mining negative passages, remains\npoorly explored or described. One of the challenging aspects of fine-tuning\nembedding models is the selection of high quality hard-negative passages for\ncontrastive learning. In this paper we propose a family of positive-aware\nmining methods that leverage the positive relevance score for more effective\nfalse negatives removal. We also provide a comprehensive ablation study on\nhard-negative mining methods over their configurations, exploring different\nteacher and base models. We demonstrate the efficacy of our proposed methods by\nintroducing the NV-Retriever-v1 model, which scores 60.9 on MTEB Retrieval\n(BEIR) benchmark and 0.65 points higher than previous methods. The model placed\n1st when it was published to MTEB Retrieval on July 07, 2024.\n","authors":["Gabriel de Souza P. Moreira","Radek Osmulski","Mengyao Xu","Ronay Ak","Benedikt Schifferer","Even Oldridge"],"pdf_url":"https://arxiv.org/pdf/2407.15831v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16212v2","updated":"2024-07-22T17:20:14Z","published":"2024-06-23T20:33:49Z","title":"A Mechanism for Optimizing Media Recommender Systems","summary":"  A mechanism is described that addresses the fundamental trade off between\nmedia producers who want to increase reach and consumers who provide attention\nbased on the rate of utility received, and where overreach negatively impacts\nthat rate. An optimal solution can be achieved when the media source considers\nthe impact of overreach in a cost function used in determining the optimal\ndistribution of content to maximize individual consumer utility and\nparticipation. The result is a Nash equilibrium between producer and consumer\nthat is also Pareto efficient. Comparison with the literature on Recommender\nsystems highlights the advantages of the mechanism, including identifying an\noptimal content volume for the consumer and improvements for optimizing with\nmultiple objectives. A practical algorithm for generating the optimal\ndistribution for each consumer is provided.\n","authors":["Brian McFadden"],"pdf_url":"https://arxiv.org/pdf/2406.16212v2.pdf","comment":"Main Paper: 20 pages, Appendix with proofs and additional material:\n  26 pages. This version fixes typos"},{"id":"http://arxiv.org/abs/2407.15748v1","updated":"2024-07-22T15:53:27Z","published":"2024-07-22T15:53:27Z","title":"MoRSE: Bridging the Gap in Cybersecurity Expertise with Retrieval\n  Augmented Generation","summary":"  In this paper, we introduce MoRSE (Mixture of RAGs Security Experts), the\nfirst specialised AI chatbot for cybersecurity. MoRSE aims to provide\ncomprehensive and complete knowledge about cybersecurity. MoRSE uses two RAG\n(Retrieval Augmented Generation) systems designed to retrieve and organize\ninformation from multidimensional cybersecurity contexts. MoRSE differs from\ntraditional RAGs by using parallel retrievers that work together to retrieve\nsemantically related information in different formats and structures. Unlike\ntraditional Large Language Models (LLMs) that rely on Parametric Knowledge\nBases, MoRSE retrieves relevant documents from Non-Parametric Knowledge Bases\nin response to user queries. Subsequently, MoRSE uses this information to\ngenerate accurate answers. In addition, MoRSE benefits from real-time updates\nto its knowledge bases, enabling continuous knowledge enrichment without\nretraining. We have evaluated the effectiveness of MoRSE against other\nstate-of-the-art LLMs, evaluating the system on 600 cybersecurity specific\nquestions. The experimental evaluation has shown that the improvement in terms\nof relevance and correctness of the answer is more than 10\\% compared to known\nsolutions such as GPT-4 and Mixtral 7x8.\n","authors":["Marco Simoni","Andrea Saracino","Vinod P.","Mauro Conti"],"pdf_url":"https://arxiv.org/pdf/2407.15748v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11879v2","updated":"2024-07-22T13:44:41Z","published":"2023-03-21T14:23:46Z","title":"Multimodal Pre-training Framework for Sequential Recommendation via\n  Contrastive Learning","summary":"  Current multimodal sequential recommendation models are often unable to\neffectively explore and capture correlations among behavior sequences of users\nand items across different modalities, either neglecting correlations among\nsequence representations or inadequately capturing associations between\nmultimodal data and sequence data in their representations. To address this\nproblem, we explore multimodal pre-training in the context of sequential\nrecommendation, with the aim of enhancing fusion and utilization of multimodal\ninformation. We propose a novel Multimodal Pre-training for Sequential\nRecommendation (MP4SR) framework, which utilizes contrastive losses to capture\nthe correlation among different modality sequences of users, as well as the\ncorrelation among different modality sequences of users and items. MP4SR\nconsists of three key components: 1) multimodal feature extraction, 2) a\nbackbone network, Multimodal Mixup Sequence Encoder (M2SE), and 3) pre-training\ntasks. After utilizing pre-trained encoders to generate initial multimodal\nfeatures of items, M2SE adopts a complementary sequence mixup strategy to fuse\ndifferent modality sequences, and leverages contrastive learning to capture\nmodality interactions at the sequence-to-sequence and sequence-to-item levels.\nExtensive experiments on four real-world datasets demonstrate that MP4SR\noutperforms state-of-the-art approaches in both normal and cold-start settings.\nWe further highlight the efficacy of incorporating multimodal pre-training in\nsequential recommendation representation learning, serving as an effective\nregularizer and optimizing the parameter space for the recommendation task.\n","authors":["Lingzi Zhang","Xin Zhou","Zhiwei Zeng","Zhiqi Shen"],"pdf_url":"https://arxiv.org/pdf/2303.11879v2.pdf","comment":"22 pages"},{"id":"http://arxiv.org/abs/2407.15620v1","updated":"2024-07-22T13:27:51Z","published":"2024-07-22T13:27:51Z","title":"Dual Test-time Training for Out-of-distribution Recommender System","summary":"  Deep learning has been widely applied in recommender systems, which has\nachieved revolutionary progress recently. However, most existing learning-based\nmethods assume that the user and item distributions remain unchanged between\nthe training phase and the test phase. However, the distribution of user and\nitem features can naturally shift in real-world scenarios, potentially\nresulting in a substantial decrease in recommendation performance. This\nphenomenon can be formulated as an Out-Of-Distribution (OOD) recommendation\nproblem. To address this challenge, we propose a novel Dual Test-Time-Training\nframework for OOD Recommendation, termed DT3OR. In DT3OR, we incorporate a\nmodel adaptation mechanism during the test-time phase to carefully update the\nrecommendation model, allowing the model to specially adapt to the shifting\nuser and item features. To be specific, we propose a self-distillation task and\na contrastive task to assist the model learning both the user's invariant\ninterest preferences and the variant user/item characteristics during the\ntest-time phase, thus facilitating a smooth adaptation to the shifting\nfeatures. Furthermore, we provide theoretical analysis to support the rationale\nbehind our dual test-time training framework. To the best of our knowledge,\nthis paper is the first work to address OOD recommendation via a\ntest-time-training strategy. We conduct experiments on three datasets with\nvarious backbones. Comprehensive experimental results have demonstrated the\neffectiveness of DT3OR compared to other state-of-the-art baselines.\n","authors":["Xihong Yang","Yiqi Wang","Jin Chen","Wenqi Fan","Xiangyu Zhao","En Zhu","Xinwang Liu","Defu Lian"],"pdf_url":"https://arxiv.org/pdf/2407.15620v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.09417v2","updated":"2024-07-22T12:28:05Z","published":"2024-07-12T16:47:34Z","title":"Mitigating Entity-Level Hallucination in Large Language Models","summary":"  The emergence of Large Language Models (LLMs) has revolutionized how users\naccess information, shifting from traditional search engines to direct\nquestion-and-answer interactions with LLMs. However, the widespread adoption of\nLLMs has revealed a significant challenge known as hallucination, wherein LLMs\ngenerate coherent yet factually inaccurate responses. This hallucination\nphenomenon has led to users' distrust in information retrieval systems based on\nLLMs. To tackle this challenge, this paper proposes Dynamic Retrieval\nAugmentation based on hallucination Detection (DRAD) as a novel method to\ndetect and mitigate hallucinations in LLMs. DRAD improves upon traditional\nretrieval augmentation by dynamically adapting the retrieval process based on\nreal-time hallucination detection. It features two main components: Real-time\nHallucination Detection (RHD) for identifying potential hallucinations without\nexternal models, and Self-correction based on External Knowledge (SEK) for\ncorrecting these errors using external knowledge. Experiment results show that\nDRAD demonstrates superior performance in both detecting and mitigating\nhallucinations in LLMs. All of our code and data are open-sourced at\nhttps://github.com/oneal2000/EntityHallucination.\n","authors":["Weihang Su","Yichen Tang","Qingyao Ai","Changyue Wang","Zhijing Wu","Yiqun Liu"],"pdf_url":"https://arxiv.org/pdf/2407.09417v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15546v1","updated":"2024-07-22T11:13:07Z","published":"2024-07-22T11:13:07Z","title":"Personalization of Dataset Retrieval Results using a Metadata-based Data\n  Valuation Method","summary":"  In this paper, we propose a novel data valuation method for a Dataset\nRetrieval (DR) use case in Ireland's National mapping agency. To the best of\nour knowledge, data valuation has not yet been applied to Dataset Retrieval. By\nleveraging metadata and a user's preferences, we estimate the personal value of\neach dataset to facilitate dataset retrieval and filtering. We then validated\nthe data value-based ranking against the stakeholders' ranking of the datasets.\nThe proposed data valuation method and use case demonstrated that data\nvaluation is promising for dataset retrieval. For instance, the outperforming\ndataset retrieval based on our approach obtained 0.8207 in terms of NDCG@5 (the\ntruncated Normalized Discounted Cumulative Gain at 5). This study is unique in\nits exploration of a data valuation-based approach to dataset retrieval and\nstands out because, unlike most existing methods, our approach is validated\nusing the stakeholders ranking of the datasets.\n","authors":["Malick Ebiele","Malika Bendechache","Eamonn Clinton","Rob Brennan"],"pdf_url":"https://arxiv.org/pdf/2407.15546v1.pdf","comment":"5 pages, 1 figure"},{"id":"http://arxiv.org/abs/2407.15476v1","updated":"2024-07-22T08:40:27Z","published":"2024-07-22T08:40:27Z","title":"MODRL-TA:A Multi-Objective Deep Reinforcement Learning Framework for\n  Traffic Allocation in E-Commerce Search","summary":"  Traffic allocation is a process of redistributing natural traffic to products\nby adjusting their positions in the post-search phase, aimed at effectively\nfostering merchant growth, precisely meeting customer demands, and ensuring the\nmaximization of interests across various parties within e-commerce platforms.\nExisting methods based on learning to rank neglect the long-term value of\ntraffic allocation, whereas approaches of reinforcement learning suffer from\nbalancing multiple objectives and the difficulties of cold starts within\nrealworld data environments. To address the aforementioned issues, this paper\npropose a multi-objective deep reinforcement learning framework consisting of\nmulti-objective Q-learning (MOQ), a decision fusion algorithm (DFM) based on\nthe cross-entropy method(CEM), and a progressive data augmentation system(PDA).\nSpecifically. MOQ constructs ensemble RL models, each dedicated to an\nobjective, such as click-through rate, conversion rate, etc. These models\nindividually determine the position of items as actions, aiming to estimate the\nlong-term value of multiple objectives from an individual perspective. Then we\nemploy DFM to dynamically adjust weights among objectives to maximize long-term\nvalue, addressing temporal dynamics in objective preferences in e-commerce\nscenarios. Initially, PDA trained MOQ with simulated data from offline logs. As\nexperiments progressed, it strategically integrated real user interaction data,\nultimately replacing the simulated dataset to alleviate distributional shifts\nand the cold start problem. Experimental results on real-world online\ne-commerce systems demonstrate the significant improvements of MODRL-TA, and we\nhave successfully deployed MODRL-TA on an e-commerce search platform.\n","authors":["Peng Cheng","Huimu Wang","Jinyuan Zhao","Yihao Wang","Enqiang Xu","Yu Zhao","Zhuojian Xiao","Songlin Wang","Guoyu Tang","Lin Liu","Sulong Xu"],"pdf_url":"https://arxiv.org/pdf/2407.15476v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15462v1","updated":"2024-07-22T08:19:34Z","published":"2024-07-22T08:19:34Z","title":"Efficient Retrieval with Learned Similarities","summary":"  Retrieval plays a fundamental role in recommendation systems, search, and\nnatural language processing by efficiently finding relevant items from a large\ncorpus given a query. Dot products have been widely used as the similarity\nfunction in such retrieval tasks, thanks to Maximum Inner Product Search (MIPS)\nthat enabled efficient retrieval based on dot products. However,\nstate-of-the-art retrieval algorithms have migrated to learned similarities.\nSuch algorithms vary in form; the queries can be represented with multiple\nembeddings, complex neural networks can be deployed, the item ids can be\ndecoded directly from queries using beam search, and multiple approaches can be\ncombined in hybrid solutions. Unfortunately, we lack efficient solutions for\nretrieval in these state-of-the-art setups. Our work investigates techniques\nfor approximate nearest neighbor search with learned similarity functions. We\nfirst prove that Mixture-of-Logits (MoL) is a universal approximator, and can\nexpress all learned similarity functions. We next propose techniques to\nretrieve the approximate top K results using MoL with a tight bound. We finally\ncompare our techniques with existing approaches, showing that MoL sets new\nstate-of-the-art results on recommendation retrieval tasks, and our approximate\ntop-k retrieval with learned similarities outperforms baselines by up to two\norders of magnitude in latency, while achieving > .99 recall rate of exact\nalgorithms.\n","authors":["Bailu Ding","Jiaqi Zhai"],"pdf_url":"https://arxiv.org/pdf/2407.15462v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.01280v2","updated":"2024-07-22T06:44:20Z","published":"2024-06-03T12:48:38Z","title":"Demo: Soccer Information Retrieval via Natural Queries using SoccerRAG","summary":"  The rapid evolution of digital sports media necessitates sophisticated\ninformation retrieval systems that can efficiently parse extensive multimodal\ndatasets. This paper demonstrates SoccerRAG, an innovative framework designed\nto harness the power of Retrieval Augmented Generation (RAG) and Large Language\nModels (LLMs) to extract soccer-related information through natural language\nqueries. By leveraging a multimodal dataset, SoccerRAG supports dynamic\nquerying and automatic data validation, enhancing user interaction and\naccessibility to sports archives. We present a novel interactive user interface\n(UI) based on the Chainlit framework which wraps around the core functionality,\nand enable users to interact with the SoccerRAG framework in a chatbot-like\nvisual manner.\n","authors":["Aleksander Theo Strand","Sushant Gautam","Cise Midoglu","Pål Halvorsen"],"pdf_url":"https://arxiv.org/pdf/2406.01280v2.pdf","comment":"accepted to CBMI 2024 as a demonstration;\n  https://github.com/simula/soccer-rag. arXiv admin note: text overlap with\n  arXiv:2406.01273"},{"id":"http://arxiv.org/abs/2406.01273v2","updated":"2024-07-22T06:42:44Z","published":"2024-06-03T12:39:04Z","title":"SoccerRAG: Multimodal Soccer Information Retrieval via Natural Queries","summary":"  The rapid evolution of digital sports media necessitates sophisticated\ninformation retrieval systems that can efficiently parse extensive multimodal\ndatasets. This paper introduces SoccerRAG, an innovative framework designed to\nharness the power of Retrieval Augmented Generation (RAG) and Large Language\nModels (LLMs) to extract soccer-related information through natural language\nqueries. By leveraging a multimodal dataset, SoccerRAG supports dynamic\nquerying and automatic data validation, enhancing user interaction and\naccessibility to sports archives. Our evaluations indicate that SoccerRAG\neffectively handles complex queries, offering significant improvements over\ntraditional retrieval systems in terms of accuracy and user engagement. The\nresults underscore the potential of using RAG and LLMs in sports analytics,\npaving the way for future advancements in the accessibility and real-time\nprocessing of sports data.\n","authors":["Aleksander Theo Strand","Sushant Gautam","Cise Midoglu","Pål Halvorsen"],"pdf_url":"https://arxiv.org/pdf/2406.01273v2.pdf","comment":"accepted to CBMI 2024 as a regular paper;\n  https://github.com/simula/soccer-rag"},{"id":"http://arxiv.org/abs/2407.15411v1","updated":"2024-07-22T06:37:24Z","published":"2024-07-22T06:37:24Z","title":"Scalable Dynamic Embedding Size Search for Streaming Recommendation","summary":"  Recommender systems typically represent users and items by learning their\nembeddings, which are usually set to uniform dimensions and dominate the model\nparameters. However, real-world recommender systems often operate in streaming\nrecommendation scenarios, where the number of users and items continues to\ngrow, leading to substantial storage resource consumption for these embeddings.\nAlthough a few methods attempt to mitigate this by employing embedding size\nsearch strategies to assign different embedding dimensions in streaming\nrecommendations, they assume that the embedding size grows with the frequency\nof users/items, which eventually still exceeds the predefined memory budget\nover time. To address this issue, this paper proposes to learn Scalable\nLightweight Embeddings for streaming recommendation, called SCALL, which can\nadaptively adjust the embedding sizes of users/items within a given memory\nbudget over time. Specifically, we propose to sample embedding sizes from a\nprobabilistic distribution, with the guarantee to meet any predefined memory\nbudget. By fixing the memory budget, the proposed embedding size sampling\nstrategy can increase and decrease the embedding sizes in accordance to the\nfrequency of the corresponding users or items. Furthermore, we develop a\nreinforcement learning-based search paradigm that models each state with mean\npooling to keep the length of the state vectors fixed, invariant to the\nchanging number of users and items. As a result, the proposed method can\nprovide embedding sizes to unseen users and items. Comprehensive empirical\nevaluations on two public datasets affirm the advantageous effectiveness of our\nproposed method.\n","authors":["Yunke Qu","Liang Qu","Tong Chen","Xiangyu Zhao","Quoc Viet Hung Nguyen","Hongzhi Yin"],"pdf_url":"https://arxiv.org/pdf/2407.15411v1.pdf","comment":"accepted to CIKM 2024"},{"id":"http://arxiv.org/abs/2402.09727v3","updated":"2024-07-22T05:33:51Z","published":"2024-02-15T05:40:21Z","title":"A Human-Inspired Reading Agent with Gist Memory of Very Long Contexts","summary":"  Current Large Language Models (LLMs) are not only limited to some maximum\ncontext length, but also are not able to robustly consume long inputs. To\naddress these limitations, we propose ReadAgent, an LLM agent system that\nincreases effective context length up to 20x in our experiments. Inspired by\nhow humans interactively read long documents, we implement ReadAgent as a\nsimple prompting system that uses the advanced language capabilities of LLMs to\n(1) decide what content to store together in a memory episode, (2) compress\nthose memory episodes into short episodic memories called gist memories, and\n(3) take actions to look up passages in the original text if ReadAgent needs to\nremind itself of relevant details to complete a task. We evaluate ReadAgent\nagainst baselines using retrieval methods, using the original long contexts,\nand using the gist memories. These evaluations are performed on three\nlong-document reading comprehension tasks: QuALITY, NarrativeQA, and QMSum.\nReadAgent outperforms the baselines on all three tasks while extending the\neffective context window by 3.5-20x.\n","authors":["Kuang-Huei Lee","Xinyun Chen","Hiroki Furuta","John Canny","Ian Fischer"],"pdf_url":"https://arxiv.org/pdf/2402.09727v3.pdf","comment":"Website: https://read-agent.github.io"},{"id":"http://arxiv.org/abs/2407.12374v2","updated":"2024-07-22T04:07:03Z","published":"2024-07-17T07:52:45Z","title":"Graph Signal Processing for Cross-Domain Recommendation","summary":"  Cross-domain recommendation (CDR) extends conventional recommender systems by\nleveraging user-item interactions from dense domains to mitigate data sparsity\nand the cold start problem. While CDR offers substantial potential for\nenhancing recommendation performance, most existing CDR methods suffer from\nsensitivity to the ratio of overlapping users and intrinsic discrepancy between\nsource and target domains. To overcome these limitations, in this work, we\nexplore the application of graph signal processing (GSP) in CDR scenarios. We\npropose CGSP, a unified CDR framework based on GSP, which employs a\ncross-domain similarity graph constructed by flexibly combining target-only\nsimilarity and source-bridged similarity. By processing personalized graph\nsignals computed for users from either the source or target domain, our\nframework effectively supports both inter-domain and intra-domain\nrecommendations. Our empirical evaluation demonstrates that CGSP consistently\noutperforms various encoder-based CDR approaches in both intra-domain and\ninter-domain recommendation scenarios, especially when the ratio of overlapping\nusers is low, highlighting its significant practical implication in real-world\napplications.\n","authors":["Jeongeun Lee","Seongku Kang","Won-Yong Shin","Jeongwhan Choi","Noseong Park","Dongha Lee"],"pdf_url":"https://arxiv.org/pdf/2407.12374v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10184v2","updated":"2024-07-22T01:10:52Z","published":"2024-07-14T13:03:35Z","title":"Towards Robust Recommendation via Decision Boundary-aware Graph\n  Contrastive Learning","summary":"  In recent years, graph contrastive learning (GCL) has received increasing\nattention in recommender systems due to its effectiveness in reducing bias\ncaused by data sparsity. However, most existing GCL models rely on heuristic\napproaches and usually assume entity independence when constructing contrastive\nviews. We argue that these methods struggle to strike a balance between\nsemantic invariance and view hardness across the dynamic training process, both\nof which are critical factors in graph contrastive learning.\n  To address the above issues, we propose a novel GCL-based recommendation\nframework RGCL, which effectively maintains the semantic invariance of\ncontrastive pairs and dynamically adapts as the model capability evolves\nthrough the training process. Specifically, RGCL first introduces decision\nboundary-aware adversarial perturbations to constrain the exploration space of\ncontrastive augmented views, avoiding the decrease of task-specific\ninformation. Furthermore, to incorporate global user-user and item-item\ncollaboration relationships for guiding on the generation of hard contrastive\nviews, we propose an adversarial-contrastive learning objective to construct a\nrelation-aware view-generator. Besides, considering that unsupervised GCL could\npotentially narrower margins between data points and the decision boundary,\nresulting in decreased model robustness, we introduce the adversarial examples\nbased on maximum perturbations to achieve margin maximization. We also provide\ntheoretical analyses on the effectiveness of our designs. Through extensive\nexperiments on five public datasets, we demonstrate the superiority of RGCL\ncompared against twelve baseline models.\n","authors":["Jiakai Tang","Sunhao Dai","Zexu Sun","Xu Chen","Jun Xu","Wenhui Yu","Lantao Hu","Peng Jiang","Han Li"],"pdf_url":"https://arxiv.org/pdf/2407.10184v2.pdf","comment":"KDD 2024"},{"id":"http://arxiv.org/abs/2403.14666v2","updated":"2024-07-22T20:37:55Z","published":"2024-03-03T03:01:14Z","title":"SyllabusQA: A Course Logistics Question Answering Dataset","summary":"  Automated teaching assistants and chatbots have significant potential to\nreduce the workload of human instructors, especially for logistics-related\nquestion answering, which is important to students yet repetitive for\ninstructors. However, due to privacy concerns, there is a lack of publicly\navailable datasets. We introduce SyllabusQA, an open-source dataset with 63\nreal course syllabi covering 36 majors, containing 5,078 open-ended course\nlogistics-related question-answer pairs that are diverse in both question types\nand answer formats. Since many logistics-related questions contain critical\ninformation like the date of an exam, it is important to evaluate the\nfactuality of answers. We benchmark several strong baselines on this task, from\nlarge language model prompting to retrieval-augmented generation. We introduce\nFact-QA, an LLM-based (GPT-4) evaluation metric to evaluate the factuality of\npredicted answers. We find that despite performing close to humans on\ntraditional metrics of textual similarity, there remains a significant gap\nbetween automated approaches and humans in terms of fact precision.\n","authors":["Nigel Fernandez","Alexander Scarlatos","Andrew Lan"],"pdf_url":"https://arxiv.org/pdf/2403.14666v2.pdf","comment":"ACL 2024: The 62nd Annual Meeting of the Association for\n  Computational Linguistics"},{"id":"http://arxiv.org/abs/2405.01458v2","updated":"2024-07-22T18:46:11Z","published":"2024-05-02T16:44:31Z","title":"UQA: Corpus for Urdu Question Answering","summary":"  This paper introduces UQA, a novel dataset for question answering and text\ncomprehension in Urdu, a low-resource language with over 70 million native\nspeakers. UQA is generated by translating the Stanford Question Answering\nDataset (SQuAD2.0), a large-scale English QA dataset, using a technique called\nEATS (Enclose to Anchor, Translate, Seek), which preserves the answer spans in\nthe translated context paragraphs. The paper describes the process of selecting\nand evaluating the best translation model among two candidates: Google\nTranslator and Seamless M4T. The paper also benchmarks several state-of-the-art\nmultilingual QA models on UQA, including mBERT, XLM-RoBERTa, and mT5, and\nreports promising results. For XLM-RoBERTa-XL, we have an F1 score of 85.99 and\n74.56 EM. UQA is a valuable resource for developing and testing multilingual\nNLP systems for Urdu and for enhancing the cross-lingual transferability of\nexisting models. Further, the paper demonstrates the effectiveness of EATS for\ncreating high-quality datasets for other languages and domains. The UQA dataset\nand the code are publicly available at www.github.com/sameearif/UQA.\n","authors":["Samee Arif","Sualeha Farid","Awais Athar","Agha Ali Raza"],"pdf_url":"https://arxiv.org/pdf/2405.01458v2.pdf","comment":null}],"Machine Learning":[{"id":"http://arxiv.org/abs/2407.07055v2","updated":"2024-07-22T17:59:15Z","published":"2024-07-09T17:21:49Z","title":"Multicell-Fold: geometric learning in folding multicellular life","summary":"  During developmental processes such as embryogenesis, how a group of cells\nfold into specific structures, is a central question in biology that defines\nhow living organisms form. Establishing tissue-level morphology critically\nrelies on how every single cell decides to position itself relative to its\nneighboring cells. Despite its importance, it remains a major challenge to\nunderstand and predict the behavior of every cell within the living tissue over\ntime during such intricate processes. To tackle this question, we propose a\ngeometric deep learning model that can predict multicellular folding and\nembryogenesis, accurately capturing the highly convoluted spatial interactions\namong cells. We demonstrate that multicellular data can be represented with\nboth granular and foam-like physical pictures through a unified graph data\nstructure, considering both cellular interactions and cell junction networks.\nWe successfully use our model to achieve two important tasks, interpretable 4-D\nmorphological sequence alignment, and predicting local cell rearrangements\nbefore they occur at single-cell resolution. Furthermore, using an activation\nmap and ablation studies, we demonstrate that cell geometries and cell junction\nnetworks together regulate local cell rearrangement which is critical for\nembryo morphogenesis. This approach provides a novel paradigm to study\nmorphogenesis, highlighting a unified data structure and harnessing the power\nof geometric deep learning to accurately model the mechanisms and behaviors of\ncells during development. It offers a pathway toward creating a unified dynamic\nmorphological atlas for a variety of developmental processes such as\nembryogenesis.\n","authors":["Haiqian Yang","Anh Q. Nguyen","Dapeng Bi","Markus J. Buehler","Ming Guo"],"pdf_url":"https://arxiv.org/pdf/2407.07055v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15845v1","updated":"2024-07-22T17:59:10Z","published":"2024-07-22T17:59:10Z","title":"Reconstructing Training Data From Real World Models Trained with\n  Transfer Learning","summary":"  Current methods for reconstructing training data from trained classifiers are\nrestricted to very small models, limited training set sizes, and low-resolution\nimages. Such restrictions hinder their applicability to real-world scenarios.\nIn this paper, we present a novel approach enabling data reconstruction in\nrealistic settings for models trained on high-resolution images. Our method\nadapts the reconstruction scheme of arXiv:2206.07758 to real-world scenarios --\nspecifically, targeting models trained via transfer learning over image\nembeddings of large pre-trained models like DINO-ViT and CLIP. Our work employs\ndata reconstruction in the embedding space rather than in the image space,\nshowcasing its applicability beyond visual data. Moreover, we introduce a novel\nclustering-based method to identify good reconstructions from thousands of\ncandidates. This significantly improves on previous works that relied on\nknowledge of the training set to identify good reconstructed images. Our\nfindings shed light on a potential privacy risk for data leakage from models\ntrained using transfer learning.\n","authors":["Yakir Oz","Gilad Yehudai","Gal Vardi","Itai Antebi","Michal Irani","Niv Haim"],"pdf_url":"https://arxiv.org/pdf/2407.15845v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15844v1","updated":"2024-07-22T17:59:01Z","published":"2024-07-22T17:59:01Z","title":"HandDGP: Camera-Space Hand Mesh Prediction with Differentiable Global\n  Positioning","summary":"  Predicting camera-space hand meshes from single RGB images is crucial for\nenabling realistic hand interactions in 3D virtual and augmented worlds.\nPrevious work typically divided the task into two stages: given a cropped image\nof the hand, predict meshes in relative coordinates, followed by lifting these\npredictions into camera space in a separate and independent stage, often\nresulting in the loss of valuable contextual and scale information. To prevent\nthe loss of these cues, we propose unifying these two stages into an end-to-end\nsolution that addresses the 2D-3D correspondence problem. This solution enables\nback-propagation from camera space outputs to the rest of the network through a\nnew differentiable global positioning module. We also introduce an image\nrectification step that harmonizes both the training dataset and the input\nimage as if they were acquired with the same camera, helping to alleviate the\ninherent scale-depth ambiguity of the problem. We validate the effectiveness of\nour framework in evaluations against several baselines and state-of-the-art\napproaches across three public benchmarks.\n","authors":["Eugene Valassakis","Guillermo Garcia-Hernando"],"pdf_url":"https://arxiv.org/pdf/2407.15844v1.pdf","comment":"To be presented at ECCV 2024"},{"id":"http://arxiv.org/abs/2406.18451v2","updated":"2024-07-22T17:52:19Z","published":"2024-06-26T16:00:35Z","title":"Detecting Brittle Decisions for Free: Leveraging Margin Consistency in\n  Deep Robust Classifiers","summary":"  Despite extensive research on adversarial training strategies to improve\nrobustness, the decisions of even the most robust deep learning models can\nstill be quite sensitive to imperceptible perturbations, creating serious risks\nwhen deploying them for high-stakes real-world applications. While detecting\nsuch cases may be critical, evaluating a model's vulnerability at a\nper-instance level using adversarial attacks is computationally too intensive\nand unsuitable for real-time deployment scenarios. The input space margin is\nthe exact score to detect non-robust samples and is intractable for deep neural\nnetworks. This paper introduces the concept of margin consistency -- a property\nthat links the input space margins and the logit margins in robust models --\nfor efficient detection of vulnerable samples. First, we establish that margin\nconsistency is a necessary and sufficient condition to use a model's logit\nmargin as a score for identifying non-robust samples. Next, through\ncomprehensive empirical analysis of various robustly trained models on CIFAR10\nand CIFAR100 datasets, we show that they indicate strong margin consistency\nwith a strong correlation between their input space margins and the logit\nmargins. Then, we show that we can effectively use the logit margin to\nconfidently detect brittle decisions with such models and accurately estimate\nrobust accuracy on an arbitrarily large test set by estimating the input\nmargins only on a small subset. Finally, we address cases where the model is\nnot sufficiently margin-consistent by learning a pseudo-margin from the feature\nrepresentation. Our findings highlight the potential of leveraging deep\nrepresentations to efficiently assess adversarial vulnerability in deployment\nscenarios.\n","authors":["Jonas Ngnawé","Sabyasachi Sahoo","Yann Pequignot","Frédéric Precioso","Christian Gagné"],"pdf_url":"https://arxiv.org/pdf/2406.18451v2.pdf","comment":"11 pages, 7 figures, 2 tables, 1 algorithm. Version Update: Figure 6"},{"id":"http://arxiv.org/abs/2405.04657v3","updated":"2024-07-22T17:48:37Z","published":"2024-05-07T20:30:14Z","title":"ACEGEN: Reinforcement learning of generative chemical agents for drug\n  discovery","summary":"  In recent years, reinforcement learning (RL) has emerged as a valuable tool\nin drug design, offering the potential to propose and optimize molecules with\ndesired properties. However, striking a balance between capabilities,\nflexibility, reliability, and efficiency remains challenging due to the\ncomplexity of advanced RL algorithms and the significant reliance on\nspecialized code. In this work, we introduce ACEGEN, a comprehensive and\nstreamlined toolkit tailored for generative drug design, built using TorchRL, a\nmodern RL library that offers thoroughly tested reusable components. We\nvalidate ACEGEN by benchmarking against other published generative modeling\nalgorithms and show comparable or improved performance. We also show examples\nof ACEGEN applied in multiple drug discovery case studies. ACEGEN is accessible\nat \\url{https://github.com/acellera/acegen-open} and available for use under\nthe MIT license.\n","authors":["Albert Bou","Morgan Thomas","Sebastian Dittert","Carles Navarro Ramírez","Maciej Majewski","Ye Wang","Shivam Patel","Gary Tresadern","Mazen Ahmad","Vincent Moens","Woody Sherman","Simone Sciabola","Gianni De Fabritiis"],"pdf_url":"https://arxiv.org/pdf/2405.04657v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.05153v2","updated":"2024-07-22T17:37:44Z","published":"2023-12-08T16:31:52Z","title":"Uncertainty Quantification and Propagation in Surrogate-based Bayesian\n  Inference","summary":"  Surrogate models are statistical or conceptual approximations for more\ncomplex simulation models. In this context, it is crucial to propagate the\nuncertainty induced by limited simulation budget and surrogate approximation\nerror to predictions, inference, and subsequent decision-relevant quantities.\nHowever, quantifying and then propagating the uncertainty of surrogates is\nusually limited to special analytic cases or is otherwise computationally very\nexpensive. In this paper, we propose a framework enabling a scalable, Bayesian\napproach to surrogate modeling with thorough uncertainty quantification,\npropagation, and validation. Specifically, we present three methods for\nBayesian inference with surrogate models given measurement data. This is a task\nwhere the propagation of surrogate uncertainty is especially relevant, because\nfailing to account for it may lead to biased and/or overconfident estimates of\nthe parameters of interest. We showcase our approach in three detailed case\nstudies for linear and nonlinear real-world modeling scenarios. Uncertainty\npropagation in surrogate models enables more reliable and safe approximation of\nexpensive simulators and will therefore be useful in various fields of\napplications.\n","authors":["Philipp Reiser","Javier Enrique Aguilar","Anneli Guthke","Paul-Christian Bürkner"],"pdf_url":"https://arxiv.org/pdf/2312.05153v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.16822v2","updated":"2024-07-22T17:31:43Z","published":"2024-02-26T18:47:27Z","title":"Rainbow Teaming: Open-Ended Generation of Diverse Adversarial Prompts","summary":"  As large language models (LLMs) become increasingly prevalent across many\nreal-world applications, understanding and enhancing their robustness to\nadversarial attacks is of paramount importance. Existing methods for\nidentifying adversarial prompts tend to focus on specific domains, lack\ndiversity, or require extensive human annotations. To address these\nlimitations, we present Rainbow Teaming, a novel black-box approach for\nproducing a diverse collection of adversarial prompts. Rainbow Teaming casts\nadversarial prompt generation as a quality-diversity problem, and uses\nopen-ended search to generate prompts that are both effective and diverse.\nFocusing on the safety domain, we use Rainbow Teaming to target various\nstate-of-the-art LLMs, including the Llama 2 and Llama 3 models. Our approach\nreveals hundreds of effective adversarial prompts, with an attack success rate\nexceeding 90% across all tested models. Furthermore, we demonstrate that\nfine-tuning models with synthetic data generated by the Rainbow Teaming method\nsignificantly enhances their safety without sacrificing general performance or\nhelpfulness. We additionally explore the versatility of Rainbow Teaming by\napplying it to question answering and cybersecurity, showcasing its potential\nto drive robust open-ended self-improvement in a wide range of applications.\n","authors":["Mikayel Samvelyan","Sharath Chandra Raparthy","Andrei Lupu","Eric Hambro","Aram H. Markosyan","Manish Bhatt","Yuning Mao","Minqi Jiang","Jack Parker-Holder","Jakob Foerster","Tim Rocktäschel","Roberta Raileanu"],"pdf_url":"https://arxiv.org/pdf/2402.16822v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15814v1","updated":"2024-07-22T17:26:12Z","published":"2024-07-22T17:26:12Z","title":"Perceptions of Linguistic Uncertainty by Language Models and Humans","summary":"  Uncertainty expressions such as ``probably'' or ``highly unlikely'' are\npervasive in human language. While prior work has established that there is\npopulation-level agreement in terms of how humans interpret these expressions,\nthere has been little inquiry into the abilities of language models to\ninterpret such expressions. In this paper, we investigate how language models\nmap linguistic expressions of uncertainty to numerical responses. Our approach\nassesses whether language models can employ theory of mind in this setting:\nunderstanding the uncertainty of another agent about a particular statement,\nindependently of the model's own certainty about that statement. We evaluate\nboth humans and 10 popular language models on a task created to assess these\nabilities. Unexpectedly, we find that 8 out of 10 models are able to map\nuncertainty expressions to probabilistic responses in a human-like manner.\nHowever, we observe systematically different behavior depending on whether a\nstatement is actually true or false. This sensitivity indicates that language\nmodels are substantially more susceptible to bias based on their prior\nknowledge (as compared to humans). These findings raise important questions and\nhave broad implications for human-AI alignment and AI-AI communication.\n","authors":["Catarina G Belem","Markelle Kelly","Mark Steyvers","Sameer Singh","Padhraic Smyth"],"pdf_url":"https://arxiv.org/pdf/2407.15814v1.pdf","comment":"In submission"},{"id":"http://arxiv.org/abs/2403.20262v2","updated":"2024-07-22T17:24:14Z","published":"2024-03-29T16:13:31Z","title":"ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language\n  Models","summary":"  Research on Large Language Models (LLMs) has recently witnessed an increasing\ninterest in extending models' context size to better capture dependencies\nwithin long documents. While benchmarks have been proposed to assess long-range\nabilities, existing efforts primarily considered generic tasks that are not\nnecessarily aligned with real-world applications. In contrast, our work\nproposes a new benchmark for long-context LLMs focused on a practical meeting\nassistant scenario. In this scenario, the long contexts consist of transcripts\nobtained by automatic speech recognition, presenting unique challenges for LLMs\ndue to the inherent noisiness and oral nature of such data. Our benchmark,\nnamed ELITR-Bench, augments the existing ELITR corpus' transcripts with 271\nmanually crafted questions and their ground-truth answers. Our experiments with\nrecent long-context LLMs on ELITR-Bench highlight a gap between open-source and\nproprietary models, especially when questions are asked sequentially within a\nconversation. We also provide a thorough analysis of our GPT-4-based evaluation\nmethod, encompassing insights from a crowdsourcing study. Our findings suggest\nthat while GPT-4's evaluation scores are correlated with human judges', its\nability to differentiate among more than three score levels may be limited.\n","authors":["Thibaut Thonet","Jos Rozen","Laurent Besacier"],"pdf_url":"https://arxiv.org/pdf/2403.20262v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15811v1","updated":"2024-07-22T17:23:28Z","published":"2024-07-22T17:23:28Z","title":"Stretching Each Dollar: Diffusion Training from Scratch on a\n  Micro-Budget","summary":"  As scaling laws in generative AI push performance, they also simultaneously\nconcentrate the development of these models among actors with large\ncomputational resources. With a focus on text-to-image (T2I) generative models,\nwe aim to address this bottleneck by demonstrating very low-cost training of\nlarge-scale T2I diffusion transformer models. As the computational cost of\ntransformers increases with the number of patches in each image, we propose to\nrandomly mask up to 75% of the image patches during training. We propose a\ndeferred masking strategy that preprocesses all patches using a patch-mixer\nbefore masking, thus significantly reducing the performance degradation with\nmasking, making it superior to model downscaling in reducing computational\ncost. We also incorporate the latest improvements in transformer architecture,\nsuch as the use of mixture-of-experts layers, to improve performance and\nfurther identify the critical benefit of using synthetic images in micro-budget\ntraining. Finally, using only 37M publicly available real and synthetic images,\nwe train a 1.16 billion parameter sparse transformer with only \\$1,890\neconomical cost and achieve a 12.7 FID in zero-shot generation on the COCO\ndataset. Notably, our model achieves competitive FID and high-quality\ngenerations while incurring 118$\\times$ lower cost than stable diffusion models\nand 14$\\times$ lower cost than the current state-of-the-art approach that costs\n\\$28,400. We aim to release our end-to-end training pipeline to further\ndemocratize the training of large-scale diffusion models on micro-budgets.\n","authors":["Vikash Sehwag","Xianghao Kong","Jingtao Li","Michael Spranger","Lingjuan Lyu"],"pdf_url":"https://arxiv.org/pdf/2407.15811v1.pdf","comment":"41 pages, 28 figures, 5 tables"},{"id":"http://arxiv.org/abs/2407.13044v2","updated":"2024-07-22T17:12:39Z","published":"2024-07-17T22:48:47Z","title":"DropKAN: Regularizing KANs by masking post-activations","summary":"  We propose DropKAN (Dropout Kolmogorov-Arnold Networks) a regularization\nmethod that prevents co-adaptation of activation function weights in\nKolmogorov-Arnold Networks (KANs). DropKAN operates by randomly masking some of\nthe post-activations within the KANs computation graph, while scaling-up the\nretained post-activations. We show that this simple procedure that require\nminimal coding effort has a regularizing effect and consistently lead to better\ngeneralization of KANs.\n  We analyze the adaptation of the standard Dropout with KANs and demonstrate\nthat Dropout applied to KANs' neurons can lead to unpredictable behaviour in\nthe feedforward pass. We carry an empirical study with real world Machine\nLearning datasets to validate our findings. Our results suggest that DropKAN is\nconsistently a better alternative to using standard Dropout with KANs, and\nimproves the generalization performance of KANs. Our implementation of DropKAN\nis available at: \\url{https://github.com/Ghaith81/dropkan}.\n","authors":["Mohammed Ghaith Altarabichi"],"pdf_url":"https://arxiv.org/pdf/2407.13044v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15793v1","updated":"2024-07-22T16:51:28Z","published":"2024-07-22T16:51:28Z","title":"CLIP with Generative Latent Replay: a Strong Baseline for Incremental\n  Learning","summary":"  With the emergence of Transformers and Vision-Language Models (VLMs) such as\nCLIP, large pre-trained models have become a common strategy to enhance\nperformance in Continual Learning scenarios. This led to the development of\nnumerous prompting strategies to effectively fine-tune transformer-based models\nwithout succumbing to catastrophic forgetting. However, these methods struggle\nto specialize the model on domains significantly deviating from the\npre-training and preserving its zero-shot capabilities. In this work, we\npropose Continual Generative training for Incremental prompt-Learning, a novel\napproach to mitigate forgetting while adapting a VLM, which exploits generative\nreplay to align prompts to tasks. We also introduce a new metric to evaluate\nzero-shot capabilities within CL benchmarks. Through extensive experiments on\ndifferent domains, we demonstrate the effectiveness of our framework in\nadapting to new tasks while improving zero-shot capabilities. Further analysis\nreveals that our approach can bridge the gap with joint prompt tuning. The\ncodebase is available at https://github.com/aimagelab/mammoth.\n","authors":["Emanuele Frascaroli","Aniello Panariello","Pietro Buzzega","Lorenzo Bonicelli","Angelo Porrello","Simone Calderara"],"pdf_url":"https://arxiv.org/pdf/2407.15793v1.pdf","comment":"15 pages, 1 figure. Accepted at the The 35th British Machine Vision\n  Conference 2024 (BMVC 2024), Glasgow, UK"},{"id":"http://arxiv.org/abs/2407.15792v1","updated":"2024-07-22T16:51:05Z","published":"2024-07-22T16:51:05Z","title":"Robust Mixture Learning when Outliers Overwhelm Small Groups","summary":"  We study the problem of estimating the means of well-separated mixtures when\nan adversary may add arbitrary outliers. While strong guarantees are available\nwhen the outlier fraction is significantly smaller than the minimum mixing\nweight, much less is known when outliers may crowd out low-weight clusters - a\nsetting we refer to as list-decodable mixture learning (LD-ML). In this case,\nadversarial outliers can simulate additional spurious mixture components.\nHence, if all means of the mixture must be recovered up to a small error in the\noutput list, the list size needs to be larger than the number of (true)\ncomponents. We propose an algorithm that obtains order-optimal error guarantees\nfor each mixture mean with a minimal list-size overhead, significantly\nimproving upon list-decodable mean estimation, the only existing method that is\napplicable for LD-ML. Although improvements are observed even when the mixture\nis non-separated, our algorithm achieves particularly strong guarantees when\nthe mixture is separated: it can leverage the mixture structure to partially\ncluster the samples before carefully iterating a base learner for\nlist-decodable mean estimation at different scales.\n","authors":["Daniil Dmitriev","Rares-Darius Buhai","Stefan Tiegel","Alexander Wolters","Gleb Novikov","Amartya Sanyal","David Steurer","Fanny Yang"],"pdf_url":"https://arxiv.org/pdf/2407.15792v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.10818v3","updated":"2024-07-22T16:47:09Z","published":"2023-10-16T20:37:36Z","title":"Uncertainty-aware transfer across tasks using hybrid model-based\n  successor feature reinforcement learning","summary":"  Sample efficiency is central to developing practical reinforcement learning\n(RL) for complex and large-scale decision-making problems. The ability to\ntransfer and generalize knowledge gained from previous experiences to\ndownstream tasks can significantly improve sample efficiency. Recent research\nindicates that successor feature (SF) RL algorithms enable knowledge\ngeneralization between tasks with different rewards but identical transition\ndynamics. It has recently been hypothesized that combining model-based (MB)\nmethods with SF algorithms can alleviate the limitation of fixed transition\ndynamics. Furthermore, uncertainty-aware exploration is widely recognized as\nanother appealing approach for improving sample efficiency. Putting together\ntwo ideas of hybrid model-based successor feature (MB-SF) and uncertainty leads\nto an approach to the problem of sample efficient uncertainty-aware knowledge\ntransfer across tasks with different transition dynamics or/and reward\nfunctions. In this paper, the uncertainty of the value of each action is\napproximated by a Kalman filter (KF)-based multiple-model adaptive estimation.\nThis KF-based framework treats the parameters of a model as random variables.\nTo the best of our knowledge, this is the first attempt at formulating a hybrid\nMB-SF algorithm capable of generalizing knowledge across large or continuous\nstate space tasks with various transition dynamics while requiring less\ncomputation at decision time than MB methods. The number of samples required to\nlearn the tasks was compared to recent SF and MB baselines. The results show\nthat our algorithm generalizes its knowledge across different transition\ndynamics, learns downstream tasks with significantly fewer samples than\nstarting from scratch, and outperforms existing approaches.\n","authors":["Parvin Malekzadeh","Ming Hou","Konstantinos N. Plataniotis"],"pdf_url":"https://arxiv.org/pdf/2310.10818v3.pdf","comment":"40 pages"},{"id":"http://arxiv.org/abs/2407.15786v1","updated":"2024-07-22T16:46:33Z","published":"2024-07-22T16:46:33Z","title":"Concept-Based Interpretable Reinforcement Learning with Limited to No\n  Human Labels","summary":"  Recent advances in reinforcement learning (RL) have predominantly leveraged\nneural network-based policies for decision-making, yet these models often lack\ninterpretability, posing challenges for stakeholder comprehension and trust.\nConcept bottleneck models offer an interpretable alternative by integrating\nhuman-understandable concepts into neural networks. However, a significant\nlimitation in prior work is the assumption that human annotations for these\nconcepts are readily available during training, necessitating continuous\nreal-time input from human annotators. To overcome this limitation, we\nintroduce a novel training scheme that enables RL algorithms to efficiently\nlearn a concept-based policy by only querying humans to label a small set of\ndata, or in the extreme case, without any human labels. Our algorithm,\nLICORICE, involves three main contributions: interleaving concept learning and\nRL training, using a concept ensembles to actively select informative data\npoints for labeling, and decorrelating the concept data with a simple strategy.\nWe show how LICORICE reduces manual labeling efforts to to 500 or fewer concept\nlabels in three environments. Finally, we present an initial study to explore\nhow we can use powerful vision-language models to infer concepts from raw\nvisual inputs without explicit labels at minimal cost to performance.\n","authors":["Zhuorui Ye","Stephanie Milani","Geoffrey J. Gordon","Fei Fang"],"pdf_url":"https://arxiv.org/pdf/2407.15786v1.pdf","comment":"23 pages, 6 figures, 9 tables"},{"id":"http://arxiv.org/abs/2407.15776v1","updated":"2024-07-22T16:29:35Z","published":"2024-07-22T16:29:35Z","title":"In Search of Quantum Advantage: Estimating the Number of Shots in\n  Quantum Kernel Methods","summary":"  Quantum Machine Learning (QML) has gathered significant attention through\napproaches like Quantum Kernel Machines. While these methods hold considerable\npromise, their quantum nature presents inherent challenges. One major challenge\nis the limited resolution of estimated kernel values caused by the finite\nnumber of circuit runs performed on a quantum device. In this study, we propose\na comprehensive system of rules and heuristics for estimating the required\nnumber of circuit runs in quantum kernel methods. We introduce two critical\neffects that necessitate an increased measurement precision through additional\ncircuit runs: the spread effect and the concentration effect. The effects are\nanalyzed in the context of fidelity and projected quantum kernels. To address\nthese phenomena, we develop an approach for estimating desired precision of\nkernel values, which, in turn, is translated into the number of circuit runs.\nOur methodology is validated through extensive numerical simulations, focusing\non the problem of exponential value concentration. We stress that quantum\nkernel methods should not only be considered from the machine learning\nperformance perspective, but also from the context of the resource consumption.\nThe results provide insights into the possible benefits of quantum kernel\nmethods, offering a guidance for their application in quantum machine learning\ntasks.\n","authors":["Artur Miroszewski","Marco Fellous Asiani","Jakub Mielczarek","Bertrand Le Saux","Jakub Nalepa"],"pdf_url":"https://arxiv.org/pdf/2407.15776v1.pdf","comment":"18 + 13 pages, 8 figures. This manuscript is a first release that\n  will be improved in future versions. We wanted to provide this preview now as\n  we recently became aware of extensive modifications in arXiv:2208.11060"},{"id":"http://arxiv.org/abs/2407.15773v1","updated":"2024-07-22T16:25:41Z","published":"2024-07-22T16:25:41Z","title":"STAMP: Outlier-Aware Test-Time Adaptation with Stable Memory Replay","summary":"  Test-time adaptation (TTA) aims to address the distribution shift between the\ntraining and test data with only unlabeled data at test time. Existing TTA\nmethods often focus on improving recognition performance specifically for test\ndata associated with classes in the training set. However, during the\nopen-world inference process, there are inevitably test data instances from\nunknown classes, commonly referred to as outliers. This paper pays attention to\nthe problem that conducts both sample recognition and outlier rejection during\ninference while outliers exist. To address this problem, we propose a new\napproach called STAble Memory rePlay (STAMP), which performs optimization over\na stable memory bank instead of the risky mini-batch. In particular, the memory\nbank is dynamically updated by selecting low-entropy and label-consistent\nsamples in a class-balanced manner. In addition, we develop a self-weighted\nentropy minimization strategy that assigns higher weight to low-entropy\nsamples. Extensive results demonstrate that STAMP outperforms existing TTA\nmethods in terms of both recognition and outlier detection performance. The\ncode is released at https://github.com/yuyongcan/STAMP.\n","authors":["Yongcan Yu","Lijun Sheng","Ran He","Jian Liang"],"pdf_url":"https://arxiv.org/pdf/2407.15773v1.pdf","comment":"Accepted by ECCV 2024"},{"id":"http://arxiv.org/abs/2407.15762v1","updated":"2024-07-22T16:13:38Z","published":"2024-07-22T16:13:38Z","title":"Conditioned Language Policy: A General Framework for Steerable\n  Multi-Objective Finetuning","summary":"  Reward-based finetuning is crucial for aligning language policies with\nintended behaviors (e.g., creativity and safety). A key challenge here is to\ndevelop steerable language models that trade-off multiple (conflicting)\nobjectives in a flexible and efficient manner. This paper presents Conditioned\nLanguage Policy (CLP), a general framework for finetuning language models on\nmultiple objectives. Building on techniques from multi-task training and\nparameter-efficient finetuning, CLP can learn steerable models that effectively\ntrade-off conflicting objectives at inference time. Notably, this does not\nrequire training or maintaining multiple models to achieve different trade-offs\nbetween the objectives. Through an extensive set of experiments and ablations,\nwe show that the CLP framework learns steerable models that outperform and\nPareto-dominate the current state-of-the-art approaches for multi-objective\nfinetuning.\n","authors":["Kaiwen Wang","Rahul Kidambi","Ryan Sullivan","Alekh Agarwal","Christoph Dann","Andrea Michi","Marco Gelmi","Yunxuan Li","Raghav Gupta","Avinava Dubey","Alexandre Ramé","Johan Ferret","Geoffrey Cideron","Le Hou","Hongkun Yu","Amr Ahmed","Aranyak Mehta","Léonard Hussenot","Olivier Bachem","Edouard Leurent"],"pdf_url":"https://arxiv.org/pdf/2407.15762v1.pdf","comment":"40 pages"},{"id":"http://arxiv.org/abs/2407.15756v1","updated":"2024-07-22T16:06:51Z","published":"2024-07-22T16:06:51Z","title":"Model editing for distribution shifts in uranium oxide morphological\n  analysis","summary":"  Deep learning still struggles with certain kinds of scientific data. Notably,\npretraining data may not provide coverage of relevant distribution shifts\n(e.g., shifts induced via the use of different measurement instruments). We\nconsider deep learning models trained to classify the synthesis conditions of\nuranium ore concentrates (UOCs) and show that model editing is particularly\neffective for improving generalization to distribution shifts common in this\ndomain. In particular, model editing outperforms finetuning on two curated\ndatasets comprising of micrographs taken of U$_{3}$O$_{8}$ aged in humidity\nchambers and micrographs acquired with different scanning electron microscopes,\nrespectively.\n","authors":["Davis Brown","Cody Nizinski","Madelyn Shapiro","Corey Fallon","Tianzhixi Yin","Henry Kvinge","Jonathan H. Tu"],"pdf_url":"https://arxiv.org/pdf/2407.15756v1.pdf","comment":"Presented at CV4MS @ CVPR 2024"},{"id":"http://arxiv.org/abs/2407.15754v1","updated":"2024-07-22T16:00:55Z","published":"2024-07-22T16:00:55Z","title":"LongVideoBench: A Benchmark for Long-context Interleaved Video-Language\n  Understanding","summary":"  Large multimodal models (LMMs) are processing increasingly longer and richer\ninputs. Albeit the progress, few public benchmark is available to measure such\ndevelopment. To mitigate this gap, we introduce LongVideoBench, a\nquestion-answering benchmark that features video-language interleaved inputs up\nto an hour long. Our benchmark includes 3,763 varying-length web-collected\nvideos with their subtitles across diverse themes, designed to comprehensively\nevaluate LMMs on long-term multimodal understanding. To achieve this, we\ninterpret the primary challenge as to accurately retrieve and reason over\ndetailed multimodal information from long inputs. As such, we formulate a novel\nvideo question-answering task termed referring reasoning. Specifically, as part\nof the question, it contains a referring query that references related video\ncontexts, called referred context. The model is then required to reason over\nrelevant video details from the referred context. Following the paradigm of\nreferring reasoning, we curate 6,678 human-annotated multiple-choice questions\nin 17 fine-grained categories, establishing one of the most comprehensive\nbenchmarks for long-form video understanding. Evaluations suggest that the\nLongVideoBench presents significant challenges even for the most advanced\nproprietary models (e.g. GPT-4o, Gemini-1.5-Pro, GPT-4-Turbo), while their\nopen-source counterparts show an even larger performance gap. In addition, our\nresults indicate that model performance on the benchmark improves only when\nthey are capable of processing more frames, positioning LongVideoBench as a\nvaluable benchmark for evaluating future-generation long-context LMMs.\n","authors":["Haoning Wu","Dongxu Li","Bei Chen","Junnan Li"],"pdf_url":"https://arxiv.org/pdf/2407.15754v1.pdf","comment":"29 pages"},{"id":"http://arxiv.org/abs/2407.15749v1","updated":"2024-07-22T15:55:08Z","published":"2024-07-22T15:55:08Z","title":"Robustness of Speech Separation Models for Similar-pitch Speakers","summary":"  Single-channel speech separation is a crucial task for enhancing speech\nrecognition systems in multi-speaker environments. This paper investigates the\nrobustness of state-of-the-art Neural Network models in scenarios where the\npitch differences between speakers are minimal. Building on earlier findings by\nDitter and Gerkmann, which identified a significant performance drop for the\n2018 Chimera++ under similar-pitch conditions, our study extends the analysis\nto more recent and sophisticated Neural Network models. Our experiments reveal\nthat modern models have substantially reduced the performance gap for matched\ntraining and testing conditions. However, a substantial performance gap\npersists under mismatched conditions, with models performing well for large\npitch differences but showing worse performance if the speakers' pitches are\nsimilar. These findings motivate further research into the generalizability of\nspeech separation models to similar-pitch speakers and unseen data.\n","authors":["Bunlong Lay","Sebastian Zaczek","Kristina Tesch","Timo Gerkmann"],"pdf_url":"https://arxiv.org/pdf/2407.15749v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15739v1","updated":"2024-07-22T15:41:37Z","published":"2024-07-22T15:41:37Z","title":"Diffusion for Out-of-Distribution Detection on Road Scenes and Beyond","summary":"  In recent years, research on out-of-distribution (OoD) detection for semantic\nsegmentation has mainly focused on road scenes -- a domain with a constrained\namount of semantic diversity. In this work, we challenge this constraint and\nextend the domain of this task to general natural images. To this end, we\nintroduce: 1. the ADE-OoD benchmark, which is based on the ADE20k dataset and\nincludes images from diverse domains with a high semantic diversity, and 2. a\nnovel approach that uses Diffusion score matching for OoD detection (DOoD) and\nis robust to the increased semantic diversity. ADE-OoD features indoor and\noutdoor images, defines 150 semantic categories as in-distribution, and\ncontains a variety of OoD objects. For DOoD, we train a diffusion model with an\nMLP architecture on semantic in-distribution embeddings and build on the score\nmatching interpretation to compute pixel-wise OoD scores at inference time. On\ncommon road scene OoD benchmarks, DOoD performs on par or better than the state\nof the art, without using outliers for training or making assumptions about the\ndata domain. On ADE-OoD, DOoD outperforms previous approaches, but leaves much\nroom for future improvements.\n","authors":["Silvio Galesso","Philipp Schröppel","Hssan Driss","Thomas Brox"],"pdf_url":"https://arxiv.org/pdf/2407.15739v1.pdf","comment":"ECCV 2024 - Benchmark page: https://ade-ood.github.io/"},{"id":"http://arxiv.org/abs/2407.15738v1","updated":"2024-07-22T15:41:23Z","published":"2024-07-22T15:41:23Z","title":"Parallel Split Learning with Global Sampling","summary":"  The expansion of IoT devices and the demands of Deep Learning have\nhighlighted significant challenges in Distributed Deep Learning (DDL) systems.\nParallel Split Learning (PSL) has emerged as a promising derivative of Split\nLearning that is well suited for distributed learning on resource-constrained\ndevices. However, PSL faces several obstacles, such as large effective batch\nsizes, non-IID data distributions, and the straggler effect. We view these\nissues as a sampling dilemma and propose to address them by orchestrating the\nmini-batch sampling process on the server side. We introduce the Uniform Global\nSampling (UGS) method to decouple the effective batch size from the number of\nclients and reduce mini-batch deviation in non-IID settings. To address the\nstraggler effect, we introduce the Latent Dirichlet Sampling (LDS) method,\nwhich generalizes UGS to balance the trade-off between batch deviation and\ntraining time. Our simulations reveal that our proposed methods enhance model\naccuracy by up to 34.1% in non-IID settings and reduce the training time in the\npresence of stragglers by up to 62%. In particular, LDS effectively mitigates\nthe straggler effect without compromising model accuracy or adding significant\ncomputational overhead compared to UGS. Our results demonstrate the potential\nof our methods as a promising solution for DDL in real applications.\n","authors":["Mohammad Kohankhaki","Ahmad Ayad","Mahdi Barhoush","Anke Schmeink"],"pdf_url":"https://arxiv.org/pdf/2407.15738v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02413v2","updated":"2024-07-22T15:37:39Z","published":"2024-01-04T18:53:50Z","title":"Simulation-Based Inference with Quantile Regression","summary":"  We present Neural Quantile Estimation (NQE), a novel Simulation-Based\nInference (SBI) method based on conditional quantile regression. NQE\nautoregressively learns individual one dimensional quantiles for each posterior\ndimension, conditioned on the data and previous posterior dimensions. Posterior\nsamples are obtained by interpolating the predicted quantiles using monotonic\ncubic Hermite spline, with specific treatment for the tail behavior and\nmulti-modal distributions. We introduce an alternative definition for the\nBayesian credible region using the local Cumulative Density Function (CDF),\noffering substantially faster evaluation than the traditional Highest Posterior\nDensity Region (HPDR). In case of limited simulation budget and/or known model\nmisspecification, a post-processing calibration step can be integrated into NQE\nto ensure the unbiasedness of the posterior estimation with negligible\nadditional computational cost. We demonstrate that NQE achieves\nstate-of-the-art performance on a variety of benchmark problems.\n","authors":["He Jia"],"pdf_url":"https://arxiv.org/pdf/2401.02413v2.pdf","comment":"9+13 pages, 8+8 figures, ICML 2024"},{"id":"http://arxiv.org/abs/2407.04856v2","updated":"2024-07-22T15:32:50Z","published":"2024-07-05T20:25:39Z","title":"Explorative Imitation Learning: A Path Signature Approach for Continuous\n  Environments","summary":"  Some imitation learning methods combine behavioural cloning with\nself-supervision to infer actions from state pairs. However, most rely on a\nlarge number of expert trajectories to increase generalisation and human\nintervention to capture key aspects of the problem, such as domain constraints.\nIn this paper, we propose Continuous Imitation Learning from Observation\n(CILO), a new method augmenting imitation learning with two important features:\n(i) exploration, allowing for more diverse state transitions, requiring less\nexpert trajectories and resulting in fewer training iterations; and (ii) path\nsignatures, allowing for automatic encoding of constraints, through the\ncreation of non-parametric representations of agents and expert trajectories.\nWe compared CILO with a baseline and two leading imitation learning methods in\nfive environments. It had the best overall performance of all methods in all\nenvironments, outperforming the expert in two of them.\n","authors":["Nathan Gavenski","Juarez Monteiro","Felipe Meneguzzi","Michael Luck","Odinaldo Rodrigues"],"pdf_url":"https://arxiv.org/pdf/2407.04856v2.pdf","comment":"This paper has been accepted in the 27th European Conference on\n  Artificial Intelligence (ECAI) 2024"},{"id":"http://arxiv.org/abs/2407.15727v1","updated":"2024-07-22T15:30:21Z","published":"2024-07-22T15:30:21Z","title":"Inferring turbulent velocity and temperature fields and their statistics\n  from Lagrangian velocity measurements using physics-informed\n  Kolmogorov-Arnold Networks","summary":"  We propose the Artificial Intelligence Velocimetry-Thermometry (AIVT) method\nto infer hidden temperature fields from experimental turbulent velocity data.\nThis physics-informed machine learning method enables us to infer continuous\ntemperature fields using only sparse velocity data, hence eliminating the need\nfor direct temperature measurements. Specifically, AIVT is based on\nphysics-informed Kolmogorov-Arnold Networks (not neural networks) and is\ntrained by optimizing a combined loss function that minimizes the residuals of\nthe velocity data, boundary conditions, and the governing equations. We apply\nAIVT to a unique set of experimental volumetric and simultaneous temperature\nand velocity data of Rayleigh-B\\'enard convection (RBC) that we acquired by\ncombining Particle Image Thermometry and Lagrangian Particle Tracking. This\nallows us to compare AIVT predictions and measurements directly. We demonstrate\nthat we can reconstruct and infer continuous and instantaneous velocity and\ntemperature fields from sparse experimental data at a fidelity comparable to\ndirect numerical simulations (DNS) of turbulence. This, in turn, enables us to\ncompute important quantities for quantifying turbulence, such as fluctuations,\nviscous and thermal dissipation, and QR distribution. This paradigm shift in\nprocessing experimental data using AIVT to infer turbulent fields at DNS-level\nfidelity is a promising avenue in breaking the current deadlock of quantitative\nunderstanding of turbulence at high Reynolds numbers, where DNS is\ncomputationally infeasible.\n","authors":["Juan Diego Toscano","Theo Käufer","Martin Maxey","Christian Cierpka","George Em Karniadakis"],"pdf_url":"https://arxiv.org/pdf/2407.15727v1.pdf","comment":"50 pages, 8 figures"},{"id":"http://arxiv.org/abs/2407.15724v1","updated":"2024-07-22T15:28:51Z","published":"2024-07-22T15:28:51Z","title":"Beyond Size and Class Balance: Alpha as a New Dataset Quality Metric for\n  Deep Learning","summary":"  In deep learning, achieving high performance on image classification tasks\nrequires diverse training sets. However, dataset diversity is incompletely\nunderstood. The current best practice is to try to maximize dataset size and\nclass balance. Yet large, class-balanced datasets are not guaranteed to be\ndiverse: images can still be arbitrarily similar. We hypothesized that, for a\ngiven model architecture, better model performance can be achieved by\nmaximizing dataset diversity more directly. This could open a path for\nperformance improvement without additional computational resources or\narchitectural advances. To test this hypothesis, we introduce a comprehensive\nframework of diversity measures, developed in ecology, that generalizes\nfamiliar quantities like Shannon entropy by accounting for similarities and\ndifferences among images. (Dataset size and class balance emerge from this\nframework as special cases.) By analyzing thousands of subsets from seven\nmedical datasets representing ultrasound, X-ray, CT, and pathology images, we\nfound that the best correlates of performance were not size or class balance\nbut $A$ -- ``big alpha'' -- a set of generalized entropy measures interpreted\nas the effective number of image-class pairs in the dataset, after accounting\nfor similarities among images. One of these, $A_0$, explained 67\\% of the\nvariance in balanced accuracy across all subsets, vs. 54\\% for class balance\nand just 39\\% for size. The best pair was size and $A_1$ (79\\%), which\noutperformed size and class balance (74\\%). $A$ performed best for subsets from\nindividual datasets as well as across datasets, supporting the generality of\nthese results. We propose maximizing $A$ as a potential new way to improve the\nperformance of deep learning in medical imaging.\n","authors":["Josiah Couch","Ramy Arnaout","Rima Arnaout"],"pdf_url":"https://arxiv.org/pdf/2407.15724v1.pdf","comment":"11 pages, 5 figures, 3 tables"},{"id":"http://arxiv.org/abs/2407.15720v1","updated":"2024-07-22T15:22:34Z","published":"2024-07-22T15:22:34Z","title":"Do Large Language Models Have Compositional Ability? An Investigation\n  into Limitations and Scalability","summary":"  Large language models (LLMs) have emerged as powerful tools for many AI\nproblems and exhibit remarkable in-context learning (ICL) capabilities.\nCompositional ability, solving unseen complex tasks that combine two or more\nsimple tasks, is an essential reasoning ability for Artificial General\nIntelligence. Despite LLM's tremendous success, how they approach composite\ntasks, especially those not encountered during the pretraining phase, remains\nan open question and largely ununderstood. In this study, we delve into the ICL\ncapabilities of LLMs on composite tasks, with only simple tasks as in-context\nexamples. We develop a test suite of composite tasks that include linguistic\nand logical challenges and perform empirical studies across different LLM\nfamilies. We observe that models exhibit divergent behaviors: (1) For simpler\ncomposite tasks that apply distinct mapping mechanisms to different input\nsegments, the models demonstrate decent compositional ability, while scaling up\nthe model enhances this ability; (2) for more complex composite tasks that\ninvolving reasoning multiple steps, where each step represent one task, models\ntypically underperform, and scaling up generally provide no improvements. We\noffer theoretical analysis in a simplified setting, explaining that models\nexhibit compositional capability when the task handles different input parts\nseparately. We believe our work sheds new light on the capabilities of LLMs in\nsolving composite tasks regarding the nature of the tasks and model scale. Our\ndataset and code are available at\n{\\url{https://github.com/OliverXUZY/LLM_Compose}}.\n","authors":["Zhuoyan Xu","Zhenmei Shi","Yingyu Liang"],"pdf_url":"https://arxiv.org/pdf/2407.15720v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15703v1","updated":"2024-07-22T15:10:41Z","published":"2024-07-22T15:10:41Z","title":"Estimating Probability Densities with Transformer and Denoising\n  Diffusion","summary":"  Transformers are often the go-to architecture to build foundation models that\ningest a large amount of training data. But these models do not estimate the\nprobability density distribution when trained on regression problems, yet\nobtaining full probabilistic outputs is crucial to many fields of science,\nwhere the probability distribution of the answer can be non-Gaussian and\nmultimodal. In this work, we demonstrate that training a probabilistic model\nusing a denoising diffusion head on top of the Transformer provides reasonable\nprobability density estimation even for high-dimensional inputs. The combined\nTransformer+Denoising Diffusion model allows conditioning the output\nprobability density on arbitrary combinations of inputs and it is thus a highly\nflexible density function emulator of all possible input/output combinations.\nWe illustrate our Transformer+Denoising Diffusion model by training it on a\nlarge dataset of astronomical observations and measured labels of stars within\nour Galaxy and we apply it to a variety of inference tasks to show that the\nmodel can infer labels accurately with reasonable distributions.\n","authors":["Henry W. Leung","Jo Bovy","Joshua S. Speagle"],"pdf_url":"https://arxiv.org/pdf/2407.15703v1.pdf","comment":"Accepted at the ICML 2024 Workshop on Foundation Models in the Wild"},{"id":"http://arxiv.org/abs/2407.10702v2","updated":"2024-07-22T15:09:46Z","published":"2024-07-15T13:17:48Z","title":"Geometric Analysis of Unconstrained Feature Models with $d=K$","summary":"  Recently, interesting empirical phenomena known as Neural Collapse have been\nobserved during the final phase of training deep neural networks for\nclassification tasks. We examine this issue when the feature dimension d is\nequal to the number of classes K. We demonstrate that two popular unconstrained\nfeature models are strict saddle functions, with every critical point being\neither a global minimum or a strict saddle point that can be exited using\nnegative curvatures. The primary findings conclusively confirm the conjecture\non the unconstrained feature models in previous articles.\n","authors":["Yi Shen","Shao Gu"],"pdf_url":"https://arxiv.org/pdf/2407.10702v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.14084v2","updated":"2024-07-22T15:02:58Z","published":"2022-12-28T20:07:43Z","title":"Multimodal Explainability via Latent Shift applied to COVID-19\n  stratification","summary":"  We are witnessing a widespread adoption of artificial intelligence in\nhealthcare. However, most of the advancements in deep learning in this area\nconsider only unimodal data, neglecting other modalities. Their multimodal\ninterpretation necessary for supporting diagnosis, prognosis and treatment\ndecisions. In this work we present a deep architecture, which jointly learns\nmodality reconstructions and sample classifications using tabular and imaging\ndata. The explanation of the decision taken is computed by applying a latent\nshift that, simulates a counterfactual prediction revealing the features of\neach modality that contribute the most to the decision and a quantitative score\nindicating the modality importance. We validate our approach in the context of\nCOVID-19 pandemic using the AIforCOVID dataset, which contains multimodal data\nfor the early identification of patients at risk of severe outcome. The results\nshow that the proposed method provides meaningful explanations without\ndegrading the classification performance.\n","authors":["Valerio Guarrasi","Lorenzo Tronchin","Domenico Albano","Eliodoro Faiella","Deborah Fazzini","Domiziana Santucci","Paolo Soda"],"pdf_url":"https://arxiv.org/pdf/2212.14084v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.04117v2","updated":"2024-07-22T14:56:46Z","published":"2024-07-04T18:39:20Z","title":"Predictive Coding Networks and Inference Learning: Tutorial and Survey","summary":"  Recent years have witnessed a growing call for renewed emphasis on\nneuroscience-inspired approaches in artificial intelligence research, under the\nbanner of NeuroAI. A prime example of this is predictive coding networks\n(PCNs), based on the neuroscientific framework of predictive coding. This\nframework views the brain as a hierarchical Bayesian inference model that\nminimizes prediction errors through feedback connections. Unlike traditional\nneural networks trained with backpropagation (BP), PCNs utilize inference\nlearning (IL), a more biologically plausible algorithm that explains patterns\nof neural activity that BP cannot. Historically, IL has been more\ncomputationally intensive, but recent advancements have demonstrated that it\ncan achieve higher efficiency than BP with sufficient parallelization.\nFurthermore, PCNs can be mathematically considered a superset of traditional\nfeedforward neural networks (FNNs), significantly extending the range of\ntrainable architectures. As inherently probabilistic (graphical) latent\nvariable models, PCNs provide a versatile framework for both supervised\nlearning and unsupervised (generative) modeling that goes beyond traditional\nartificial neural networks. This work provides a comprehensive review and\ndetailed formal specification of PCNs, particularly situating them within the\ncontext of modern ML methods. Additionally, we introduce a Python library\n(PRECO) for practical implementation. This positions PC as a promising\nframework for future ML innovations.\n","authors":["Björn van Zwol","Ro Jefferson","Egon L. van den Broek"],"pdf_url":"https://arxiv.org/pdf/2407.04117v2.pdf","comment":"46 pages, 13 figures, 8 tables"},{"id":"http://arxiv.org/abs/2407.15687v1","updated":"2024-07-22T14:54:12Z","published":"2024-07-22T14:54:12Z","title":"SoftCVI: contrastive variational inference with self-generated soft\n  labels","summary":"  Estimating a distribution given access to its unnormalized density is pivotal\nin Bayesian inference, where the posterior is generally known only up to an\nunknown normalizing constant. Variational inference and Markov chain Monte\nCarlo methods are the predominant tools for this task; however, both methods\nare often challenging to apply reliably, particularly when the posterior has\ncomplex geometry. Here, we introduce Soft Contrastive Variational Inference\n(SoftCVI), which allows a family of variational objectives to be derived\nthrough a contrastive estimation framework. These objectives have zero variance\ngradient when the variational approximation is exact, without the need for\nspecialized gradient estimators. The approach involves parameterizing a\nclassifier in terms of the variational distribution, which allows the inference\ntask to be reframed as a contrastive estimation problem, aiming to identify a\nsingle true posterior sample among a set of samples. Despite this framing, we\ndo not require positive or negative samples, but rather learn by sampling the\nvariational distribution and computing ground truth soft classification labels\nfrom the unnormalized posterior itself. We empirically investigate the\nperformance on a variety of Bayesian inference tasks, using both using both\nsimple (e.g. normal) and expressive (normalizing flow) variational\ndistributions. We find that SoftCVI objectives often outperform other commonly\nused variational objectives.\n","authors":["Daniel Ward","Mark Beaumont","Matteo Fasiolo"],"pdf_url":"https://arxiv.org/pdf/2407.15687v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15680v1","updated":"2024-07-22T14:49:51Z","published":"2024-07-22T14:49:51Z","title":"HaloQuest: A Visual Hallucination Dataset for Advancing Multimodal\n  Reasoning","summary":"  Hallucination has been a major problem for large language models and remains\na critical challenge when it comes to multimodality in which vision-language\nmodels (VLMs) have to deal with not just textual but also visual inputs.\nDespite rapid progress in VLMs, resources for evaluating and addressing\nmultimodal hallucination are limited and mostly focused on evaluation. This\nwork introduces HaloQuest, a novel visual question answering dataset that\ncaptures various aspects of multimodal hallucination such as false premises,\ninsufficient contexts, and visual challenges. A novel idea from HaloQuest is to\nleverage synthetic images, apart from real ones, to enable dataset creation at\nscale. With over 7.7K examples spanning across a wide variety of categories,\nHaloQuest was designed to be both a challenging benchmark for VLMs and a\nfine-tuning dataset for advancing multimodal reasoning. Our experiments reveal\nthat current models struggle with HaloQuest, with all open-source VLMs\nachieving below 36% accuracy. On the other hand, fine-tuning on HaloQuest\nsignificantly reduces hallucination rates while preserving performance on\nstandard reasoning tasks. Our results discover that benchmarking with generated\nimages is highly correlated (r=0.97) with real images. Last but not least, we\npropose a novel Auto-Eval mechanism that is highly correlated with human raters\n(r=0.99) for evaluating VLMs. In sum, this work makes concrete strides towards\nunderstanding, evaluating, and mitigating hallucination in VLMs, serving as an\nimportant step towards more reliable multimodal AI systems in the future.\n","authors":["Zhecan Wang","Garrett Bingham","Adams Yu","Quoc Le","Thang Luong","Golnaz Ghiasi"],"pdf_url":"https://arxiv.org/pdf/2407.15680v1.pdf","comment":"Accepted as a main conference paper at ECCV 2024\n  (https://github.com/google/haloquest)"},{"id":"http://arxiv.org/abs/2206.02969v6","updated":"2024-07-22T14:45:09Z","published":"2022-06-07T02:10:30Z","title":"A Simple and Optimal Policy Design with Safety against Heavy-Tailed Risk\n  for Stochastic Bandits","summary":"  We study the stochastic multi-armed bandit problem and design new policies\nthat enjoy both worst-case optimality for expected regret and light-tailed risk\nfor regret distribution. Specifically, our policy design (i) enjoys the\nworst-case optimality for the expected regret at order $O(\\sqrt{KT\\ln T})$ and\n(ii) has the worst-case tail probability of incurring a regret larger than any\n$x>0$ being upper bounded by $\\exp(-\\Omega(x/\\sqrt{KT}))$, a rate that we prove\nto be best achievable with respect to $T$ for all worst-case optimal policies.\nOur proposed policy achieves a delicate balance between doing more exploration\nat the beginning of the time horizon and doing more exploitation when\napproaching the end, compared to standard confidence-bound-based policies. We\nalso enhance the policy design to accommodate the \"any-time\" setting where $T$\nis unknown a priori, and prove equivalently desired policy performances as\ncompared to the \"fixed-time\" setting with known $T$. Numerical experiments are\nconducted to illustrate the theoretical findings. We find that from a\nmanagerial perspective, our new policy design yields better tail distributions\nand is preferable than celebrated policies especially when (i) there is a risk\nof under-estimating the volatility profile, or (ii) there is a challenge of\ntuning policy hyper-parameters. We conclude by extending our proposed policy\ndesign to the stochastic linear bandit setting that leads to both worst-case\noptimality in terms of expected regret and light-tailed risk on the regret\ndistribution.\n","authors":["David Simchi-Levi","Zeyu Zheng","Feng Zhu"],"pdf_url":"https://arxiv.org/pdf/2206.02969v6.pdf","comment":"Preliminary version appeared in NeurIPS 2022"},{"id":"http://arxiv.org/abs/2407.15309v1","updated":"2024-07-22T14:37:58Z","published":"2024-07-22T14:37:58Z","title":"vTensor: Flexible Virtual Tensor Management for Efficient LLM Serving","summary":"  Large Language Models (LLMs) are widely used across various domains,\nprocessing millions of daily requests. This surge in demand poses significant\nchallenges in optimizing throughput and latency while keeping costs manageable.\nThe Key-Value (KV) cache, a standard method for retaining previous\ncomputations, makes LLM inference highly bounded by memory. While batching\nstrategies can enhance performance, they frequently lead to significant memory\nfragmentation. Even though cutting-edge systems like vLLM mitigate KV cache\nfragmentation using paged Attention mechanisms, they still suffer from\ninefficient memory and computational operations due to the tightly coupled page\nmanagement and computation kernels.\n  This study introduces the vTensor, an innovative tensor structure for LLM\ninference based on GPU virtual memory management (VMM). vTensor addresses\nexisting limitations by decoupling computation from memory defragmentation and\noffering dynamic extensibility. Our framework employs a CPU-GPU heterogeneous\napproach, ensuring efficient, fragmentation-free memory management while\naccommodating various computation kernels across different LLM architectures.\nExperimental results indicate that vTensor achieves an average speedup of 1.86x\nacross different models, with up to 2.42x in multi-turn chat scenarios.\nAdditionally, vTensor provides average speedups of 2.12x and 3.15x in kernel\nevaluation, reaching up to 3.92x and 3.27x compared to SGLang Triton\nprefix-prefilling kernels and vLLM paged Attention kernel, respectively.\nFurthermore, it frees approximately 71.25% (57GB) of memory on the NVIDIA A100\nGPU compared to vLLM, enabling more memory-intensive workloads.\n","authors":["Jiale Xu","Rui Zhang","Cong Guo","Weiming Hu","Zihan Liu","Feiyang Wu","Yu Feng","Shixuan Sun","Changxu Shao","Yuhong Guo","Junping Zhao","Ke Zhang","Minyi Guo","Jingwen Leng"],"pdf_url":"https://arxiv.org/pdf/2407.15309v1.pdf","comment":"16 pages, 12 figures"},{"id":"http://arxiv.org/abs/2401.02938v2","updated":"2024-07-22T14:34:04Z","published":"2024-01-01T23:10:23Z","title":"Fast and Effective Weight Update for Pruned Large Language Models","summary":"  Pruning large language models (LLMs) is a challenging task due to their\nenormous size. The primary difficulty is fine-tuning the model after pruning,\nwhich is needed to recover the lost performance caused by dropping weights.\nRecent approaches have either ignored fine-tuning entirely, focusing on\nefficient pruning criteria, or attempted layer-wise weight updates, preserving\nthe behavior of each layer. However, even layer-wise weight updates can be\ncostly for LLMs, and previous works have resorted to various approximations.\n  In our paper, we propose a fast and effective weight update algorithm for\npruned layers based on the Alternating Direction Method of Multipliers (ADMM).\nWe further extend it with a simple gradual pruning mask selection and achieve\nstate-of-the-art pruning performance across a wide range of LLMs. Code is\navailable at https://github.com/fmfi-compbio/admm-pruning.\n","authors":["Vladimír Boža"],"pdf_url":"https://arxiv.org/pdf/2401.02938v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.11904v2","updated":"2024-07-22T14:32:46Z","published":"2024-02-19T07:45:04Z","title":"Automated Deterministic Auction Design with Objective Decomposition","summary":"  Identifying high-revenue mechanisms that are both dominant strategy incentive\ncompatible (DSIC) and individually rational (IR) is a fundamental challenge in\nauction design. While theoretical approaches have encountered bottlenecks in\nmulti-item auctions, there has been much empirical progress in automated\ndesigning such mechanisms using machine learning. However, existing research\nprimarily focuses on randomized auctions, with less attention given to the more\npractical deterministic auctions. Therefore, this paper investigates the\nautomated design of deterministic auctions and introduces OD-VVCA, an objective\ndecomposition approach for automated designing Virtual Valuations Combinatorial\nAuctions (VVCAs). Firstly, we restrict our mechanism to deterministic VVCAs,\nwhich are inherently DSIC and IR. Afterward, we utilize a parallelizable\ndynamic programming algorithm to compute the allocation and revenue outcomes of\na VVCA efficiently. We then decompose the revenue objective function into\ncontinuous and piecewise constant discontinuous components, optimizing each\nusing distinct methods. Extensive experiments show that OD-VVCA achieves high\nrevenue in multi-item auctions, especially in large-scale settings where it\noutperforms both randomized and deterministic baselines, indicating its\nefficacy and scalability.\n","authors":["Zhijian Duan","Haoran Sun","Yichong Xia","Siqiang Wang","Zhilin Zhang","Chuan Yu","Jian Xu","Bo Zheng","Xiaotie Deng"],"pdf_url":"https://arxiv.org/pdf/2402.11904v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15665v1","updated":"2024-07-22T14:28:46Z","published":"2024-07-22T14:28:46Z","title":"A spatiotemporal deep learning framework for prediction of crack\n  dynamics in heterogeneous solids: efficient mapping of concrete\n  microstructures to its fracture properties","summary":"  A spatiotemporal deep learning framework is proposed that is capable of 2D\nfull-field prediction of fracture in concrete mesostructures. This framework\nnot only predicts fractures but also captures the entire history of the\nfracture process, from the crack initiation in the interfacial transition zone\nto the subsequent propagation of the cracks in the mortar matrix. In addition,\na convolutional neural network is developed which can predict the averaged\nstress-strain curve of the mesostructures. The UNet modeling framework, which\ncomprises an encoder-decoder section with skip connections, is used as the deep\nlearning surrogate model. Training and test data are generated from\nhigh-fidelity fracture simulations of randomly generated concrete\nmesostructures. These mesostructures include geometric variabilities such as\ndifferent aggregate particle geometrical features, spatial distribution, and\nthe total volume fraction of aggregates. The fracture simulations are carried\nout in Abaqus, utilizing the cohesive phase-field fracture modeling technique\nas the fracture modeling approach. In this work, to reduce the number of\ntraining datasets, the spatial distribution of three sets of material\nproperties for three-phase concrete mesostructures, along with the spatial\nphase-field damage index, are fed to the UNet to predict the corresponding\nstress and spatial damage index at the subsequent step. It is shown that after\nthe training process using this methodology, the UNet model is capable of\naccurately predicting damage on the unseen test dataset by using 470 datasets.\nMoreover, another novel aspect of this work is the conversion of irregular\nfinite element data into regular grids using a developed pipeline. This\napproach allows for the implementation of less complex UNet architecture and\nfacilitates the integration of phase-field fracture equations into surrogate\nmodels for future developments.\n","authors":["Rasoul Najafi Koopas","Shahed Rezaei","Natalie Rauter","Richard Ostwald","Rolf Lammering"],"pdf_url":"https://arxiv.org/pdf/2407.15665v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.04474v3","updated":"2024-07-22T14:27:56Z","published":"2023-12-07T17:51:43Z","title":"Chain of Code: Reasoning with a Language Model-Augmented Code Emulator","summary":"  Code provides a general syntactic structure to build complex programs and\nperform precise computations when paired with a code interpreter - we\nhypothesize that language models (LMs) can leverage code-writing to improve\nChain of Thought reasoning not only for logic and arithmetic tasks, but also\nfor semantic ones (and in particular, those that are a mix of both). For\nexample, consider prompting an LM to write code that counts the number of times\nit detects sarcasm in an essay: the LM may struggle to write an implementation\nfor \"detect_sarcasm(string)\" that can be executed by the interpreter (handling\nthe edge cases would be insurmountable). However, LMs may still produce a valid\nsolution if they not only write code, but also selectively \"emulate\" the\ninterpreter by generating the expected output of \"detect_sarcasm(string)\". In\nthis work, we propose Chain of Code (CoC), a simple yet surprisingly effective\nextension that improves LM code-driven reasoning. The key idea is to encourage\nLMs to format semantic sub-tasks in a program as flexible pseudocode that the\ninterpreter can explicitly catch undefined behaviors and hand off to simulate\nwith an LM (as an \"LMulator\"). Experiments demonstrate that Chain of Code\noutperforms Chain of Thought and other baselines across a variety of\nbenchmarks; on BIG-Bench Hard, Chain of Code achieves 84%, a gain of 12% over\nChain of Thought. In a nutshell, CoC broadens the scope of reasoning questions\nthat LMs can answer by \"thinking in code\".\n","authors":["Chengshu Li","Jacky Liang","Andy Zeng","Xinyun Chen","Karol Hausman","Dorsa Sadigh","Sergey Levine","Li Fei-Fei","Fei Xia","Brian Ichter"],"pdf_url":"https://arxiv.org/pdf/2312.04474v3.pdf","comment":"ICML 2024 Oral; Project webpage: https://chain-of-code.github.io"},{"id":"http://arxiv.org/abs/2405.01557v3","updated":"2024-07-22T14:25:39Z","published":"2024-03-22T13:08:22Z","title":"An Experimental Study on the Rashomon Effect of Balancing Methods in\n  Imbalanced Classification","summary":"  Predictive models may generate biased predictions when classifying imbalanced\ndatasets. This happens when the model favors the majority class, leading to low\nperformance in accurately predicting the minority class. To address this issue,\nbalancing or resampling methods are critical data-centric AI approaches in the\nmodeling process to improve prediction performance. However, there have been\ndebates and questions about the functionality of these methods in recent years.\nIn particular, many candidate models may exhibit very similar predictive\nperformance, called the Rashomon effect, in model selection, and they may even\nproduce different predictions for the same observations. Selecting one of these\nmodels without considering the predictive multiplicity -- which is the case of\nyielding conflicting models' predictions for any sample -- can result in blind\nselection. In this paper, the impact of balancing methods on predictive\nmultiplicity is examined using the Rashomon effect. It is crucial because the\nblind model selection in data-centric AI is risky from a set of approximately\nequally accurate models. This may lead to severe problems in model selection,\nvalidation, and explanation. To tackle this matter, we conducted real dataset\nexperiments to observe the impact of balancing methods on predictive\nmultiplicity through the Rashomon effect by using a newly proposed metric\nobscurity in addition to the existing ones: ambiguity and discrepancy. Our\nfindings showed that balancing methods inflate the predictive multiplicity and\nyield varying results. To monitor the trade-off between the prediction\nperformance and predictive multiplicity for conducting the modeling process\nresponsibly, we proposed using the extended version of the performance-gain\nplot when balancing the training data.\n","authors":["Mustafa Cavus","Przemysław Biecek"],"pdf_url":"https://arxiv.org/pdf/2405.01557v3.pdf","comment":"16 pages, 6 figures"},{"id":"http://arxiv.org/abs/2407.15662v1","updated":"2024-07-22T14:19:19Z","published":"2024-07-22T14:19:19Z","title":"How to Shrink Confidence Sets for Many Equivalent Discrete\n  Distributions?","summary":"  We consider the situation when a learner faces a set of unknown discrete\ndistributions $(p_k)_{k\\in \\mathcal K}$ defined over a common alphabet\n$\\mathcal X$, and can build for each distribution $p_k$ an individual\nhigh-probability confidence set thanks to $n_k$ observations sampled from\n$p_k$. The set $(p_k)_{k\\in \\mathcal K}$ is structured: each distribution $p_k$\nis obtained from the same common, but unknown, distribution q via applying an\nunknown permutation to $\\mathcal X$. We call this\n\\emph{permutation-equivalence}. The goal is to build refined confidence sets\n\\emph{exploiting} this structural property. Like other popular notions of\nstructure (Lipschitz smoothness, Linearity, etc.) permutation-equivalence\nnaturally appears in machine learning problems, and to benefit from its\npotential gain calls for a specific approach. We present a strategy to\neffectively exploit permutation-equivalence, and provide a finite-time\nhigh-probability bound on the size of the refined confidence sets output by the\nstrategy. Since a refinement is not possible for too few observations in\ngeneral, under mild technical assumptions, our finite-time analysis establish\nwhen the number of observations $(n_k)_{k\\in \\mathcal K}$ are large enough so\nthat the output confidence sets improve over initial individual sets. We\ncarefully characterize this event and the corresponding improvement. Further,\nour result implies that the size of confidence sets shrink at asymptotic rates\nof $O(1/\\sqrt{\\sum_{k\\in \\mathcal K} n_k})$ and $O(1/\\max_{k\\in K} n_{k})$,\nrespectively for elements inside and outside the support of q, when the size of\neach individual confidence set shrinks at respective rates of $O(1/\\sqrt{n_k})$\nand $O(1/n_k)$. We illustrate the practical benefit of exploiting permutation\nequivalence on a reinforcement learning task.\n","authors":["Odalric-Ambrym Maillard","Mohammad Sadegh Talebi"],"pdf_url":"https://arxiv.org/pdf/2407.15662v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15660v1","updated":"2024-07-22T14:18:52Z","published":"2024-07-22T14:18:52Z","title":"MuTT: A Multimodal Trajectory Transformer for Robot Skills","summary":"  High-level robot skills represent an increasingly popular paradigm in robot\nprogramming. However, configuring the skills' parameters for a specific task\nremains a manual and time-consuming endeavor. Existing approaches for learning\nor optimizing these parameters often require numerous real-world executions or\ndo not work in dynamic environments. To address these challenges, we propose\nMuTT, a novel encoder-decoder transformer architecture designed to predict\nenvironment-aware executions of robot skills by integrating vision, trajectory,\nand robot skill parameters. Notably, we pioneer the fusion of vision and\ntrajectory, introducing a novel trajectory projection. Furthermore, we\nillustrate MuTT's efficacy as a predictor when combined with a model-based\nrobot skill optimizer. This approach facilitates the optimization of robot\nskill parameters for the current environment, without the need for real-world\nexecutions during optimization. Designed for compatibility with any\nrepresentation of robot skills, MuTT demonstrates its versatility across three\ncomprehensive experiments, showcasing superior performance across two different\nskill representations.\n","authors":["Claudius Kienle","Benjamin Alt","Onur Celik","Philipp Becker","Darko Katic","Rainer Jäkel","Gerhard Neumann"],"pdf_url":"https://arxiv.org/pdf/2407.15660v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.02866v3","updated":"2024-07-22T14:18:42Z","published":"2024-02-05T10:28:20Z","title":"Quantum Normalizing Flows for Anomaly Detection","summary":"  A Normalizing Flow computes a bijective mapping from an arbitrary\ndistribution to a predefined (e.g. normal) distribution. Such a flow can be\nused to address different tasks, e.g. anomaly detection, once such a mapping\nhas been learned. In this work we introduce Normalizing Flows for Quantum\narchitectures, describe how to model and optimize such a flow and evaluate our\nmethod on example datasets. Our proposed models show competitive performance\nfor anomaly detection compared to classical methods, esp. those ones where\nthere are already quantum inspired algorithms available. In the experiments we\ncompare our performance to isolation forests (IF), the local outlier factor\n(LOF) or single-class SVMs.\n","authors":["Bodo Rosenhahn","Christoph Hirche"],"pdf_url":"https://arxiv.org/pdf/2402.02866v3.pdf","comment":"v3: 15 pages, 8 figures"},{"id":"http://arxiv.org/abs/2407.15643v1","updated":"2024-07-22T14:02:28Z","published":"2024-07-22T14:02:28Z","title":"Link Polarity Prediction from Sparse and Noisy Labels via Multiscale\n  Social Balance","summary":"  Signed Graph Neural Networks (SGNNs) have recently gained attention as an\neffective tool for several learning tasks on signed networks, i.e., graphs\nwhere edges have an associated polarity. One of these tasks is to predict the\npolarity of the links for which this information is missing, starting from the\nnetwork structure and the other available polarities. However, when the\navailable polarities are few and potentially noisy, such a task becomes\nchallenging.\n  In this work, we devise a semi-supervised learning framework that builds\naround the novel concept of \\emph{multiscale social balance} to improve the\nprediction of link polarities in settings characterized by limited data\nquantity and quality. Our model-agnostic approach can seamlessly integrate with\nany SGNN architecture, dynamically reweighting the importance of each data\nsample while making strategic use of the structural information from unlabeled\nedges combined with social balance theory.\n  Empirical validation demonstrates that our approach outperforms established\nbaseline models, effectively addressing the limitations imposed by noisy and\nsparse data. This result underlines the benefits of incorporating multiscale\nsocial balance into SGNNs, opening new avenues for robust and accurate\npredictions in signed network analysis.\n","authors":["Marco Minici","Federico Cinus","Francesco Bonchi","Giuseppe Manco"],"pdf_url":"https://arxiv.org/pdf/2407.15643v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15641v1","updated":"2024-07-22T13:59:58Z","published":"2024-07-22T13:59:58Z","title":"Generating Sample-Based Musical Instruments Using Neural Audio Codec\n  Language Models","summary":"  In this paper, we propose and investigate the use of neural audio codec\nlanguage models for the automatic generation of sample-based musical\ninstruments based on text or reference audio prompts. Our approach extends a\ngenerative audio framework to condition on pitch across an 88-key spectrum,\nvelocity, and a combined text/audio embedding. We identify maintaining timbral\nconsistency within the generated instruments as a major challenge. To tackle\nthis issue, we introduce three distinct conditioning schemes. We analyze our\nmethods through objective metrics and human listening tests, demonstrating that\nour approach can produce compelling musical instruments. Specifically, we\nintroduce a new objective metric to evaluate the timbral consistency of the\ngenerated instruments and adapt the average Contrastive Language-Audio\nPretraining (CLAP) score for the text-to-instrument case, noting that its naive\napplication is unsuitable for assessing this task. Our findings reveal a\ncomplex interplay between timbral consistency, the quality of generated\nsamples, and their correspondence to the input prompt.\n","authors":["Shahan Nercessian","Johannes Imort","Ninon Devis","Frederik Blang"],"pdf_url":"https://arxiv.org/pdf/2407.15641v1.pdf","comment":"8 pages, 2 figures. Accepted to the 25th Conference of the\n  International Society for Music Information Retrieval (ISMIR)"},{"id":"http://arxiv.org/abs/2407.14066v2","updated":"2024-07-22T13:50:55Z","published":"2024-07-19T06:50:24Z","title":"360VFI: A Dataset and Benchmark for Omnidirectional Video Frame\n  Interpolation","summary":"  With the development of VR-related techniques, viewers can enjoy a realistic\nand immersive experience through a head-mounted display, while omnidirectional\nvideo with a low frame rate can lead to user dizziness. However, the prevailing\nplane frame interpolation methodologies are unsuitable for Omnidirectional\nVideo Interpolation, chiefly due to the lack of models tailored to such videos\nwith strong distortion, compounded by the scarcity of valuable datasets for\nOmnidirectional Video Frame Interpolation. In this paper, we introduce the\nbenchmark dataset, 360VFI, for Omnidirectional Video Frame Interpolation. We\npresent a practical implementation that introduces a distortion prior from\nomnidirectional video into the network to modulate distortions. We especially\npropose a pyramid distortion-sensitive feature extractor that uses the unique\ncharacteristics of equirectangular projection (ERP) format as prior\ninformation. Moreover, we devise a decoder that uses an affine transformation\nto facilitate the synthesis of intermediate frames further. 360VFI is the first\ndataset and benchmark that explores the challenge of Omnidirectional Video\nFrame Interpolation. Through our benchmark analysis, we presented four\ndifferent distortion conditions scenes in the proposed 360VFI dataset to\nevaluate the challenge triggered by distortion during interpolation. Besides,\nexperimental results demonstrate that Omnidirectional Video Interpolation can\nbe effectively improved by modeling for omnidirectional distortion.\n","authors":["Wenxuan Lu","Mengshun Hu","Yansheng Qiu","Liang Liao","Zheng Wang"],"pdf_url":"https://arxiv.org/pdf/2407.14066v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.19379v6","updated":"2024-07-22T13:50:27Z","published":"2024-02-29T17:27:59Z","title":"Wisdom of the Silicon Crowd: LLM Ensemble Prediction Capabilities Rival\n  Human Crowd Accuracy","summary":"  Human forecasting accuracy in practice relies on the 'wisdom of the crowd'\neffect, in which predictions about future events are significantly improved by\naggregating across a crowd of individual forecasters. Past work on the\nforecasting ability of large language models (LLMs) suggests that frontier\nLLMs, as individual forecasters, underperform compared to the gold standard of\na human crowd forecasting tournament aggregate. In Study 1, we expand this\nresearch by using an LLM ensemble approach consisting of a crowd of twelve\nLLMs. We compare the aggregated LLM predictions on 31 binary questions to that\nof a crowd of 925 human forecasters from a three-month forecasting tournament.\nOur preregistered main analysis shows that the LLM crowd outperforms a simple\nno-information benchmark and is not statistically different from the human\ncrowd. In exploratory analyses, we find that these two approaches are\nequivalent with respect to medium-effect-size equivalence bounds. We also\nobserve an acquiescence effect, with mean model predictions being significantly\nabove 50%, despite an almost even split of positive and negative resolutions.\nMoreover, in Study 2, we test whether LLM predictions (of GPT-4 and Claude 2)\ncan be improved by drawing on human cognitive output. We find that both models'\nforecasting accuracy benefits from exposure to the median human prediction as\ninformation, improving accuracy by between 17% and 28%: though this leads to\nless accurate predictions than simply averaging human and machine forecasts.\nOur results suggest that LLMs can achieve forecasting accuracy rivaling that of\nhuman crowd forecasting tournaments: via the simple, practically applicable\nmethod of forecast aggregation. This replicates the 'wisdom of the crowd'\neffect for LLMs, and opens up their use for a variety of applications\nthroughout society.\n","authors":["Philipp Schoenegger","Indre Tuminauskaite","Peter S. Park","Philip E. Tetlock"],"pdf_url":"https://arxiv.org/pdf/2402.19379v6.pdf","comment":"20 pages; 13 visualizations (nine figures, four tables)"},{"id":"http://arxiv.org/abs/2306.05722v3","updated":"2024-07-22T13:48:36Z","published":"2023-06-09T07:38:38Z","title":"Ridge Estimation with Nonlinear Transformations","summary":"  Ridge estimation is an important manifold learning technique. The goal of\nthis paper is to examine the effects of nonlinear transformations on the ridge\nsets. The main result proves the inclusion relationship between ridges:\n$\\cR(f\\circ p)\\subseteq \\cR(p)$, provided that the transformation $f$ is\nstrictly increasing and concave on the range of the function $p$. Additionally,\ngiven an underlying true manifold $\\cM$, we show that the Hausdorff distance\nbetween $\\cR(f\\circ p)$ and its projection onto $\\cM$ is smaller than the\nHausdorff distance between $\\cR(p)$ and the corresponding projection. This\nmotivates us to apply an increasing and concave transformation before the ridge\nestimation. In specific, we show that the power transformations\n$f^{q}(y)=y^q/q,-\\infty<q\\leq 1$ are increasing and concave on $\\RR_+$, and\nthus we can use such power transformations when $p$ is strictly positive.\nNumerical experiments demonstrate the advantages of the proposed methods.\n","authors":["Zheng Zhai","Hengchao Chen","Zhigang Yao"],"pdf_url":"https://arxiv.org/pdf/2306.05722v3.pdf","comment":"There are some flaws in the proofs for Lemma 1 and Theorem 1. We want\n  to withdraw this version to prevent any potential misunderstanding for\n  readers"},{"id":"http://arxiv.org/abs/2206.04359v2","updated":"2024-07-22T13:47:46Z","published":"2022-06-09T08:59:46Z","title":"Learning Non-Vacuous Generalization Bounds from Optimization","summary":"  One of the fundamental challenges in the deep learning community is to\ntheoretically understand how well a deep neural network generalizes to unseen\ndata. However, current approaches often yield generalization bounds that are\neither too loose to be informative of the true generalization error or only\nvalid to the compressed nets. In this study, we present a simple yet\nnon-vacuous generalization bound from the optimization perspective. We achieve\nthis goal by leveraging that the hypothesis set accessed by stochastic gradient\nalgorithms is essentially fractal-like and thus can derive a tighter bound over\nthe algorithm-dependent Rademacher complexity. The main argument rests on\nmodeling the discrete-time recursion process via a continuous-time stochastic\ndifferential equation driven by fractional Brownian motion. Numerical studies\ndemonstrate that our approach is able to yield plausible generalization\nguarantees for modern neural networks such as ResNet and Vision Transformer,\neven when they are trained on a large-scale dataset (e.g. ImageNet-1K).\n","authors":["Chengli Tan","Jiangshe Zhang","Junmin Liu"],"pdf_url":"https://arxiv.org/pdf/2206.04359v2.pdf","comment":"35pages"},{"id":"http://arxiv.org/abs/2308.04428v2","updated":"2024-07-22T13:36:50Z","published":"2023-08-08T17:56:20Z","title":"Sample-Efficient Linear Representation Learning from Non-IID\n  Non-Isotropic Data","summary":"  A powerful concept behind much of the recent progress in machine learning is\nthe extraction of common features across data from heterogeneous sources or\ntasks. Intuitively, using all of one's data to learn a common representation\nfunction benefits both computational effort and statistical generalization by\nleaving a smaller number of parameters to fine-tune on a given task. Toward\ntheoretically grounding these merits, we propose a general setting of\nrecovering linear operators $M$ from noisy vector measurements $y = Mx + w$,\nwhere the covariates $x$ may be both non-i.i.d. and non-isotropic. We\ndemonstrate that existing isotropy-agnostic representation learning approaches\nincur biases on the representation update, which causes the scaling of the\nnoise terms to lose favorable dependence on the number of source tasks. This in\nturn can cause the sample complexity of representation learning to be\nbottlenecked by the single-task data size. We introduce an adaptation,\n$\\texttt{De-bias & Feature-Whiten}$ ($\\texttt{DFW}$), of the popular\nalternating minimization-descent scheme proposed independently in Collins et\nal., (2021) and Nayer and Vaswani (2022), and establish linear convergence to\nthe optimal representation with noise level scaling down with the\n$\\textit{total}$ source data size. This leads to generalization bounds on the\nsame order as an oracle empirical risk minimizer. We verify the vital\nimportance of $\\texttt{DFW}$ on various numerical simulations. In particular,\nwe show that vanilla alternating-minimization descent fails catastrophically\neven for iid, but mildly non-isotropic data. Our analysis unifies and\ngeneralizes prior work, and provides a flexible framework for a wider range of\napplications, such as in controls and dynamical systems.\n","authors":["Thomas T. C. K. Zhang","Leonardo F. Toso","James Anderson","Nikolai Matni"],"pdf_url":"https://arxiv.org/pdf/2308.04428v2.pdf","comment":"Appeared at ICLR 2024 (spotlight presentation)"},{"id":"http://arxiv.org/abs/2401.01905v2","updated":"2024-07-22T13:33:25Z","published":"2023-12-21T10:20:10Z","title":"Machine-learning-based particle identification with missing data","summary":"  In this work, we introduce a novel method for Particle Identification (PID)\nwithin the scope of the ALICE experiment at the Large Hadron Collider at CERN.\nIdentifying products of ultrarelativisitc collisions delivered by the LHC is\none of the crucial objectives of ALICE. Typically employed PID methods rely on\nhand-crafted selections, which compare experimental data to theoretical\nsimulations. To improve the performance of the baseline methods, novel\napproaches use machine learning models that learn the proper assignment in a\nclassification task. However, because of the various detection techniques used\nby different subdetectors, as well as the limited detector efficiency and\nacceptance, produced particles do not always yield signals in all of the ALICE\ncomponents. This results in data with missing values. Machine learning\ntechniques cannot be trained with such examples, so a significant part of the\ndata is skipped during training. In this work, we propose the first method for\nPID that can be trained with all of the available data examples, including\nincomplete ones. Our approach improves the PID purity and efficiency of the\nselected sample for all investigated particle species.\n","authors":["Miłosz Kasak","Kamil Deja","Maja Karwowska","Monika Jakubowska","Łukasz Graczykowski","Małgorzata Janik"],"pdf_url":"https://arxiv.org/pdf/2401.01905v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15621v1","updated":"2024-07-22T13:29:56Z","published":"2024-07-22T13:29:56Z","title":"RadioRAG: Factual Large Language Models for Enhanced Diagnostics in\n  Radiology Using Dynamic Retrieval Augmented Generation","summary":"  Large language models (LLMs) have advanced the field of artificial\nintelligence (AI) in medicine. However LLMs often generate outdated or\ninaccurate information based on static training datasets. Retrieval augmented\ngeneration (RAG) mitigates this by integrating outside data sources. While\nprevious RAG systems used pre-assembled, fixed databases with limited\nflexibility, we have developed Radiology RAG (RadioRAG) as an end-to-end\nframework that retrieves data from authoritative radiologic online sources in\nreal-time. RadioRAG is evaluated using a dedicated radiologic\nquestion-and-answer dataset (RadioQA). We evaluate the diagnostic accuracy of\nvarious LLMs when answering radiology-specific questions with and without\naccess to additional online information via RAG. Using 80 questions from RSNA\nCase Collection across radiologic subspecialties and 24 additional\nexpert-curated questions, for which the correct gold-standard answers were\navailable, LLMs (GPT-3.5-turbo, GPT-4, Mistral-7B, Mixtral-8x7B, and Llama3 [8B\nand 70B]) were prompted with and without RadioRAG. RadioRAG retrieved\ncontext-specific information from www.radiopaedia.org in real-time and\nincorporated them into its reply. RadioRAG consistently improved diagnostic\naccuracy across all LLMs, with relative improvements ranging from 2% to 54%. It\nmatched or exceeded question answering without RAG across radiologic\nsubspecialties, particularly in breast imaging and emergency radiology.\nHowever, degree of improvement varied among models; GPT-3.5-turbo and\nMixtral-8x7B-instruct-v0.1 saw notable gains, while Mistral-7B-instruct-v0.2\nshowed no improvement, highlighting variability in its effectiveness. LLMs\nbenefit when provided access to domain-specific data beyond their training\ndata. For radiology, RadioRAG establishes a robust framework that substantially\nimproves diagnostic accuracy and factuality in radiological question answering.\n","authors":["Soroosh Tayebi Arasteh","Mahshad Lotfinia","Keno Bressem","Robert Siepmann","Dyke Ferber","Christiane Kuhl","Jakob Nikolas Kather","Sven Nebelung","Daniel Truhn"],"pdf_url":"https://arxiv.org/pdf/2407.15621v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15620v1","updated":"2024-07-22T13:27:51Z","published":"2024-07-22T13:27:51Z","title":"Dual Test-time Training for Out-of-distribution Recommender System","summary":"  Deep learning has been widely applied in recommender systems, which has\nachieved revolutionary progress recently. However, most existing learning-based\nmethods assume that the user and item distributions remain unchanged between\nthe training phase and the test phase. However, the distribution of user and\nitem features can naturally shift in real-world scenarios, potentially\nresulting in a substantial decrease in recommendation performance. This\nphenomenon can be formulated as an Out-Of-Distribution (OOD) recommendation\nproblem. To address this challenge, we propose a novel Dual Test-Time-Training\nframework for OOD Recommendation, termed DT3OR. In DT3OR, we incorporate a\nmodel adaptation mechanism during the test-time phase to carefully update the\nrecommendation model, allowing the model to specially adapt to the shifting\nuser and item features. To be specific, we propose a self-distillation task and\na contrastive task to assist the model learning both the user's invariant\ninterest preferences and the variant user/item characteristics during the\ntest-time phase, thus facilitating a smooth adaptation to the shifting\nfeatures. Furthermore, we provide theoretical analysis to support the rationale\nbehind our dual test-time training framework. To the best of our knowledge,\nthis paper is the first work to address OOD recommendation via a\ntest-time-training strategy. We conduct experiments on three datasets with\nvarious backbones. Comprehensive experimental results have demonstrated the\neffectiveness of DT3OR compared to other state-of-the-art baselines.\n","authors":["Xihong Yang","Yiqi Wang","Jin Chen","Wenqi Fan","Xiangyu Zhao","En Zhu","Xinwang Liu","Defu Lian"],"pdf_url":"https://arxiv.org/pdf/2407.15620v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.06821v3","updated":"2024-07-22T13:19:50Z","published":"2024-01-11T21:04:28Z","title":"Surrogate Neural Networks Local Stability for Aircraft Predictive\n  Maintenance","summary":"  Surrogate Neural Networks are nowadays routinely used in industry as\nsubstitutes for computationally demanding engineering simulations (e.g., in\nstructural analysis). They allow to generate faster predictions and thus\nanalyses in industrial applications e.g., during a product design, testing or\nmonitoring phases. Due to their performance and time-efficiency, these\nsurrogate models are now being developed for use in safety-critical\napplications. Neural network verification and in particular the assessment of\ntheir robustness (e.g., to perturbations) is the next critical step to allow\ntheir inclusion in real-life applications and certification. We assess the\napplicability and scalability of empirical and formal methods in the context of\naircraft predictive maintenance for surrogate neural networks designed to\npredict the stress sustained by an aircraft part from external loads. The case\nstudy covers a high-dimensional input and output space and the verification\nprocess thus accommodates multi-objective constraints. We explore the\ncomplementarity of verification methods in assessing the local stability\nproperty of such surrogate models to input noise. We showcase the effectiveness\nof sequentially combining methods in one verification 'pipeline' and\ndemonstrate the subsequent gain in runtime required to assess the targeted\nproperty.\n","authors":["Mélanie Ducoffe","Guillaume Povéda","Audrey Galametz","Ryma Boumazouza","Marion-Cécile Martin","Julien Baris","Derk Daverschot","Eugene O'Higgins"],"pdf_url":"https://arxiv.org/pdf/2401.06821v3.pdf","comment":"Peer-reviewed and accepted at the 29th International Conference on\n  Formal Methods for Industrial Critical Systems (FMICS 2024) - 15 pages"},{"id":"http://arxiv.org/abs/2407.15611v1","updated":"2024-07-22T13:08:50Z","published":"2024-07-22T13:08:50Z","title":"Distance-based mutual congestion feature selection with genetic\n  algorithm for high-dimensional medical datasets","summary":"  Feature selection poses a challenge in small-sample high-dimensional\ndatasets, where the number of features exceeds the number of observations, as\nseen in microarray, gene expression, and medical datasets. There isn't a\nuniversally optimal feature selection method applicable to any data\ndistribution, and as a result, the literature consistently endeavors to address\nthis issue. One recent approach in feature selection is termed frequency-based\nfeature selection. However, existing methods in this domain tend to overlook\nfeature values, focusing solely on the distribution in the response variable.\nIn response, this paper introduces the Distance-based Mutual Congestion (DMC)\nas a filter method that considers both the feature values and the distribution\nof observations in the response variable. DMC sorts the features of datasets,\nand the top 5% are retained and clustered by KMeans to mitigate\nmulticollinearity. This is achieved by randomly selecting one feature from each\ncluster. The selected features form the feature space, and the search space for\nthe Genetic Algorithm with Adaptive Rates (GAwAR) will be approximated using\nthis feature space. GAwAR approximates the combination of the top 10 features\nthat maximizes prediction accuracy within a wrapper scheme. To prevent\npremature convergence, GAwAR adaptively updates the crossover and mutation\nrates. The hybrid DMC-GAwAR is applicable to binary classification datasets,\nand experimental results demonstrate its superiority over some recent works.\nThe implementation and corresponding data are available at\nhttps://github.com/hnematzadeh/DMC-GAwAR\n","authors":["Hossein Nematzadeh","Joseph Mani","Zahra Nematzadeh","Ebrahim Akbari","Radziah Mohamad"],"pdf_url":"https://arxiv.org/pdf/2407.15611v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.11720v2","updated":"2024-07-22T12:39:21Z","published":"2024-01-22T06:47:00Z","title":"Graph Condensation: A Survey","summary":"  The rapid growth of graph data poses significant challenges in storage,\ntransmission, and particularly the training of graph neural networks (GNNs). To\naddress these challenges, graph condensation (GC) has emerged as an innovative\nsolution. GC focuses on synthesizing a compact yet highly representative graph,\nenabling GNNs trained on it to achieve performance comparable to those trained\non the original large graph. The notable efficacy of GC and its broad prospects\nhave garnered significant attention and spurred extensive research. This survey\npaper provides an up-to-date and systematic overview of GC, organizing existing\nresearch into five categories aligned with critical GC evaluation criteria:\neffectiveness, generalization, efficiency, fairness, and robustness. To\nfacilitate an in-depth and comprehensive understanding of GC, this paper\nexamines various methods under each category and thoroughly discusses two\nessential components within GC: optimization strategies and condensed graph\ngeneration. We also empirically compare and analyze representative GC methods\nwith diverse optimization strategies based on the five proposed GC evaluation\ncriteria. Finally, we explore the applications of GC in various fields, outline\nthe related open-source libraries, and highlight the present challenges and\nnovel insights, with the aim of promoting advancements in future research. The\nrelated resources can be found at\nhttps://github.com/XYGaoG/Graph-Condensation-Papers.\n","authors":["Xinyi Gao","Junliang Yu","Tong Chen","Guanhua Ye","Wentao Zhang","Hongzhi Yin"],"pdf_url":"https://arxiv.org/pdf/2401.11720v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15595v1","updated":"2024-07-22T12:33:27Z","published":"2024-07-22T12:33:27Z","title":"Discrete Flow Matching","summary":"  Despite Flow Matching and diffusion models having emerged as powerful\ngenerative paradigms for continuous variables such as images and videos, their\napplication to high-dimensional discrete data, such as language, is still\nlimited. In this work, we present Discrete Flow Matching, a novel discrete flow\nparadigm designed specifically for generating discrete data. Discrete Flow\nMatching offers several key contributions: (i) it works with a general family\nof probability paths interpolating between source and target distributions;\n(ii) it allows for a generic formula for sampling from these probability paths\nusing learned posteriors such as the probability denoiser ($x$-prediction) and\nnoise-prediction ($\\epsilon$-prediction); (iii) practically, focusing on\nspecific probability paths defined with different schedulers considerably\nimproves generative perplexity compared to previous discrete diffusion and flow\nmodels; and (iv) by scaling Discrete Flow Matching models up to 1.7B\nparameters, we reach 6.7% Pass@1 and 13.4% Pass@10 on HumanEval and 6.7% Pass@1\nand 20.6% Pass@10 on 1-shot MBPP coding benchmarks. Our approach is capable of\ngenerating high-quality discrete data in a non-autoregressive fashion,\nsignificantly closing the gap between autoregressive models and discrete flow\nmodels.\n","authors":["Itai Gat","Tal Remez","Neta Shaul","Felix Kreuk","Ricky T. Q. Chen","Gabriel Synnaeve","Yossi Adi","Yaron Lipman"],"pdf_url":"https://arxiv.org/pdf/2407.15595v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15589v1","updated":"2024-07-22T12:26:08Z","published":"2024-07-22T12:26:08Z","title":"Exploring the Effectiveness of Object-Centric Representations in Visual\n  Question Answering: Comparative Insights with Foundation Models","summary":"  Object-centric (OC) representations, which represent the state of a visual\nscene by modeling it as a composition of objects, have the potential to be used\nin various downstream tasks to achieve systematic compositional generalization\nand facilitate reasoning. However, these claims have not been thoroughly\nanalyzed yet. Recently, foundation models have demonstrated unparalleled\ncapabilities across diverse domains from language to computer vision, marking\nthem as a potential cornerstone of future research for a multitude of\ncomputational tasks. In this paper, we conduct an extensive empirical study on\nrepresentation learning for downstream Visual Question Answering (VQA), which\nrequires an accurate compositional understanding of the scene. We thoroughly\ninvestigate the benefits and trade-offs of OC models and alternative approaches\nincluding large pre-trained foundation models on both synthetic and real-world\ndata, and demonstrate a viable way to achieve the best of both worlds. The\nextensiveness of our study, encompassing over 800 downstream VQA models and 15\ndifferent types of upstream representations, also provides several additional\ninsights that we believe will be of interest to the community at large.\n","authors":["Amir Mohammad Karimi Mamaghan","Samuele Papa","Karl Henrik Johansson","Stefan Bauer","Andrea Dittadi"],"pdf_url":"https://arxiv.org/pdf/2407.15589v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15586v1","updated":"2024-07-22T12:23:26Z","published":"2024-07-22T12:23:26Z","title":"Data driven weather forecasts trained and initialised directly from\n  observations","summary":"  Skilful Machine Learned weather forecasts have challenged our approach to\nnumerical weather prediction, demonstrating competitive performance compared to\ntraditional physics-based approaches. Data-driven systems have been trained to\nforecast future weather by learning from long historical records of past\nweather such as the ECMWF ERA5. These datasets have been made freely available\nto the wider research community, including the commercial sector, which has\nbeen a major factor in the rapid rise of ML forecast systems and the levels of\naccuracy they have achieved. However, historical reanalyses used for training\nand real-time analyses used for initial conditions are produced by data\nassimilation, an optimal blending of observations with a physics-based forecast\nmodel. As such, many ML forecast systems have an implicit and unquantified\ndependence on the physics-based models they seek to challenge. Here we propose\na new approach, training a neural network to predict future weather purely from\nhistorical observations with no dependence on reanalyses. We use raw\nobservations to initialise a model of the atmosphere (in observation space)\nlearned directly from the observations themselves. Forecasts of crucial weather\nparameters (such as surface temperature and wind) are obtained by predicting\nweather parameter observations (e.g. SYNOP surface data) at future times and\narbitrary locations. We present preliminary results on forecasting observations\n12-hours into the future. These already demonstrate successful learning of time\nevolutions of the physical processes captured in real observations. We argue\nthat this new approach, by staying purely in observation space, avoids many of\nthe challenges of traditional data assimilation, can exploit a wider range of\nobservations and is readily expanded to simultaneous forecasting of the full\nEarth system (atmosphere, land, ocean and composition).\n","authors":["Anthony McNally","Christian Lessig","Peter Lean","Eulalie Boucher","Mihai Alexe","Ewan Pinnington","Matthew Chantry","Simon Lang","Chris Burrows","Marcin Chrust","Florian Pinault","Ethel Villeneuve","Niels Bormann","Sean Healy"],"pdf_url":"https://arxiv.org/pdf/2407.15586v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15580v1","updated":"2024-07-22T12:16:56Z","published":"2024-07-22T12:16:56Z","title":"Annealed Multiple Choice Learning: Overcoming limitations of\n  Winner-takes-all with annealing","summary":"  We introduce Annealed Multiple Choice Learning (aMCL) which combines\nsimulated annealing with MCL. MCL is a learning framework handling ambiguous\ntasks by predicting a small set of plausible hypotheses. These hypotheses are\ntrained using the Winner-takes-all (WTA) scheme, which promotes the diversity\nof the predictions. However, this scheme may converge toward an arbitrarily\nsuboptimal local minimum, due to the greedy nature of WTA. We overcome this\nlimitation using annealing, which enhances the exploration of the hypothesis\nspace during training. We leverage insights from statistical physics and\ninformation theory to provide a detailed description of the model training\ntrajectory. Additionally, we validate our algorithm by extensive experiments on\nsynthetic datasets, on the standard UCI benchmark, and on speech separation.\n","authors":["David Perera","Victor Letzelter","Théo Mariotte","Adrien Cortés","Mickael Chen","Slim Essid","Gaël Richard"],"pdf_url":"https://arxiv.org/pdf/2407.15580v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.12707v2","updated":"2024-07-22T12:08:35Z","published":"2024-07-17T16:30:27Z","title":"TTSDS -- Text-to-Speech Distribution Score","summary":"  Many recently published Text-to-Speech (TTS) systems produce audio close to\nreal speech. However, TTS evaluation needs to be revisited to make sense of the\nresults obtained with the new architectures, approaches and datasets. We\npropose evaluating the quality of synthetic speech as a combination of multiple\nfactors such as prosody, speaker identity, and intelligibility. Our approach\nassesses how well synthetic speech mirrors real speech by obtaining correlates\nof each factor and measuring their distance from both real speech datasets and\nnoise datasets. We benchmark 35 TTS systems developed between 2008 and 2024 and\nshow that our score computed as an unweighted average of factors strongly\ncorrelates with the human evaluations from each time period.\n","authors":["Christoph Minixhofer","Ondřej Klejch","Peter Bell"],"pdf_url":"https://arxiv.org/pdf/2407.12707v2.pdf","comment":"Under review for SLT 2024"},{"id":"http://arxiv.org/abs/2406.17630v2","updated":"2024-07-22T12:00:43Z","published":"2024-06-25T15:17:01Z","title":"KANQAS: Kolmogorov-Arnold Network for Quantum Architecture Search","summary":"  Quantum architecture search (QAS) is a promising direction for optimization\nand automated design of quantum circuits towards quantum advantage. Recent\ntechniques in QAS focus on machine learning-based approaches from reinforcement\nlearning, like deep Q-network. While multi-layer perceptron-based deep\nQ-networks have been applied for QAS, their interpretability remains\nchallenging due to the high number of parameters. In this work, we evaluate the\npracticality of Kolmogorov-Arnold Networks (KANs) in QAS problems, analyzing\ntheir efficiency in the task of quantum state preparation and quantum\nchemistry. In quantum state preparation, our results show that in a noiseless\nscenario, the probability of success and the number of optimal quantum circuit\nconfigurations to generate the multi-qubit maximally entangled states are\n$2\\times$ to $5\\times$ higher than Multi-Layer perceptions (MLPs). Moreover, in\nnoisy scenarios, KAN can achieve a better fidelity in approximating maximally\nentangled state than MLPs, where the performance of the MLP significantly\ndepends on the choice of activation function. In tackling quantum chemistry\nproblems, we enhance the recently proposed QAS algorithm by integrating\nCurriculum Reinforcement Learning (CRL) with a KAN structure instead of the\ntraditional MLP. This modification allows us to design a parameterized quantum\ncircuit that contains fewer 2-qubit gates and has a shallower depth, thereby\nimproving the efficiency of finding the ground state of a chemical Hamiltonian.\nFurther investigation reveals that KAN requires a significantly smaller number\nof learnable parameters compared to MLPs; however, the average time of\nexecuting each episode for KAN is higher.\n","authors":["Akash Kundu","Aritra Sarkar","Abhishek Sadhu"],"pdf_url":"https://arxiv.org/pdf/2406.17630v2.pdf","comment":"11 pages and 5 figures, 7 tables. New experiments added and typo\n  removed"},{"id":"http://arxiv.org/abs/2407.15567v1","updated":"2024-07-22T11:52:58Z","published":"2024-07-22T11:52:58Z","title":"A New Theoretical Perspective on Data Heterogeneity in Federated\n  Optimization","summary":"  In federated learning (FL), data heterogeneity is the main reason that\nexisting theoretical analyses are pessimistic about the convergence rate. In\nparticular, for many FL algorithms, the convergence rate grows dramatically\nwhen the number of local updates becomes large, especially when the product of\nthe gradient divergence and local Lipschitz constant is large. However,\nempirical studies can show that more local updates can improve the convergence\nrate even when these two parameters are large, which is inconsistent with the\ntheoretical findings. This paper aims to bridge this gap between theoretical\nunderstanding and practical performance by providing a theoretical analysis\nfrom a new perspective on data heterogeneity. In particular, we propose a new\nand weaker assumption compared to the local Lipschitz gradient assumption,\nnamed the heterogeneity-driven pseudo-Lipschitz assumption. We show that this\nand the gradient divergence assumptions can jointly characterize the effect of\ndata heterogeneity. By deriving a convergence upper bound for FedAvg and its\nextensions, we show that, compared to the existing works, local Lipschitz\nconstant is replaced by the much smaller heterogeneity-driven pseudo-Lipschitz\nconstant and the corresponding convergence upper bound can be significantly\nreduced for the same number of local updates, although its order stays the\nsame. In addition, when the local objective function is quadratic, more\ninsights on the impact of data heterogeneity can be obtained using the\nheterogeneity-driven pseudo-Lipschitz constant. For example, we can identify a\nregion where FedAvg can outperform mini-batch SGD even when the gradient\ndivergence can be arbitrarily large. Our findings are validated using\nexperiments.\n","authors":["Jiayi Wang","Shiqiang Wang","Rong-Rong Chen","Mingyue Ji"],"pdf_url":"https://arxiv.org/pdf/2407.15567v1.pdf","comment":"ICML 2024"},{"id":"http://arxiv.org/abs/2306.00037v4","updated":"2024-07-22T11:40:15Z","published":"2023-05-31T09:12:35Z","title":"BotArtist: Generic approach for bot detection in Twitter via\n  semi-automatic machine learning pipeline","summary":"  Twitter, as one of the most popular social networks, provides a platform for\ncommunication and online discourse. Unfortunately, it has also become a target\nfor bots and fake accounts, resulting in the spread of false information and\nmanipulation. This paper introduces a semi-automatic machine learning pipeline\n(SAMLP) designed to address the challenges correlated with machine learning\nmodel development. Through this pipeline, we develop a comprehensive bot\ndetection model named BotArtist, based on user profile features. SAMLP\nleverages nine distinct publicly available datasets to train the BotArtist\nmodel. To assess BotArtist's performance against current state-of-the-art\nsolutions, we select 35 existing Twitter bot detection methods, each utilizing\na diverse range of features. Our comparative evaluation of BotArtist and these\nexisting methods, conducted across nine public datasets under standardized\nconditions, reveals that the proposed model outperforms existing solutions by\nalmost 10%, in terms of F1-score, achieving an average score of 83.19 and 68.5\nover specific and general approaches respectively. As a result of this\nresearch, we provide a dataset of the extracted features combined with\nBotArtist predictions over the 10.929.533 Twitter user profiles, collected via\nTwitter API during the 2022 Russo-Ukrainian War, over a 16-month period. This\ndataset was created in collaboration with [Shevtsov et al., 2022a] where the\noriginal authors share anonymized tweets on the discussion of the\nRusso-Ukrainian war with a total amount of 127.275.386 tweets. The combination\nof the existing text dataset and the provided labeled bot and human profiles\nwill allow for the future development of a more advanced bot detection large\nlanguage model in the post-Twitter API era.\n","authors":["Alexander Shevtsov","Despoina Antonakaki","Ioannis Lamprou","Polyvios Pratikakis","Sotiris Ioannidis"],"pdf_url":"https://arxiv.org/pdf/2306.00037v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15555v1","updated":"2024-07-22T11:34:47Z","published":"2024-07-22T11:34:47Z","title":"The Rlign Algorithm for Enhanced Electrocardiogram Analysis through\n  R-Peak Alignment for Explainable Classification and Clustering","summary":"  Electrocardiogram (ECG) recordings have long been vital in diagnosing\ndifferent cardiac conditions. Recently, research in the field of automatic ECG\nprocessing using machine learning methods has gained importance, mainly by\nutilizing deep learning methods on raw ECG signals. A major advantage of models\nlike convolutional neural networks (CNNs) is their ability to effectively\nprocess biomedical imaging or signal data. However, this strength is tempered\nby challenges related to their lack of explainability, the need for a large\namount of training data, and the complexities involved in adapting them for\nunsupervised clustering tasks. In addressing these tasks, we aim to reintroduce\nshallow learning techniques, including support vector machines and principal\ncomponents analysis, into ECG signal processing by leveraging their\nsemi-structured, cyclic form. To this end, we developed and evaluated a\ntransformation that effectively restructures ECG signals into a fully\nstructured format, facilitating their subsequent analysis using shallow\nlearning algorithms. In this study, we present this adaptive transformative\napproach that aligns R-peaks across all signals in a dataset and resamples the\nsegments between R-peaks, both with and without heart rate dependencies. We\nillustrate the substantial benefit of this transformation for traditional\nanalysis techniques in the areas of classification, clustering, and\nexplainability, outperforming commercial software for median beat\ntransformation and CNN approaches. Our approach demonstrates a significant\nadvantage for shallow machine learning methods over CNNs, especially when\ndealing with limited training data. Additionally, we release a fully tested and\npublicly accessible code framework, providing a robust alignment pipeline to\nsupport future research, available at https://github.com/ imi-ms/rlign.\n","authors":["Lucas Plagwitz","Lucas Bickmann","Michael Fujarski","Alexander Brenner","Warnes Gobalakrishnan","Lars Eckardt","Antonius Büscher","Julian Varghese"],"pdf_url":"https://arxiv.org/pdf/2407.15555v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10177v2","updated":"2024-07-22T11:31:08Z","published":"2024-03-20T14:22:12Z","title":"Consistent Diffusion Meets Tweedie: Training Exact Ambient Diffusion\n  Models with Noisy Data","summary":"  Ambient diffusion is a recently proposed framework for training diffusion\nmodels using corrupted data. Both Ambient Diffusion and alternative SURE-based\napproaches for learning diffusion models from corrupted data resort to\napproximations which deteriorate performance. We present the first framework\nfor training diffusion models that provably sample from the uncorrupted\ndistribution given only noisy training data, solving an open problem in this\nspace. Our key technical contribution is a method that uses a double\napplication of Tweedie's formula and a consistency loss function that allows us\nto extend sampling at noise levels below the observed data noise. We also\nprovide further evidence that diffusion models memorize from their training\nsets by identifying extremely corrupted images that are almost perfectly\nreconstructed, raising copyright and privacy concerns. Our method for training\nusing corrupted samples can be used to mitigate this problem. We demonstrate\nthis by fine-tuning Stable Diffusion XL to generate samples from a distribution\nusing only noisy samples. Our framework reduces the amount of memorization of\nthe fine-tuning dataset, while maintaining competitive performance.\n","authors":["Giannis Daras","Alexandros G. Dimakis","Constantinos Daskalakis"],"pdf_url":"https://arxiv.org/pdf/2404.10177v2.pdf","comment":"Accepted to ICML 2024"},{"id":"http://arxiv.org/abs/2407.15549v1","updated":"2024-07-22T11:19:14Z","published":"2024-07-22T11:19:14Z","title":"Targeted Latent Adversarial Training Improves Robustness to Persistent\n  Harmful Behaviors in LLMs","summary":"  Large language models (LLMs) can often be made to behave in undesirable ways\nthat they are explicitly fine-tuned not to. For example, the LLM red-teaming\nliterature has produced a wide variety of `jailbreaking' techniques to elicit\nharmful text from models that were fine-tuned to be harmless. Recent work on\nred-teaming, model editing, and interpretability suggests that this challenge\nstems from how (adversarial) fine-tuning largely serves to suppress rather than\nremove undesirable capabilities from LLMs. Prior work has introduced latent\nadversarial training (LAT) as a way to improve robustness to broad classes of\nfailures. These prior works have considered untargeted latent space attacks\nwhere the adversary perturbs latent activations to maximize loss on examples of\ndesirable behavior. Untargeted LAT can provide a generic type of robustness but\ndoes not leverage information about specific failure modes. Here, we experiment\nwith targeted LAT where the adversary seeks to minimize loss on a specific\ncompeting task. We find that it can augment a wide variety of state-of-the-art\nmethods. First, we use targeted LAT to improve robustness to jailbreaks,\noutperforming a strong R2D2 baseline with orders of magnitude less compute.\nSecond, we use it to more effectively remove backdoors with no knowledge of the\ntrigger. Finally, we use it to more effectively unlearn knowledge for specific\nundesirable tasks in a way that is also more robust to re-learning. Overall,\nour results suggest that targeted LAT can be an effective tool for defending\nagainst harmful behaviors from LLMs.\n","authors":["Abhay Sheshadri","Aidan Ewart","Phillip Guo","Aengus Lynch","Cindy Wu","Vivek Hebbar","Henry Sleight","Asa Cooper Stickland","Ethan Perez","Dylan Hadfield-Menell","Stephen Casper"],"pdf_url":"https://arxiv.org/pdf/2407.15549v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.12048v2","updated":"2024-07-22T11:11:28Z","published":"2023-11-18T08:55:08Z","title":"One Size Fits All for Semantic Shifts: Adaptive Prompt Tuning for\n  Continual Learning","summary":"  In real-world continual learning (CL) scenarios, tasks often exhibit\nintricate and unpredictable semantic shifts, posing challenges for fixed prompt\nmanagement strategies which are tailored to only handle semantic shifts of\nuniform degree (i.e., uniformly mild or uniformly abrupt). To address this\nlimitation, we propose an adaptive prompting approach that effectively\naccommodates semantic shifts of varying degree where mild and abrupt shifts are\nmixed. AdaPromptCL employs the assign-and-refine semantic grouping mechanism\nthat dynamically manages prompt groups in accordance with the semantic\nsimilarity between tasks, enhancing the quality of grouping through continuous\nrefinement. Our experiment results demonstrate that AdaPromptCL outperforms\nexisting prompting methods by up to 21.3%, especially in the benchmark datasets\nwith diverse semantic shifts between tasks.\n","authors":["Doyoung Kim","Susik Yoon","Dongmin Park","Youngjun Lee","Hwanjun Song","Jihwan Bang","Jae-Gil Lee"],"pdf_url":"https://arxiv.org/pdf/2311.12048v2.pdf","comment":"ICML 2024"},{"id":"http://arxiv.org/abs/2407.15545v1","updated":"2024-07-22T11:11:17Z","published":"2024-07-22T11:11:17Z","title":"Inverted Activations","summary":"  The scaling of neural networks with increasing data and model sizes\nnecessitates more efficient deep learning algorithms. This paper addresses the\nmemory footprint challenge in neural network training by proposing a\nmodification to the handling of activation tensors in pointwise nonlinearity\nlayers. Traditionally, these layers save the entire input tensor for the\nbackward pass, leading to substantial memory use. Our method involves saving\nthe output tensor instead, reducing the memory required when the subsequent\nlayer also saves its input tensor. This approach is particularly beneficial for\ntransformer-based architectures like GPT, BERT, Mistral, and Llama. Application\nof our method involves taken an inverse function of nonlinearity. To the best\nof our knowledge, that can not be done analitically and instead we buid an\naccurate approximations using simpler functions. Experimental results confirm\nthat our method significantly reduces memory usage without affecting training\naccuracy. The implementation is available at\nhttps://github.com/PgLoLo/optiacts.\n","authors":["Georgii Novikov","Ivan Oseledets"],"pdf_url":"https://arxiv.org/pdf/2407.15545v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.00774v2","updated":"2024-07-22T11:06:22Z","published":"2024-06-30T17:24:55Z","title":"Harnessing Quantum Support Vector Machines for Cross-Domain\n  Classification of Quantum States","summary":"  In the present study, we use cross-domain classification using quantum\nmachine learning for quantum advantages to readdress the entanglement versus\nseparability paradigm. The inherent structure of quantum states and its\nrelation to a particular class of quantum states are used to intuitively\nclassify testing states from domains different from training states, called\n\\textit{cross-domain classification}. Using our quantum machine learning\nalgorithm, we demonstrate efficient classifications of two-qubit mixed states\ninto entangled and separable classes. For analyzing the quantumness of\ncorrelations, our model adequately classifies Bell diagonal states as zero and\nnon-zero discord states. In addition, we also extend our analysis to evaluate\nthe robustness of our model using random local unitary transformations. Our\nresults demonstrate the potential of the quantum support vector machine for\nclassifying quantum states across the multi-dimensional Hilbert space in\ncomparison to classical support vector machines and neural networks.\n","authors":["Diksha Sharma","Vivek Balasaheb Sabale","Parvinder Singh","Atul Kumar"],"pdf_url":"https://arxiv.org/pdf/2407.00774v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.20204v4","updated":"2024-07-22T11:01:54Z","published":"2023-10-31T06:04:18Z","title":"General-Purpose Retrieval-Enhanced Medical Prediction Model Using\n  Near-Infinite History","summary":"  Machine learning (ML) has recently shown promising results in medical\npredictions using electronic health records (EHRs). However, since ML models\ntypically have a limited capability in terms of input sizes, selecting specific\nmedical events from EHRs for use as input is necessary. This selection process,\noften relying on expert opinion, can cause bottlenecks in development. We\npropose Retrieval-Enhanced Medical prediction model (REMed) to address such\nchallenges. REMed can essentially evaluate unlimited medical events, select the\nrelevant ones, and make predictions. This allows for an unrestricted input\nsize, eliminating the need for manual event selection. We verified these\nproperties through experiments involving 27 clinical prediction tasks across\nfour independent cohorts, where REMed outperformed the baselines. Notably, we\nfound that the preferences of REMed align closely with those of medical\nexperts. We expect our approach to significantly expedite the development of\nEHR prediction models by minimizing clinicians' need for manual involvement.\n","authors":["Junu Kim","Chaeeun Shim","Bosco Seong Kyu Yang","Chami Im","Sung Yoon Lim","Han-Gil Jeong","Edward Choi"],"pdf_url":"https://arxiv.org/pdf/2310.20204v4.pdf","comment":"The source codes corresponding to this paper are available at:\n  https://github.com/starmpcc/REMed"},{"id":"http://arxiv.org/abs/2407.15537v1","updated":"2024-07-22T10:57:32Z","published":"2024-07-22T10:57:32Z","title":"Exterior Penalty Policy Optimization with Penalty Metric Network under\n  Constraints","summary":"  In Constrained Reinforcement Learning (CRL), agents explore the environment\nto learn the optimal policy while satisfying constraints. The penalty function\nmethod has recently been studied as an effective approach for handling\nconstraints, which imposes constraints penalties on the objective to transform\nthe constrained problem into an unconstrained one. However, it is challenging\nto choose appropriate penalties that balance policy performance and constraint\nsatisfaction efficiently. In this paper, we propose a theoretically guaranteed\npenalty function method, Exterior Penalty Policy Optimization (EPO), with\nadaptive penalties generated by a Penalty Metric Network (PMN). PMN responds\nappropriately to varying degrees of constraint violations, enabling efficient\nconstraint satisfaction and safe exploration. We theoretically prove that EPO\nconsistently improves constraint satisfaction with a convergence guarantee. We\npropose a new surrogate function and provide worst-case constraint violation\nand approximation error. In practice, we propose an effective smooth penalty\nfunction, which can be easily implemented with a first-order optimizer.\nExtensive experiments are conducted, showing that EPO outperforms the baselines\nin terms of policy performance and constraint satisfaction with a stable\ntraining process, particularly on complex tasks.\n","authors":["Shiqing Gao","Jiaxin Ding","Luoyi Fu","Xinbing Wang","Chenghu Zhou"],"pdf_url":"https://arxiv.org/pdf/2407.15537v1.pdf","comment":"To be published in the 33rd International Joint Conference on\n  Artificial Intelligence (IJCAI 2024)"},{"id":"http://arxiv.org/abs/2404.14527v4","updated":"2024-07-22T10:56:19Z","published":"2024-04-22T18:56:18Z","title":"Mélange: Cost Efficient Large Language Model Serving by Exploiting GPU\n  Heterogeneity","summary":"  Large language models (LLMs) are increasingly integrated into many online\nservices, yet they remain cost-prohibitive to deploy due to the requirement of\nexpensive GPU instances. Prior work has addressed the high cost of LLM serving\nby improving the inference engine, but less attention has been given to\nselecting the most cost-efficient GPU type(s) for a specific LLM service. There\nis a large and growing landscape of GPU types and, within these options, higher\ncost does not always lead to increased performance. Instead, through a\ncomprehensive investigation, we find that three key LLM service characteristics\n(request size, request rate, SLO) strongly influence GPU cost efficiency, and\ndiffering GPU types are most cost efficient for differing LLM service settings.\nAs a result, the most cost-efficient allocation for a given service is\ntypically a mix of heterogeneous GPU types. Based on this analysis, we\nintroduce M\\'elange, a GPU allocation framework that navigates these diverse\nLLM service characteristics and heterogeneous GPU option space to automatically\nand efficiently derive the minimal-cost GPU allocation for a given LLM service.\nWe formulate the GPU allocation task as a cost-aware bin packing problem where\nGPUs are bins and items are slices of the service workload. Our formulation's\nconstraints account for a service's unique characteristics, allowing M\\'elange\nto be flexible to support diverse service settings and heterogeneity-aware to\nadapt the GPU allocation to a specific service. Compared to using only a single\nGPU type, M\\'elange reduces deployment costs by up to 77% in conversational\nsettings, 33% in document-based settings, and 51% in a mixed setting.\n","authors":["Tyler Griggs","Xiaoxuan Liu","Jiaxiang Yu","Doyoung Kim","Wei-Lin Chiang","Alvin Cheung","Ion Stoica"],"pdf_url":"https://arxiv.org/pdf/2404.14527v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15527v1","updated":"2024-07-22T10:32:48Z","published":"2024-07-22T10:32:48Z","title":"Interpretable Concept-Based Memory Reasoning","summary":"  The lack of transparency in the decision-making processes of deep learning\nsystems presents a significant challenge in modern artificial intelligence\n(AI), as it impairs users' ability to rely on and verify these systems. To\naddress this challenge, Concept Bottleneck Models (CBMs) have made significant\nprogress by incorporating human-interpretable concepts into deep learning\narchitectures. This approach allows predictions to be traced back to specific\nconcept patterns that users can understand and potentially intervene on.\nHowever, existing CBMs' task predictors are not fully interpretable, preventing\na thorough analysis and any form of formal verification of their\ndecision-making process prior to deployment, thereby raising significant\nreliability concerns. To bridge this gap, we introduce Concept-based Memory\nReasoner (CMR), a novel CBM designed to provide a human-understandable and\nprovably-verifiable task prediction process. Our approach is to model each task\nprediction as a neural selection mechanism over a memory of learnable logic\nrules, followed by a symbolic evaluation of the selected rule. The presence of\nan explicit memory and the symbolic evaluation allow domain experts to inspect\nand formally verify the validity of certain global properties of interest for\nthe task prediction process. Experimental results demonstrate that CMR achieves\ncomparable accuracy-interpretability trade-offs to state-of-the-art CBMs,\ndiscovers logic rules consistent with ground truths, allows for rule\ninterventions, and allows pre-deployment verification.\n","authors":["David Debot","Pietro Barbiero","Francesco Giannini","Gabriele Ciravegna","Michelangelo Diligenti","Giuseppe Marra"],"pdf_url":"https://arxiv.org/pdf/2407.15527v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15526v1","updated":"2024-07-22T10:31:07Z","published":"2024-07-22T10:31:07Z","title":"Synthetic Image Learning: Preserving Performance and Preventing\n  Membership Inference Attacks","summary":"  Generative artificial intelligence has transformed the generation of\nsynthetic data, providing innovative solutions to challenges like data scarcity\nand privacy, which are particularly critical in fields such as medicine.\nHowever, the effective use of this synthetic data to train high-performance\nmodels remains a significant challenge. This paper addresses this issue by\nintroducing Knowledge Recycling (KR), a pipeline designed to optimise the\ngeneration and use of synthetic data for training downstream classifiers. At\nthe heart of this pipeline is Generative Knowledge Distillation (GKD), the\nproposed technique that significantly improves the quality and usefulness of\nthe information provided to classifiers through a synthetic dataset\nregeneration and soft labelling mechanism. The KR pipeline has been tested on a\nvariety of datasets, with a focus on six highly heterogeneous medical image\ndatasets, ranging from retinal images to organ scans. The results show a\nsignificant reduction in the performance gap between models trained on real and\nsynthetic data, with models based on synthetic data outperforming those trained\non real data in some cases. Furthermore, the resulting models show almost\ncomplete immunity to Membership Inference Attacks, manifesting privacy\nproperties missing in models trained with conventional techniques.\n","authors":["Eugenio Lomurno","Matteo Matteucci"],"pdf_url":"https://arxiv.org/pdf/2407.15526v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08483v4","updated":"2024-07-22T10:30:21Z","published":"2024-04-12T14:03:41Z","title":"Semantic Communication for Cooperative Multi-Task Processing over\n  Wireless Networks","summary":"  In this paper, we investigated semantic communication for multi-task\nprocessing using an information-theoretic approach. We introduced the concept\nof a \"semantic source\", allowing multiple semantic interpretations from a\nsingle observation. We formulated an end-to-end optimization problem taking\ninto account the communication channel, maximizing mutual information (infomax)\nto design the semantic encoding and decoding process exploiting the statistical\nrelations between semantic variables. To solve the problem we perform\ndata-driven deep learning employing variational approximation techniques. Our\nsemantic encoder is divided into a common unit and multiple specific units to\nfacilitate cooperative multi-task processing. Simulation results demonstrate\nthe effectiveness of our proposed semantic source and system design when\nstatistical relationships exist, comparing cooperative task processing with\nindependent task processing. However, our findings highlight that cooperative\nmulti-tasking is not always beneficial, emphasizing the importance of\nstatistical relationships between tasks and indicating the need for further\ninvestigation into the semantically processing of multiple tasks.\n","authors":["Ahmad Halimi Razlighi","Carsten Bockelmann","Armin Dekorsy"],"pdf_url":"https://arxiv.org/pdf/2404.08483v4.pdf","comment":"This work has been submitted to the IEEE Wireless Communications\n  Letters for possible publication"},{"id":"http://arxiv.org/abs/2407.15525v1","updated":"2024-07-22T10:28:56Z","published":"2024-07-22T10:28:56Z","title":"Multiple importance sampling for stochastic gradient estimation","summary":"  We introduce a theoretical and practical framework for efficient importance\nsampling of mini-batch samples for gradient estimation from single and multiple\nprobability distributions. To handle noisy gradients, our framework dynamically\nevolves the importance distribution during training by utilizing a\nself-adaptive metric. Our framework combines multiple, diverse sampling\ndistributions, each tailored to specific parameter gradients. This approach\nfacilitates the importance sampling of vector-valued gradient estimation.\nRather than naively combining multiple distributions, our framework involves\noptimally weighting data contribution across multiple distributions. This\nadapted combination of multiple importance yields superior gradient estimates,\nleading to faster training convergence. We demonstrate the effectiveness of our\napproach through empirical evaluations across a range of optimization tasks\nlike classification and regression on both image and point cloud datasets.\n","authors":["Corentin Salaün","Xingchang Huang","Iliyan Georgiev","Niloy J. Mitra","Gurprit Singh"],"pdf_url":"https://arxiv.org/pdf/2407.15525v1.pdf","comment":"13 pages, 11 figures"},{"id":"http://arxiv.org/abs/2407.10768v2","updated":"2024-07-22T10:26:41Z","published":"2024-07-15T14:50:15Z","title":"MSegRNN:Enhanced SegRNN Model with Mamba for Long-Term Time Series\n  Forecasting","summary":"  The field of long-term time series forecasting demands handling extensive\nlook-back windows and long-range prediction steps, posing significant\nchallenges for RNN-based methodologies. Among these, SegRNN, a robust\nRNN-driven model, has gained considerable attention in LTSF analysis for\nachieving state-of-the-art results while maintaining a remarkably streamlined\narchitecture. Concurrently, the Mamba structure has demonstrated its advantages\nin small to medium-sized models due to its capability for information\nselection. This study introduces a variant of SegRNN that preprocesses\ninformation using a fine-tuned single-layer Mamba structure. Additionally, it\nincorporates implicit segmentation and residual structures into the model's\nencoding section to further reduce the inherent data iterative cycles of RNN\narchitectures and implicitly integrate inter-channel correlations. This\nvariant, named MSegRNN, utilizes the Mamba structure to select useful\ninformation, resulting in a transformed sequence. The linear-strategy-adapted\nderivative retains the superior memory efficiency of the original SegRNN while\ndemonstrating enhanced performance. Empirical evaluations on real-world LTSF\ndatasets demonstrate the superior performance of our model, thereby\ncontributing to the advancement of LTSF methodologies.\n","authors":["GaoXiang Zhao","XiaoQiang Wang"],"pdf_url":"https://arxiv.org/pdf/2407.10768v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.06368v3","updated":"2024-07-22T10:21:49Z","published":"2024-05-10T10:10:37Z","title":"DP-DyLoRA: Fine-Tuning Transformer-Based Models On-Device under\n  Differentially Private Federated Learning using Dynamic Low-Rank Adaptation","summary":"  Federated learning (FL) allows clients to collaboratively train a global\nmodel without sharing their local data with a server. However, clients'\ncontributions to the server can still leak sensitive information. Differential\nprivacy (DP) addresses such leakage by providing formal privacy guarantees,\nwith mechanisms that add randomness to the clients' contributions. The\nrandomness makes it infeasible to train large transformer-based models, common\nin modern federated learning systems. In this work, we empirically evaluate the\npracticality of fine-tuning large scale on-device transformer-based models with\ndifferential privacy in a federated learning system. We conduct comprehensive\nexperiments on various system properties for tasks spanning a multitude of\ndomains: speech recognition, computer vision (CV) and natural language\nunderstanding (NLU). Our results show that full fine-tuning under\ndifferentially private federated learning (DP-FL) generally leads to huge\nperformance degradation which can be alleviated by reducing the dimensionality\nof contributions through parameter-efficient fine-tuning (PEFT). Our benchmarks\nof existing DP-PEFT methods show that DP-Low-Rank Adaptation (DP-LoRA)\nconsistently outperforms other methods. An even more promising approach,\nDyLoRA, which makes the low rank variable, when naively combined with FL would\nstraightforwardly break differential privacy. We therefore propose an\nadaptation method that can be combined with differential privacy and call it\nDP-DyLoRA. Finally, we are able to reduce the accuracy degradation and word\nerror rate (WER) increase due to DP to less than 2% and 7% respectively with 1\nmillion clients and a stringent privacy budget of $\\epsilon=2$.\n","authors":["Jie Xu","Karthikeyan Saravanan","Rogier van Dalen","Haaris Mehmood","David Tuckey","Mete Ozay"],"pdf_url":"https://arxiv.org/pdf/2405.06368v3.pdf","comment":"16 pages, 10 figures, 5 tables"},{"id":"http://arxiv.org/abs/2404.07713v2","updated":"2024-07-22T10:09:39Z","published":"2024-04-11T12:59:38Z","title":"Progressive Semantic-Guided Vision Transformer for Zero-Shot Learning","summary":"  Zero-shot learning (ZSL) recognizes the unseen classes by conducting\nvisual-semantic interactions to transfer semantic knowledge from seen classes\nto unseen ones, supported by semantic information (e.g., attributes). However,\nexisting ZSL methods simply extract visual features using a pre-trained network\nbackbone (i.e., CNN or ViT), which fail to learn matched visual-semantic\ncorrespondences for representing semantic-related visual features as lacking of\nthe guidance of semantic information, resulting in undesirable visual-semantic\ninteractions. To tackle this issue, we propose a progressive semantic-guided\nvision transformer for zero-shot learning (dubbed ZSLViT). ZSLViT mainly\nconsiders two properties in the whole network: i) discover the semantic-related\nvisual representations explicitly, and ii) discard the semantic-unrelated\nvisual information. Specifically, we first introduce semantic-embedded token\nlearning to improve the visual-semantic correspondences via semantic\nenhancement and discover the semantic-related visual tokens explicitly with\nsemantic-guided token attention. Then, we fuse low semantic-visual\ncorrespondence visual tokens to discard the semantic-unrelated visual\ninformation for visual enhancement. These two operations are integrated into\nvarious encoders to progressively learn semantic-related visual representations\nfor accurate visual-semantic interactions in ZSL. The extensive experiments\nshow that our ZSLViT achieves significant performance gains on three popular\nbenchmark datasets, i.e., CUB, SUN, and AWA2. Codes are available at:\nhttps://github.com/shiming-chen/ZSLViT .\n","authors":["Shiming Chen","Wenjin Hou","Salman Khan","Fahad Shahbaz Khan"],"pdf_url":"https://arxiv.org/pdf/2404.07713v2.pdf","comment":"Accepted to CVPR'24"},{"id":"http://arxiv.org/abs/2407.15516v1","updated":"2024-07-22T10:09:05Z","published":"2024-07-22T10:09:05Z","title":"Attention Is All You Need But You Don't Need All Of It For Inference of\n  Large Language Models","summary":"  The inference demand for LLMs has skyrocketed in recent months, and serving\nmodels with low latencies remains challenging due to the quadratic input length\ncomplexity of the attention layers. In this work, we investigate the effect of\ndropping MLP and attention layers at inference time on the performance of\nLlama-v2 models. We find that dropping dreeper attention layers only marginally\ndecreases performance but leads to the best speedups alongside dropping entire\nlayers. For example, removing 33\\% of attention layers in a 13B Llama2 model\nresults in a 1.8\\% drop in average performance over the OpenLLM benchmark. We\nalso observe that skipping layers except the latter layers reduces performances\nfor more layers skipped, except for skipping the attention layers.\n","authors":["Georgy Tyukin","Gbetondji J-S Dovonon","Jean Kaddour","Pasquale Minervini"],"pdf_url":"https://arxiv.org/pdf/2407.15516v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15512v1","updated":"2024-07-22T09:58:29Z","published":"2024-07-22T09:58:29Z","title":"Increasing the Robustness of Model Predictions to Missing Sensors in\n  Earth Observation","summary":"  Multi-sensor ML models for EO aim to enhance prediction accuracy by\nintegrating data from various sources. However, the presence of missing data\nposes a significant challenge, particularly in non-persistent sensors that can\nbe affected by external factors. Existing literature has explored strategies\nlike temporal dropout and sensor-invariant models to address the generalization\nto missing data issues. Inspired by these works, we study two novel methods\ntailored for multi-sensor scenarios, namely Input Sensor Dropout (ISensD) and\nEnsemble Sensor Invariant (ESensI). Through experimentation on three\nmulti-sensor temporal EO datasets, we demonstrate that these methods\neffectively increase the robustness of model predictions to missing sensors.\nParticularly, we focus on how the predictive performance of models drops when\nsensors are missing at different levels. We observe that ensemble multi-sensor\nmodels are the most robust to the lack of sensors. In addition, the sensor\ndropout component in ISensD shows promising robustness results.\n","authors":["Francisco Mena","Diego Arenas","Andreas Dengel"],"pdf_url":"https://arxiv.org/pdf/2407.15512v1.pdf","comment":"Accepted at the MACLEAN workshop in the ECML/PKDD 2024"},{"id":"http://arxiv.org/abs/2309.00169v3","updated":"2024-07-22T09:53:44Z","published":"2023-08-31T23:26:10Z","title":"RepCodec: A Speech Representation Codec for Speech Tokenization","summary":"  With recent rapid growth of large language models (LLMs), discrete speech\ntokenization has played an important role for injecting speech into LLMs.\nHowever, this discretization gives rise to a loss of information, consequently\nimpairing overall performance. To improve the performance of these discrete\nspeech tokens, we present RepCodec, a novel speech representation codec for\nsemantic speech tokenization. In contrast to audio codecs which reconstruct the\nraw audio, RepCodec learns a vector quantization codebook through\nreconstructing speech representations from speech encoders like HuBERT or\ndata2vec. Together, the speech encoder, the codec encoder and the vector\nquantization codebook form a pipeline for converting speech waveforms into\nsemantic tokens. The extensive experiments illustrate that RepCodec, by virtue\nof its enhanced information retention capacity, significantly outperforms the\nwidely used k-means clustering approach in both speech understanding and\ngeneration. Furthermore, this superiority extends across various speech\nencoders and languages, affirming the robustness of RepCodec. We believe our\nmethod can facilitate large language modeling research on speech processing.\n","authors":["Zhichao Huang","Chutong Meng","Tom Ko"],"pdf_url":"https://arxiv.org/pdf/2309.00169v3.pdf","comment":"ACL 2024 (Main)"},{"id":"http://arxiv.org/abs/2407.04538v3","updated":"2024-07-22T09:41:39Z","published":"2024-07-05T14:24:37Z","title":"PDiscoFormer: Relaxing Part Discovery Constraints with Vision\n  Transformers","summary":"  Computer vision methods that explicitly detect object parts and reason on\nthem are a step towards inherently interpretable models. Existing approaches\nthat perform part discovery driven by a fine-grained classification task make\nvery restrictive assumptions on the geometric properties of the discovered\nparts; they should be small and compact. Although this prior is useful in some\ncases, in this paper we show that pre-trained transformer-based vision models,\nsuch as self-supervised DINOv2 ViT, enable the relaxation of these constraints.\nIn particular, we find that a total variation (TV) prior, which allows for\nmultiple connected components of any size, substantially outperforms previous\nwork. We test our approach on three fine-grained classification benchmarks:\nCUB, PartImageNet and Oxford Flowers, and compare our results to previously\npublished methods as well as a re-implementation of the state-of-the-art method\nPDiscoNet with a transformer-based backbone. We consistently obtain substantial\nimprovements across the board, both on part discovery metrics and the\ndownstream classification task, showing that the strong inductive biases in\nself-supervised ViT models require to rethink the geometric priors that can be\nused for unsupervised part discovery.\n","authors":["Ananthu Aniraj","Cassio F. Dantas","Dino Ienco","Diego Marcos"],"pdf_url":"https://arxiv.org/pdf/2407.04538v3.pdf","comment":"Accepted as a main conference paper at the European Conference of\n  Computer Vision (ECCV) 2024"},{"id":"http://arxiv.org/abs/2407.15504v1","updated":"2024-07-22T09:40:13Z","published":"2024-07-22T09:40:13Z","title":"Fundamental Limits of Prompt Compression: A Rate-Distortion Framework\n  for Black-Box Language Models","summary":"  We formalize the problem of prompt compression for large language models\n(LLMs) and present a framework to unify token-level prompt compression methods\nwhich create hard prompts for black-box models. We derive the distortion-rate\nfunction for this setup as a linear program, and provide an efficient algorithm\nto compute this fundamental limit via the dual of the linear program. Using the\ndistortion-rate function as the baseline, we study the performance of existing\ncompression schemes on a synthetic dataset consisting of prompts generated from\na Markov chain, natural language queries, and their respective answers. Our\nempirical analysis demonstrates the criticality of query-aware prompt\ncompression, where the compressor has knowledge of the downstream task/query\nfor the black-box LLM. We show that there is a large gap between the\nperformance of current prompt compression methods and the optimal strategy, and\npropose a query-aware, variable-rate adaptation of a prior work to close the\ngap. We extend our experiments to a small natural language dataset to further\nconfirm our findings on our synthetic dataset.\n","authors":["Adway Girish","Alliot Nagle","Marco Bondaschi","Michael Gastpar","Ashok Vardhan Makkuva","Hyeji Kim"],"pdf_url":"https://arxiv.org/pdf/2407.15504v1.pdf","comment":"40 pages, 15 figures. Under review"},{"id":"http://arxiv.org/abs/2405.15434v2","updated":"2024-07-22T09:37:36Z","published":"2024-05-24T11:02:55Z","title":"Biometrics and Behavior Analysis for Detecting Distractions in\n  e-Learning","summary":"  In this article, we explore computer vision approaches to detect abnormal\nhead pose during e-learning sessions and we introduce a study on the effects of\nmobile phone usage during these sessions. We utilize behavioral data collected\nfrom 120 learners monitored while participating in a MOOC learning sessions.\nOur study focuses on the influence of phone-usage events on behavior and\nphysiological responses, specifically attention, heart rate, and meditation,\nbefore, during, and after phone usage. Additionally, we propose an approach for\nestimating head pose events using images taken by the webcam during the MOOC\nlearning sessions to detect phone-usage events. Our hypothesis suggests that\nhead posture undergoes significant changes when learners interact with a mobile\nphone, contrasting with the typical behavior seen when learners face a computer\nduring e-learning sessions. We propose an approach designed to detect\ndeviations in head posture from the average observed during a learner's\nsession, operating as a semi-supervised method. This system flags events\nindicating alterations in head posture for subsequent human review and\nselection of mobile phone usage occurrences with a sensitivity over 90%.\n","authors":["Álvaro Becerra","Javier Irigoyen","Roberto Daza","Ruth Cobos","Aythami Morales","Julian Fierrez","Mutlu Cukurova"],"pdf_url":"https://arxiv.org/pdf/2405.15434v2.pdf","comment":"Accepted in CEDI 2024 (VII Congreso Espa\\~nol de Inform\\'atica), A\n  Coru\\~na, Spain"},{"id":"http://arxiv.org/abs/2406.18417v2","updated":"2024-07-22T09:35:36Z","published":"2024-06-26T15:11:15Z","title":"Towards diffusion models for large-scale sea-ice modelling","summary":"  We make the first steps towards diffusion models for unconditional generation\nof multivariate and Arctic-wide sea-ice states. While targeting to reduce the\ncomputational costs by diffusion in latent space, latent diffusion models also\noffer the possibility to integrate physical knowledge into the generation\nprocess. We tailor latent diffusion models to sea-ice physics with a censored\nGaussian distribution in data space to generate data that follows the physical\nbounds of the modelled variables. Our latent diffusion models reach similar\nscores as the diffusion model trained in data space, but they smooth the\ngenerated fields as caused by the latent mapping. While enforcing physical\nbounds cannot reduce the smoothing, it improves the representation of the\nmarginal ice zone. Therefore, for large-scale Earth system modelling, latent\ndiffusion models can have many advantages compared to diffusion in data space\nif the significant barrier of smoothing can be resolved.\n","authors":["Tobias Sebastian Finn","Charlotte Durand","Alban Farchi","Marc Bocquet","Julien Brajard"],"pdf_url":"https://arxiv.org/pdf/2406.18417v2.pdf","comment":"21 pages, 5 Figures, Camera-ready version for the ICML 2024 Machine\n  Learning for Earth System Modeling workshop"},{"id":"http://arxiv.org/abs/2406.02255v2","updated":"2024-07-22T09:34:46Z","published":"2024-06-04T12:21:55Z","title":"MidiCaps: A large-scale MIDI dataset with text captions","summary":"  Generative models guided by text prompts are increasingly becoming more\npopular. However, no text-to-MIDI models currently exist due to the lack of a\ncaptioned MIDI dataset. This work aims to enable research that combines LLMs\nwith symbolic music by presenting, the first openly available large-scale MIDI\ndataset with text captions. MIDI (Musical Instrument Digital Interface) files\nare widely used for encoding musical information and can capture the nuances of\nmusical composition. They are widely used by music producers, composers,\nmusicologists, and performers alike. Inspired by recent advancements in\ncaptioning techniques, we present a curated dataset of over 168k MIDI files\nwith textual descriptions. Each MIDI caption describes the musical content,\nincluding tempo, chord progression, time signature, instruments, genre, and\nmood, thus facilitating multi-modal exploration and analysis. The dataset\nencompasses various genres, styles, and complexities, offering a rich data\nsource for training and evaluating models for tasks such as music information\nretrieval, music understanding, and cross-modal translation. We provide\ndetailed statistics about the dataset and have assessed the quality of the\ncaptions in an extensive listening study. We anticipate that this resource will\nstimulate further research at the intersection of music and natural language\nprocessing, fostering advancements in both fields.\n","authors":["Jan Melechovsky","Abhinaba Roy","Dorien Herremans"],"pdf_url":"https://arxiv.org/pdf/2406.02255v2.pdf","comment":"Accepted in ISMIR2024"},{"id":"http://arxiv.org/abs/2406.10840v2","updated":"2024-07-22T09:22:37Z","published":"2024-06-16T08:20:24Z","title":"CBGBench: Fill in the Blank of Protein-Molecule Complex Binding Graph","summary":"  Structure-based drug design (SBDD) aims to generate potential drugs that can\nbind to a target protein and is greatly expedited by the aid of AI techniques\nin generative models. However, a lack of systematic understanding persists due\nto the diverse settings, complex implementation, difficult reproducibility, and\ntask singularity. Firstly, the absence of standardization can lead to unfair\ncomparisons and inconclusive insights. To address this dilemma, we propose\nCBGBench, a comprehensive benchmark for SBDD, that unifies the task as a\ngenerative heterogeneous graph completion, analogous to fill-in-the-blank of\nthe 3D complex binding graph. By categorizing existing methods based on their\nattributes, CBGBench facilitates a modular and extensible framework that\nimplements various cutting-edge methods. Secondly, a single task on \\textit{de\nnovo} molecule generation can hardly reflect their capabilities. To broaden the\nscope, we have adapted these models to a range of tasks essential in drug\ndesign, which are considered sub-tasks within the graph fill-in-the-blank\ntasks. These tasks include the generative designation of \\textit{de novo}\nmolecules, linkers, fragments, scaffolds, and sidechains, all conditioned on\nthe structures of protein pockets. Our evaluations are conducted with fairness,\nencompassing comprehensive perspectives on interaction, chemical properties,\ngeometry authenticity, and substructure validity. We further provide the\npre-trained versions of the state-of-the-art models and deep insights with\nanalysis from empirical studies. The codebase for CBGBench is publicly\naccessible at \\url{https://github.com/Edapinenut/CBGBench}.\n","authors":["Haitao Lin","Guojiang Zhao","Odin Zhang","Yufei Huang","Lirong Wu","Zicheng Liu","Siyuan Li","Cheng Tan","Zhifeng Gao","Stan Z. Li"],"pdf_url":"https://arxiv.org/pdf/2406.10840v2.pdf","comment":"9 pages main context"},{"id":"http://arxiv.org/abs/2403.09549v2","updated":"2024-07-22T09:22:09Z","published":"2024-03-14T16:38:02Z","title":"Generalizing Denoising to Non-Equilibrium Structures Improves\n  Equivariant Force Fields","summary":"  Understanding the interactions of atoms such as forces in 3D atomistic\nsystems is fundamental to many applications like molecular dynamics and\ncatalyst design. However, simulating these interactions requires\ncompute-intensive ab initio calculations and thus results in limited data for\ntraining neural networks. In this paper, we propose to use denoising\nnon-equilibrium structures (DeNS) as an auxiliary task to better leverage\ntraining data and improve performance. For training with DeNS, we first corrupt\na 3D structure by adding noise to its 3D coordinates and then predict the\nnoise. Different from previous works on denoising, which are limited to\nequilibrium structures, the proposed method generalizes denoising to a much\nlarger set of non-equilibrium structures. The main difference is that a\nnon-equilibrium structure does not correspond to local energy minima and has\nnon-zero forces, and therefore it can have many possible atomic positions\ncompared to an equilibrium structure. This makes denoising non-equilibrium\nstructures an ill-posed problem since the target of denoising is not uniquely\ndefined. Our key insight is to additionally encode the forces of the original\nnon-equilibrium structure to specify which non-equilibrium structure we are\ndenoising. Concretely, given a corrupted non-equilibrium structure and the\nforces of the original one, we predict the non-equilibrium structure satisfying\nthe input forces instead of any arbitrary structures. Since DeNS requires\nencoding forces, DeNS favors equivariant networks, which can easily incorporate\nforces and other higher-order tensors in node embeddings. We study the\neffectiveness of training equivariant networks with DeNS on OC20, OC22 and MD17\ndatasets and demonstrate that DeNS can achieve new state-of-the-art results on\nOC20 and OC22 and significantly improve training efficiency on MD17.\n","authors":["Yi-Lun Liao","Tess Smidt","Muhammed Shuaibi","Abhishek Das"],"pdf_url":"https://arxiv.org/pdf/2403.09549v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.04284v2","updated":"2024-07-22T09:11:04Z","published":"2024-06-06T17:28:56Z","title":"What is Dataset Distillation Learning?","summary":"  Dataset distillation has emerged as a strategy to overcome the hurdles\nassociated with large datasets by learning a compact set of synthetic data that\nretains essential information from the original dataset. While distilled data\ncan be used to train high performing models, little is understood about how the\ninformation is stored. In this study, we posit and answer three questions about\nthe behavior, representativeness, and point-wise information content of\ndistilled data. We reveal distilled data cannot serve as a substitute for real\ndata during training outside the standard evaluation setting for dataset\ndistillation. Additionally, the distillation process retains high task\nperformance by compressing information related to the early training dynamics\nof real models. Finally, we provide an framework for interpreting distilled\ndata and reveal that individual distilled data points contain meaningful\nsemantic information. This investigation sheds light on the intricate nature of\ndistilled data, providing a better understanding on how they can be effectively\nutilized.\n","authors":["William Yang","Ye Zhu","Zhiwei Deng","Olga Russakovsky"],"pdf_url":"https://arxiv.org/pdf/2406.04284v2.pdf","comment":"ICML 2024"},{"id":"http://arxiv.org/abs/2301.12065v2","updated":"2024-07-22T09:06:22Z","published":"2023-01-28T02:47:42Z","title":"Decentralized Entropic Optimal Transport for Distributed Distribution\n  Comparison","summary":"  Distributed distribution comparison aims to measure the distance between the\ndistributions whose data are scattered across different agents in a distributed\nsystem and cannot even be shared directly among the agents. In this study, we\npropose a novel decentralized entropic optimal transport (DEOT) method, which\nprovides a communication-efficient and privacy-preserving solution to this\nproblem with theoretical guarantees. In particular, we design a mini-batch\nrandomized block-coordinate descent (MRBCD) scheme to optimize the DEOT\ndistance in its dual form. The dual variables are scattered across different\nagents and updated locally and iteratively with limited communications among\npartial agents. The kernel matrix involved in the gradients of the dual\nvariables is estimated by a decentralized kernel approximation method, in which\neach agent only needs to approximate and store a sub-kernel matrix by one-shot\ncommunication and without sharing raw data. Besides computing entropic\nWasserstein distance, we show that the proposed MRBCD scheme and kernel\napproximation method also apply to entropic Gromov-Wasserstein distance. We\nanalyze our method's communication complexity and, under mild assumptions,\nprovide a theoretical bound for the approximation error caused by the\nconvergence error, the estimated kernel, and the mismatch between the storage\nand communication protocols. In addition, we discuss the trade-off between the\nprecision of the EOT distance and the strength of privacy protection when\nimplementing our method. Experiments on synthetic data and real-world\ndistributed domain adaptation tasks demonstrate the effectiveness of our\nmethod.\n","authors":["Xiangfeng Wang","Hongteng Xu","Moyi Yang"],"pdf_url":"https://arxiv.org/pdf/2301.12065v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.04378v3","updated":"2024-07-22T09:00:18Z","published":"2023-11-07T22:52:54Z","title":"Watermarks in the Sand: Impossibility of Strong Watermarking for\n  Generative Models","summary":"  Watermarking generative models consists of planting a statistical signal\n(watermark) in a model's output so that it can be later verified that the\noutput was generated by the given model. A strong watermarking scheme satisfies\nthe property that a computationally bounded attacker cannot erase the watermark\nwithout causing significant quality degradation. In this paper, we study the\n(im)possibility of strong watermarking schemes. We prove that, under\nwell-specified and natural assumptions, strong watermarking is impossible to\nachieve. This holds even in the private detection algorithm setting, where the\nwatermark insertion and detection algorithms share a secret key, unknown to the\nattacker. To prove this result, we introduce a generic efficient watermark\nattack; the attacker is not required to know the private key of the scheme or\neven which scheme is used. Our attack is based on two assumptions: (1) The\nattacker has access to a \"quality oracle\" that can evaluate whether a candidate\noutput is a high-quality response to a prompt, and (2) The attacker has access\nto a \"perturbation oracle\" which can modify an output with a nontrivial\nprobability of maintaining quality, and which induces an efficiently mixing\nrandom walk on high-quality outputs. We argue that both assumptions can be\nsatisfied in practice by an attacker with weaker computational capabilities\nthan the watermarked model itself, to which the attacker has only black-box\naccess. Furthermore, our assumptions will likely only be easier to satisfy over\ntime as models grow in capabilities and modalities. We demonstrate the\nfeasibility of our attack by instantiating it to attack three existing\nwatermarking schemes for large language models: Kirchenbauer et al. (2023),\nKuditipudi et al. (2023), and Zhao et al. (2023). The same attack successfully\nremoves the watermarks planted by all three schemes, with only minor quality\ndegradation.\n","authors":["Hanlin Zhang","Benjamin L. Edelman","Danilo Francati","Daniele Venturi","Giuseppe Ateniese","Boaz Barak"],"pdf_url":"https://arxiv.org/pdf/2311.04378v3.pdf","comment":"ICML 2024. Website: https://hanlin-zhang.com/impossibility-watermarks"},{"id":"http://arxiv.org/abs/2407.15479v1","updated":"2024-07-22T08:46:20Z","published":"2024-07-22T08:46:20Z","title":"Affordance Labeling and Exploration: A Manifold-Based Approach","summary":"  The advancement in computing power has significantly reduced the training\ntimes for deep learning, fostering the rapid development of networks designed\nfor object recognition. However, the exploration of object utility, which is\nthe affordance of the object, as opposed to object recognition, has received\ncomparatively less attention. This work focuses on the problem of exploration\nof object affordances using existing networks trained on the object\nclassification dataset. While pre-trained networks have proven to be\ninstrumental in transfer learning for classification tasks, this work diverges\nfrom conventional object classification methods. Instead, it employs\npre-trained networks to discern affordance labels without the need for\nspecialized layers, abstaining from modifying the final layers through the\naddition of classification layers. To facilitate the determination of\naffordance labels without such modifications, two approaches, i.e. subspace\nclustering and manifold curvature methods are tested. These methods offer a\ndistinct perspective on affordance label recognition. Especially, manifold\ncurvature method has been successfully tested with nine distinct pre-trained\nnetworks, each achieving an accuracy exceeding 95%. Moreover, it is observed\nthat manifold curvature and subspace clustering methods explore affordance\nlabels that are not marked in the ground truth, but object affords in various\ncases.\n","authors":["İsmail Özçil","A. Buğra Koku"],"pdf_url":"https://arxiv.org/pdf/2407.15479v1.pdf","comment":"17 Pages, 3 Figures, 3 Tables"},{"id":"http://arxiv.org/abs/2407.04189v2","updated":"2024-07-22T08:45:22Z","published":"2024-07-04T23:47:10Z","title":"Meta-Learning and representation learner: A short theoretical note","summary":"  Meta-learning, or \"learning to learn,\" is a subfield of machine learning\nwhere the goal is to develop models and algorithms that can learn from various\ntasks and improve their learning process over time. Unlike traditional machine\nlearning methods focusing on learning a specific task, meta-learning aims to\nleverage experience from previous tasks to enhance future learning. This\napproach is particularly beneficial in scenarios where the available data for a\nnew task is limited, but there exists abundant data from related tasks. By\nextracting and utilizing the underlying structure and patterns across these\ntasks, meta-learning algorithms can achieve faster convergence and better\nperformance with fewer data. The following notes are mainly inspired from\n\\cite{vanschoren2018meta}, \\cite{baxter2019learning}, and\n\\cite{maurer2005algorithmic}.\n","authors":["Mouad El Bouchattaoui"],"pdf_url":"https://arxiv.org/pdf/2407.04189v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15476v1","updated":"2024-07-22T08:40:27Z","published":"2024-07-22T08:40:27Z","title":"MODRL-TA:A Multi-Objective Deep Reinforcement Learning Framework for\n  Traffic Allocation in E-Commerce Search","summary":"  Traffic allocation is a process of redistributing natural traffic to products\nby adjusting their positions in the post-search phase, aimed at effectively\nfostering merchant growth, precisely meeting customer demands, and ensuring the\nmaximization of interests across various parties within e-commerce platforms.\nExisting methods based on learning to rank neglect the long-term value of\ntraffic allocation, whereas approaches of reinforcement learning suffer from\nbalancing multiple objectives and the difficulties of cold starts within\nrealworld data environments. To address the aforementioned issues, this paper\npropose a multi-objective deep reinforcement learning framework consisting of\nmulti-objective Q-learning (MOQ), a decision fusion algorithm (DFM) based on\nthe cross-entropy method(CEM), and a progressive data augmentation system(PDA).\nSpecifically. MOQ constructs ensemble RL models, each dedicated to an\nobjective, such as click-through rate, conversion rate, etc. These models\nindividually determine the position of items as actions, aiming to estimate the\nlong-term value of multiple objectives from an individual perspective. Then we\nemploy DFM to dynamically adjust weights among objectives to maximize long-term\nvalue, addressing temporal dynamics in objective preferences in e-commerce\nscenarios. Initially, PDA trained MOQ with simulated data from offline logs. As\nexperiments progressed, it strategically integrated real user interaction data,\nultimately replacing the simulated dataset to alleviate distributional shifts\nand the cold start problem. Experimental results on real-world online\ne-commerce systems demonstrate the significant improvements of MODRL-TA, and we\nhave successfully deployed MODRL-TA on an e-commerce search platform.\n","authors":["Peng Cheng","Huimu Wang","Jinyuan Zhao","Yihao Wang","Enqiang Xu","Yu Zhao","Zhuojian Xiao","Songlin Wang","Guoyu Tang","Lin Liu","Sulong Xu"],"pdf_url":"https://arxiv.org/pdf/2407.15476v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.12404v2","updated":"2024-07-22T08:28:31Z","published":"2024-07-17T08:32:03Z","title":"Analyzing the Generalization and Reliability of Steering Vectors","summary":"  Steering vectors (SVs) are a new approach to efficiently adjust language\nmodel behaviour at inference time by intervening on intermediate model\nactivations. They have shown promise in terms of improving both capabilities\nand model alignment. However, the reliability and generalisation properties of\nthis approach are unknown. In this work, we rigorously investigate these\nproperties, and show that steering vectors have substantial limitations both\nin- and out-of-distribution. In-distribution, steerability is highly variable\nacross different inputs. Depending on the concept, spurious biases can\nsubstantially contribute to how effective steering is for each input,\npresenting a challenge for the widespread use of steering vectors.\nOut-of-distribution, while steering vectors often generalise well, for several\nconcepts they are brittle to reasonable changes in the prompt, resulting in\nthem failing to generalise well. Overall, our findings show that while steering\ncan work well in the right circumstances, there remain many technical\ndifficulties of applying steering vectors to guide models' behaviour at scale.\n","authors":["Daniel Tan","David Chanin","Aengus Lynch","Dimitrios Kanoulas","Brooks Paige","Adria Garriga-Alonso","Robert Kirk"],"pdf_url":"https://arxiv.org/pdf/2407.12404v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15464v1","updated":"2024-07-22T08:24:45Z","published":"2024-07-22T08:24:45Z","title":"The Diversity Bonus: Learning from Dissimilar Distributed Clients in\n  Personalized Federated Learning","summary":"  Personalized Federated Learning (PFL) is a commonly used framework that\nallows clients to collaboratively train their personalized models. PFL is\nparticularly useful for handling situations where data from different clients\nare not independent and identically distributed (non-IID). Previous research in\nPFL implicitly assumes that clients can gain more benefits from those with\nsimilar data distributions. Correspondingly, methods such as personalized\nweight aggregation are developed to assign higher weights to similar clients\nduring training. We pose a question: can a client benefit from other clients\nwith dissimilar data distributions and if so, how? This question is\nparticularly relevant in scenarios with a high degree of non-IID, where clients\nhave widely different data distributions, and learning from only similar\nclients will lose knowledge from many other clients. We note that when dealing\nwith clients with similar data distributions, methods such as personalized\nweight aggregation tend to enforce their models to be close in the parameter\nspace. It is reasonable to conjecture that a client can benefit from dissimilar\nclients if we allow their models to depart from each other. Based on this idea,\nwe propose DiversiFed which allows each client to learn from clients with\ndiversified data distribution in personalized federated learning. DiversiFed\npushes personalized models of clients with dissimilar data distributions apart\nin the parameter space while pulling together those with similar distributions.\nIn addition, to achieve the above effect without using prior knowledge of data\ndistribution, we design a loss function that leverages the model similarity to\ndetermine the degree of attraction and repulsion between any two models.\nExperiments on several datasets show that DiversiFed can benefit from\ndissimilar clients and thus outperform the state-of-the-art methods.\n","authors":["Xinghao Wu","Xuefeng Liu","Jianwei Niu","Guogang Zhu","Shaojie Tang","Xiaotian Li","Jiannong Cao"],"pdf_url":"https://arxiv.org/pdf/2407.15464v1.pdf","comment":"14 pages, 9 figures"},{"id":"http://arxiv.org/abs/2407.15462v1","updated":"2024-07-22T08:19:34Z","published":"2024-07-22T08:19:34Z","title":"Efficient Retrieval with Learned Similarities","summary":"  Retrieval plays a fundamental role in recommendation systems, search, and\nnatural language processing by efficiently finding relevant items from a large\ncorpus given a query. Dot products have been widely used as the similarity\nfunction in such retrieval tasks, thanks to Maximum Inner Product Search (MIPS)\nthat enabled efficient retrieval based on dot products. However,\nstate-of-the-art retrieval algorithms have migrated to learned similarities.\nSuch algorithms vary in form; the queries can be represented with multiple\nembeddings, complex neural networks can be deployed, the item ids can be\ndecoded directly from queries using beam search, and multiple approaches can be\ncombined in hybrid solutions. Unfortunately, we lack efficient solutions for\nretrieval in these state-of-the-art setups. Our work investigates techniques\nfor approximate nearest neighbor search with learned similarity functions. We\nfirst prove that Mixture-of-Logits (MoL) is a universal approximator, and can\nexpress all learned similarity functions. We next propose techniques to\nretrieve the approximate top K results using MoL with a tight bound. We finally\ncompare our techniques with existing approaches, showing that MoL sets new\nstate-of-the-art results on recommendation retrieval tasks, and our approximate\ntop-k retrieval with learned similarities outperforms baselines by up to two\norders of magnitude in latency, while achieving > .99 recall rate of exact\nalgorithms.\n","authors":["Bailu Ding","Jiaqi Zhai"],"pdf_url":"https://arxiv.org/pdf/2407.15462v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.15870v3","updated":"2024-07-22T08:18:38Z","published":"2023-07-29T02:35:37Z","title":"SemiSFL: Split Federated Learning on Unlabeled and Non-IID Data","summary":"  Federated Learning (FL) has emerged to allow multiple clients to\ncollaboratively train machine learning models on their private data at the\nnetwork edge. However, training and deploying large-scale models on\nresource-constrained devices is challenging. Fortunately, Split Federated\nLearning (SFL) offers a feasible solution by alleviating the computation and/or\ncommunication burden on clients. However, existing SFL works often assume\nsufficient labeled data on clients, which is usually impractical. Besides, data\nnon-IIDness poses another challenge to ensure efficient model training. To our\nbest knowledge, the above two issues have not been simultaneously addressed in\nSFL. Herein, we propose a novel Semi-supervised SFL system, termed SemiSFL,\nwhich incorporates clustering regularization to perform SFL with unlabeled and\nnon-IID client data. Moreover, our theoretical and experimental investigations\ninto model convergence reveal that the inconsistent training processes on\nlabeled and unlabeled data have an influence on the effectiveness of clustering\nregularization. To mitigate the training inconsistency, we develop an algorithm\nfor dynamically adjusting the global updating frequency, so as to improve\ntraining performance. Extensive experiments on benchmark models and datasets\nshow that our system provides a 3.8x speed-up in training time, reduces the\ncommunication cost by about 70.3% while reaching the target accuracy, and\nachieves up to 5.8% improvement in accuracy under non-IID scenarios compared to\nthe state-of-the-art baselines.\n","authors":["Yang Xu","Yunming Liao","Hongli Xu","Zhipeng Sun","Liusheng Huang","Chunming Qiao"],"pdf_url":"https://arxiv.org/pdf/2307.15870v3.pdf","comment":"16 pages"},{"id":"http://arxiv.org/abs/2407.15455v1","updated":"2024-07-22T08:13:13Z","published":"2024-07-22T08:13:13Z","title":"Score matching for bridges without time-reversals","summary":"  We propose a new algorithm for learning a bridged diffusion process using\nscore-matching methods. Our method relies on reversing the dynamics of the\nforward process and using this to learn a score function, which, via Doob's\n$h$-transform, gives us a bridged diffusion process; that is, a process\nconditioned on an endpoint. In contrast to prior methods, ours learns the score\nterm $\\nabla_x \\log p(t, x; T, y)$, for given $t, Y$ directly, completely\navoiding the need for first learning a time reversal. We compare the\nperformance of our algorithm with existing methods and see that it outperforms\nusing the (learned) time-reversals to learn the score term. The code can be\nfound at https://github.com/libbylbaker/forward_bridge.\n","authors":["Elizabeth L. Baker","Moritz Schauer","Stefan Sommer"],"pdf_url":"https://arxiv.org/pdf/2407.15455v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15453v1","updated":"2024-07-22T08:11:58Z","published":"2024-07-22T08:11:58Z","title":"Regression under demographic parity constraints via unlabeled\n  post-processing","summary":"  We address the problem of performing regression while ensuring demographic\nparity, even without access to sensitive attributes during inference. We\npresent a general-purpose post-processing algorithm that, using accurate\nestimates of the regression function and a sensitive attribute predictor,\ngenerates predictions that meet the demographic parity constraint. Our method\ninvolves discretization and stochastic minimization of a smooth convex\nfunction. It is suitable for online post-processing and multi-class\nclassification tasks only involving unlabeled data for the post-processing.\nUnlike prior methods, our approach is fully theory-driven. We require precise\ncontrol over the gradient norm of the convex function, and thus, we rely on\nmore advanced techniques than standard stochastic gradient descent. Our\nalgorithm is backed by finite-sample analysis and post-processing bounds, with\nexperimental results validating our theoretical findings.\n","authors":["Evgenii Chzhen","Mohamed Hebiri","Gayane Taturyan"],"pdf_url":"https://arxiv.org/pdf/2407.15453v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15452v1","updated":"2024-07-22T08:09:36Z","published":"2024-07-22T08:09:36Z","title":"GraphScale: A Framework to Enable Machine Learning over Billion-node\n  Graphs","summary":"  Graph Neural Networks (GNNs) have emerged as powerful tools for supervised\nmachine learning over graph-structured data, while sampling-based node\nrepresentation learning is widely utilized in unsupervised learning. However,\nscalability remains a major challenge in both supervised and unsupervised\nlearning for large graphs (e.g., those with over 1 billion nodes). The\nscalability bottleneck largely stems from the mini-batch sampling phase in GNNs\nand the random walk sampling phase in unsupervised methods. These processes\noften require storing features or embeddings in memory. In the context of\ndistributed training, they require frequent, inefficient random access to data\nstored across different workers. Such repeated inter-worker communication for\neach mini-batch leads to high communication overhead and computational\ninefficiency.\n  We propose GraphScale, a unified framework for both supervised and\nunsupervised learning to store and process large graph data distributedly. The\nkey insight in our design is the separation of workers who store data and those\nwho perform the training. This separation allows us to decouple computing and\nstorage in graph training, thus effectively building a pipeline where data\nfetching and data computation can overlap asynchronously. Our experiments show\nthat GraphScale outperforms state-of-the-art methods for distributed training\nof both GNNs and node embeddings. We evaluate GraphScale both on public and\nproprietary graph datasets and observe a reduction of at least 40% in\nend-to-end training times compared to popular distributed frameworks, without\nany loss in performance. While most existing methods don't support billion-node\ngraphs for training node embeddings, GraphScale is currently deployed in\nproduction at TikTok enabling efficient learning over such large graphs.\n","authors":["Vipul Gupta","Xin Chen","Ruoyun Huang","Fanlong Meng","Jianjun Chen","Yujun Yan"],"pdf_url":"https://arxiv.org/pdf/2407.15452v1.pdf","comment":"Published in the Proceedings of the 33rd ACM International Conference\n  on Information and Knowledge Management (CIKM 2024), 8 Pages, 12 Figures"},{"id":"http://arxiv.org/abs/2406.14697v2","updated":"2024-07-22T08:03:26Z","published":"2024-06-20T19:39:17Z","title":"A Benchmark Study of Deep-RL Methods for Maximum Coverage Problems over\n  Graphs","summary":"  Recent years have witnessed a growing trend toward employing deep\nreinforcement learning (Deep-RL) to derive heuristics for combinatorial\noptimization (CO) problems on graphs. Maximum Coverage Problem (MCP) and its\nprobabilistic variant on social networks, Influence Maximization (IM), have\nbeen particularly prominent in this line of research. In this paper, we present\na comprehensive benchmark study that thoroughly investigates the effectiveness\nand efficiency of five recent Deep-RL methods for MCP and IM. These methods\nwere published in top data science venues, namely S2V-DQN, Geometric-QN, GCOMB,\nRL4IM, and LeNSE. Our findings reveal that, across various scenarios, the Lazy\nGreedy algorithm consistently outperforms all Deep-RL methods for MCP. In the\ncase of IM, theoretically sound algorithms like IMM and OPIM demonstrate\nsuperior performance compared to Deep-RL methods in most scenarios. Notably, we\nobserve an abnormal phenomenon in IM problem where Deep-RL methods slightly\noutperform IMM and OPIM when the influence spread nearly does not increase as\nthe budget increases. Furthermore, our experimental results highlight common\nissues when applying Deep-RL methods to MCP and IM in practical settings.\nFinally, we discuss potential avenues for improving Deep-RL methods. Our\nbenchmark study sheds light on potential challenges in current deep\nreinforcement learning research for solving combinatorial optimization\nproblems.\n","authors":["Zhicheng Liang","Yu Yang","Xiangyu Ke","Xiaokui Xiao","Yunjun Gao"],"pdf_url":"https://arxiv.org/pdf/2406.14697v2.pdf","comment":"This paper has been accepted by VLDB 2024"},{"id":"http://arxiv.org/abs/2312.05910v5","updated":"2024-07-22T07:45:40Z","published":"2023-12-10T15:22:30Z","title":"Ensemble Kalman Filtering Meets Gaussian Process SSM for Non-Mean-Field\n  and Online Inference","summary":"  The Gaussian process state-space models (GPSSMs) represent a versatile class\nof data-driven nonlinear dynamical system models. However, the presence of\nnumerous latent variables in GPSSM incurs unresolved issues for existing\nvariational inference approaches, particularly under the more realistic\nnon-mean-field (NMF) assumption, including extensive training effort,\ncompromised inference accuracy, and infeasibility for online applications,\namong others. In this paper, we tackle these challenges by incorporating the\nensemble Kalman filter (EnKF), a well-established model-based filtering\ntechnique, into the NMF variational inference framework to approximate the\nposterior distribution of the latent states. This novel marriage between EnKF\nand GPSSM not only eliminates the need for extensive parameterization in\nlearning variational distributions, but also enables an interpretable,\nclosed-form approximation of the evidence lower bound (ELBO). Moreover, owing\nto the streamlined parameterization via the EnKF, the new GPSSM model can be\neasily accommodated in online learning applications. We demonstrate that the\nresulting EnKF-aided online algorithm embodies a principled objective function\nby ensuring data-fitting accuracy while incorporating model regularizations to\nmitigate overfitting. We also provide detailed analysis and fresh insights for\nthe proposed algorithms. Comprehensive evaluation across diverse real and\nsynthetic datasets corroborates the superior learning and inference performance\nof our EnKF-aided variational inference algorithms compared to existing\nmethods.\n","authors":["Zhidi Lin","Yiyong Sun","Feng Yin","Alexandre Hoang Thiéry"],"pdf_url":"https://arxiv.org/pdf/2312.05910v5.pdf","comment":"Gaussian process, state-space model, ensemble Kalman filter, online\n  learning, variational inference"},{"id":"http://arxiv.org/abs/2407.15439v1","updated":"2024-07-22T07:36:27Z","published":"2024-07-22T07:36:27Z","title":"Merit-based Fair Combinatorial Semi-Bandit with Unrestricted Feedback\n  Delays","summary":"  We study the stochastic combinatorial semi-bandit problem with unrestricted\nfeedback delays under merit-based fairness constraints. This is motivated by\napplications such as crowdsourcing, and online advertising, where immediate\nfeedback is not immediately available and fairness among different choices (or\narms) is crucial. We consider two types of unrestricted feedback delays:\nreward-independent delays where the feedback delays are independent of the\nrewards, and reward-dependent delays where the feedback delays are correlated\nwith the rewards. Furthermore, we introduce merit-based fairness constraints to\nensure a fair selection of the arms. We define the reward regret and the\nfairness regret and present new bandit algorithms to select arms under\nunrestricted feedback delays based on their merits. We prove that our\nalgorithms all achieve sublinear expected reward regret and expected fairness\nregret, with a dependence on the quantiles of the delay distribution. We also\nconduct extensive experiments using synthetic and real-world data and show that\nour algorithms can fairly select arms with different feedback delays.\n","authors":["Ziqun Chen","Kechao Cai","Zhuoyue Chen","Jinbei Zhang","John C. S. Lui"],"pdf_url":"https://arxiv.org/pdf/2407.15439v1.pdf","comment":"28 pages, 9 figures, accepted for 27th European Conference on\n  Artificial Intelligence"},{"id":"http://arxiv.org/abs/2407.15431v1","updated":"2024-07-22T07:24:21Z","published":"2024-07-22T07:24:21Z","title":"Pre-Training and Prompting for Few-Shot Node Classification on\n  Text-Attributed Graphs","summary":"  The text-attributed graph (TAG) is one kind of important real-world\ngraph-structured data with each node associated with raw texts. For TAGs,\ntraditional few-shot node classification methods directly conduct training on\nthe pre-processed node features and do not consider the raw texts. The\nperformance is highly dependent on the choice of the feature pre-processing\nmethod. In this paper, we propose P2TAG, a framework designed for few-shot node\nclassification on TAGs with graph pre-training and prompting. P2TAG first\npre-trains the language model (LM) and graph neural network (GNN) on TAGs with\nself-supervised loss. To fully utilize the ability of language models, we adapt\nthe masked language modeling objective for our framework. The pre-trained model\nis then used for the few-shot node classification with a mixed prompt method,\nwhich simultaneously considers both text and graph information. We conduct\nexperiments on six real-world TAGs, including paper citation networks and\nproduct co-purchasing networks. Experimental results demonstrate that our\nproposed framework outperforms existing graph few-shot learning methods on\nthese datasets with +18.98% ~ +35.98% improvements.\n","authors":["Huanjing Zhao","Beining Yang","Yukuo Cen","Junyu Ren","Chenhui Zhang","Yuxiao Dong","Evgeny Kharlamov","Shu Zhao","Jie Tang"],"pdf_url":"https://arxiv.org/pdf/2407.15431v1.pdf","comment":"Accepted to KDD'24"},{"id":"http://arxiv.org/abs/2407.04495v3","updated":"2024-07-22T07:19:24Z","published":"2024-07-05T13:35:14Z","title":"Speed-accuracy trade-off for the diffusion models: Wisdom from\n  nonequilibrium thermodynamics and optimal transport","summary":"  We discuss a connection between a generative model, called the diffusion\nmodel, and nonequilibrium thermodynamics for the Fokker-Planck equation, called\nstochastic thermodynamics. Based on the techniques of stochastic\nthermodynamics, we derive the speed-accuracy trade-off for the diffusion\nmodels, which is a trade-off relationship between the speed and accuracy of\ndata generation in diffusion models. Our result implies that the entropy\nproduction rate in the forward process affects the errors in data generation.\nFrom a stochastic thermodynamic perspective, our results provide quantitative\ninsight into how best to generate data in diffusion models. The optimal\nlearning protocol is introduced by the conservative force in stochastic\nthermodynamics and the geodesic of space by the 2-Wasserstein distance in\noptimal transport theory. We numerically illustrate the validity of the\nspeed-accuracy trade-off for the diffusion models with different noise\nschedules such as the cosine schedule, the conditional optimal transport, and\nthe optimal transport.\n","authors":["Kotaro Ikeda","Tomoya Uda","Daisuke Okanohara","Sosuke Ito"],"pdf_url":"https://arxiv.org/pdf/2407.04495v3.pdf","comment":"26 pages, 5 figures"},{"id":"http://arxiv.org/abs/2405.07761v2","updated":"2024-07-22T07:13:18Z","published":"2024-05-13T14:03:49Z","title":"LLM4ED: Large Language Models for Automatic Equation Discovery","summary":"  Equation discovery is aimed at directly extracting physical laws from data\nand has emerged as a pivotal research domain. Previous methods based on\nsymbolic mathematics have achieved substantial advancements, but often require\nthe design of implementation of complex algorithms. In this paper, we introduce\na new framework that utilizes natural language-based prompts to guide large\nlanguage models (LLMs) in automatically mining governing equations from data.\nSpecifically, we first utilize the generation capability of LLMs to generate\ndiverse equations in string form, and then evaluate the generated equations\nbased on observations. In the optimization phase, we propose two alternately\niterated strategies to optimize generated equations collaboratively. The first\nstrategy is to take LLMs as a black-box optimizer and achieve equation\nself-improvement based on historical samples and their performance. The second\nstrategy is to instruct LLMs to perform evolutionary operators for global\nsearch. Experiments are extensively conducted on both partial differential\nequations and ordinary differential equations. Results demonstrate that our\nframework can discover effective equations to reveal the underlying physical\nlaws under various nonlinear dynamic systems. Further comparisons are made with\nstate-of-the-art models, demonstrating good stability and usability. Our\nframework substantially lowers the barriers to learning and applying equation\ndiscovery techniques, demonstrating the application potential of LLMs in the\nfield of knowledge discovery.\n","authors":["Mengge Du","Yuntian Chen","Zhongzheng Wang","Longfeng Nie","Dongxiao Zhang"],"pdf_url":"https://arxiv.org/pdf/2405.07761v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.11070v2","updated":"2024-07-22T07:08:31Z","published":"2024-07-12T18:34:55Z","title":"Optimal Defender Strategies for CAGE-2 using Causal Modeling and Tree\n  Search","summary":"  The CAGE-2 challenge is considered a standard benchmark to compare methods\nfor autonomous cyber defense. Current state-of-the-art methods evaluated\nagainst this benchmark are based on model-free (offline) reinforcement\nlearning, which does not provide provably optimal defender strategies. We\naddress this limitation and present a formal (causal) model of CAGE-2 together\nwith a method that produces a provably optimal defender strategy, which we call\nCausal Partially Observable Monte-Carlo Planning (C-POMCP). It has two key\nproperties. First, it incorporates the causal structure of the target system,\ni.e., the causal relationships among the system variables. This structure\nallows for a significant reduction of the search space of defender strategies.\nSecond, it is an online method that updates the defender strategy at each time\nstep via tree search. Evaluations against the CAGE-2 benchmark show that\nC-POMCP achieves state-of-the-art performance with respect to effectiveness and\nis two orders of magnitude more efficient in computing time than the closest\ncompetitor method.\n","authors":["Kim Hammar","Neil Dhir","Rolf Stadler"],"pdf_url":"https://arxiv.org/pdf/2407.11070v2.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible"},{"id":"http://arxiv.org/abs/2407.15426v1","updated":"2024-07-22T07:06:17Z","published":"2024-07-22T07:06:17Z","title":"Resource-Efficient Federated Multimodal Learning via Layer-wise and\n  Progressive Training","summary":"  Combining different data modalities enables deep neural networks to tackle\ncomplex tasks more effectively, making multimodal learning increasingly\npopular. To harness multimodal data closer to end users, it is essential to\nintegrate multimodal learning with privacy-preserving training approaches such\nas federated learning (FL). However, compared to conventional unimodal\nlearning, multimodal setting requires dedicated encoders for each modality,\nresulting in larger and more complex models that demand significant resources.\nThis presents a substantial challenge for FL clients operating with limited\ncomputational resources and communication bandwidth. To address these\nchallenges, we introduce LW-FedMML, a layer-wise federated multimodal learning\napproach, which decomposes the training process into multiple steps. Each step\nfocuses on training only a portion of the model, thereby significantly reducing\nthe memory and computational requirements. Moreover, FL clients only need to\nexchange the trained model portion with the central server, lowering the\nresulting communication cost. We conduct extensive experiments across various\nFL scenarios and multimodal learning setups to validate the effectiveness of\nour proposed method. The results demonstrate that LW-FedMML can compete with\nconventional end-to-end federated multimodal learning (FedMML) while\nsignificantly reducing the resource burden on FL clients. Specifically,\nLW-FedMML reduces memory usage by up to $2.7\\times$, computational operations\n(FLOPs) by $2.4\\times$, and total communication cost by $2.3\\times$. We also\nintroduce a progressive training approach called Prog-FedMML. While it offers\nlesser resource efficiency than LW-FedMML, Prog-FedMML has the potential to\nsurpass the performance of end-to-end FedMML, making it a viable option for\nscenarios with fewer resource constraints.\n","authors":["Ye Lin Tun","Chu Myaet Thwal","Minh N. H. Nguyen","Choong Seon Hong"],"pdf_url":"https://arxiv.org/pdf/2407.15426v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15425v1","updated":"2024-07-22T07:02:15Z","published":"2024-07-22T07:02:15Z","title":"Empirical Capacity Model for Self-Attention Neural Networks","summary":"  Large pretrained self-attention neural networks, or transformers, have been\nvery successful in various tasks recently. The performance of a model on a\ngiven task depends on its ability to memorize and generalize the training data.\nLarge transformer models, which may have billions of parameters, in theory have\na huge capacity to memorize content. However, the current algorithms for the\noptimization fall short of the theoretical capacity, and the capacity is also\nhighly dependent on the content. In this paper, we focus on the memory capacity\nof these models obtained using common training algorithms and synthetic\ntraining data. Based on the results, we derive an empirical capacity model\n(ECM) for a generic transformer. The ECM can be used to design task-specific\ntransformer models with an optimal number of parameters in cases where the\ntarget memorization capability of the task can be defined.\n","authors":["Aki Härmä","Marcin Pietrasik","Anna Wilbik"],"pdf_url":"https://arxiv.org/pdf/2407.15425v1.pdf","comment":"Submitted to BNAIC'24, 14 pages + refs"},{"id":"http://arxiv.org/abs/2407.15421v1","updated":"2024-07-22T06:57:34Z","published":"2024-07-22T06:57:34Z","title":"Planning behavior in a recurrent neural network that plays Sokoban","summary":"  To predict how advanced neural networks generalize to novel situations, it is\nessential to understand how they reason. Guez et al. (2019, \"An investigation\nof model-free planning\") trained a recurrent neural network (RNN) to play\nSokoban with model-free reinforcement learning. They found that adding extra\ncomputation steps to the start of episodes at test time improves the RNN's\nsuccess rate. We further investigate this phenomenon, finding that it rapidly\nemerges early on in training and then slowly fades, but only for comparatively\neasier levels. The RNN also often takes redundant actions at episode starts,\nand these are reduced by adding extra computation steps. Our results suggest\nthat the RNN learns to take time to think by `pacing', despite the per-step\npenalties, indicating that training incentivizes planning capabilities. The\nsmall size (1.29M parameters) and interesting behavior of this model make it an\nexcellent model organism for mechanistic interpretability.\n","authors":["Adrià Garriga-Alonso","Mohammad Taufeeque","Adam Gleave"],"pdf_url":"https://arxiv.org/pdf/2407.15421v1.pdf","comment":"Mechanistic Interpretability workshop, ICML 2024"},{"id":"http://arxiv.org/abs/2311.13348v2","updated":"2024-07-22T06:43:13Z","published":"2023-11-22T12:25:02Z","title":"MergeSFL: Split Federated Learning with Feature Merging and Batch Size\n  Regulation","summary":"  Recently, federated learning (FL) has emerged as a popular technique for edge\nAI to mine valuable knowledge in edge computing (EC) systems. To mitigate the\ncomputing/communication burden on resource-constrained workers and protect\nmodel privacy, split federated learning (SFL) has been released by integrating\nboth data and model parallelism. Despite resource limitations, SFL still faces\ntwo other critical challenges in EC, i.e., statistical heterogeneity and system\nheterogeneity. To address these challenges, we propose a novel SFL framework,\ntermed MergeSFL, by incorporating feature merging and batch size regulation in\nSFL. Concretely, feature merging aims to merge the features from workers into a\nmixed feature sequence, which is approximately equivalent to the features\nderived from IID data and is employed to promote model accuracy. While batch\nsize regulation aims to assign diverse and suitable batch sizes for\nheterogeneous workers to improve training efficiency. Moreover, MergeSFL\nexplores to jointly optimize these two strategies upon their coupled\nrelationship to better enhance the performance of SFL. Extensive experiments\nare conducted on a physical platform with 80 NVIDIA Jetson edge devices, and\nthe experimental results show that MergeSFL can improve the final model\naccuracy by 5.82% to 26.22%, with a speedup by about 1.74x to 4.14x, compared\nto the baselines.\n","authors":["Yunming Liao","Yang Xu","Hongli Xu","Lun Wang","Zhiwei Yao","Chunming Qiao"],"pdf_url":"https://arxiv.org/pdf/2311.13348v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15414v1","updated":"2024-07-22T06:41:59Z","published":"2024-07-22T06:41:59Z","title":"Weights Shuffling for Improving DPSGD in Transformer-based Models","summary":"  Differential Privacy (DP) mechanisms, especially in high-dimensional\nsettings, often face the challenge of maintaining privacy without compromising\nthe data utility. This work introduces an innovative shuffling mechanism in\nDifferentially-Private Stochastic Gradient Descent (DPSGD) to enhance the\nutility of large models at the same privacy guarantee of the unshuffled case.\nSpecifically, we reveal that random shuffling brings additional randomness to\nthe trajectory of gradient descent while not impacting the model accuracy by\nthe permutation invariance property -- the model can be equivalently computed\nin both forward and backward propagations under permutation. We show that\npermutation indeed improves the privacy guarantee of DPSGD in theory, but\ntracking the exact privacy loss on shuffled model is particularly challenging.\nHence we exploit the approximation on sum of lognormal distributions to derive\nthe condition for the shuffled DPSGD to meet the DP guarantee. Auditing results\nshow that our condition offers a DP guarantee quite close to the audited\nprivacy level, demonstrating our approach an effective estimation in practice.\nExperimental results have verified our theoretical derivation and illustrate\nthat our mechanism improves the accuracy of DPSGD over the state-of-the-art\nbaselines on a variety of models and tasks.\n","authors":["Jungang Yang","Zhe Ji","Liyao Xiang"],"pdf_url":"https://arxiv.org/pdf/2407.15414v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.17042v2","updated":"2024-07-22T06:25:54Z","published":"2024-05-27T10:54:42Z","title":"LabObf: A Label Protection Scheme for Vertical Federated Learning\n  Through Label Obfuscation","summary":"  Split Neural Network, as one of the most common architectures used in\nvertical federated learning, is popular in industry due to its\nprivacy-preserving characteristics. In this architecture, the party holding the\nlabels seeks cooperation from other parties to improve model performance due to\ninsufficient feature data. Each of these participants has a self-defined bottom\nmodel to learn hidden representations from its own feature data and uploads the\nembedding vectors to the top model held by the label holder for final\npredictions. This design allows participants to conduct joint training without\ndirectly exchanging data. However, existing research points out that malicious\nparticipants may still infer label information from the uploaded embeddings,\nleading to privacy leakage. In this paper, we first propose an embedding\nextension attack manipulating embeddings to undermine existing defense\nstrategies, which rely on constraining the correlation between the embeddings\nuploaded by participants and the labels. Subsequently, we propose a new label\nobfuscation defense strategy, called `LabObf', which randomly maps each\noriginal integer-valued label to multiple real-valued soft labels with values\nintertwined, significantly increasing the difficulty for attackers to infer the\nlabels. We conduct experiments on four different types of datasets, and the\nresults show that LabObf significantly reduces the attacker's success rate\ncompared to raw models while maintaining desirable model accuracy.\n","authors":["Ying He","Mingyang Niu","Jingyu Hua","Yunlong Mao","Xu Huang","Chen Li","Sheng Zhong"],"pdf_url":"https://arxiv.org/pdf/2405.17042v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.17052v2","updated":"2024-07-22T06:23:02Z","published":"2024-01-30T14:33:18Z","title":"Retrieval Augmented Deep Anomaly Detection for Tabular Data","summary":"  Deep learning for tabular data has garnered increasing attention in recent\nyears, yet employing deep models for structured data remains challenging. While\nthese models excel with unstructured data, their efficacy with structured data\nhas been limited. Recent research has introduced retrieval-augmented models to\naddress this gap, demonstrating promising results in supervised tasks such as\nclassification and regression. In this work, we investigate using\nretrieval-augmented models for anomaly detection on tabular data. We propose a\nreconstruction-based approach in which a transformer model learns to\nreconstruct masked features of \\textit{normal} samples. We test the\neffectiveness of KNN-based and attention-based modules to select relevant\nsamples to help in the reconstruction process of the target sample. Our\nexperiments on a benchmark of 31 tabular datasets reveal that augmenting this\nreconstruction-based anomaly detection (AD) method with sample-sample\ndependencies via retrieval modules significantly boosts performance. The\npresent work supports the idea that retrieval module are useful to augment any\ndeep AD method to enhance anomaly detection on tabular data.\n","authors":["Hugo Thimonier","Fabrice Popineau","Arpad Rimmel","Bich-Liên Doan"],"pdf_url":"https://arxiv.org/pdf/2401.17052v2.pdf","comment":"Accepted at CIKM 2024"},{"id":"http://arxiv.org/abs/2407.15017v1","updated":"2024-07-22T06:15:59Z","published":"2024-07-22T06:15:59Z","title":"Knowledge Mechanisms in Large Language Models: A Survey and Perspective","summary":"  Understanding knowledge mechanisms in Large Language Models (LLMs) is crucial\nfor advancing towards trustworthy AGI. This paper reviews knowledge mechanism\nanalysis from a novel taxonomy including knowledge utilization and evolution.\nKnowledge utilization delves into the mechanism of memorization, comprehension\nand application, and creation. Knowledge evolution focuses on the dynamic\nprogression of knowledge within individual and group LLMs. Moreover, we discuss\nwhat knowledge LLMs have learned, the reasons for the fragility of parametric\nknowledge, and the potential dark knowledge (hypothesis) that will be\nchallenging to address. We hope this work can help understand knowledge in LLMs\nand provide insights for future research.\n","authors":["Mengru Wang","Yunzhi Yao","Ziwen Xu","Shuofei Qiao","Shumin Deng","Peng Wang","Xiang Chen","Jia-Chen Gu","Yong Jiang","Pengjun Xie","Fei Huang","Huajun Chen","Ningyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2407.15017v1.pdf","comment":"Ongoing work (v1); 34 pages, 5 figures"},{"id":"http://arxiv.org/abs/2407.15403v1","updated":"2024-07-22T06:12:21Z","published":"2024-07-22T06:12:21Z","title":"Offline Imitation Learning Through Graph Search and Retrieval","summary":"  Imitation learning is a powerful machine learning algorithm for a robot to\nacquire manipulation skills. Nevertheless, many real-world manipulation tasks\ninvolve precise and dexterous robot-object interactions, which make it\ndifficult for humans to collect high-quality expert demonstrations. As a\nresult, a robot has to learn skills from suboptimal demonstrations and\nunstructured interactions, which remains a key challenge. Existing works\ntypically use offline deep reinforcement learning (RL) to solve this challenge,\nbut in practice these algorithms are unstable and fragile due to the deadly\ntriad issue. To overcome this problem, we propose GSR, a simple yet effective\nalgorithm that learns from suboptimal demonstrations through Graph Search and\nRetrieval. We first use pretrained representation to organize the interaction\nexperience into a graph and perform a graph search to calculate the values of\ndifferent behaviors. Then, we apply a retrieval-based procedure to identify the\nbest behavior (actions) on each state and use behavior cloning to learn that\nbehavior. We evaluate our method in both simulation and real-world robotic\nmanipulation tasks with complex visual inputs, covering various precise and\ndexterous manipulation skills with objects of different physical properties.\nGSR can achieve a 10% to 30% higher success rate and over 30% higher\nproficiency compared to baselines. Our project page is at\nhttps://zhaohengyin.github.io/gsr.\n","authors":["Zhao-Heng Yin","Pieter Abbeel"],"pdf_url":"https://arxiv.org/pdf/2407.15403v1.pdf","comment":"Robotics: Science and Systems (RSS) 2024"},{"id":"http://arxiv.org/abs/2407.15402v1","updated":"2024-07-22T06:08:13Z","published":"2024-07-22T06:08:13Z","title":"Tackling Selfish Clients in Federated Learning","summary":"  Federated Learning (FL) is a distributed machine learning paradigm\nfacilitating participants to collaboratively train a model without revealing\ntheir local data. However, when FL is deployed into the wild, some intelligent\nclients can deliberately deviate from the standard training process to make the\nglobal model inclined toward their local model, thereby prioritizing their\nlocal data distribution. We refer to this novel category of misbehaving clients\nas selfish. In this paper, we propose a Robust aggregation strategy for FL\nserver to mitigate the effect of Selfishness (in short RFL-Self). RFL-Self\nincorporates an innovative method to recover (or estimate) the true updates of\nselfish clients from the received ones, leveraging robust statistics (median of\nnorms) of the updates at every round. By including the recovered updates in\naggregation, our strategy offers strong robustness against selfishness. Our\nexperimental results, obtained on MNIST and CIFAR-10 datasets, demonstrate that\njust 2% of clients behaving selfishly can decrease the accuracy by up to 36%,\nand RFL-Self can mitigate that effect without degrading the global model\nperformance.\n","authors":["Andrea Augello","Ashish Gupta","Giuseppe Lo Re","Sajal K. Das"],"pdf_url":"https://arxiv.org/pdf/2407.15402v1.pdf","comment":"10 pages, 16 figures. European Conference on Artificial Intelligence\n  (ECAI) 2024"},{"id":"http://arxiv.org/abs/2403.09793v2","updated":"2024-07-22T05:48:50Z","published":"2024-03-14T18:25:40Z","title":"Socially Integrated Navigation: A Social Acting Robot with Deep\n  Reinforcement Learning","summary":"  Mobile robots are being used on a large scale in various crowded situations\nand become part of our society. The socially acceptable navigation behavior of\na mobile robot with individual human consideration is an essential requirement\nfor scalable applications and human acceptance. Deep Reinforcement Learning\n(DRL) approaches are recently used to learn a robot's navigation policy and to\nmodel the complex interactions between robots and humans. We propose to divide\nexisting DRL-based navigation approaches based on the robot's exhibited social\nbehavior and distinguish between social collision avoidance with a lack of\nsocial behavior and socially aware approaches with explicit predefined social\nbehavior. In addition, we propose a novel socially integrated navigation\napproach where the robot's social behavior is adaptive and emerges from the\ninteraction with humans. The formulation of our approach is derived from a\nsociological definition, which states that social acting is oriented toward the\nacting of others. The DRL policy is trained in an environment where other\nagents interact socially integrated and reward the robot's behavior\nindividually. The simulation results indicate that the proposed socially\nintegrated navigation approach outperforms a socially aware approach in terms\nof ego navigation performance while significantly reducing the negative impact\non all agents within the environment.\n","authors":["Daniel Flögel","Lars Fischer","Thomas Rudolf","Tobias Schürmann","Sören Hohmann"],"pdf_url":"https://arxiv.org/pdf/2403.09793v2.pdf","comment":"Accepted at 2024 IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS)"},{"id":"http://arxiv.org/abs/2407.15389v1","updated":"2024-07-22T05:34:47Z","published":"2024-07-22T05:34:47Z","title":"Poisoning with A Pill: Circumventing Detection in Federated Learning","summary":"  Without direct access to the client's data, federated learning (FL) is\nwell-known for its unique strength in data privacy protection among existing\ndistributed machine learning techniques. However, its distributive and\niterative nature makes FL inherently vulnerable to various poisoning attacks.\nTo counteract these threats, extensive defenses have been proposed to filter\nout malicious clients, using various detection metrics. Based on our analysis\nof existing attacks and defenses, we find that there is a lack of attention to\nmodel redundancy. In neural networks, various model parameters contribute\ndifferently to the model's performance. However, existing attacks in FL\nmanipulate all the model update parameters with the same strategy, making them\neasily detectable by common defenses. Meanwhile, the defenses also tend to\nanalyze the overall statistical features of the entire model updates, leaving\nroom for sophisticated attacks. Based on these observations, this paper\nproposes a generic and attack-agnostic augmentation approach designed to\nenhance the effectiveness and stealthiness of existing FL poisoning attacks\nagainst detection in FL, pointing out the inherent flaws of existing defenses\nand exposing the necessity of fine-grained FL security. Specifically, we employ\na three-stage methodology that strategically constructs, generates, and injects\npoison (generated by existing attacks) into a pill (a tiny subnet with a novel\nstructure) during the FL training, named as pill construction, pill poisoning,\nand pill injection accordingly. Extensive experimental results show that FL\npoisoning attacks enhanced by our method can bypass all the popular defenses,\nand can gain an up to 7x error rate increase, as well as on average a more than\n2x error rate increase on both IID and non-IID data, in both cross-silo and\ncross-device FL systems.\n","authors":["Hanxi Guo","Hao Wang","Tao Song","Tianhang Zheng","Yang Hua","Haibing Guan","Xiangyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2407.15389v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08192v2","updated":"2024-07-22T05:26:19Z","published":"2024-07-11T05:22:04Z","title":"ARCO:Adaptive Multi-Agent Reinforcement Learning-Based Hardware/Software\n  Co-Optimization Compiler for Improved Performance in DNN Accelerator Design","summary":"  This paper presents ARCO, an adaptive Multi-Agent Reinforcement Learning\n(MARL)-based co-optimizing compilation framework designed to enhance the\nefficiency of mapping machine learning (ML) models - such as Deep Neural\nNetworks (DNNs) - onto diverse hardware platforms. The framework incorporates\nthree specialized actor-critic agents within MARL, each dedicated to a distinct\naspect of compilation/optimization at an abstract level: one agent focuses on\nhardware, while two agents focus on software optimizations. This integration\nresults in a collaborative hardware/software co-optimization strategy that\nimproves the precision and speed of DNN deployments. Concentrating on\nhigh-confidence configurations simplifies the search space and delivers\nsuperior performance compared to current optimization methods. The ARCO\nframework surpasses existing leading frameworks, achieving a throughput\nincrease of up to 37.95% while reducing the optimization time by up to 42.2%\nacross various DNNs.\n","authors":["Arya Fayyazi","Mehdi Kamal","Massoud Pedram"],"pdf_url":"https://arxiv.org/pdf/2407.08192v2.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2301.12554v5","updated":"2024-07-22T03:41:03Z","published":"2023-01-29T22:05:28Z","title":"Improving the Accuracy-Robustness Trade-Off of Classifiers via Adaptive\n  Smoothing","summary":"  While prior research has proposed a plethora of methods that build neural\nclassifiers robust against adversarial robustness, practitioners are still\nreluctant to adopt them due to their unacceptably severe clean accuracy\npenalties. This paper significantly alleviates this accuracy-robustness\ntrade-off by mixing the output probabilities of a standard classifier and a\nrobust classifier, where the standard network is optimized for clean accuracy\nand is not robust in general. We show that the robust base classifier's\nconfidence difference for correct and incorrect examples is the key to this\nimprovement. In addition to providing intuitions and empirical evidence, we\ntheoretically certify the robustness of the mixed classifier under realistic\nassumptions. Furthermore, we adapt an adversarial input detector into a mixing\nnetwork that adaptively adjusts the mixture of the two base models, further\nreducing the accuracy penalty of achieving robustness. The proposed flexible\nmethod, termed \"adaptive smoothing\", can work in conjunction with existing or\neven future methods that improve clean accuracy, robustness, or adversary\ndetection. Our empirical evaluation considers strong attack methods, including\nAutoAttack and adaptive attack. On the CIFAR-100 dataset, our method achieves\nan 85.21% clean accuracy while maintaining a 38.72% $\\ell_\\infty$-AutoAttacked\n($\\epsilon = 8/255$) accuracy, becoming the second most robust method on the\nRobustBench CIFAR-100 benchmark as of submission, while improving the clean\naccuracy by ten percentage points compared with all listed models. The code\nthat implements our method is available at\nhttps://github.com/Bai-YT/AdaptiveSmoothing.\n","authors":["Yatong Bai","Brendon G. Anderson","Aerin Kim","Somayeh Sojoudi"],"pdf_url":"https://arxiv.org/pdf/2301.12554v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15351v1","updated":"2024-07-22T03:36:38Z","published":"2024-07-22T03:36:38Z","title":"LLMExplainer: Large Language Model based Bayesian Inference for Graph\n  Explanation Generation","summary":"  Recent studies seek to provide Graph Neural Network (GNN) interpretability\nvia multiple unsupervised learning models. Due to the scarcity of datasets,\ncurrent methods easily suffer from learning bias. To solve this problem, we\nembed a Large Language Model (LLM) as knowledge into the GNN explanation\nnetwork to avoid the learning bias problem. We inject LLM as a Bayesian\nInference (BI) module to mitigate learning bias. The efficacy of the BI module\nhas been proven both theoretically and experimentally. We conduct experiments\non both synthetic and real-world datasets. The innovation of our work lies in\ntwo parts: 1. We provide a novel view of the possibility of an LLM functioning\nas a Bayesian inference to improve the performance of existing algorithms; 2.\nWe are the first to discuss the learning bias issues in the GNN explanation\nproblem.\n","authors":["Jiaxing Zhang","Jiayi Liu","Dongsheng Luo","Jennifer Neville","Hua Wei"],"pdf_url":"https://arxiv.org/pdf/2407.15351v1.pdf","comment":"Preprint Paper with 13 pages"},{"id":"http://arxiv.org/abs/2105.08666v4","updated":"2024-07-22T03:34:57Z","published":"2021-05-18T16:50:42Z","title":"Reinforcement Learning With Sparse-Executing Actions via Sparsity\n  Regularization","summary":"  Reinforcement learning (RL) has demonstrated impressive performance in\ndecision-making tasks like embodied control, autonomous driving and financial\ntrading. In many decision-making tasks, the agents often encounter the problem\nof executing actions under limited budgets. However, classic RL methods\ntypically overlook the challenges posed by such sparse-executing actions. They\noperate under the assumption that all actions can be taken for a unlimited\nnumber of times, both in the formulation of the problem and in the development\nof effective algorithms. To tackle the issue of limited action execution in RL,\nthis paper first formalizes the problem as a Sparse Action Markov Decision\nProcess (SA-MDP), in which specific actions in the action space can only be\nexecuted for a limited time. Then, we propose a policy optimization algorithm,\nAction Sparsity REgularization (ASRE), which adaptively handles each action\nwith a distinct preference. ASRE operates through two steps: First, ASRE\nevaluates action sparsity by constrained action sampling. Following this, ASRE\nincorporates the sparsity evaluation into policy learning by way of an action\ndistribution regularization. We provide theoretical identification that\nvalidates the convergence of ASRE to a regularized optimal value function.\nExperiments on tasks with known sparse-executing actions, where classical RL\nalgorithms struggle to train policy efficiently, ASRE effectively constrains\nthe action sampling and outperforms baselines. Moreover, we present that ASRE\ncan generally improve the performance in Atari games, demonstrating its broad\napplicability.\n","authors":["Jing-Cheng Pang","Tian Xu","Shengyi Jiang","Yu-Ren Liu","Yang Yu"],"pdf_url":"https://arxiv.org/pdf/2105.08666v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.00280v3","updated":"2024-07-22T02:51:05Z","published":"2023-12-30T16:56:24Z","title":"Advancing TTP Analysis: Harnessing the Power of Large Language Models\n  with Retrieval Augmented Generation","summary":"  Tactics, Techniques, and Procedures (TTPs) outline the methods attackers use\nto exploit vulnerabilities. The interpretation of TTPs in the MITRE ATT&CK\nframework can be challenging for cybersecurity practitioners due to presumed\nexpertise and complex dependencies. Meanwhile, advancements with Large Language\nModels (LLMs) have led to recent surge in studies exploring its uses in\ncybersecurity operations. It is, however, unclear how LLMs can be used in an\nefficient and proper way to provide accurate responses for critical domains\nsuch as cybersecurity. This leads us to investigate how to better use two types\nof LLMs: small-scale encoder-only (e.g., RoBERTa) and larger decoder-only\n(e.g., GPT-3.5) LLMs to comprehend and summarize TTPs with the intended\npurposes (i.e., tactics) of a cyberattack procedure. This work studies and\ncompares the uses of supervised fine-tuning (SFT) of encoder-only LLMs vs.\nRetrieval Augmented Generation (RAG) for decoder-only LLMs (without\nfine-tuning). Both SFT and RAG techniques presumably enhance the LLMs with\nrelevant contexts for each cyberattack procedure. Our studies show decoder-only\nLLMs with RAG achieves better performance than encoder-only models with SFT,\nparticularly when directly relevant context is extracted by RAG. The\ndecoder-only results could suffer low `Precision' while achieving high\n`Recall'. Our findings further highlight a counter-intuitive observation that\nmore generic prompts tend to yield better predictions of cyberattack tactics\nthan those that are more specifically tailored.\n","authors":["Reza Fayyazi","Rozhina Taghdimi","Shanchieh Jay Yang"],"pdf_url":"https://arxiv.org/pdf/2401.00280v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14008v2","updated":"2024-07-22T02:13:58Z","published":"2024-07-19T03:45:27Z","title":"Investigating the Indirect Object Identification circuit in Mamba","summary":"  How well will current interpretability techniques generalize to future\nmodels? A relevant case study is Mamba, a recent recurrent architecture with\nscaling comparable to Transformers. We adapt pre-Mamba techniques to Mamba and\npartially reverse-engineer the circuit responsible for the Indirect Object\nIdentification (IOI) task. Our techniques provide evidence that 1) Layer 39 is\na key bottleneck, 2) Convolutions in layer 39 shift names one position forward,\nand 3) The name entities are stored linearly in Layer 39's SSM. Finally, we\nadapt an automatic circuit discovery tool, positional Edge Attribution\nPatching, to identify a Mamba IOI circuit. Our contributions provide initial\nevidence that circuit-based mechanistic interpretability tools work well for\nthe Mamba architecture.\n","authors":["Danielle Ensign","Adrià Garriga-Alonso"],"pdf_url":"https://arxiv.org/pdf/2407.14008v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.17147v2","updated":"2024-07-22T02:11:20Z","published":"2024-04-26T04:34:45Z","title":"On the Federated Learning Framework for Cooperative Perception","summary":"  Cooperative perception is essential to enhance the efficiency and safety of\nfuture transportation systems, requiring extensive data sharing among vehicles\non the road, which raises significant privacy concerns. Federated learning\noffers a promising solution by enabling data privacy-preserving collaborative\nenhancements in perception, decision-making, and planning among connected and\nautonomous vehicles (CAVs). However, federated learning is impeded by\nsignificant challenges arising from data heterogeneity across diverse clients,\npotentially diminishing model accuracy and prolonging convergence periods. This\nstudy introduces a specialized federated learning framework for CP, termed the\nfederated dynamic weighted aggregation (FedDWA) algorithm, facilitated by\ndynamic adjusting loss (DALoss) function. This framework employs dynamic client\nweighting to direct model convergence and integrates a novel loss function that\nutilizes Kullback-Leibler divergence (KLD) to counteract the detrimental\neffects of non-independently and identically distributed (Non-IID) and\nunbalanced data. Utilizing the BEV transformer as the primary model, our\nrigorous testing on the OpenV2V dataset, augmented with FedBEVT data,\ndemonstrates significant improvements in the average intersection over union\n(IoU). These results highlight the substantial potential of our federated\nlearning framework to address data heterogeneity challenges in CP, thereby\nenhancing the accuracy of environmental perception models and facilitating more\nrobust and efficient collaborative learning solutions in the transportation\nsector.\n","authors":["Zhenrong Zhang","Jianan Liu","Xi Zhou","Tao Huang","Qing-Long Han","Jingxin Liu","Hongbin Liu"],"pdf_url":"https://arxiv.org/pdf/2404.17147v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.06886v5","updated":"2024-07-22T01:59:21Z","published":"2024-07-09T14:14:47Z","title":"Aligning Cyber Space with Physical World: A Comprehensive Survey on\n  Embodied AI","summary":"  Embodied Artificial Intelligence (Embodied AI) is crucial for achieving\nArtificial General Intelligence (AGI) and serves as a foundation for various\napplications that bridge cyberspace and the physical world. Recently, the\nemergence of Multi-modal Large Models (MLMs) and World Models (WMs) have\nattracted significant attention due to their remarkable perception,\ninteraction, and reasoning capabilities, making them a promising architecture\nfor the brain of embodied agents. However, there is no comprehensive survey for\nEmbodied AI in the era of MLMs. In this survey, we give a comprehensive\nexploration of the latest advancements in Embodied AI. Our analysis firstly\nnavigates through the forefront of representative works of embodied robots and\nsimulators, to fully understand the research focuses and their limitations.\nThen, we analyze four main research targets: 1) embodied perception, 2)\nembodied interaction, 3) embodied agent, and 4) sim-to-real adaptation,\ncovering the state-of-the-art methods, essential paradigms, and comprehensive\ndatasets. Additionally, we explore the complexities of MLMs in virtual and real\nembodied agents, highlighting their significance in facilitating interactions\nin dynamic digital and physical environments. Finally, we summarize the\nchallenges and limitations of embodied AI and discuss their potential future\ndirections. We hope this survey will serve as a foundational reference for the\nresearch community and inspire continued innovation. The associated project can\nbe found at https://github.com/HCPLab-SYSU/Embodied_AI_Paper_List.\n","authors":["Yang Liu","Weixing Chen","Yongjie Bai","Guanbin Li","Wen Gao","Liang Lin"],"pdf_url":"https://arxiv.org/pdf/2407.06886v5.pdf","comment":"The first comprehensive review of Embodied AI in the era of MLMs, 35\n  pages. We also provide the paper list for Embodied AI:\n  https://github.com/HCPLab-SYSU/Embodied_AI_Paper_List"},{"id":"http://arxiv.org/abs/2401.03506v8","updated":"2024-07-22T01:29:37Z","published":"2024-01-07T14:54:57Z","title":"DiarizationLM: Speaker Diarization Post-Processing with Large Language\n  Models","summary":"  In this paper, we introduce DiarizationLM, a framework to leverage large\nlanguage models (LLM) to post-process the outputs from a speaker diarization\nsystem. Various goals can be achieved with the proposed framework, such as\nimproving the readability of the diarized transcript, or reducing the word\ndiarization error rate (WDER). In this framework, the outputs of the automatic\nspeech recognition (ASR) and speaker diarization systems are represented as a\ncompact textual format, which is included in the prompt to an optionally\nfinetuned LLM. The outputs of the LLM can be used as the refined diarization\nresults with the desired enhancement. As a post-processing step, this framework\ncan be easily applied to any off-the-shelf ASR and speaker diarization systems\nwithout retraining existing components. Our experiments show that a finetuned\nPaLM 2-S model can reduce the WDER by rel. 55.5% on the Fisher telephone\nconversation dataset, and rel. 44.9% on the Callhome English dataset.\n","authors":["Quan Wang","Yiling Huang","Guanlong Zhao","Evan Clark","Wei Xia","Hank Liao"],"pdf_url":"https://arxiv.org/pdf/2401.03506v8.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.00526v3","updated":"2024-07-22T01:26:32Z","published":"2022-03-01T15:12:05Z","title":"Multi-Objective Latent Space Optimization of Generative Molecular Design\n  Models","summary":"  Molecular design based on generative models, such as variational autoencoders\n(VAEs), has become increasingly popular in recent years due to its efficiency\nfor exploring high-dimensional molecular space to identify molecules with\ndesired properties. While the efficacy of the initial model strongly depends on\nthe training data, the sampling efficiency of the model for suggesting novel\nmolecules with enhanced properties can be further enhanced via latent space\noptimization. In this paper, we propose a multi-objective latent space\noptimization (LSO) method that can significantly enhance the performance of\ngenerative molecular design (GMD). The proposed method adopts an iterative\nweighted retraining approach, where the respective weights of the molecules in\nthe training data are determined by their Pareto efficiency. We demonstrate\nthat our multi-objective GMD LSO method can significantly improve the\nperformance of GMD for jointly optimizing multiple molecular properties.\n","authors":["A N M Nafiz Abeer","Nathan Urban","M Ryan Weil","Francis J. Alexander","Byung-Jun Yoon"],"pdf_url":"https://arxiv.org/pdf/2203.00526v3.pdf","comment":"23 pages, 9 figures"},{"id":"http://arxiv.org/abs/2304.07687v3","updated":"2024-07-22T00:40:17Z","published":"2023-04-16T03:49:50Z","title":"MLRegTest: A Benchmark for the Machine Learning of Regular Languages","summary":"  Synthetic datasets constructed from formal languages allow fine-grained\nexamination of the learning and generalization capabilities of machine learning\nsystems for sequence classification. This article presents a new benchmark for\nmachine learning systems on sequence classification called MLRegTest, which\ncontains training, development, and test sets from 1,800 regular languages.\nDifferent kinds of formal languages represent different kinds of long-distance\ndependencies, and correctly identifying long-distance dependencies in sequences\nis a known challenge for ML systems to generalize successfully. MLRegTest\norganizes its languages according to their logical complexity (monadic second\norder, first order, propositional, or monomial expressions) and the kind of\nlogical literals (string, tier-string, subsequence, or combinations thereof).\nThe logical complexity and choice of literal provides a systematic way to\nunderstand different kinds of long-distance dependencies in regular languages,\nand therefore to understand the capacities of different ML systems to learn\nsuch long-distance dependencies. Finally, the performance of different neural\nnetworks (simple RNN, LSTM, GRU, transformer) on MLRegTest is examined. The\nmain conclusion is that performance depends significantly on the kind of test\nset, the class of language, and the neural network architecture.\n","authors":["Sam van der Poel","Dakotah Lambert","Kalina Kostyszyn","Tiantian Gao","Rahul Verma","Derek Andersen","Joanne Chau","Emily Peterson","Cody St. Clair","Paul Fodor","Chihiro Shibata","Jeffrey Heinz"],"pdf_url":"https://arxiv.org/pdf/2304.07687v3.pdf","comment":"43 pages, MLRegTest benchmark available at\n  https://doi.org/10.5061/dryad.dncjsxm4h , associated code at\n  https://github.com/heinz-jeffrey/subregular-learning"},{"id":"http://arxiv.org/abs/2407.15302v1","updated":"2024-07-22T00:05:31Z","published":"2024-07-22T00:05:31Z","title":"Fever Detection with Infrared Thermography: Enhancing Accuracy through\n  Machine Learning Techniques","summary":"  The COVID-19 pandemic has underscored the necessity for advanced diagnostic\ntools in global health systems. Infrared Thermography (IRT) has proven to be a\ncrucial non-contact method for measuring body temperature, vital for\nidentifying febrile conditions associated with infectious diseases like\nCOVID-19. Traditional non-contact infrared thermometers (NCITs) often exhibit\nsignificant variability in readings. To address this, we integrated machine\nlearning algorithms with IRT to enhance the accuracy and reliability of\ntemperature measurements. Our study systematically evaluated various regression\nmodels using heuristic feature engineering techniques, focusing on features'\nphysiological relevance and statistical significance. The Convolutional Neural\nNetwork (CNN) model, utilizing these techniques, achieved the lowest RMSE of\n0.2223, demonstrating superior performance compared to results reported in\nprevious literature. Among non-neural network models, the Binning method\nachieved the best performance with an RMSE of 0.2296. Our findings highlight\nthe potential of combining advanced feature engineering with machine learning\nto improve diagnostic tools' effectiveness, with implications extending to\nother non-contact or remote sensing biomedical applications. This paper offers\na comprehensive analysis of these methodologies, providing a foundation for\nfuture research in the field of non-invasive medical diagnostics.\n","authors":["Parsa Razmara","Tina Khezresmaeilzadeh","B. Keith Jenkins"],"pdf_url":"https://arxiv.org/pdf/2407.15302v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15301v1","updated":"2024-07-22T00:03:51Z","published":"2024-07-22T00:03:51Z","title":"U-learning for Prediction Inference via Combinatory Multi-Subsampling:\n  With Applications to LASSO and Neural Networks","summary":"  Epigenetic aging clocks play a pivotal role in estimating an individual's\nbiological age through the examination of DNA methylation patterns at numerous\nCpG (Cytosine-phosphate-Guanine) sites within their genome. However, making\nvalid inferences on predicted epigenetic ages, or more broadly, on predictions\nderived from high-dimensional inputs, presents challenges. We introduce a novel\nU-learning approach via combinatory multi-subsampling for making ensemble\npredictions and constructing confidence intervals for predictions of continuous\noutcomes when traditional asymptotic methods are not applicable. More\nspecifically, our approach conceptualizes the ensemble estimators within the\nframework of generalized U-statistics and invokes the H\\'ajek projection for\nderiving the variances of predictions and constructing confidence intervals\nwith valid conditional coverage probabilities. We apply our approach to two\ncommonly used predictive algorithms, Lasso and deep neural networks (DNNs), and\nillustrate the validity of inferences with extensive numerical studies. We have\napplied these methods to predict the DNA methylation age (DNAmAge) of patients\nwith various health conditions, aiming to accurately characterize the aging\nprocess and potentially guide anti-aging interventions.\n","authors":["Zhe Fei","Yi Li"],"pdf_url":"https://arxiv.org/pdf/2407.15301v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16094v1","updated":"2024-07-22T23:31:10Z","published":"2024-07-22T23:31:10Z","title":"Universal Spectral Transfer with Physical Prior-Informed Deep Generative\n  Learning","summary":"  Spectroscopy is a powerful analytical technique for characterizing matter\nacross physical and biological realms1-5. However, its fundamental principle\nnecessitates specialized instrumentation per physical phenomena probed,\nlimiting broad adoption and use in all relevant research. In this study, we\nintroduce SpectroGen, a novel physical prior-informed deep generative model for\ngenerating relevant spectral signatures across modalities using experimentally\ncollected spectral input only from a single modality. We achieve this by\nreimagining the representation of spectral data as mathematical constructs of\ndistributions instead of their traditional physical and molecular state\nrepresentations. The results from 319 standard mineral samples tested\ndemonstrate generating with 99% correlation and 0.01 root mean square error\nwith superior resolution than experimentally acquired ground truth spectra. We\nshowed transferring capability across Raman, Infrared, and X-ray Diffraction\nmodalities with Gaussian, Lorentzian, and Voigt distribution priors\nrespectively6-10. This approach however is globally generalizable for any\nspectral input that can be represented by a distribution prior, making it\nuniversally applicable. We believe our work revolutionizes the application\nsphere of spectroscopy, which has traditionally been limited by access to the\nrequired sophisticated and often expensive equipment towards accelerating\nmaterial, pharmaceutical, and biological discoveries.\n","authors":["Yanmin Zhu","Loza F. Tadesse"],"pdf_url":"https://arxiv.org/pdf/2407.16094v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16091v1","updated":"2024-07-22T23:24:02Z","published":"2024-07-22T23:24:02Z","title":"Early Recognition of Parkinson's Disease Through Acoustic Analysis and\n  Machine Learning","summary":"  Parkinson's Disease (PD) is a progressive neurodegenerative disorder that\nsignificantly impacts both motor and non-motor functions, including speech.\nEarly and accurate recognition of PD through speech analysis can greatly\nenhance patient outcomes by enabling timely intervention. This paper provides a\ncomprehensive review of methods for PD recognition using speech data,\nhighlighting advances in machine learning and data-driven approaches. We\ndiscuss the process of data wrangling, including data collection, cleaning,\ntransformation, and exploratory data analysis, to prepare the dataset for\nmachine learning applications. Various classification algorithms are explored,\nincluding logistic regression, SVM, and neural networks, with and without\nfeature selection. Each method is evaluated based on accuracy, precision, and\ntraining time. Our findings indicate that specific acoustic features and\nadvanced machine-learning techniques can effectively differentiate between\nindividuals with PD and healthy controls. The study concludes with a comparison\nof the different models, identifying the most effective approaches for PD\nrecognition, and suggesting potential directions for future research.\n","authors":["Niloofar Fadavi","Nazanin Fadavi"],"pdf_url":"https://arxiv.org/pdf/2407.16091v1.pdf","comment":"N/A"},{"id":"http://arxiv.org/abs/2407.16083v1","updated":"2024-07-22T23:04:49Z","published":"2024-07-22T23:04:49Z","title":"Self-driving lab discovers principles for steering spontaneous emission","summary":"  We developed an autonomous experimentation platform to accelerate\ninterpretable scientific discovery in ultrafast nanophotonics, targeting a\nnovel method to steer spontaneous emission from reconfigurable semiconductor\nmetasurfaces. Controlling spontaneous emission is crucial for clean-energy\nsolutions in illumination, thermal radiation engineering, and remote sensing.\nDespite the potential of reconfigurable semiconductor metasurfaces with\nembedded sources for spatiotemporal control, achieving arbitrary far-field\ncontrol remains challenging. Here, we present a self-driving lab (SDL) platform\nthat addresses this challenge by discovering the governing equations for\npredicting the far-field emission profile from light-emitting metasurfaces. We\ndiscover that both the spatial gradient (grating-like) and the curvature\n(lens-like) of the local refractive index are key factors in steering\nspontaneous emission. The SDL employs a machine-learning framework comprising:\n(1) a variational autoencoder for generating complex spatial refractive index\nprofiles, (2) an active learning agent for guiding experiments with real-time\nclosed-loop feedback, and (3) a neural network-based equation learner to\nuncover structure-property relationships. The SDL demonstrated a four-fold\nenhancement in peak emission directivity (up to 77%) over a 72{\\deg} field of\nview within ~300 experiments. Our findings reveal that combinations of positive\ngratings and lenses are as effective as negative lenses and gratings for all\nemission angles, offering a novel strategy for controlling spontaneous emission\nbeyond conventional Fourier optics.\n","authors":["Saaketh Desai","Sadhvikas Addamane","Jeffery Y. Tsao","Igal Brener","Remi Dingreville","Prasad P. Iyer"],"pdf_url":"https://arxiv.org/pdf/2407.16083v1.pdf","comment":"25 pages, 4 figures in main text, 5 figures in supplementary\n  information"},{"id":"http://arxiv.org/abs/2404.15943v3","updated":"2024-07-22T21:58:05Z","published":"2024-04-24T16:03:34Z","title":"Decentralized Personalized Federated Learning based on a Conditional\n  Sparse-to-Sparser Scheme","summary":"  Decentralized Federated Learning (DFL) has become popular due to its\nrobustness and avoidance of centralized coordination. In this paradigm, clients\nactively engage in training by exchanging models with their networked\nneighbors. However, DFL introduces increased costs in terms of training and\ncommunication. Existing methods focus on minimizing communication often\noverlooking training efficiency and data heterogeneity. To address this gap, we\npropose a novel \\textit{sparse-to-sparser} training scheme: DA-DPFL. DA-DPFL\ninitializes with a subset of model parameters, which progressively reduces\nduring training via \\textit{dynamic aggregation} and leads to substantial\nenergy savings while retaining adequate information during critical learning\nperiods.\n  Our experiments showcase that DA-DPFL substantially outperforms DFL baselines\nin test accuracy, while achieving up to $5$ times reduction in energy costs. We\nprovide a theoretical analysis of DA-DPFL's convergence by solidifying its\napplicability in decentralized and personalized learning. The code is available\nat:https://github.com/EricLoong/da-dpfl\n","authors":["Qianyu Long","Qiyuan Wang","Christos Anagnostopoulos","Daning Bi"],"pdf_url":"https://arxiv.org/pdf/2404.15943v3.pdf","comment":"15 pages, 9 figures, 3 pages theory"},{"id":"http://arxiv.org/abs/2407.16067v1","updated":"2024-07-22T21:54:19Z","published":"2024-07-22T21:54:19Z","title":"LCA-on-the-Line: Benchmarking Out-of-Distribution Generalization with\n  Class Taxonomies","summary":"  We tackle the challenge of predicting models' Out-of-Distribution (OOD)\nperformance using in-distribution (ID) measurements without requiring OOD data.\nExisting evaluations with \"Effective Robustness\", which use ID accuracy as an\nindicator of OOD accuracy, encounter limitations when models are trained with\ndiverse supervision and distributions, such as class labels (Vision Models,\nVMs, on ImageNet) and textual descriptions (Visual-Language Models, VLMs, on\nLAION). VLMs often generalize better to OOD data than VMs despite having\nsimilar or lower ID performance. To improve the prediction of models' OOD\nperformance from ID measurements, we introduce the Lowest Common Ancestor\n(LCA)-on-the-Line framework. This approach revisits the established concept of\nLCA distance, which measures the hierarchical distance between labels and\npredictions within a predefined class hierarchy, such as WordNet. We assess 75\nmodels using ImageNet as the ID dataset and five significantly shifted OOD\nvariants, uncovering a strong linear correlation between ID LCA distance and\nOOD top-1 accuracy. Our method provides a compelling alternative for\nunderstanding why VLMs tend to generalize better. Additionally, we propose a\ntechnique to construct a taxonomic hierarchy on any dataset using K-means\nclustering, demonstrating that LCA distance is robust to the constructed\ntaxonomic hierarchy. Moreover, we demonstrate that aligning model predictions\nwith class taxonomies, through soft labels or prompt engineering, can enhance\nmodel generalization. Open source code in our Project Page:\nhttps://elvishelvis.github.io/papers/lca/.\n","authors":["Jia Shi","Gautam Gare","Jinjin Tian","Siqi Chai","Zhiqiu Lin","Arun Vasudevan","Di Feng","Francesco Ferroni","Shu Kong"],"pdf_url":"https://arxiv.org/pdf/2407.16067v1.pdf","comment":"ICML 2024 Oral Presentation; Project Page:\n  https://elvishelvis.github.io/papers/lca/"},{"id":"http://arxiv.org/abs/2310.11991v2","updated":"2024-07-22T21:40:55Z","published":"2023-10-18T14:22:36Z","title":"Removing Spurious Concepts from Neural Network Representations via Joint\n  Subspace Estimation","summary":"  Out-of-distribution generalization in neural networks is often hampered by\nspurious correlations. A common strategy is to mitigate this by removing\nspurious concepts from the neural network representation of the data. Existing\nconcept-removal methods tend to be overzealous by inadvertently eliminating\nfeatures associated with the main task of the model, thereby harming model\nperformance. We propose an iterative algorithm that separates spurious from\nmain-task concepts by jointly identifying two low-dimensional orthogonal\nsubspaces in the neural network representation. We evaluate the algorithm on\nbenchmark datasets for computer vision (Waterbirds, CelebA) and natural\nlanguage processing (MultiNLI), and show that it outperforms existing concept\nremoval methods\n","authors":["Floris Holstege","Bram Wouters","Noud van Giersbergen","Cees Diks"],"pdf_url":"https://arxiv.org/pdf/2310.11991v2.pdf","comment":"Preprint. Under Review. 33 pages"},{"id":"http://arxiv.org/abs/2407.16062v1","updated":"2024-07-22T21:39:34Z","published":"2024-07-22T21:39:34Z","title":"Artificial Intelligence-based Decision Support Systems for Precision and\n  Digital Health","summary":"  Precision health, increasingly supported by digital technologies, is a domain\nof research that broadens the paradigm of precision medicine, advancing\neveryday healthcare. This vision goes hand in hand with the groundbreaking\nadvent of artificial intelligence (AI), which is reshaping the way we diagnose,\ntreat, and monitor both clinical subjects and the general population. AI tools\npowered by machine learning have shown considerable improvements in a variety\nof healthcare domains. In particular, reinforcement learning (RL) holds great\npromise for sequential and dynamic problems such as dynamic treatment regimes\nand just-in-time adaptive interventions in digital health. In this work, we\ndiscuss the opportunity offered by AI, more specifically RL, to current trends\nin healthcare, providing a methodological survey of RL methods in the context\nof precision and digital health. Focusing on the area of adaptive\ninterventions, we expand the methodological survey with illustrative case\nstudies that used RL in real practice.\n  This invited article has undergone anonymous review and is intended as a book\nchapter for the volume \"Frontiers of Statistics and Data Science\" edited by\nSubhashis Ghoshal and Anindya Roy for the International Indian Statistical\nAssociation Series on Statistics and Data Science, published by Springer. It\ncovers the material from a short course titled \"Artificial Intelligence in\nPrecision and Digital Health\" taught by the author Bibhas Chakraborty at the\nIISA 2022 Conference, December 26-30 2022, at the Indian Institute of Science,\nBengaluru.\n","authors":["Nina Deliu","Bibhas Chakraborty"],"pdf_url":"https://arxiv.org/pdf/2407.16062v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2203.02605"},{"id":"http://arxiv.org/abs/2407.16058v1","updated":"2024-07-22T21:26:39Z","published":"2024-07-22T21:26:39Z","title":"Revisiting Score Function Estimators for $k$-Subset Sampling","summary":"  Are score function estimators an underestimated approach to learning with\n$k$-subset sampling? Sampling $k$-subsets is a fundamental operation in many\nmachine learning tasks that is not amenable to differentiable parametrization,\nimpeding gradient-based optimization. Prior work has focused on relaxed\nsampling or pathwise gradient estimators. Inspired by the success of score\nfunction estimators in variational inference and reinforcement learning, we\nrevisit them within the context of $k$-subset sampling. Specifically, we\ndemonstrate how to efficiently compute the $k$-subset distribution's score\nfunction using a discrete Fourier transform, and reduce the estimator's\nvariance with control variates. The resulting estimator provides both exact\nsamples and unbiased gradient estimates while also applying to\nnon-differentiable downstream models, unlike existing methods. Experiments in\nfeature selection show results competitive with current methods, despite weaker\nassumptions.\n","authors":["Klas Wijk","Ricardo Vinuesa","Hossein Azizpour"],"pdf_url":"https://arxiv.org/pdf/2407.16058v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16048v1","updated":"2024-07-22T20:55:13Z","published":"2024-07-22T20:55:13Z","title":"HIERVAR: A Hierarchical Feature Selection Method for Time Series\n  Analysis","summary":"  Time series classification stands as a pivotal and intricate challenge across\nvarious domains, including finance, healthcare, and industrial systems. In\ncontemporary research, there has been a notable upsurge in exploring feature\nextraction through random sampling. Unlike deep convolutional networks, these\nmethods sidestep elaborate training procedures, yet they often necessitate\ngenerating a surplus of features to comprehensively encapsulate time series\nnuances. Consequently, some features may lack relevance to labels or exhibit\nmulti-collinearity with others. In this paper, we propose a novel hierarchical\nfeature selection method aided by ANOVA variance analysis to address this\nchallenge. Through meticulous experimentation, we demonstrate that our method\nsubstantially reduces features by over 94% while preserving accuracy -- a\nsignificant advancement in the field of time series analysis and feature\nselection.\n","authors":["Alireza Keshavarzian","Shahrokh Valaee"],"pdf_url":"https://arxiv.org/pdf/2407.16048v1.pdf","comment":"6 pages, 5 figures, IEEE Machine Learning and Signal processing 2024"},{"id":"http://arxiv.org/abs/2403.14666v2","updated":"2024-07-22T20:37:55Z","published":"2024-03-03T03:01:14Z","title":"SyllabusQA: A Course Logistics Question Answering Dataset","summary":"  Automated teaching assistants and chatbots have significant potential to\nreduce the workload of human instructors, especially for logistics-related\nquestion answering, which is important to students yet repetitive for\ninstructors. However, due to privacy concerns, there is a lack of publicly\navailable datasets. We introduce SyllabusQA, an open-source dataset with 63\nreal course syllabi covering 36 majors, containing 5,078 open-ended course\nlogistics-related question-answer pairs that are diverse in both question types\nand answer formats. Since many logistics-related questions contain critical\ninformation like the date of an exam, it is important to evaluate the\nfactuality of answers. We benchmark several strong baselines on this task, from\nlarge language model prompting to retrieval-augmented generation. We introduce\nFact-QA, an LLM-based (GPT-4) evaluation metric to evaluate the factuality of\npredicted answers. We find that despite performing close to humans on\ntraditional metrics of textual similarity, there remains a significant gap\nbetween automated approaches and humans in terms of fact precision.\n","authors":["Nigel Fernandez","Alexander Scarlatos","Andrew Lan"],"pdf_url":"https://arxiv.org/pdf/2403.14666v2.pdf","comment":"ACL 2024: The 62nd Annual Meeting of the Association for\n  Computational Linguistics"},{"id":"http://arxiv.org/abs/2407.16040v1","updated":"2024-07-22T20:34:00Z","published":"2024-07-22T20:34:00Z","title":"Generalizing Teacher Networks for Effective Knowledge Distillation\n  Across Student Architectures","summary":"  Knowledge distillation (KD) is a model compression method that entails\ntraining a compact student model to emulate the performance of a more complex\nteacher model. However, the architectural capacity gap between the two models\nlimits the effectiveness of knowledge transfer. Addressing this issue, previous\nworks focused on customizing teacher-student pairs to improve compatibility, a\ncomputationally expensive process that needs to be repeated every time either\nmodel changes. Hence, these methods are impractical when a teacher model has to\nbe compressed into different student models for deployment on multiple hardware\ndevices with distinct resource constraints. In this work, we propose Generic\nTeacher Network (GTN), a one-off KD-aware training to create a generic teacher\ncapable of effectively transferring knowledge to any student model sampled from\na given finite pool of architectures. To this end, we represent the student\npool as a weight-sharing supernet and condition our generic teacher to align\nwith the capacities of various student architectures sampled from this\nsupernet. Experimental evaluation shows that our method both improves overall\nKD effectiveness and amortizes the minimal additional training cost of the\ngeneric teacher across students in the pool.\n","authors":["Kuluhan Binici","Weiming Wu","Tulika Mitra"],"pdf_url":"https://arxiv.org/pdf/2407.16040v1.pdf","comment":"Accepted by the BMVC-24"},{"id":"http://arxiv.org/abs/2407.16036v1","updated":"2024-07-22T20:21:40Z","published":"2024-07-22T20:21:40Z","title":"Transformer-based Capacity Prediction for Lithium-ion Batteries with\n  Data Augmentation","summary":"  Lithium-ion batteries are pivotal to technological advancements in\ntransportation, electronics, and clean energy storage. The optimal operation\nand safety of these batteries require proper and reliable estimation of battery\ncapacities to monitor the state of health. Current methods for estimating the\ncapacities fail to adequately account for long-term temporal dependencies of\nkey variables (e.g., voltage, current, and temperature) associated with battery\naging and degradation. In this study, we explore the usage of transformer\nnetworks to enhance the estimation of battery capacity. We develop a\ntransformer-based battery capacity prediction model that accounts for both\nlong-term and short-term patterns in battery data. Further, to tackle the data\nscarcity issue, data augmentation is used to increase the data size, which\nhelps to improve the performance of the model. Our proposed method is validated\nwith benchmark datasets. Simulation results show the effectiveness of data\naugmentation and the transformer network in improving the accuracy and\nrobustness of battery capacity prediction.\n","authors":["Gift Modekwe","Saif Al-Wahaibi","Qiugang Lu"],"pdf_url":"https://arxiv.org/pdf/2407.16036v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16030v1","updated":"2024-07-22T20:13:10Z","published":"2024-07-22T20:13:10Z","title":"Enhancing Temporal Understanding in LLMs for Semi-structured Tables","summary":"  Temporal reasoning over tabular data presents substantial challenges for\nlarge language models (LLMs), as evidenced by recent research. In this study,\nwe conduct a comprehensive analysis of temporal datasets to pinpoint the\nspecific limitations of LLMs. Our investigation leads to enhancements in\nTempTabQA, a dataset specifically designed for tabular temporal question\nanswering. We provide critical insights for improving LLM performance in\ntemporal reasoning tasks with tabular data. Furthermore, we introduce a novel\napproach, C.L.E.A.R to strengthen LLM capabilities in this domain. Our findings\ndemonstrate that our method significantly improves evidence-based reasoning\nacross various models. Additionally, our experimental results reveal that\nindirect supervision with auxiliary data substantially boosts model performance\nin these tasks. This work contributes to a deeper understanding of LLMs'\ntemporal reasoning abilities over tabular data and promotes advancements in\ntheir application across diverse fields.\n","authors":["Irwin Deng","Kushagra Dixit","Vivek Gupta","Dan Roth"],"pdf_url":"https://arxiv.org/pdf/2407.16030v1.pdf","comment":"Total Pages 18, Total Tables 6, Total figures 7"},{"id":"http://arxiv.org/abs/2407.16025v1","updated":"2024-07-22T20:03:36Z","published":"2024-07-22T20:03:36Z","title":"Exploring and Addressing Reward Confusion in Offline Preference Learning","summary":"  Spurious correlations in a reward model's training data can prevent\nReinforcement Learning from Human Feedback (RLHF) from identifying the desired\ngoal and induce unwanted behaviors. This paper shows that offline RLHF is\nsusceptible to reward confusion, especially in the presence of spurious\ncorrelations in offline data. We create a benchmark to study this problem and\npropose a method that can significantly reduce reward confusion by leveraging\ntransitivity of preferences while building a global preference chain with\nactive learning.\n","authors":["Xin Chen","Sam Toyer","Florian Shkurti"],"pdf_url":"https://arxiv.org/pdf/2407.16025v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16021v1","updated":"2024-07-22T19:56:03Z","published":"2024-07-22T19:56:03Z","title":"Pavement Fatigue Crack Detection and Severity Classification Based on\n  Convolutional Neural Network","summary":"  Due to the varying intensity of pavement cracks, the complexity of\ntopological structure, and the noise of texture background, image\nclassification for asphalt pavement cracking has proven to be a challenging\nproblem. Fatigue cracking, also known as alligator cracking, is one of the\ncommon distresses of asphalt pavement. It is thus important to detect and\nmonitor the condition of alligator cracking on roadway pavements. Most research\nin this area has typically focused on pixel-level detection of cracking using\nlimited datasets. A novel deep convolutional neural network that can achieve\ntwo objectives is proposed. The first objective of the proposed neural network\nis to classify presence of fatigue cracking based on pavement surface images.\nThe second objective is to classify the fatigue cracking severity level based\non the Distress Identification Manual (DIM) standard. In this paper, a databank\nof 4484 high-resolution pavement surface images is established in which images\nare taken locally in the Town of Blacksburg, Virginia, USA. In the data\npre-preparation, over 4000 images are labeled into 4 categories manually\naccording to DIM standards. A four-layer convolutional neural network model is\nthen built to achieve the goal of classification of images by pavement crack\nseverity category. The trained model reached the highest accuracy among all\nexisting methods. After only 30 epochs of training, the model achieved a crack\nexistence classification accuracy of 96.23% and a severity level classification\naccuracy of 96.74%. After 20 epochs of training, the model achieved a pavement\nmarking presence classification accuracy of 97.64%.\n","authors":["Zhen Wang","Dylan G. Ildefonzo","Linbing Wang"],"pdf_url":"https://arxiv.org/pdf/2407.16021v1.pdf","comment":"10 pages, 14 figures, 3 tables"},{"id":"http://arxiv.org/abs/2307.07679v3","updated":"2024-07-22T19:54:38Z","published":"2023-07-15T01:53:09Z","title":"Sharp Convergence Rates for Matching Pursuit","summary":"  We study the fundamental limits of matching pursuit, or the pure greedy\nalgorithm, for approximating a target function $ f $ by a linear combination\n$f_n$ of $n$ elements from a dictionary. When the target function is contained\nin the variation space corresponding to the dictionary, many impressive works\nover the past few decades have obtained upper and lower bounds on the error\n$\\|f-f_n\\|$ of matching pursuit, but they do not match. The main contribution\nof this paper is to close this gap and obtain a sharp characterization of the\ndecay rate, $n^{-\\alpha}$, of matching pursuit. Specifically, we construct a\nworst case dictionary which shows that the existing best upper bound cannot be\nsignificantly improved. It turns out that, unlike other greedy algorithm\nvariants which converge at the optimal rate $ n^{-1/2}$, the convergence rate\n$n^{-\\alpha}$ is suboptimal. Here, $\\alpha \\approx 0.182$ is determined by the\nsolution to a certain non-linear equation.\n","authors":["Jason M. Klusowski","Jonathan W. Siegel"],"pdf_url":"https://arxiv.org/pdf/2307.07679v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.03098v2","updated":"2024-07-22T19:41:11Z","published":"2023-05-04T18:28:09Z","title":"Unsupervised anomaly localization in high-resolution breast scans using\n  deep pluralistic image completion","summary":"  Automated tumor detection in Digital Breast Tomosynthesis (DBT) is a\ndifficult task due to natural tumor rarity, breast tissue variability, and high\nresolution. Given the scarcity of abnormal images and the abundance of normal\nimages for this problem, an anomaly detection/localization approach could be\nwell-suited. However, most anomaly localization research in machine learning\nfocuses on non-medical datasets, and we find that these methods fall short when\nadapted to medical imaging datasets. The problem is alleviated when we solve\nthe task from the image completion perspective, in which the presence of\nanomalies can be indicated by a discrepancy between the original appearance and\nits auto-completion conditioned on the surroundings. However, there are often\nmany valid normal completions given the same surroundings, especially in the\nDBT dataset, making this evaluation criterion less precise. To address such an\nissue, we consider pluralistic image completion by exploring the distribution\nof possible completions instead of generating fixed predictions. This is\nachieved through our novel application of spatial dropout on the completion\nnetwork during inference time only, which requires no additional training cost\nand is effective at generating diverse completions. We further propose minimum\ncompletion distance (MCD), a new metric for detecting anomalies, thanks to\nthese stochastic completions. We provide theoretical as well as empirical\nsupport for the superiority over existing methods of using the proposed method\nfor anomaly localization. On the DBT dataset, our model outperforms other\nstate-of-the-art methods by at least 10\\% AUROC for pixel-level detection.\n","authors":["Nicholas Konz","Haoyu Dong","Maciej A. Mazurowski"],"pdf_url":"https://arxiv.org/pdf/2305.03098v2.pdf","comment":"Accepted in Medical Image Analysis (2023). Our code is at\n  https://github.com/mazurowski-lab/picard"},{"id":"http://arxiv.org/abs/2407.10336v2","updated":"2024-07-22T19:39:07Z","published":"2024-07-14T21:29:28Z","title":"Thyroidiomics: An Automated Pipeline for Segmentation and Classification\n  of Thyroid Pathologies from Scintigraphy Images","summary":"  The objective of this study was to develop an automated pipeline that\nenhances thyroid disease classification using thyroid scintigraphy images,\naiming to decrease assessment time and increase diagnostic accuracy. Anterior\nthyroid scintigraphy images from 2,643 patients were collected and categorized\ninto diffuse goiter (DG), multinodal goiter (MNG), and thyroiditis (TH) based\non clinical reports, and then segmented by an expert. A ResUNet model was\ntrained to perform auto-segmentation. Radiomic features were extracted from\nboth physician (scenario 1) and ResUNet segmentations (scenario 2), followed by\nomitting highly correlated features using Spearman's correlation, and feature\nselection using Recursive Feature Elimination (RFE) with XGBoost as the core.\nAll models were trained under leave-one-center-out cross-validation (LOCOCV)\nscheme, where nine instances of algorithms were iteratively trained and\nvalidated on data from eight centers and tested on the ninth for both scenarios\nseparately. Segmentation performance was assessed using the Dice similarity\ncoefficient (DSC), while classification performance was assessed using metrics,\nsuch as precision, recall, F1-score, accuracy, area under the Receiver\nOperating Characteristic (ROC AUC), and area under the precision-recall curve\n(PRC AUC). ResUNet achieved DSC values of 0.84$\\pm$0.03, 0.71$\\pm$0.06, and\n0.86$\\pm$0.02 for MNG, TH, and DG, respectively. Classification in scenario 1\nachieved an accuracy of 0.76$\\pm$0.04 and a ROC AUC of 0.92$\\pm$0.02 while in\nscenario 2, classification yielded an accuracy of 0.74$\\pm$0.05 and a ROC AUC\nof 0.90$\\pm$0.02. The automated pipeline demonstrated comparable performance to\nphysician segmentations on several classification metrics across different\nclasses, effectively reducing assessment time while maintaining high diagnostic\naccuracy. Code available at: https://github.com/ahxmeds/thyroidiomics.git.\n","authors":["Maziar Sabouri","Shadab Ahamed","Azin Asadzadeh","Atlas Haddadi Avval","Soroush Bagheri","Mohsen Arabi","Seyed Rasoul Zakavi","Emran Askari","Ali Rasouli","Atena Aghaee","Mohaddese Sehati","Fereshteh Yousefirizi","Carlos Uribe","Ghasem Hajianfar","Habib Zaidi","Arman Rahmim"],"pdf_url":"https://arxiv.org/pdf/2407.10336v2.pdf","comment":"7 pages, 4 figures, 2 tables"}],"Multimedia":[{"id":"http://arxiv.org/abs/2407.15680v1","updated":"2024-07-22T14:49:51Z","published":"2024-07-22T14:49:51Z","title":"HaloQuest: A Visual Hallucination Dataset for Advancing Multimodal\n  Reasoning","summary":"  Hallucination has been a major problem for large language models and remains\na critical challenge when it comes to multimodality in which vision-language\nmodels (VLMs) have to deal with not just textual but also visual inputs.\nDespite rapid progress in VLMs, resources for evaluating and addressing\nmultimodal hallucination are limited and mostly focused on evaluation. This\nwork introduces HaloQuest, a novel visual question answering dataset that\ncaptures various aspects of multimodal hallucination such as false premises,\ninsufficient contexts, and visual challenges. A novel idea from HaloQuest is to\nleverage synthetic images, apart from real ones, to enable dataset creation at\nscale. With over 7.7K examples spanning across a wide variety of categories,\nHaloQuest was designed to be both a challenging benchmark for VLMs and a\nfine-tuning dataset for advancing multimodal reasoning. Our experiments reveal\nthat current models struggle with HaloQuest, with all open-source VLMs\nachieving below 36% accuracy. On the other hand, fine-tuning on HaloQuest\nsignificantly reduces hallucination rates while preserving performance on\nstandard reasoning tasks. Our results discover that benchmarking with generated\nimages is highly correlated (r=0.97) with real images. Last but not least, we\npropose a novel Auto-Eval mechanism that is highly correlated with human raters\n(r=0.99) for evaluating VLMs. In sum, this work makes concrete strides towards\nunderstanding, evaluating, and mitigating hallucination in VLMs, serving as an\nimportant step towards more reliable multimodal AI systems in the future.\n","authors":["Zhecan Wang","Garrett Bingham","Adams Yu","Quoc Le","Thang Luong","Golnaz Ghiasi"],"pdf_url":"https://arxiv.org/pdf/2407.15680v1.pdf","comment":"Accepted as a main conference paper at ECCV 2024\n  (https://github.com/google/haloquest)"},{"id":"http://arxiv.org/abs/2407.14066v2","updated":"2024-07-22T13:50:55Z","published":"2024-07-19T06:50:24Z","title":"360VFI: A Dataset and Benchmark for Omnidirectional Video Frame\n  Interpolation","summary":"  With the development of VR-related techniques, viewers can enjoy a realistic\nand immersive experience through a head-mounted display, while omnidirectional\nvideo with a low frame rate can lead to user dizziness. However, the prevailing\nplane frame interpolation methodologies are unsuitable for Omnidirectional\nVideo Interpolation, chiefly due to the lack of models tailored to such videos\nwith strong distortion, compounded by the scarcity of valuable datasets for\nOmnidirectional Video Frame Interpolation. In this paper, we introduce the\nbenchmark dataset, 360VFI, for Omnidirectional Video Frame Interpolation. We\npresent a practical implementation that introduces a distortion prior from\nomnidirectional video into the network to modulate distortions. We especially\npropose a pyramid distortion-sensitive feature extractor that uses the unique\ncharacteristics of equirectangular projection (ERP) format as prior\ninformation. Moreover, we devise a decoder that uses an affine transformation\nto facilitate the synthesis of intermediate frames further. 360VFI is the first\ndataset and benchmark that explores the challenge of Omnidirectional Video\nFrame Interpolation. Through our benchmark analysis, we presented four\ndifferent distortion conditions scenes in the proposed 360VFI dataset to\nevaluate the challenge triggered by distortion during interpolation. Besides,\nexperimental results demonstrate that Omnidirectional Video Interpolation can\nbe effectively improved by modeling for omnidirectional distortion.\n","authors":["Wenxuan Lu","Mengshun Hu","Yansheng Qiu","Liang Liao","Zheng Wang"],"pdf_url":"https://arxiv.org/pdf/2407.14066v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11879v2","updated":"2024-07-22T13:44:41Z","published":"2023-03-21T14:23:46Z","title":"Multimodal Pre-training Framework for Sequential Recommendation via\n  Contrastive Learning","summary":"  Current multimodal sequential recommendation models are often unable to\neffectively explore and capture correlations among behavior sequences of users\nand items across different modalities, either neglecting correlations among\nsequence representations or inadequately capturing associations between\nmultimodal data and sequence data in their representations. To address this\nproblem, we explore multimodal pre-training in the context of sequential\nrecommendation, with the aim of enhancing fusion and utilization of multimodal\ninformation. We propose a novel Multimodal Pre-training for Sequential\nRecommendation (MP4SR) framework, which utilizes contrastive losses to capture\nthe correlation among different modality sequences of users, as well as the\ncorrelation among different modality sequences of users and items. MP4SR\nconsists of three key components: 1) multimodal feature extraction, 2) a\nbackbone network, Multimodal Mixup Sequence Encoder (M2SE), and 3) pre-training\ntasks. After utilizing pre-trained encoders to generate initial multimodal\nfeatures of items, M2SE adopts a complementary sequence mixup strategy to fuse\ndifferent modality sequences, and leverages contrastive learning to capture\nmodality interactions at the sequence-to-sequence and sequence-to-item levels.\nExtensive experiments on four real-world datasets demonstrate that MP4SR\noutperforms state-of-the-art approaches in both normal and cold-start settings.\nWe further highlight the efficacy of incorporating multimodal pre-training in\nsequential recommendation representation learning, serving as an effective\nregularizer and optimizing the parameter space for the recommendation task.\n","authors":["Lingzi Zhang","Xin Zhou","Zhiwei Zeng","Zhiqi Shen"],"pdf_url":"https://arxiv.org/pdf/2303.11879v2.pdf","comment":"22 pages"},{"id":"http://arxiv.org/abs/2403.05427v2","updated":"2024-07-22T09:51:02Z","published":"2024-03-08T16:24:42Z","title":"Reply with Sticker: New Dataset and Model for Sticker Retrieval","summary":"  Using stickers in online chatting is very prevalent on social media\nplatforms, where the stickers used in the conversation can express someone's\nintention/emotion/attitude in a vivid, tactful, and intuitive way. Existing\nsticker retrieval research typically retrieves stickers based on context and\nthe current utterance delivered by the user. That is, the stickers serve as a\nsupplement to the current utterance. However, in the real-world scenario, using\nstickers to express what we want to say rather than as a supplement to our\nwords only is also important. Therefore, in this paper, we create a new dataset\nfor sticker retrieval in conversation, called \\textbf{StickerInt}, where\nstickers are used to reply to previous conversations or supplement our\nwords\\footnote{We believe that the release of this dataset will provide a more\ncomplete paradigm than existing work for the research of sticker retrieval in\nthe open-domain online conversation.}. Based on the created dataset, we present\na simple yet effective framework for sticker retrieval in conversation based on\nthe learning of intention and the cross-modal relationships between\nconversation context and stickers, coined as \\textbf{Int-RA}. Specifically, we\nfirst devise a knowledge-enhanced intention predictor to introduce the\nintention information into the conversation representations. Subsequently, a\nrelation-aware sticker selector is devised to retrieve the response sticker via\ncross-modal relationships. Extensive experiments on the created dataset show\nthat the proposed model achieves state-of-the-art performance in sticker\nretrieval\\footnote{The dataset and source code of this work are released at\n\\url{https://github.com/HITSZ-HLT/Int-RA}.}.\n","authors":["Bin Liang","Bingbing Wang","Zhixin Bai","Qiwei Lang","Mingwei Sun","Kaiheng Hou","Lanjun Zhou","Ruifeng Xu","Kam-Fai Wong"],"pdf_url":"https://arxiv.org/pdf/2403.05427v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.02255v2","updated":"2024-07-22T09:34:46Z","published":"2024-06-04T12:21:55Z","title":"MidiCaps: A large-scale MIDI dataset with text captions","summary":"  Generative models guided by text prompts are increasingly becoming more\npopular. However, no text-to-MIDI models currently exist due to the lack of a\ncaptioned MIDI dataset. This work aims to enable research that combines LLMs\nwith symbolic music by presenting, the first openly available large-scale MIDI\ndataset with text captions. MIDI (Musical Instrument Digital Interface) files\nare widely used for encoding musical information and can capture the nuances of\nmusical composition. They are widely used by music producers, composers,\nmusicologists, and performers alike. Inspired by recent advancements in\ncaptioning techniques, we present a curated dataset of over 168k MIDI files\nwith textual descriptions. Each MIDI caption describes the musical content,\nincluding tempo, chord progression, time signature, instruments, genre, and\nmood, thus facilitating multi-modal exploration and analysis. The dataset\nencompasses various genres, styles, and complexities, offering a rich data\nsource for training and evaluating models for tasks such as music information\nretrieval, music understanding, and cross-modal translation. We provide\ndetailed statistics about the dataset and have assessed the quality of the\ncaptions in an extensive listening study. We anticipate that this resource will\nstimulate further research at the intersection of music and natural language\nprocessing, fostering advancements in both fields.\n","authors":["Jan Melechovsky","Abhinaba Roy","Dorien Herremans"],"pdf_url":"https://arxiv.org/pdf/2406.02255v2.pdf","comment":"Accepted in ISMIR2024"},{"id":"http://arxiv.org/abs/2407.15423v1","updated":"2024-07-22T07:00:21Z","published":"2024-07-22T07:00:21Z","title":"Integrating IP Broadcasting with Audio Tags- Workflow and Challenges","summary":"  The broadcasting industry is increasingly adopting IP techniques,\nrevolutionising both live and pre-recorded content production, from news\ngathering to live music events. IP broadcasting allows for the transport of\naudio and video signals in an easily configurable way, aligning with modern\nnetworking techniques. This shift towards an IP workflow allows for much\ngreater flexibility, not only in routing signals but with the integration of\ntools using standard web development techniques. One possible tool could\ninclude the use of live audio tagging, which has a number of uses in the\nproduction of content. These include from automated closed captioning to\nidentifying unwanted sound events within a scene. In this paper, we describe\nthe process of containerising an audio tagging model into a microservice, a\nsmall segregated code module that can be integrated into a multitude of\ndifferent network setups. The goal is to develop a modular, accessible, and\nflexible tool capable of seamless deployment into broadcasting workflows of all\nsizes, from small productions to large corporations. Challenges surrounding\nlatency of the selected audio tagging model and its effect on the usefulness of\nthe end product are discussed.\n","authors":["Rhys Burchett-Vass","Arshdeep Singh","Gabriel Bibbó","Mark D. Plumbley"],"pdf_url":"https://arxiv.org/pdf/2407.15423v1.pdf","comment":"Submitted to DCASE 2024 Workshop"},{"id":"http://arxiv.org/abs/2406.01280v2","updated":"2024-07-22T06:44:20Z","published":"2024-06-03T12:48:38Z","title":"Demo: Soccer Information Retrieval via Natural Queries using SoccerRAG","summary":"  The rapid evolution of digital sports media necessitates sophisticated\ninformation retrieval systems that can efficiently parse extensive multimodal\ndatasets. This paper demonstrates SoccerRAG, an innovative framework designed\nto harness the power of Retrieval Augmented Generation (RAG) and Large Language\nModels (LLMs) to extract soccer-related information through natural language\nqueries. By leveraging a multimodal dataset, SoccerRAG supports dynamic\nquerying and automatic data validation, enhancing user interaction and\naccessibility to sports archives. We present a novel interactive user interface\n(UI) based on the Chainlit framework which wraps around the core functionality,\nand enable users to interact with the SoccerRAG framework in a chatbot-like\nvisual manner.\n","authors":["Aleksander Theo Strand","Sushant Gautam","Cise Midoglu","Pål Halvorsen"],"pdf_url":"https://arxiv.org/pdf/2406.01280v2.pdf","comment":"accepted to CBMI 2024 as a demonstration;\n  https://github.com/simula/soccer-rag. arXiv admin note: text overlap with\n  arXiv:2406.01273"},{"id":"http://arxiv.org/abs/2406.01273v2","updated":"2024-07-22T06:42:44Z","published":"2024-06-03T12:39:04Z","title":"SoccerRAG: Multimodal Soccer Information Retrieval via Natural Queries","summary":"  The rapid evolution of digital sports media necessitates sophisticated\ninformation retrieval systems that can efficiently parse extensive multimodal\ndatasets. This paper introduces SoccerRAG, an innovative framework designed to\nharness the power of Retrieval Augmented Generation (RAG) and Large Language\nModels (LLMs) to extract soccer-related information through natural language\nqueries. By leveraging a multimodal dataset, SoccerRAG supports dynamic\nquerying and automatic data validation, enhancing user interaction and\naccessibility to sports archives. Our evaluations indicate that SoccerRAG\neffectively handles complex queries, offering significant improvements over\ntraditional retrieval systems in terms of accuracy and user engagement. The\nresults underscore the potential of using RAG and LLMs in sports analytics,\npaving the way for future advancements in the accessibility and real-time\nprocessing of sports data.\n","authors":["Aleksander Theo Strand","Sushant Gautam","Cise Midoglu","Pål Halvorsen"],"pdf_url":"https://arxiv.org/pdf/2406.01273v2.pdf","comment":"accepted to CBMI 2024 as a regular paper;\n  https://github.com/simula/soccer-rag"},{"id":"http://arxiv.org/abs/2401.03890v4","updated":"2024-07-22T05:13:49Z","published":"2024-01-08T13:42:59Z","title":"A Survey on 3D Gaussian Splatting","summary":"  3D Gaussian splatting (GS) has recently emerged as a transformative technique\nin the realm of explicit radiance field and computer graphics. This innovative\napproach, characterized by the utilization of millions of learnable 3D\nGaussians, represents a significant departure from mainstream neural radiance\nfield approaches, which predominantly use implicit, coordinate-based models to\nmap spatial coordinates to pixel values. 3D GS, with its explicit scene\nrepresentation and differentiable rendering algorithm, not only promises\nreal-time rendering capability but also introduces unprecedented levels of\neditability. This positions 3D GS as a potential game-changer for the next\ngeneration of 3D reconstruction and representation. In the present paper, we\nprovide the first systematic overview of the recent developments and critical\ncontributions in the domain of 3D GS. We begin with a detailed exploration of\nthe underlying principles and the driving forces behind the emergence of 3D GS,\nlaying the groundwork for understanding its significance. A focal point of our\ndiscussion is the practical applicability of 3D GS. By enabling unprecedented\nrendering speed, 3D GS opens up a plethora of applications, ranging from\nvirtual reality to interactive media and beyond. This is complemented by a\ncomparative analysis of leading 3D GS models, evaluated across various\nbenchmark tasks to highlight their performance and practical utility. The\nsurvey concludes by identifying current challenges and suggesting potential\navenues for future research in this domain. Through this survey, we aim to\nprovide a valuable resource for both newcomers and seasoned researchers,\nfostering further exploration and advancement in applicable and explicit\nradiance field representation.\n","authors":["Guikun Chen","Wenguan Wang"],"pdf_url":"https://arxiv.org/pdf/2401.03890v4.pdf","comment":"Ongoing project"},{"id":"http://arxiv.org/abs/2407.15376v1","updated":"2024-07-22T04:56:13Z","published":"2024-07-22T04:56:13Z","title":"Structure-Aware Residual-Center Representation for Self-Supervised\n  Open-Set 3D Cross-Modal Retrieval","summary":"  Existing methods of 3D cross-modal retrieval heavily lean on category\ndistribution priors within the training set, which diminishes their efficacy\nwhen tasked with unseen categories under open-set environments. To tackle this\nproblem, we propose the Structure-Aware Residual-Center Representation (SRCR)\nframework for self-supervised open-set 3D cross-modal retrieval. To address the\ncenter deviation due to category distribution differences, we utilize the\nResidual-Center Embedding (RCE) for each object by nested auto-encoders, rather\nthan directly mapping them to the modality or category centers. Besides, we\nperform the Hierarchical Structure Learning (HSL) approach to leverage the\nhigh-order correlations among objects for generalization, by constructing a\nheterogeneous hypergraph structure based on hierarchical inter-modality,\nintra-object, and implicit-category correlations. Extensive experiments and\nablation studies on four benchmarks demonstrate the superiority of our proposed\nframework compared to state-of-the-art methods.\n","authors":["Yang Xu","Yifan Feng","Yu Jiang"],"pdf_url":"https://arxiv.org/pdf/2407.15376v1.pdf","comment":"ICME 2024"},{"id":"http://arxiv.org/abs/2407.15346v1","updated":"2024-07-22T03:05:32Z","published":"2024-07-22T03:05:32Z","title":"Knowledge Acquisition Disentanglement for Knowledge-based Visual\n  Question Answering with Large Language Models","summary":"  Knowledge-based Visual Question Answering (KVQA) requires both image and\nworld knowledge to answer questions. Current methods first retrieve knowledge\nfrom the image and external knowledge base with the original complex question,\nthen generate answers with Large Language Models (LLMs). However, since the\noriginal question contains complex elements that require knowledge from\ndifferent sources, acquiring different kinds of knowledge in a coupled manner\nmay confuse models and hinder them from retrieving precise knowledge.\nFurthermore, the ``forward-only'' answering process fails to explicitly capture\nthe knowledge needs of LLMs, which can further hurt answering quality. To cope\nwith the above limitations, we propose DKA: Disentangled Knowledge Acquisition\nfrom LLM feedback, a training-free framework that disentangles knowledge\nacquisition to avoid confusion and uses LLM's feedback to specify the required\nknowledge. Specifically, DKA requires LLMs to specify what knowledge they need\nto answer the question and decompose the original complex question into two\nsimple sub-questions: Image-based sub-question and Knowledge-based\nsub-question. Then we use the two sub-questions to retrieve knowledge from the\nimage and knowledge base, respectively. In this way, two knowledge acquisition\nmodels can focus on the content that corresponds to them and avoid disturbance\nof irrelevant elements in the original complex question, which can help to\nprovide more precise knowledge and better align the knowledge needs of LLMs to\nyield correct answers. Experiments on benchmark datasets show that DKA\nsignificantly outperforms SOTA models. To facilitate future research, our data\nand code are available at \\url{https://github.com/Lackel/DKA}.\n","authors":["Wenbin An","Feng Tian","Jiahao Nie","Wenkai Shi","Haonan Lin","Yan Chen","QianYing Wang","Yaqiang Wu","Guang Dai","Ping Chen"],"pdf_url":"https://arxiv.org/pdf/2407.15346v1.pdf","comment":"Pre-print"},{"id":"http://arxiv.org/abs/2308.12636v3","updated":"2024-07-22T02:40:29Z","published":"2023-08-24T08:22:21Z","title":"Exploring Transferability of Multimodal Adversarial Samples for\n  Vision-Language Pre-training Models with Contrastive Learning","summary":"  The integration of visual and textual data in Vision-Language Pre-training\n(VLP) models is crucial for enhancing vision-language understanding. However,\nthe adversarial robustness of these models, especially in the alignment of\nimage-text features, has not yet been sufficiently explored. In this paper, we\nintroduce a novel gradient-based multimodal adversarial attack method,\nunderpinned by contrastive learning, to improve the transferability of\nmultimodal adversarial samples in VLP models. This method concurrently\ngenerates adversarial texts and images within imperceptive perturbation,\nemploying both image-text and intra-modal contrastive loss. We evaluate the\neffectiveness of our approach on image-text retrieval and visual entailment\ntasks, using publicly available datasets in a black-box setting. Extensive\nexperiments indicate a significant advancement over existing single-modal\ntransfer-based adversarial attack methods and current multimodal adversarial\nattack approaches.\n","authors":["Youze Wang","Wenbo Hu","Yinpeng Dong","Hanwang Zhang","Hang Su","Richang Hong"],"pdf_url":"https://arxiv.org/pdf/2308.12636v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16076v1","updated":"2024-07-22T22:27:57Z","published":"2024-07-22T22:27:57Z","title":"PLayerTV: Advanced Player Tracking and Identification for Automatic\n  Soccer Highlight Clips","summary":"  In the rapidly evolving field of sports analytics, the automation of targeted\nvideo processing is a pivotal advancement. We propose PlayerTV, an innovative\nframework which harnesses state-of-the-art AI technologies for automatic player\ntracking and identification in soccer videos. By integrating object detection\nand tracking, Optical Character Recognition (OCR), and color analysis, PlayerTV\nfacilitates the generation of player-specific highlight clips from extensive\ngame footage, significantly reducing the manual labor traditionally associated\nwith such tasks. Preliminary results from the evaluation of our core pipeline,\ntested on a dataset from the Norwegian Eliteserien league, indicate that\nPlayerTV can accurately and efficiently identify teams and players, and our\ninteractive Graphical User Interface (GUI) serves as a user-friendly\napplication wrapping this functionality for streamlined use.\n","authors":["Håkon Maric Solberg","Mehdi Houshmand Sarkhoosh","Sushant Gautam","Saeed Shafiee Sabet","Pål Halvorsen","Cise Midoglu"],"pdf_url":"https://arxiv.org/pdf/2407.16076v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16015v1","updated":"2024-07-22T19:46:27Z","published":"2024-07-22T19:46:27Z","title":"Wallcamera: Reinventing the Wheel?","summary":"  Developed at MIT CSAIL, the Wallcamera has captivated the public's\nimagination. Here, we show that the key insight underlying the Wallcamera is\nthe same one that underpins the concept and the prototype of differential\nimaging forensics (DIF), both of which were validated and reported several\nyears prior to the Wallcamera's debut. Rather than being the first to extract\nand amplify invisible signals -- aka latent evidence in the forensics context\n-- from wall reflections in a video, or the first to propose activity\nrecognition following that approach, the Wallcamera's actual innovation is\nachieving activity recognition at a finer granularity than DIF demonstrated. In\naddition to activity recognition, DIF as conceived has a number of other\napplications in forensics, including 1) the recovery of a photographer's\npersonal identifiable information such as body width, height, and even the\ncolor of their clothing, from a single photo, and 2) the detection of image\ntampering and deepfake videos.\n","authors":["Aurélien Bourquard","Jeff Yan"],"pdf_url":"https://arxiv.org/pdf/2407.16015v1.pdf","comment":null}]},"2024-07-21T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2407.15296v1","updated":"2024-07-21T23:43:24Z","published":"2024-07-21T23:43:24Z","title":"Weak-to-Strong Compositional Learning from Generative Models for\n  Language-based Object Detection","summary":"  Vision-language (VL) models often exhibit a limited understanding of complex\nexpressions of visual objects (e.g., attributes, shapes, and their relations),\ngiven complex and diverse language queries. Traditional approaches attempt to\nimprove VL models using hard negative synthetic text, but their effectiveness\nis limited. In this paper, we harness the exceptional compositional\nunderstanding capabilities of generative foundational models. We introduce a\nnovel method for structured synthetic data generation aimed at enhancing the\ncompositional understanding of VL models in language-based object detection.\nOur framework generates densely paired positive and negative triplets (image,\ntext descriptions, and bounding boxes) in both image and text domains. By\nleveraging these synthetic triplets, we transform 'weaker' VL models into\n'stronger' models in terms of compositional understanding, a process we call\n\"Weak-to-Strong Compositional Learning\" (WSCL). To achieve this, we propose a\nnew compositional contrastive learning formulation that discovers semantics and\nstructures in complex descriptions from synthetic triplets. As a result, VL\nmodels trained with our synthetic data generation exhibit a significant\nperformance boost in the Omnilabel benchmark by up to +5AP and the D3 benchmark\nby +6.9AP upon existing baselines.\n","authors":["Kwanyong Park","Kuniaki Saito","Donghyun Kim"],"pdf_url":"https://arxiv.org/pdf/2407.15296v1.pdf","comment":"ECCV 2024"},{"id":"http://arxiv.org/abs/2402.17553v3","updated":"2024-07-21T23:16:13Z","published":"2024-02-27T14:47:53Z","title":"OmniACT: A Dataset and Benchmark for Enabling Multimodal Generalist\n  Autonomous Agents for Desktop and Web","summary":"  For decades, human-computer interaction has fundamentally been manual. Even\ntoday, almost all productive work done on the computer necessitates human input\nat every step. Autonomous virtual agents represent an exciting step in\nautomating many of these menial tasks. Virtual agents would empower users with\nlimited technical proficiency to harness the full possibilities of computer\nsystems. They could also enable the efficient streamlining of numerous computer\ntasks, ranging from calendar management to complex travel bookings, with\nminimal human intervention. In this paper, we introduce OmniACT, the\nfirst-of-a-kind dataset and benchmark for assessing an agent's capability to\ngenerate executable programs to accomplish computer tasks. Our scope extends\nbeyond traditional web automation, covering a diverse range of desktop\napplications. The dataset consists of fundamental tasks such as \"Play the next\nsong\", as well as longer horizon tasks such as \"Send an email to John Doe\nmentioning the time and place to meet\". Specifically, given a pair of screen\nimage and a visually-grounded natural language task, the goal is to generate a\nscript capable of fully executing the task. We run several strong baseline\nlanguage model agents on our benchmark. The strongest baseline, GPT-4, performs\nthe best on our benchmark However, its performance level still reaches only 15%\nof the human proficiency in generating executable scripts capable of completing\nthe task, demonstrating the challenge of our task for conventional web agents.\nOur benchmark provides a platform to measure and evaluate the progress of\nlanguage model agents in automating computer tasks and motivates future work\ntowards building multimodal models that bridge large language models and the\nvisual grounding of computer screens.\n","authors":["Raghav Kapoor","Yash Parag Butala","Melisa Russak","Jing Yu Koh","Kiran Kamble","Waseem Alshikh","Ruslan Salakhutdinov"],"pdf_url":"https://arxiv.org/pdf/2402.17553v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.12777v4","updated":"2024-07-21T22:57:58Z","published":"2022-10-23T16:34:39Z","title":"Retrieval-Augmented and Knowledge-Grounded Language Models for Faithful\n  Clinical Medicine","summary":"  Language models (LMs), including large language models (such as ChatGPT),\nhave the potential to assist clinicians in generating various clinical notes.\nHowever, LMs are prone to produce ``hallucinations'', i.e., generated content\nthat is not aligned with facts and knowledge. In this paper, we propose the\nRe$^3$Writer method with retrieval-augmented generation and knowledge-grounded\nreasoning to enable LMs to generate faithful clinical texts. We demonstrate the\neffectiveness of our method in generating patient discharge instructions. It\nrequires the LMs not to only understand the patients' long clinical documents,\ni.e., the health records during hospitalization, but also to generate critical\ninstructional information provided both to carers and to the patient at the\ntime of discharge. The proposed Re$^3$Writer imitates the working patterns of\nphysicians to first \\textbf{re}trieve related working experience from\nhistorical instructions written by physicians, then \\textbf{re}ason related\nmedical knowledge. Finally, it \\textbf{re}fines the retrieved working\nexperience and reasoned medical knowledge to extract useful information, which\nis used to generate the discharge instructions for previously-unseen patients.\nOur experiments show that, using our method, the performance of five\nrepresentative LMs can be substantially boosted across all metrics. Meanwhile,\nwe show results from human evaluations to measure the effectiveness in terms of\nfluency, faithfulness, and comprehensiveness.\n","authors":["Fenglin Liu","Bang Yang","Chenyu You","Xian Wu","Shen Ge","Zhangdaihong Liu","Xu Sun","Yang Yang","David A. Clifton"],"pdf_url":"https://arxiv.org/pdf/2210.12777v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15286v1","updated":"2024-07-21T22:50:11Z","published":"2024-07-21T22:50:11Z","title":"Intrinsic Self-correction for Enhanced Morality: An Analysis of Internal\n  Mechanisms and the Superficial Hypothesis","summary":"  Large Language Models (LLMs) are capable of producing content that\nperpetuates stereotypes, discrimination, and toxicity. The recently proposed\nmoral self-correction is a computationally efficient method for reducing\nharmful content in the responses of LLMs. However, the process of how injecting\nself-correction instructions can modify the behavior of LLMs remains\nunder-explored. In this paper, we explore the effectiveness of moral\nself-correction by answering three research questions: (1) In what scenarios\ndoes moral self-correction work? (2) What are the internal mechanisms of LLMs,\ne.g., hidden states, that are influenced by moral self-correction instructions?\n(3) Is intrinsic moral self-correction actually superficial? We argue that\nself-correction can help LLMs find a shortcut to more morally correct output,\nrather than truly reducing the immorality stored in hidden states. Through\nempirical investigation with tasks of language generation and multi-choice\nquestion answering, we conclude: (i) LLMs exhibit good performance across both\ntasks, and self-correction instructions are particularly beneficial when the\ncorrect answer is already top-ranked; (ii) The morality levels in intermediate\nhidden states are strong indicators as to whether one instruction would be more\neffective than another; (iii) Based on our analysis of intermediate hidden\nstates and task case studies of self-correction behaviors, we are first to\npropose the hypothesis that intrinsic moral self-correction is in fact\nsuperficial.\n","authors":["Guangliang Liu","Haitao Mao","Jiliang Tang","Kristen Marie Johnson"],"pdf_url":"https://arxiv.org/pdf/2407.15286v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15281v1","updated":"2024-07-21T22:07:14Z","published":"2024-07-21T22:07:14Z","title":"SynCPKL: Harnessing LLMs to Generate Synthetic Data for Commonsense\n  Persona Knowledge Linking","summary":"  Understanding rich dialogues often requires NLP systems to access relevant\ncommonsense persona knowledge, but retrieving this knowledge is challenging due\nto complex contexts and the implicit nature of commonsense. This paper presents\nour approach to the Commonsense Persona Knowledge Linking (CPKL) challenge,\naddressing the critical need for integrating persona and commonsense knowledge\nin open-domain dialogue systems. We introduce SynCPKL Pipeline, a pipeline that\nleverages Large Language Models to generate high-quality synthetic datasets for\ntraining commonsense persona knowledge linkers. To demonstrate the efficacy of\nour approach, we present SynCPKL, a new dataset specifically designed for this\ntask. Our experiments validate the effectiveness of SynCPKL for training\ncommonsense persona knowledge linkers. Additionally, our top-performing model,\nDerberta-SynCPKL, secured first place in the CPKL challenge by a 16%\nimprovement in F1 score. We released both SynCPKL and Derberta-SynCPKL at\nhttps://github.com/irislin1006/CPKL.\n","authors":["Kuan-Yen Lin"],"pdf_url":"https://arxiv.org/pdf/2407.15281v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.18922v2","updated":"2024-07-21T21:48:54Z","published":"2024-04-29T17:58:30Z","title":"DPO Meets PPO: Reinforced Token Optimization for RLHF","summary":"  In the classical Reinforcement Learning from Human Feedback (RLHF) framework,\nProximal Policy Optimization (PPO) is employed to learn from sparse,\nsentence-level rewards -- a challenging scenario in traditional deep\nreinforcement learning. Despite the great successes of PPO in the alignment of\nstate-of-the-art closed-source large language models (LLMs), its open-source\nimplementation is still largely sub-optimal, as widely reported by numerous\nresearch studies. To address these issues, we introduce a framework that models\nRLHF problems as a Markov decision process (MDP), enabling the capture of\nfine-grained token-wise information. Furthermore, we provide theoretical\ninsights that demonstrate the superiority of our MDP framework over the\nprevious sentence-level bandit formulation. Under this framework, we introduce\nan algorithm, dubbed as Reinforced Token Optimization (\\texttt{RTO}), which\nlearns the token-wise reward function from preference data and performs policy\noptimization based on this learned token-wise reward signal. Theoretically,\n\\texttt{RTO} is proven to have the capability of finding the near-optimal\npolicy sample-efficiently. For its practical implementation, \\texttt{RTO}\ninnovatively integrates Direct Preference Optimization (DPO) and PPO. DPO,\noriginally derived from sparse sentence rewards, surprisingly provides us with\na token-wise characterization of response quality, which is seamlessly\nincorporated into our subsequent PPO training stage. Extensive real-world\nalignment experiments verify the effectiveness of the proposed approach.\n","authors":["Han Zhong","Guhao Feng","Wei Xiong","Xinle Cheng","Li Zhao","Di He","Jiang Bian","Liwei Wang"],"pdf_url":"https://arxiv.org/pdf/2404.18922v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15268v1","updated":"2024-07-21T21:04:28Z","published":"2024-07-21T21:04:28Z","title":"Fact-Aware Multimodal Retrieval Augmentation for Accurate Medical\n  Radiology Report Generation","summary":"  Multimodal foundation models hold significant potential for automating\nradiology report generation, thereby assisting clinicians in diagnosing cardiac\ndiseases. However, generated reports often suffer from serious factual\ninaccuracy. In this paper, we introduce a fact-aware multimodal\nretrieval-augmented pipeline in generating accurate radiology reports\n(FactMM-RAG). We first leverage RadGraph to mine factual report pairs, then\nintegrate factual knowledge to train a universal multimodal retriever. Given a\nradiology image, our retriever can identify high-quality reference reports to\naugment multimodal foundation models, thus enhancing the factual completeness\nand correctness of report generation. Experiments on two benchmark datasets\nshow that our multimodal retriever outperforms state-of-the-art retrievers on\nboth language generation and radiology-specific metrics, up to 6.5% and 2%\nscore in F1CheXbert and F1RadGraph. Further analysis indicates that employing\nour factually-informed training strategy imposes an effective supervision\nsignal, without relying on explicit diagnostic label guidance, and successfully\npropagates fact-aware capabilities from the multimodal retriever to the\nmultimodal foundation model in radiology report generation.\n","authors":["Liwen Sun","James Zhao","Megan Han","Chenyan Xiong"],"pdf_url":"https://arxiv.org/pdf/2407.15268v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15248v1","updated":"2024-07-21T19:23:45Z","published":"2024-07-21T19:23:45Z","title":"XAI meets LLMs: A Survey of the Relation between Explainable AI and\n  Large Language Models","summary":"  In this survey, we address the key challenges in Large Language Models (LLM)\nresearch, focusing on the importance of interpretability. Driven by increasing\ninterest from AI and business sectors, we highlight the need for transparency\nin LLMs. We examine the dual paths in current LLM research and eXplainable\nArtificial Intelligence (XAI): enhancing performance through XAI and the\nemerging focus on model interpretability. Our paper advocates for a balanced\napproach that values interpretability equally with functional advancements.\nRecognizing the rapid development in LLM research, our survey includes both\npeer-reviewed and preprint (arXiv) papers, offering a comprehensive overview of\nXAI's role in LLM research. We conclude by urging the research community to\nadvance both LLM and XAI fields together.\n","authors":["Erik Cambria","Lorenzo Malandri","Fabio Mercorio","Navid Nobani","Andrea Seveso"],"pdf_url":"https://arxiv.org/pdf/2407.15248v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.16832v2","updated":"2024-07-21T18:11:34Z","published":"2024-02-26T18:56:48Z","title":"Cross-Modal Projection in Multimodal LLMs Doesn't Really Project Visual\n  Attributes to Textual Space","summary":"  Multimodal large language models (MLLMs) like LLaVA and GPT-4(V) enable\ngeneral-purpose conversations about images with the language modality. As\noff-the-shelf MLLMs may have limited capabilities on images from domains like\ndermatology and agriculture, they must be fine-tuned to unlock domain-specific\napplications. The prevalent architecture of current open-source MLLMs comprises\ntwo major modules: an image-language (cross-modal) projection network and a\nlarge language model. It is desirable to understand the roles of these two\nmodules in modeling domain-specific visual attributes to inform the design of\nfuture models and streamline the interpretability efforts on the current\nmodels. To this end, via experiments on 4 datasets and under 2 fine-tuning\nsettings, we find that as the MLLM is fine-tuned, it indeed gains\ndomain-specific visual capabilities, but the updates do not lead to the\nprojection extracting relevant domain-specific visual attributes. Our results\nindicate that the domain-specific visual attributes are modeled by the LLM,\neven when only the projection is fine-tuned. Through this study, we offer a\npotential reinterpretation of the role of cross-modal projections in MLLM\narchitectures. Project webpage:\nhttps://claws-lab.github.io/projection-in-MLLMs/\n","authors":["Gaurav Verma","Minje Choi","Kartik Sharma","Jamelle Watson-Daniels","Sejoon Oh","Srijan Kumar"],"pdf_url":"https://arxiv.org/pdf/2402.16832v2.pdf","comment":"Accepted at ACL 2024 (Main, Short)"},{"id":"http://arxiv.org/abs/2407.15237v1","updated":"2024-07-21T18:00:10Z","published":"2024-07-21T18:00:10Z","title":"Two eyes, Two views, and finally, One summary! Towards Multi-modal\n  Multi-tasking Knowledge-Infused Medical Dialogue Summarization","summary":"  We often summarize a multi-party conversation in two stages: chunking with\nhomogeneous units and summarizing the chunks. Thus, we hypothesize that there\nexists a correlation between homogeneous speaker chunking and overall\nsummarization tasks. In this work, we investigate the effectiveness of a\nmulti-faceted approach that simultaneously produces summaries of medical\nconcerns, doctor impressions, and an overall view. We introduce a multi-modal,\nmulti-tasking, knowledge-infused medical dialogue summary generation\n(MMK-Summation) model, which is incorporated with adapter-based fine-tuning\nthrough a gated mechanism for multi-modal information integration. The model,\nMMK-Summation, takes dialogues as input, extracts pertinent external knowledge\nbased on the context, integrates the knowledge and visual cues from the\ndialogues into the textual content, and ultimately generates concise summaries\nencompassing medical concerns, doctor impressions, and a comprehensive\noverview. The introduced model surpasses multiple baselines and traditional\nsummarization models across all evaluation metrics (including human\nevaluation), which firmly demonstrates the efficacy of the knowledge-guided\nmulti-tasking, multimodal medical conversation summarization. The code is\navailable at https://github.com/NLP-RL/MMK-Summation.\n","authors":["Anisha Saha","Abhisek Tiwari","Sai Ruthvik","Sriparna Saha"],"pdf_url":"https://arxiv.org/pdf/2407.15237v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15235v1","updated":"2024-07-21T17:59:20Z","published":"2024-07-21T17:59:20Z","title":"TAGCOS: Task-agnostic Gradient Clustered Coreset Selection for\n  Instruction Tuning Data","summary":"  Instruction tuning has achieved unprecedented success in NLP, turning large\nlanguage models into versatile chatbots. However, the increasing variety and\nvolume of instruction datasets demand significant computational resources. To\naddress this, it is essential to extract a small and highly informative subset\n(i.e., Coreset) that achieves comparable performance to the full dataset.\nAchieving this goal poses non-trivial challenges: 1) data selection requires\naccurate data representations that reflect the training samples' quality, 2)\nconsidering the diverse nature of instruction datasets, and 3) ensuring the\nefficiency of the coreset selection algorithm for large models. To address\nthese challenges, we propose Task-Agnostic Gradient Clustered COreset Selection\n(TAGCOS). Specifically, we leverage sample gradients as the data\nrepresentations, perform clustering to group similar data, and apply an\nefficient greedy algorithm for coreset selection. Experimental results show\nthat our algorithm, selecting only 5% of the data, surpasses other unsupervised\nmethods and achieves performance close to that of the full dataset.\n","authors":["Jipeng Zhang","Yaxuan Qin","Renjie Pi","Weizhong Zhang","Rui Pan","Tong Zhang"],"pdf_url":"https://arxiv.org/pdf/2407.15235v1.pdf","comment":"Preprint. Our code and models are available at:\n  https://github.com/2003pro/TAGCOS"},{"id":"http://arxiv.org/abs/2407.15229v1","updated":"2024-07-21T17:35:20Z","published":"2024-07-21T17:35:20Z","title":"The Hitchhiker's Guide to Human Alignment with *PO","summary":"  With the growing utilization of large language models (LLMs) across domains,\nalignment towards human preferences has become one of the most critical aspects\nof training models. At the forefront of state-of-the-art human alignment\nmethods are preference optimization methods (*PO). However, prior research has\noften concentrated on identifying the best-performing method, typically\ninvolving a grid search over hyperparameters, which can be impractical for\ngeneral practitioners. In this paper, we aim to identify the algorithm that,\nwhile being performant, is simultaneously more robust to varying\nhyperparameters, thereby increasing the likelihood of achieving better results.\nWe focus on a realistic out-of-distribution (OOD) scenario that mirrors\nreal-world applications of human alignment, offering practical insights into\nthe strengths and weaknesses of these methods. Furthermore, to better\nunderstand the shortcomings of generations from the different methods, we\nanalyze the model generations through the lens of KL divergence of the SFT\nmodel and the response length statistics. Our analysis reveals that the widely\nadopted DPO method consistently produces lengthy responses of inferior quality\nthat are very close to the SFT responses. Motivated by these findings, we\npropose an embarrassingly simple extension to the DPO algorithm, LN-DPO,\nresulting in more concise responses without sacrificing quality compared to the\npolicy obtained by vanilla DPO.\n","authors":["Kian Ahrabian","Xihui Lin","Barun Patra","Vishrav Chaudhary","Alon Benhaim","Jay Pujara","Xia Song"],"pdf_url":"https://arxiv.org/pdf/2407.15229v1.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2407.15227v1","updated":"2024-07-21T17:27:17Z","published":"2024-07-21T17:27:17Z","title":"A Community-Centric Perspective for Characterizing and Detecting\n  Anti-Asian Violence-Provoking Speech","summary":"  Violence-provoking speech -- speech that implicitly or explicitly promotes\nviolence against the members of the targeted community, contributed to a\nmassive surge in anti-Asian crimes during the pandemic. While previous works\nhave characterized and built tools for detecting other forms of harmful speech,\nlike fear speech and hate speech, our work takes a community-centric approach\nto studying anti-Asian violence-provoking speech. Using data from ~420k Twitter\nposts spanning a 3-year duration (January 1, 2020 to February 1, 2023), we\ndevelop a codebook to characterize anti-Asian violence-provoking speech and\ncollect a community-crowdsourced dataset to facilitate its large-scale\ndetection using state-of-the-art classifiers. We contrast the capabilities of\nnatural language processing classifiers, ranging from BERT-based to LLM-based\nclassifiers, in detecting violence-provoking speech with their capabilities to\ndetect anti-Asian hateful speech. In contrast to prior work that has\ndemonstrated the effectiveness of such classifiers in detecting hateful speech\n($F_1 = 0.89$), our work shows that accurate and reliable detection of\nviolence-provoking speech is a challenging task ($F_1 = 0.69$). We discuss the\nimplications of our findings, particularly the need for proactive interventions\nto support Asian communities during public health crises. The resources related\nto the study are available at\nhttps://claws-lab.github.io/violence-provoking-speech/.\n","authors":["Gaurav Verma","Rynaa Grover","Jiawei Zhou","Binny Mathew","Jordan Kraemer","Munmun De Choudhury","Srijan Kumar"],"pdf_url":"https://arxiv.org/pdf/2407.15227v1.pdf","comment":"Accepted to ACL 2024 Main"},{"id":"http://arxiv.org/abs/2407.15211v1","updated":"2024-07-21T16:27:24Z","published":"2024-07-21T16:27:24Z","title":"When Do Universal Image Jailbreaks Transfer Between Vision-Language\n  Models?","summary":"  The integration of new modalities into frontier AI systems offers exciting\ncapabilities, but also increases the possibility such systems can be\nadversarially manipulated in undesirable ways. In this work, we focus on a\npopular class of vision-language models (VLMs) that generate text outputs\nconditioned on visual and textual inputs. We conducted a large-scale empirical\nstudy to assess the transferability of gradient-based universal image\n\"jailbreaks\" using a diverse set of over 40 open-parameter VLMs, including 18\nnew VLMs that we publicly release. Overall, we find that transferable\ngradient-based image jailbreaks are extremely difficult to obtain. When an\nimage jailbreak is optimized against a single VLM or against an ensemble of\nVLMs, the jailbreak successfully jailbreaks the attacked VLM(s), but exhibits\nlittle-to-no transfer to any other VLMs; transfer is not affected by whether\nthe attacked and target VLMs possess matching vision backbones or language\nmodels, whether the language model underwent instruction-following and/or\nsafety-alignment training, or many other factors. Only two settings display\npartially successful transfer: between identically-pretrained and\nidentically-initialized VLMs with slightly different VLM training data, and\nbetween different training checkpoints of a single VLM. Leveraging these\nresults, we then demonstrate that transfer can be significantly improved\nagainst a specific target VLM by attacking larger ensembles of \"highly-similar\"\nVLMs. These results stand in stark contrast to existing evidence of universal\nand transferable text jailbreaks against language models and transferable\nadversarial attacks against image classifiers, suggesting that VLMs may be more\nrobust to gradient-based transfer attacks.\n","authors":["Rylan Schaeffer","Dan Valentine","Luke Bailey","James Chua","Cristóbal Eyzaguirre","Zane Durante","Joe Benton","Brando Miranda","Henry Sleight","John Hughes","Rajashree Agrawal","Mrinank Sharma","Scott Emmons","Sanmi Koyejo","Ethan Perez"],"pdf_url":"https://arxiv.org/pdf/2407.15211v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.13447v3","updated":"2024-07-21T15:38:23Z","published":"2023-10-20T12:26:04Z","title":"Superpixel Semantics Representation and Pre-training for Vision-Language\n  Task","summary":"  The key to integrating visual language tasks is to establish a good alignment\nstrategy. Recently, visual semantic representation has achieved fine-grained\nvisual understanding by dividing grids or image patches. However, the\ncoarse-grained semantic interactions in image space should not be ignored,\nwhich hinders the extraction of complex contextual semantic relations at the\nscene boundaries. This paper proposes superpixels as comprehensive and robust\nvisual primitives, which mine coarse-grained semantic interactions by\nclustering perceptually similar pixels, speeding up the subsequent processing\nof primitives. To capture superpixel-level semantic features, we propose a\nMultiscale Difference Graph Convolutional Network (MDGCN). It allows parsing\nthe entire image as a fine-to-coarse visual hierarchy. To reason actual\nsemantic relations, we reduce potential noise interference by aggregating\ndifference information between adjacent graph nodes. Finally, we propose a\nmulti-level fusion rule in a bottom-up manner to avoid understanding deviation\nby mining complementary spatial information at different levels. Experiments\nshow that the proposed method can effectively promote the learning of multiple\ndownstream tasks. Encouragingly, our method outperforms previous methods on all\nmetrics. Our code will be released upon publication.\n","authors":["Siyu Zhang","Yeming Chen","Yaoru Sun","Fang Wang","Jun Yang","Lizhi Bai","Shangce Gao"],"pdf_url":"https://arxiv.org/pdf/2310.13447v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.11484v3","updated":"2024-07-21T15:02:49Z","published":"2024-07-16T08:20:39Z","title":"The Oscars of AI Theater: A Survey on Role-Playing with Language Models","summary":"  This survey explores the burgeoning field of role-playing with language\nmodels, focusing on their development from early persona-based models to\nadvanced character-driven simulations facilitated by Large Language Models\n(LLMs). Initially confined to simple persona consistency due to limited model\ncapabilities, role-playing tasks have now expanded to embrace complex character\nportrayals involving character consistency, behavioral alignment, and overall\nattractiveness. We provide a comprehensive taxonomy of the critical components\nin designing these systems, including data, models and alignment, agent\narchitecture and evaluation. This survey not only outlines the current\nmethodologies and challenges, such as managing dynamic personal profiles and\nachieving high-level persona consistency but also suggests avenues for future\nresearch in improving the depth and realism of role-playing applications. The\ngoal is to guide future research by offering a structured overview of current\nmethodologies and identifying potential areas for improvement. Related\nresources and papers are available at\nhttps://github.com/nuochenpku/Awesome-Role-Play-Papers.\n","authors":["Nuo Chen","Yang Deng","Jia Li"],"pdf_url":"https://arxiv.org/pdf/2407.11484v3.pdf","comment":"28 pages"},{"id":"http://arxiv.org/abs/2407.15186v1","updated":"2024-07-21T14:48:23Z","published":"2024-07-21T14:48:23Z","title":"A Survey on Employing Large Language Models for Text-to-SQL Tasks","summary":"  The increasing volume of data stored in relational databases has led to the\nneed for efficient querying and utilization of this data in various sectors.\nHowever, writing SQL queries requires specialized knowledge, which poses a\nchallenge for non-professional users trying to access and query databases.\nText-to-SQL parsing solves this issue by converting natural language queries\ninto SQL queries, thus making database access more accessible for non-expert\nusers. To take advantage of the recent developments in Large Language Models\n(LLMs), a range of new methods have emerged, with a primary focus on prompt\nengineering and fine-tuning. This survey provides a comprehensive overview of\nLLMs in text-to-SQL tasks, discussing benchmark datasets, prompt engineering,\nfine-tuning methods, and future research directions. We hope this review will\nenable readers to gain a broader understanding of the recent advances in this\nfield and offer some insights into its future trajectory.\n","authors":["Liang Shi","Zhengju Tang","Zhi Yang"],"pdf_url":"https://arxiv.org/pdf/2407.15186v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15184v1","updated":"2024-07-21T14:48:13Z","published":"2024-07-21T14:48:13Z","title":"Decoding Multilingual Moral Preferences: Unveiling LLM's Biases Through\n  the Moral Machine Experiment","summary":"  Large language models (LLMs) increasingly find their way into the most\ndiverse areas of our everyday lives. They indirectly influence people's\ndecisions or opinions through their daily use. Therefore, understanding how and\nwhich moral judgements these LLMs make is crucial. However, morality is not\nuniversal and depends on the cultural background. This raises the question of\nwhether these cultural preferences are also reflected in LLMs when prompted in\ndifferent languages or whether moral decision-making is consistent across\ndifferent languages. So far, most research has focused on investigating the\ninherent values of LLMs in English. While a few works conduct multilingual\nanalyses of moral bias in LLMs in a multilingual setting, these analyses do not\ngo beyond atomic actions. To the best of our knowledge, a multilingual analysis\nof moral bias in dilemmas has not yet been conducted.\n  To address this, our paper builds on the moral machine experiment (MME) to\ninvestigate the moral preferences of five LLMs, Falcon, Gemini, Llama, GPT, and\nMPT, in a multilingual setting and compares them with the preferences collected\nfrom humans belonging to different cultures. To accomplish this, we generate\n6500 scenarios of the MME and prompt the models in ten languages on which\naction to take. Our analysis reveals that all LLMs inhibit different moral\nbiases to some degree and that they not only differ from the human preferences\nbut also across multiple languages within the models themselves. Moreover, we\nfind that almost all models, particularly Llama 3, divert greatly from human\nvalues and, for instance, prefer saving fewer people over saving more.\n","authors":["Karina Vida","Fabian Damken","Anne Lauscher"],"pdf_url":"https://arxiv.org/pdf/2407.15184v1.pdf","comment":"to be published in AIES 2024 Proceedings"},{"id":"http://arxiv.org/abs/2407.15176v1","updated":"2024-07-21T14:23:37Z","published":"2024-07-21T14:23:37Z","title":"Farewell to Length Extrapolation, a Training-Free Infinite Context with\n  Finite Attention Scope","summary":"  The maximum supported context length is a critical bottleneck limiting the\npractical application of the Large Language Model (LLM). Although existing\nlength extrapolation methods can extend the context of LLMs to millions of\ntokens, these methods all have an explicit upper bound. In this work, we\npropose LongCache, a training-free approach that enables LLM to support an\ninfinite context with finite context scope, through full-context cache\nselection and training-free integration. This effectively frees LLMs from the\nlength extrapolation issue. We validate LongCache on the LongBench and L-Eval\nand demonstrate its performance is on par with traditional full-attention\nmechanisms. Furthermore, we have applied LongCache on mainstream LLMs,\nincluding LLaMA3 and Mistral-v0.3, enabling them to support context lengths of\nat least 400K in Needle-In-A-Haystack tests. We will improve the efficiency of\nLongCache by GPU-aware optimization soon.\n","authors":["Xiaoran Liu","Qipeng Guo","Yuerong Song","Zhigeng Liu","Kai Lv","Hang Yan","Linlin Li","Qun Liu","Xipeng Qiu"],"pdf_url":"https://arxiv.org/pdf/2407.15176v1.pdf","comment":"8 pages, 7 figures"},{"id":"http://arxiv.org/abs/2407.11550v2","updated":"2024-07-21T14:08:42Z","published":"2024-07-16T09:53:32Z","title":"Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for\n  Efficient LLM Inference","summary":"  Large Language Models have excelled in various fields but encounter\nefficiency limitations due to the substantial Key-Value (KV) cache required for\nlong-sequence inference. Recent efforts try to evict non-critical cache\nelements during runtime, thereby reducing cache size within given memory\nbudgets while preserving generation quality. Our reexamination of foundational\nprinciples reveals that prevailing methods aim to minimize an upper bound of\neviction loss, quantified as the L1 distance between the pre- and post-eviction\noutputs of multi-head self-attention mechanisms. Moreover, our analysis\nindicates that the common practices of uniformly assigning budgets across\ndifferent attention heads during cache eviction hinder their budget\nutilization, negatively impacting generation quality. In light of these\nfindings, we propose a simple yet effective adaptive budget allocation\nalgorithm. This algorithm not only optimizes the loss upper bound in theory but\nalso reduces the eviction loss in practice by aligning with the intrinsic\npatterns of self-attention mechanisms. Integrating this algorithm into two\nadvanced methods, we develop Ada-SnapKV and Ada-Pyramid. Extensive evaluations\non 16 datasets and the Needle-in-a-Haystack test confirm that they both\nsignificantly boost performance across various tasks.\n","authors":["Yuan Feng","Junlin Lv","Yukun Cao","Xike Xie","S. Kevin Zhou"],"pdf_url":"https://arxiv.org/pdf/2407.11550v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15160v1","updated":"2024-07-21T13:31:02Z","published":"2024-07-21T13:31:02Z","title":"When Can Transformers Count to n?","summary":"  Large language models based on the transformer architectures can solve highly\ncomplex tasks. But are there simple tasks that such models cannot solve? Here\nwe focus on very simple counting tasks, that involve counting how many times a\ntoken in the vocabulary have appeared in a string. We show that if the\ndimension of the transformer state is linear in the context length, this task\ncan be solved. However, the solution we propose does not scale beyond this\nlimit, and we provide theoretical arguments for why it is likely impossible for\na size limited transformer to implement this task. Our empirical results\ndemonstrate the same phase-transition in performance, as anticipated by the\ntheoretical argument. Our results demonstrate the importance of understanding\nhow transformers can solve simple tasks.\n","authors":["Gilad Yehudai","Haim Kaplan","Asma Ghandeharioun","Mor Geva","Amir Globerson"],"pdf_url":"https://arxiv.org/pdf/2407.15160v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15699v3","updated":"2024-07-21T13:27:02Z","published":"2024-03-23T03:32:26Z","title":"FEEL: A Framework for Evaluating Emotional Support Capability with Large\n  Language Models","summary":"  Emotional Support Conversation (ESC) is a typical dialogue that can\neffectively assist the user in mitigating emotional pressures. However, owing\nto the inherent subjectivity involved in analyzing emotions, current\nnon-artificial methodologies face challenges in effectively appraising the\nemotional support capability. These metrics exhibit a low correlation with\nhuman judgments. Concurrently, manual evaluation methods extremely will cause\nhigh costs. To solve these problems, we propose a novel model FEEL (Framework\nfor Evaluating Emotional Support Capability with Large Lan-guage Models),\nemploying Large Language Models (LLMs) as evaluators to assess emotional\nsupport capabilities. The model meticulously considers various evaluative\naspects of ESC to apply a more comprehensive and accurate evaluation method for\nESC. Additionally, it employs a probability distribution approach for a more\nstable result and integrates an ensemble learning strategy, leveraging multiple\nLLMs with assigned weights to enhance evaluation accuracy. To appraise the\nperformance of FEEL, we conduct extensive experiments on existing ESC model\ndialogues. Experimental results demonstrate our model exhibits a substantial\nenhancement in alignment with human evaluations compared to the baselines. Our\nsource code is available at https://github.com/Ansisy/FEEL.\n","authors":["Huaiwen Zhang","Yu Chen","Ming Wang","Shi Feng"],"pdf_url":"https://arxiv.org/pdf/2403.15699v3.pdf","comment":"Accepted to ICIC 2024"},{"id":"http://arxiv.org/abs/2407.15154v1","updated":"2024-07-21T13:15:00Z","published":"2024-07-21T13:15:00Z","title":"Fine-grained Gender Control in Machine Translation with Large Language\n  Models","summary":"  In machine translation, the problem of ambiguously gendered input has been\npointed out, where the gender of an entity is not available in the source\nsentence. To address this ambiguity issue, the task of controlled translation\nthat takes the gender of the ambiguous entity as additional input have been\nproposed. However, most existing works have only considered a simplified setup\nof one target gender for input. In this paper, we tackle controlled translation\nin a more realistic setting of inputs with multiple entities and propose\nGender-of-Entity (GoE) prompting method for LLMs. Our proposed method instructs\nthe model with fine-grained entity-level gender information to translate with\ncorrect gender inflections. By utilizing four evaluation benchmarks, we\ninvestigate the controlled translation capability of LLMs in multiple\ndimensions and find that LLMs reach state-of-the-art performance in controlled\ntranslation. Furthermore, we discover an emergence of gender interference\nphenomenon when controlling the gender of multiple entities. Finally, we\naddress the limitations of existing gender accuracy evaluation metrics and\npropose leveraging LLMs as an evaluator for gender inflection in machine\ntranslation.\n","authors":["Minwoo Lee","Hyukhun Koh","Minsung Kim","Kyomin Jung"],"pdf_url":"https://arxiv.org/pdf/2407.15154v1.pdf","comment":"NAACL 2024 Main track long paper"},{"id":"http://arxiv.org/abs/2311.07172v2","updated":"2024-07-21T12:41:18Z","published":"2023-11-13T09:06:58Z","title":"VerityMath: Advancing Mathematical Reasoning by Self-Verification\n  Through Unit Consistency","summary":"  Large Language Models (LLMs), combined with program-based solving techniques,\nare increasingly demonstrating proficiency in mathematical reasoning. For\nexample, closed-source models such as OpenAI GPT-4 and Claude show excellent\nresults in solving math word problems. However, progress in math word\nproblem-solving for open-source LLMs is limited, and the challenges these\nmodels face are not well-studied. In this paper, we study the performance of\nstrong open-source LLMs, including Llama 2 (7B), Code Llama (7B), and Mistral\n(7B) on math word problems using program-based solving techniques.\nSpecifically, we analyze the outputs of these models when applied to math word\nproblems and identify a category of problems that pose a significant challenge,\nparticularly those involving quantities spanning multiple units. To address\nthis issue, we propose a systematic approach by defining the units for each\nquantity and ensuring the consistency of these units during mathematical\noperations. We developed Unit Consistency Programs (UCPs), an annotated dataset\nof math word problems, each paired with programs containing unit specifications\nand unit verification routines. We fine-tuned Llama 2 (7B), Code Llama (7B),\nand Mistral (7B) models with UCPs to produce theirVerityMath variants. Our\nfindings indicate that our approach, which incorporates unit consistency,\ncurrently slightly underperforms compared to an approach that does not. To\nunderstand the reasons behind this, we conduct an in-depth error analysis and\nsuggest options for future improvements. Our code and dataset are available at\nhttps://github.com/vernontoh/VerityMath.\n","authors":["Vernon Toh Yan Han","Ratish Puduppully","Nancy F. Chen"],"pdf_url":"https://arxiv.org/pdf/2311.07172v2.pdf","comment":"AI4MATH Workshop @ ICML 2024"},{"id":"http://arxiv.org/abs/2407.15136v1","updated":"2024-07-21T12:14:45Z","published":"2024-07-21T12:14:45Z","title":"A multi-level multi-label text classification dataset of 19th century\n  Ottoman and Russian literary and critical texts","summary":"  This paper introduces a multi-level, multi-label text classification dataset\ncomprising over 3000 documents. The dataset features literary and critical\ntexts from 19th-century Ottoman Turkish and Russian. It is the first study to\napply large language models (LLMs) to this dataset, sourced from prominent\nliterary periodicals of the era. The texts have been meticulously organized and\nlabeled. This was done according to a taxonomic framework that takes into\naccount both their structural and semantic attributes. Articles are categorized\nand tagged with bibliometric metadata by human experts. We present baseline\nclassification results using a classical bag-of-words (BoW) naive Bayes model\nand three modern LLMs: multilingual BERT, Falcon, and Llama-v2. We found that\nin certain cases, Bag of Words (BoW) outperforms Large Language Models (LLMs),\nemphasizing the need for additional research, especially in low-resource\nlanguage settings. This dataset is expected to be a valuable resource for\nresearchers in natural language processing and machine learning, especially for\nhistorical and low-resource languages. The dataset is publicly available^1.\n","authors":["Gokcen Gokceoglu","Devrim Cavusoglu","Emre Akbas","Özen Nergis Dolcerocca"],"pdf_url":"https://arxiv.org/pdf/2407.15136v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.10517v3","updated":"2024-07-21T12:01:08Z","published":"2024-05-17T03:52:01Z","title":"Towards Better Question Generation in QA-based Event Extraction","summary":"  Event Extraction (EE) is an essential information extraction task that aims\nto extract event-related information from unstructured texts. The paradigm of\nthis task has shifted from conventional classification-based methods to more\ncontemporary question-answering-based (QA-based) approaches. However, in\nQA-based EE, the quality of the questions dramatically affects the extraction\naccuracy, and how to generate high-quality questions for QA-based EE remains a\nchallenge. In this work, to tackle this challenge, we suggest four criteria to\nevaluate the quality of a question and propose a reinforcement learning method,\nRLQG, for QA-based EE that can generate generalizable, high-quality, and\ncontext-dependent questions and provides clear guidance to QA models. The\nextensive experiments conducted on ACE and RAMS datasets have strongly\nvalidated our approach's effectiveness, which also demonstrates its robustness\nin scenarios with limited training data. The corresponding code of RLQG is\nreleased for further research.\n","authors":["Zijin Hong","Jian Liu"],"pdf_url":"https://arxiv.org/pdf/2405.10517v3.pdf","comment":"Accepted to ACL2024 Findings"},{"id":"http://arxiv.org/abs/2407.15130v1","updated":"2024-07-21T11:54:49Z","published":"2024-07-21T11:54:49Z","title":"DOPRA: Decoding Over-accumulation Penalization and Re-allocation in\n  Specific Weighting Layer","summary":"  In this work, we introduce DOPRA, a novel approach designed to mitigate\nhallucinations in multi-modal large language models (MLLMs). Unlike existing\nsolutions that typically involve costly supplementary training data or the\nintegration of external knowledge sources, DOPRA innovatively addresses\nhallucinations by decoding specific weighted layer penalties and\nredistribution, offering an economical and effective solution without\nadditional resources. DOPRA is grounded in unique insights into the intrinsic\nmechanisms controlling hallucinations within MLLMs, especially the models'\ntendency to over-rely on a subset of summary tokens in the self-attention\nmatrix, neglecting critical image-related information. This phenomenon is\nparticularly pronounced in certain strata. To counteract this over-reliance,\nDOPRA employs a strategy of weighted overlay penalties and redistribution in\nspecific layers, such as the 12th layer, during the decoding process.\nFurthermore, DOPRA includes a retrospective allocation process that re-examines\nthe sequence of generated tokens, allowing the algorithm to reallocate token\nselection to better align with the actual image content, thereby reducing the\nincidence of hallucinatory descriptions in auto-generated captions. Overall,\nDOPRA represents a significant step forward in improving the output quality of\nMLLMs by systematically reducing hallucinations through targeted adjustments\nduring the decoding process.\n","authors":["Jinfeng Wei","Xiaofeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2407.15130v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.13868v2","updated":"2024-07-21T11:42:32Z","published":"2024-05-22T17:50:04Z","title":"Automatically Identifying Local and Global Circuits with Linear\n  Computation Graphs","summary":"  Circuit analysis of any certain model behavior is a central task in\nmechanistic interpretability. We introduce our circuit discovery pipeline with\nSparse Autoencoders (SAEs) and a variant called Transcoders. With these two\nmodules inserted into the model, the model's computation graph with respect to\nOV and MLP circuits becomes strictly linear. Our methods do not require linear\napproximation to compute the causal effect of each node. This fine-grained\ngraph identifies both end-to-end and local circuits accounting for either\nlogits or intermediate features. We can scalably apply this pipeline with a\ntechnique called Hierarchical Attribution. We analyze three kinds of circuits\nin GPT-2 Small: bracket, induction, and Indirect Object Identification\ncircuits. Our results reveal new findings underlying existing discoveries.\n","authors":["Xuyang Ge","Fukang Zhu","Wentao Shu","Junxuan Wang","Zhengfu He","Xipeng Qiu"],"pdf_url":"https://arxiv.org/pdf/2405.13868v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.11111v2","updated":"2024-07-21T11:11:49Z","published":"2024-02-16T22:24:13Z","title":"Language Models as Science Tutors","summary":"  NLP has recently made exciting progress toward training language models (LMs)\nwith strong scientific problem-solving skills. However, model development has\nnot focused on real-life use-cases of LMs for science, including applications\nin education that require processing long scientific documents. To address\nthis, we introduce TutorEval and TutorChat. TutorEval is a diverse\nquestion-answering benchmark consisting of questions about long chapters from\nSTEM textbooks, written by experts. TutorEval helps measure real-life usability\nof LMs as scientific assistants, and it is the first benchmark combining long\ncontexts, free-form generation, and multi-disciplinary scientific knowledge.\nMoreover, we show that fine-tuning base models with existing dialogue datasets\nleads to poor performance on TutorEval. Therefore, we create TutorChat, a\ndataset of 80,000 long synthetic dialogues about textbooks. We use TutorChat to\nfine-tune Llemma models with 7B and 34B parameters. These LM tutors specialized\nin math have a 32K-token context window, and they excel at TutorEval while\nperforming strongly on GSM8K and MATH. Our datasets build on open-source\nmaterials, and we release our models, data, and evaluations.\n","authors":["Alexis Chevalier","Jiayi Geng","Alexander Wettig","Howard Chen","Sebastian Mizera","Toni Annala","Max Jameson Aragon","Arturo Rodríguez Fanlo","Simon Frieder","Simon Machado","Akshara Prabhakar","Ellie Thieu","Jiachen T. Wang","Zirui Wang","Xindi Wu","Mengzhou Xia","Wenhan Xia","Jiatong Yu","Jun-Jie Zhu","Zhiyong Jason Ren","Sanjeev Arora","Danqi Chen"],"pdf_url":"https://arxiv.org/pdf/2402.11111v2.pdf","comment":"8 pages without bibliography and appendix, 26 pages total"},{"id":"http://arxiv.org/abs/2406.18665v3","updated":"2024-07-21T10:33:08Z","published":"2024-06-26T18:10:22Z","title":"RouteLLM: Learning to Route LLMs with Preference Data","summary":"  Large language models (LLMs) exhibit impressive capabilities across a wide\nrange of tasks, yet the choice of which model to use often involves a trade-off\nbetween performance and cost. More powerful models, though effective, come with\nhigher expenses, while less capable models are more cost-effective. To address\nthis dilemma, we propose several efficient router models that dynamically\nselect between a stronger and a weaker LLM during inference, aiming to optimize\nthe balance between cost and response quality. We develop a training framework\nfor these routers leveraging human preference data and data augmentation\ntechniques to enhance performance. Our evaluation on widely-recognized\nbenchmarks shows that our approach significantly reduces costs-by over 2 times\nin certain cases-without compromising the quality of responses. Interestingly,\nour router models also demonstrate significant transfer learning capabilities,\nmaintaining their performance even when the strong and weak models are changed\nat test time. This highlights the potential of these routers to provide a\ncost-effective yet high-performance solution for deploying LLMs.\n","authors":["Isaac Ong","Amjad Almahairi","Vincent Wu","Wei-Lin Chiang","Tianhao Wu","Joseph E. Gonzalez","M Waleed Kadous","Ion Stoica"],"pdf_url":"https://arxiv.org/pdf/2406.18665v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.12246v5","updated":"2024-07-21T08:01:00Z","published":"2023-02-23T18:58:59Z","title":"Active Prompting with Chain-of-Thought for Large Language Models","summary":"  The increasing scale of large language models (LLMs) brings emergent\nabilities to various complex tasks requiring reasoning, such as arithmetic and\ncommonsense reasoning. It is known that the effective design of task-specific\nprompts is critical for LLMs' ability to produce high-quality answers. In\nparticular, an effective approach for complex question-and-answer tasks is\nexample-based prompting with chain-of-thought (CoT) reasoning, which\nsignificantly improves the performance of LLMs. However, current CoT methods\nrely on a fixed set of human-annotated exemplars, which are not necessarily the\nmost effective examples for different tasks. This paper proposes a new method,\nActive-Prompt, to adapt LLMs to different tasks with task-specific example\nprompts (annotated with human-designed CoT reasoning). For this purpose, we\npropose a solution to the key problem of determining which questions are the\nmost important and helpful ones to annotate from a pool of task-specific\nqueries. By borrowing ideas from the related problem of uncertainty-based\nactive learning, we introduce several metrics to characterize the uncertainty\nso as to select the most uncertain questions for annotation. Experimental\nresults demonstrate the superiority of our proposed method, achieving\nstate-of-the-art on eight complex reasoning tasks. Further analyses of\ndifferent uncertainty metrics, pool sizes, zero-shot learning, and\naccuracy-uncertainty relationship demonstrate the effectiveness of our method.\nOur code will be available at https://github.com/shizhediao/active-prompt.\n","authors":["Shizhe Diao","Pengcheng Wang","Yong Lin","Rui Pan","Xiang Liu","Tong Zhang"],"pdf_url":"https://arxiv.org/pdf/2302.12246v5.pdf","comment":"Published in ACL 2024"},{"id":"http://arxiv.org/abs/2404.01461v3","updated":"2024-07-21T07:54:33Z","published":"2024-04-01T20:15:06Z","title":"Will the Real Linda Please Stand up...to Large Language Models?\n  Examining the Representativeness Heuristic in LLMs","summary":"  Although large language models (LLMs) have demonstrated remarkable\nproficiency in modeling text and generating human-like text, they may exhibit\nbiases acquired from training data in doing so. Specifically, LLMs may be\nsusceptible to a common cognitive trap in human decision-making called the\nrepresentativeness heuristic. This is a concept in psychology that refers to\njudging the likelihood of an event based on how closely it resembles a\nwell-known prototype or typical example, versus considering broader facts or\nstatistical evidence. This research investigates the impact of the\nrepresentativeness heuristic on LLM reasoning. We created ReHeAT\n(Representativeness Heuristic AI Testing), a dataset containing a series of\nproblems spanning six common types of representativeness heuristics.\nExperiments reveal that four LLMs applied to ReHeAT all exhibited\nrepresentativeness heuristic biases. We further identify that the model's\nreasoning steps are often incorrectly based on a stereotype rather than on the\nproblem's description. Interestingly, the performance improves when adding a\nhint in the prompt to remind the model to use its knowledge. This suggests the\nuniqueness of the representativeness heuristic compared to traditional biases.\nIt can occur even when LLMs possess the correct knowledge while falling into a\ncognitive trap. This highlights the importance of future research focusing on\nthe representativeness heuristic in model reasoning and decision-making and on\ndeveloping solutions to address it.\n","authors":["Pengda Wang","Zilin Xiao","Hanjie Chen","Frederick L. Oswald"],"pdf_url":"https://arxiv.org/pdf/2404.01461v3.pdf","comment":"work in progress"},{"id":"http://arxiv.org/abs/2306.13421v2","updated":"2024-07-21T07:35:23Z","published":"2023-06-23T10:18:02Z","title":"Retrieval-Pretrained Transformer: Long-range Language Modeling with\n  Self-retrieval","summary":"  Retrieval-augmented language models (LMs) have received much attention\nrecently. However, typically the retriever is not trained jointly as a native\ncomponent of the LM, but added post-hoc to an already-pretrained LM, which\nlimits the ability of the LM and the retriever to adapt to one another. In this\nwork, we propose the Retrieval-Pretrained Transformer (RPT), an architecture\nand training procedure for jointly training a retrieval-augmented LM from\nscratch and apply it to the task of modeling long texts. Given a recently\ngenerated text chunk in a long document, the LM computes query representations,\nwhich are then used to retrieve earlier chunks in the document, located\npotentially tens of thousands of tokens before. Information from retrieved\nchunks is fused into the LM representations to predict the next target chunk.\nWe train the retriever component with a semantic objective, where the goal is\nto retrieve chunks that increase the probability of the next chunk, according\nto a reference LM. We evaluate RPT on four long-range language modeling tasks,\nspanning books, code, and mathematical writing, and demonstrate that RPT\nimproves retrieval quality and subsequently perplexity across the board\ncompared to strong baselines.\n","authors":["Ohad Rubin","Jonathan Berant"],"pdf_url":"https://arxiv.org/pdf/2306.13421v2.pdf","comment":"Accepted to TACL 2024"},{"id":"http://arxiv.org/abs/2406.00032v2","updated":"2024-07-21T06:52:40Z","published":"2024-05-25T06:57:33Z","title":"Paths of A Million People: Extracting Life Trajectories from Wikipedia","summary":"  The life trajectories of notable people have been studied to pinpoint the\ntimes and places of significant events such as birth, death, education,\nmarriage, competition, work, speeches, scientific discoveries, artistic\nachievements, and battles. Understanding how these individuals interact with\nothers provides valuable insights for broader research into human dynamics.\nHowever, the scarcity of trajectory data in terms of volume, density, and\ninter-person interactions, limits relevant studies from being comprehensive and\ninteractive. We mine millions of biography pages from Wikipedia and tackle the\ngeneralization problem stemming from the variety and heterogeneity of the\ntrajectory descriptions. Our ensemble model COSMOS, which combines the idea of\nsemi-supervised learning and contrastive learning, achieves an F1 score of\n85.95%. For this task, we also create a hand-curated dataset,\nWikiLifeTrajectory, consisting of 8,852 (person, time, location) triplets as\nground truth. Besides, we perform an empirical analysis on the trajectories of\n8,272 historians to demonstrate the validity of the extracted results. To\nfacilitate the research on trajectory extractions and help the analytical\nstudies to construct grand narratives, we make our code, the million-level\nextracted trajectories, and the WikiLifeTrajectory dataset publicly available.\n","authors":["Ying Zhang","Xiaofeng Li","Zhaoyang Liu","Haipeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.00032v2.pdf","comment":"Accepted to ICWSM 2025. 15 pages"},{"id":"http://arxiv.org/abs/2407.15073v1","updated":"2024-07-21T06:21:47Z","published":"2024-07-21T06:21:47Z","title":"Multi-Agent Causal Discovery Using Large Language Models","summary":"  Large Language Models (LLMs) have demonstrated significant potential in\ncausal discovery tasks by utilizing their vast expert knowledge from extensive\ntext corpora. However, the multi-agent capabilities of LLMs in causal discovery\nremain underexplored. This paper introduces a general framework to investigate\nthis potential. The first is the Meta Agents Model, which relies exclusively on\nreasoning and discussions among LLM agents to conduct causal discovery. The\nsecond is the Coding Agents Model, which leverages the agents' ability to plan,\nwrite, and execute code, utilizing advanced statistical libraries for causal\ndiscovery. The third is the Hybrid Model, which integrates both the Meta Agents\nModel and CodingAgents Model approaches, combining the statistical analysis and\nreasoning skills of multiple agents. Our proposed framework shows promising\nresults by effectively utilizing LLMs expert knowledge, reasoning capabilities,\nmulti-agent cooperation, and statistical causal methods. By exploring the\nmulti-agent potential of LLMs, we aim to establish a foundation for further\nresearch in utilizing LLMs multi-agent for solving causal-related problems.\n","authors":["Hao Duong Le","Xin Xia","Zhang Chen"],"pdf_url":"https://arxiv.org/pdf/2407.15073v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15071v1","updated":"2024-07-21T06:19:10Z","published":"2024-07-21T06:19:10Z","title":"Relational Database Augmented Large Language Model","summary":"  Large language models (LLMs) excel in many natural language processing (NLP)\ntasks. However, since LLMs can only incorporate new knowledge through training\nor supervised fine-tuning processes, they are unsuitable for applications that\ndemand precise, up-to-date, and private information not available in the\ntraining corpora. This precise, up-to-date, and private information is\ntypically stored in relational databases. Thus, a promising solution is to\naugment LLMs with the inclusion of relational databases as external memory.\nThis can ensure the timeliness, correctness, and consistency of data, and\nassist LLMs in performing complex arithmetic operations beyond their inherent\ncapabilities. However, bridging the gap between LLMs and relational databases\nis challenging. It requires the awareness of databases and data values stored\nin databases to select correct databases and issue correct SQL queries.\nBesides, it is necessary for the external memory to be independent of the LLM\nto meet the needs of real-world applications. We introduce a novel LLM-agnostic\nmemory architecture comprising a database selection memory, a data value\nmemory, and relational databases. And we design an elegant pipeline to retrieve\ninformation from it. Besides, we carefully design the prompts to instruct the\nLLM to maximize the framework's potential. To evaluate our method, we compose a\nnew dataset with various types of questions. Experimental results show that our\nframework enables LLMs to effectively answer database-related questions, which\nis beyond their direct ability.\n","authors":["Zongyue Qin","Chen Luo","Zhengyang Wang","Haoming Jiang","Yizhou Sun"],"pdf_url":"https://arxiv.org/pdf/2407.15071v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15055v1","updated":"2024-07-21T04:52:38Z","published":"2024-07-21T04:52:38Z","title":"Natural Language Task-Oriented Dialog System 2.0","summary":"  Task-oriented dialog (TOD) systems play a crucial role in facilitating\nefficient interactions between users and machines by focusing on achieving\nspecific goals through natural language communication. These systems\ntraditionally rely on manually annotated metadata, such as dialog states and\npolicy annotations, which is labor-intensive, expensive, inconsistent, and\nprone to errors, thereby limiting the potential to leverage the vast amounts of\navailable conversational data. A critical aspect of TOD systems involves\naccessing and integrating information from external sources to effectively\nengage users. The process of determining when and how to query external\nresources represents a fundamental challenge in system design, however existing\napproaches expect this information to provided in the context. In this paper,\nwe introduce Natural Language Task Oriented Dialog System (NL-ToD), a novel\nmodel that removes the dependency on manually annotated turn-wise data by\nutilizing dialog history and domain schemas to create a Zero Shot Generalizable\nTOD system. We also incorporate query generation as a core task of the system,\nwhere the output of the system could be a response to the user or an API query\nto communicate with an external resource. To achieve a more granular analysis\nof the system output, we classify the output into multiple categories: slot\nfilling, retrieval, and query generation. Our analysis reveals that slot\nfilling is the most challenging TOD task for all models. Experimental results\non three popular TOD datasets (SGD, KETOD and BiToD) shows the effectiveness of\nour approach as NL-ToD outperforms state-of-the-art approaches, particularly\nwith a \\textbf{31.4\\%} and \\textbf{82.1\\%} improvement in the BLEU-4 score on\nthe SGD and KETOD dataset.\n","authors":["Adib Mosharrof","A. B. Siddique"],"pdf_url":"https://arxiv.org/pdf/2407.15055v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.06671v3","updated":"2024-07-21T04:14:21Z","published":"2023-04-13T16:58:33Z","title":"Diagnostic Benchmark and Iterative Inpainting for Layout-Guided Image\n  Generation","summary":"  Spatial control is a core capability in controllable image generation.\nAdvancements in layout-guided image generation have shown promising results on\nin-distribution (ID) datasets with similar spatial configurations. However, it\nis unclear how these models perform when facing out-of-distribution (OOD)\nsamples with arbitrary, unseen layouts. In this paper, we propose LayoutBench,\na diagnostic benchmark for layout-guided image generation that examines four\ncategories of spatial control skills: number, position, size, and shape. We\nbenchmark two recent representative layout-guided image generation methods and\nobserve that the good ID layout control may not generalize well to arbitrary\nlayouts in the wild (e.g., objects at the boundary). Next, we propose\nIterInpaint, a new baseline that generates foreground and background regions\nstep-by-step via inpainting, demonstrating stronger generalizability than\nexisting models on OOD layouts in LayoutBench. We perform quantitative and\nqualitative evaluation and fine-grained analysis on the four LayoutBench skills\nto pinpoint the weaknesses of existing models. We show comprehensive ablation\nstudies on IterInpaint, including training task ratio, crop&paste vs. repaint,\nand generation order. Lastly, we evaluate the zero-shot performance of\ndifferent pretrained layout-guided image generation models on LayoutBench-COCO,\nour new benchmark for OOD layouts with real objects, where our IterInpaint\nconsistently outperforms SOTA baselines in all four splits. Project website:\nhttps://layoutbench.github.io\n","authors":["Jaemin Cho","Linjie Li","Zhengyuan Yang","Zhe Gan","Lijuan Wang","Mohit Bansal"],"pdf_url":"https://arxiv.org/pdf/2304.06671v3.pdf","comment":"CVPR 2024 Workshop; Project website: https://layoutbench.github.io"},{"id":"http://arxiv.org/abs/2407.15047v1","updated":"2024-07-21T04:09:37Z","published":"2024-07-21T04:09:37Z","title":"End-to-End Video Question Answering with Frame Scoring Mechanisms and\n  Adaptive Sampling","summary":"  Video Question Answering (VideoQA) has emerged as a challenging frontier in\nthe field of multimedia processing, requiring intricate interactions between\nvisual and textual modalities. Simply uniformly sampling frames or\nindiscriminately aggregating frame-level visual features often falls short in\ncapturing the nuanced and relevant contexts of videos to well perform VideoQA.\nTo mitigate these issues, we propose VidF4, a novel VideoQA framework equipped\nwith tailored frame selection strategy for effective and efficient VideoQA. We\npropose three frame-scoring mechanisms that consider both question relevance\nand inter-frame similarity to evaluate the importance of each frame for a given\nquestion on the video. Furthermore, we design a differentiable adaptive frame\nsampling mechanism to facilitate end-to-end training for the frame selector and\nanswer generator. The experimental results across three widely adopted\nbenchmarks demonstrate that our model consistently outperforms existing VideoQA\nmethods, establishing a new SOTA across NExT-QA (+0.3%), STAR (+0.9%), and TVQA\n(+1.0%). Furthermore, through both quantitative and qualitative analyses, we\nvalidate the effectiveness of each design choice.\n","authors":["Jianxin Liang","Xiaojun Meng","Yueqian Wang","Chang Liu","Qun Liu","Dongyan Zhao"],"pdf_url":"https://arxiv.org/pdf/2407.15047v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15046v1","updated":"2024-07-21T03:59:14Z","published":"2024-07-21T03:59:14Z","title":"Audio-visual training for improved grounding in video-text LLMs","summary":"  Recent advances in multimodal LLMs, have led to several video-text models\nbeing proposed for critical video-related tasks. However, most of the previous\nworks support visual input only, essentially muting the audio signal in the\nvideo. Few models that support both audio and visual input, are not explicitly\ntrained on audio data. Hence, the effect of audio towards video understanding\nis largely unexplored. To this end, we propose a model architecture that\nhandles audio-visual inputs explicitly. We train our model with both audio and\nvisual data from a video instruction-tuning dataset. Comparison with\nvision-only baselines, and other audio-visual models showcase that training on\naudio data indeed leads to improved grounding of responses. For better\nevaluation of audio-visual models, we also release a human-annotated benchmark\ndataset, with audio-aware question-answer pairs.\n","authors":["Shivprasad Sagare","Hemachandran S","Kinshuk Sarabhai","Prashant Ullegaddi","Rajeshkumar SA"],"pdf_url":"https://arxiv.org/pdf/2407.15046v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08495v4","updated":"2024-07-21T03:00:04Z","published":"2024-03-13T13:04:58Z","title":"Automatic Interactive Evaluation for Large Language Models with State\n  Aware Patient Simulator","summary":"  Large Language Models (LLMs) have demonstrated remarkable proficiency in\nhuman interactions, yet their application within the medical field remains\ninsufficiently explored. Previous works mainly focus on the performance of\nmedical knowledge with examinations, which is far from the realistic scenarios,\nfalling short in assessing the abilities of LLMs on clinical tasks. In the\nquest to enhance the application of Large Language Models (LLMs) in healthcare,\nthis paper introduces the Automated Interactive Evaluation (AIE) framework and\nthe State-Aware Patient Simulator (SAPS), targeting the gap between traditional\nLLM evaluations and the nuanced demands of clinical practice. Unlike prior\nmethods that rely on static medical knowledge assessments, AIE and SAPS provide\na dynamic, realistic platform for assessing LLMs through multi-turn\ndoctor-patient simulations. This approach offers a closer approximation to real\nclinical scenarios and allows for a detailed analysis of LLM behaviors in\nresponse to complex patient interactions. Our extensive experimental validation\ndemonstrates the effectiveness of the AIE framework, with outcomes that align\nwell with human evaluations, underscoring its potential to revolutionize\nmedical LLM testing for improved healthcare delivery.\n","authors":["Yusheng Liao","Yutong Meng","Yuhao Wang","Hongcheng Liu","Yanfeng Wang","Yu Wang"],"pdf_url":"https://arxiv.org/pdf/2403.08495v4.pdf","comment":"23 pages, 5 figures"},{"id":"http://arxiv.org/abs/2407.08454v2","updated":"2024-07-21T02:37:11Z","published":"2024-07-11T12:50:42Z","title":"Model Tells You Where to Merge: Adaptive KV Cache Merging for LLMs on\n  Long-Context Tasks","summary":"  How to efficiently serve Large Language Models (LLMs) has become a pressing\nissue because of their huge computational cost in their autoregressive\ngeneration process. To mitigate computational costs, LLMs often employ the KV\nCache technique to improve the generation speed. While improving the\ncomputational efficiency, the storage requirements of the KV cache are\nsubstantial, particularly in long-context scenarios, leading to significant\nmemory consumption. Existing KV cache eviction methods often degrade the\nperformance of LLMs in long-context scenarios due to the information loss\nintroduced by eviction. In this paper, we propose a novel KV cache merging\napproach, called KVMerger, to achieve adaptive KV cache compression for\nlong-context tasks without significant performance degradation under\nconstrained memory budgets. Our approach is inspired by the intriguing\nobservation that key states exhibit high similarity at the token level within a\nsingle sequence. To facilitate merging, we develop an effective yet\nstraightforward merging set identification algorithm to identify suitable KV\nstates for merging. Our merging set identification algorithm stimulates the\nsecond observation that KV cache sparsity, from similarity perspective, is\nindependent of the dataset and remains persistent at the model level.\nSubsequently, we propose a Gaussian kernel weighted merging algorithm to\nselectively merge all states within each merging set. We conduct extensive\nexperiments to demonstrate the effectiveness of KVMerger for long-context tasks\nunder constrained memory budgets, applying it to models including\nLlama2-7B-chat and Llama2-13B-chat. Using the LongBench and ZeroScroll\nbenchmarks, we compare our method with other KV cache compression techniques,\nincluding H2O and CaM, showing that our method achieves superior performance\nacross tasks with both 50% and 35% KV cache budgets.\n","authors":["Zheng Wang","Boxiao Jin","Zhongzhi Yu","Minjia Zhang"],"pdf_url":"https://arxiv.org/pdf/2407.08454v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.13337v2","updated":"2024-07-21T00:54:08Z","published":"2024-06-19T08:39:09Z","title":"Medical Spoken Named Entity Recognition","summary":"  Spoken Named Entity Recognition (NER) aims to extracting named entities from\nspeech and categorizing them into types like person, location, organization,\netc. In this work, we present VietMed-NER - the first spoken NER dataset in the\nmedical domain. To our best knowledge, our real-world dataset is the largest\nspoken NER dataset in the world in terms of the number of entity types,\nfeaturing 18 distinct types. Secondly, we present baseline results using\nvarious state-of-the-art pre-trained models: encoder-only and\nsequence-to-sequence. We found that pre-trained multilingual models XLM-R\noutperformed all monolingual models on both reference text and ASR output. Also\nin general, encoders perform better than sequence-to-sequence models for the\nNER task. By simply translating, the transcript is applicable not just to\nVietnamese but to other languages as well. All code, data and models are made\npublicly available here: https://github.com/leduckhai/MultiMed\n","authors":["Khai Le-Duc","David Thulke","Hung-Phong Tran","Long Vo-Dang","Khai-Nguyen Nguyen","Truong-Son Hy","Ralf Schlüter"],"pdf_url":"https://arxiv.org/pdf/2406.13337v2.pdf","comment":"Preprint, 41 pages"},{"id":"http://arxiv.org/abs/2405.11192v2","updated":"2024-07-21T00:30:07Z","published":"2024-05-18T06:08:07Z","title":"BrainStorm @ iREL at #SMM4H 2024: Leveraging Translation and Topical\n  Embeddings for Annotation Detection in Tweets","summary":"  The proliferation of LLMs in various NLP tasks has sparked debates regarding\ntheir reliability, particularly in annotation tasks where biases and\nhallucinations may arise. In this shared task, we address the challenge of\ndistinguishing annotations made by LLMs from those made by human domain experts\nin the context of COVID-19 symptom detection from tweets in Latin American\nSpanish. This paper presents BrainStorm @ iRELs approach to the SMM4H 2024\nShared Task, leveraging the inherent topical information in tweets, we propose\na novel approach to identify and classify annotations, aiming to enhance the\ntrustworthiness of annotated data.\n","authors":["Manav Chaudhary","Harshit Gupta","Vasudeva Varma"],"pdf_url":"https://arxiv.org/pdf/2405.11192v2.pdf","comment":"Accepted at SMM4H, colocated at ACL 2024"},{"id":"http://arxiv.org/abs/2407.15021v1","updated":"2024-07-21T00:23:33Z","published":"2024-07-21T00:23:33Z","title":"Enhancing Incremental Summarization with Structured Representations","summary":"  Large language models (LLMs) often struggle with processing extensive input\ncontexts, which can lead to redundant, inaccurate, or incoherent summaries.\nRecent methods have used unstructured memory to incrementally process these\ncontexts, but they still suffer from information overload due to the volume of\nunstructured data handled. In our study, we introduce structured knowledge\nrepresentations ($GU_{json}$), which significantly improve summarization\nperformance by 40% and 14% across two public datasets. Most notably, we propose\nthe Chain-of-Key strategy ($CoK_{json}$) that dynamically updates or augments\nthese representations with new information, rather than recreating the\nstructured memory for each new source. This method further enhances performance\nby 7% and 4% on the datasets.\n","authors":["EunJeong Hwang","Yichao Zhou","James Bradley Wendt","Beliz Gunel","Nguyen Vo","Jing Xie","Sandeep Tata"],"pdf_url":"https://arxiv.org/pdf/2407.15021v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15018v1","updated":"2024-07-21T00:10:23Z","published":"2024-07-21T00:10:23Z","title":"Answer, Assemble, Ace: Understanding How Transformers Answer Multiple\n  Choice Questions","summary":"  Multiple-choice question answering (MCQA) is a key competence of performant\ntransformer language models that is tested by mainstream benchmarks. However,\nrecent evidence shows that models can have quite a range of performance,\nparticularly when the task format is diversified slightly (such as by shuffling\nanswer choice order). In this work we ask: how do successful models perform\nformatted MCQA? We employ vocabulary projection and activation patching methods\nto localize key hidden states that encode relevant information for predicting\nthe correct answer. We find that prediction of a specific answer symbol is\ncausally attributed to a single middle layer, and specifically its multi-head\nself-attention mechanism. We show that subsequent layers increase the\nprobability of the predicted answer symbol in vocabulary space, and that this\nprobability increase is associated with a sparse set of attention heads with\nunique roles. We additionally uncover differences in how different models\nadjust to alternative symbols. Finally, we demonstrate that a synthetic task\ncan disentangle sources of model error to pinpoint when a model has learned\nformatted MCQA, and show that an inability to separate answer symbol tokens in\nvocabulary space is a property of models unable to perform formatted MCQA\ntasks.\n","authors":["Sarah Wiegreffe","Oyvind Tafjord","Yonatan Belinkov","Hannaneh Hajishirzi","Ashish Sabharwal"],"pdf_url":"https://arxiv.org/pdf/2407.15018v1.pdf","comment":"Preprint. Code will be available at\n  https://github.com/allenai/understanding_mcqa"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2407.15291v1","updated":"2024-07-21T23:13:05Z","published":"2024-07-21T23:13:05Z","title":"Evidence-Based Temporal Fact Verification","summary":"  Automated fact verification plays an essential role in fostering trust in the\ndigital space. Despite the growing interest, the verification of temporal facts\nhas not received much attention in the community. Temporal fact verification\nbrings new challenges where cues of the temporal information need to be\nextracted and temporal reasoning involving various temporal aspects of the text\nmust be applied. In this work, we propose an end-to-end solution for temporal\nfact verification that considers the temporal information in claims to obtain\nrelevant evidence sentences and harness the power of large language model for\ntemporal reasoning. Recognizing that temporal facts often involve events, we\nmodel these events in the claim and evidence sentences. We curate two temporal\nfact datasets to learn time-sensitive representations that encapsulate not only\nthe semantic relationships among the events, but also their chronological\nproximity. This allows us to retrieve the top-k relevant evidence sentences and\nprovide the context for a large language model to perform temporal reasoning\nand outputs whether a claim is supported or refuted by the retrieved evidence\nsentences. Experiment results demonstrate that the proposed approach\nsignificantly enhances the accuracy of temporal claim verification, thereby\nadvancing current state-of-the-art in automated fact verification.\n","authors":["Anab Maulana Barik","Wynne Hsu","Mong Li Lee"],"pdf_url":"https://arxiv.org/pdf/2407.15291v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.02059v3","updated":"2024-07-21T21:05:40Z","published":"2024-04-02T15:58:36Z","title":"IISAN: Efficiently Adapting Multimodal Representation for Sequential\n  Recommendation with Decoupled PEFT","summary":"  Multimodal foundation models are transformative in sequential recommender\nsystems, leveraging powerful representation learning capabilities. While\nParameter-efficient Fine-tuning (PEFT) is commonly used to adapt foundation\nmodels for recommendation tasks, most research prioritizes parameter\nefficiency, often overlooking critical factors like GPU memory efficiency and\ntraining speed. Addressing this gap, our paper introduces IISAN (Intra- and\nInter-modal Side Adapted Network for Multimodal Representation), a simple\nplug-and-play architecture using a Decoupled PEFT structure and exploiting both\nintra- and inter-modal adaptation.\n  IISAN matches the performance of full fine-tuning (FFT) and state-of-the-art\nPEFT. More importantly, it significantly reduces GPU memory usage - from 47GB\nto just 3GB for multimodal sequential recommendation tasks. Additionally, it\naccelerates training time per epoch from 443s to 22s compared to FFT. This is\nalso a notable improvement over the Adapter and LoRA, which require 37-39 GB\nGPU memory and 350-380 seconds per epoch for training.\n  Furthermore, we propose a new composite efficiency metric, TPME\n(Training-time, Parameter, and GPU Memory Efficiency) to alleviate the\nprevalent misconception that \"parameter efficiency represents overall\nefficiency\". TPME provides more comprehensive insights into practical\nefficiency comparisons between different methods. Besides, we give an\naccessible efficiency analysis of all PEFT and FFT approaches, which\ndemonstrate the superiority of IISAN. We release our codes and other materials\nat https://github.com/GAIR-Lab/IISAN.\n","authors":["Junchen Fu","Xuri Ge","Xin Xin","Alexandros Karatzoglou","Ioannis Arapakis","Jie Wang","Joemon M. Jose"],"pdf_url":"https://arxiv.org/pdf/2404.02059v3.pdf","comment":"Accepted by SIGIR2024"},{"id":"http://arxiv.org/abs/2407.15239v1","updated":"2024-07-21T18:08:44Z","published":"2024-07-21T18:08:44Z","title":"Assessing Brittleness of Image-Text Retrieval Benchmarks from\n  Vision-Language Models Perspective","summary":"  Image-text retrieval (ITR), an important task in information retrieval (IR),\nis driven by pretrained vision-language models (VLMs) that consistently achieve\nstate-of-the-art performance. However, a significant challenge lies in the\nbrittleness of existing ITR benchmarks. In standard datasets for the task,\ncaptions often provide broad summaries of scenes, neglecting detailed\ninformation about specific concepts. Additionally, the current evaluation setup\nassumes simplistic binary matches between images and texts and focuses on\nintra-modality rather than cross-modal relationships, which can lead to\nmisinterpretations of model performance. Motivated by this gap, in this study,\nwe focus on examining the brittleness of the ITR evaluation pipeline with a\nfocus on concept granularity. We start by analyzing two common benchmarks,\nMS-COCO and Flickr30k, and compare them with their augmented versions,\nMS-COCO-FG and Flickr30k-FG, given a specified set of linguistic features\ncapturing concept granularity. We discover that Flickr30k-FG and MS COCO-FG\nconsistently achieve higher scores across all the selected features. To\ninvestigate the performance of VLMs on coarse and fine-grained datasets, we\nintroduce a taxonomy of perturbations. We apply these perturbations to the\nselected datasets. We evaluate four state-of-the-art models - ALIGN, AltCLIP,\nCLIP, and GroupViT - on the standard and fine-grained datasets under zero-shot\nconditions, with and without the applied perturbations. The results demonstrate\nthat although perturbations generally degrade model performance, the\nfine-grained datasets exhibit a smaller performance drop than their standard\ncounterparts. Moreover, the relative performance drop across all setups is\nconsistent across all models and datasets, indicating that the issue lies\nwithin the benchmarks. We conclude the paper by providing an agenda for\nimproving ITR evaluation pipelines.\n","authors":["Mariya Hendriksen","Shuo Zhang","Ridho Reinanda","Mohamed Yahya","Edgar Meij","Maarten de Rijke"],"pdf_url":"https://arxiv.org/pdf/2407.15239v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15124v1","updated":"2024-07-21T11:27:27Z","published":"2024-07-21T11:27:27Z","title":"Chemical Reaction Extraction for Chemical Knowledge Base","summary":"  The task of searching through patent documents is crucial for chemical patent\nrecommendation and retrieval. This can be enhanced by creating a patent\nknowledge base (ChemPatKB) to aid in prior art searches and to provide a\nplatform for domain experts to explore new innovations in chemical compound\nsynthesis and use-cases. An essential foundational component of this KB is the\nextraction of important reaction snippets from long patents documents which\nfacilitates multiple downstream tasks such as reaction co-reference resolution\nand chemical entity role identification. In this work, we explore the problem\nof extracting reactions spans from chemical patents in order to create a\nreactions resource database. We formulate this task as a paragraph-level\nsequence tagging problem, where the system is required to return a sequence of\nparagraphs that contain a description of a reaction. We propose several\napproaches and modifications of the baseline models and study how different\nmethods generalize across different domains of chemical patents.\n","authors":["Aishwarya Jadhav","Ritam Dutt"],"pdf_url":"https://arxiv.org/pdf/2407.15124v1.pdf","comment":"Work completed in 2022 at Carnegie Mellon University"},{"id":"http://arxiv.org/abs/2406.00032v2","updated":"2024-07-21T06:52:40Z","published":"2024-05-25T06:57:33Z","title":"Paths of A Million People: Extracting Life Trajectories from Wikipedia","summary":"  The life trajectories of notable people have been studied to pinpoint the\ntimes and places of significant events such as birth, death, education,\nmarriage, competition, work, speeches, scientific discoveries, artistic\nachievements, and battles. Understanding how these individuals interact with\nothers provides valuable insights for broader research into human dynamics.\nHowever, the scarcity of trajectory data in terms of volume, density, and\ninter-person interactions, limits relevant studies from being comprehensive and\ninteractive. We mine millions of biography pages from Wikipedia and tackle the\ngeneralization problem stemming from the variety and heterogeneity of the\ntrajectory descriptions. Our ensemble model COSMOS, which combines the idea of\nsemi-supervised learning and contrastive learning, achieves an F1 score of\n85.95%. For this task, we also create a hand-curated dataset,\nWikiLifeTrajectory, consisting of 8,852 (person, time, location) triplets as\nground truth. Besides, we perform an empirical analysis on the trajectories of\n8,272 historians to demonstrate the validity of the extracted results. To\nfacilitate the research on trajectory extractions and help the analytical\nstudies to construct grand narratives, we make our code, the million-level\nextracted trajectories, and the WikiLifeTrajectory dataset publicly available.\n","authors":["Ying Zhang","Xiaofeng Li","Zhaoyang Liu","Haipeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.00032v2.pdf","comment":"Accepted to ICWSM 2025. 15 pages"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2407.15296v1","updated":"2024-07-21T23:43:24Z","published":"2024-07-21T23:43:24Z","title":"Weak-to-Strong Compositional Learning from Generative Models for\n  Language-based Object Detection","summary":"  Vision-language (VL) models often exhibit a limited understanding of complex\nexpressions of visual objects (e.g., attributes, shapes, and their relations),\ngiven complex and diverse language queries. Traditional approaches attempt to\nimprove VL models using hard negative synthetic text, but their effectiveness\nis limited. In this paper, we harness the exceptional compositional\nunderstanding capabilities of generative foundational models. We introduce a\nnovel method for structured synthetic data generation aimed at enhancing the\ncompositional understanding of VL models in language-based object detection.\nOur framework generates densely paired positive and negative triplets (image,\ntext descriptions, and bounding boxes) in both image and text domains. By\nleveraging these synthetic triplets, we transform 'weaker' VL models into\n'stronger' models in terms of compositional understanding, a process we call\n\"Weak-to-Strong Compositional Learning\" (WSCL). To achieve this, we propose a\nnew compositional contrastive learning formulation that discovers semantics and\nstructures in complex descriptions from synthetic triplets. As a result, VL\nmodels trained with our synthetic data generation exhibit a significant\nperformance boost in the Omnilabel benchmark by up to +5AP and the D3 benchmark\nby +6.9AP upon existing baselines.\n","authors":["Kwanyong Park","Kuniaki Saito","Donghyun Kim"],"pdf_url":"https://arxiv.org/pdf/2407.15296v1.pdf","comment":"ECCV 2024"},{"id":"http://arxiv.org/abs/2210.12777v4","updated":"2024-07-21T22:57:58Z","published":"2022-10-23T16:34:39Z","title":"Retrieval-Augmented and Knowledge-Grounded Language Models for Faithful\n  Clinical Medicine","summary":"  Language models (LMs), including large language models (such as ChatGPT),\nhave the potential to assist clinicians in generating various clinical notes.\nHowever, LMs are prone to produce ``hallucinations'', i.e., generated content\nthat is not aligned with facts and knowledge. In this paper, we propose the\nRe$^3$Writer method with retrieval-augmented generation and knowledge-grounded\nreasoning to enable LMs to generate faithful clinical texts. We demonstrate the\neffectiveness of our method in generating patient discharge instructions. It\nrequires the LMs not to only understand the patients' long clinical documents,\ni.e., the health records during hospitalization, but also to generate critical\ninstructional information provided both to carers and to the patient at the\ntime of discharge. The proposed Re$^3$Writer imitates the working patterns of\nphysicians to first \\textbf{re}trieve related working experience from\nhistorical instructions written by physicians, then \\textbf{re}ason related\nmedical knowledge. Finally, it \\textbf{re}fines the retrieved working\nexperience and reasoned medical knowledge to extract useful information, which\nis used to generate the discharge instructions for previously-unseen patients.\nOur experiments show that, using our method, the performance of five\nrepresentative LMs can be substantially boosted across all metrics. Meanwhile,\nwe show results from human evaluations to measure the effectiveness in terms of\nfluency, faithfulness, and comprehensiveness.\n","authors":["Fenglin Liu","Bang Yang","Chenyu You","Xian Wu","Shen Ge","Zhangdaihong Liu","Xu Sun","Yang Yang","David A. Clifton"],"pdf_url":"https://arxiv.org/pdf/2210.12777v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15284v1","updated":"2024-07-21T22:37:24Z","published":"2024-07-21T22:37:24Z","title":"Revisiting Neighborhood Aggregation in Graph Neural Networks for Node\n  Classification using Statistical Signal Processing","summary":"  We delve into the issue of node classification within graphs, specifically\nreevaluating the concept of neighborhood aggregation, which is a fundamental\ncomponent in graph neural networks (GNNs). Our analysis reveals conceptual\nflaws within certain benchmark GNN models when operating under the assumption\nof edge-independent node labels, a condition commonly observed in benchmark\ngraphs employed for node classification. Approaching neighborhood aggregation\nfrom a statistical signal processing perspective, our investigation provides\nnovel insights which may be used to design more efficient GNN models.\n","authors":["Mounir Ghogho"],"pdf_url":"https://arxiv.org/pdf/2407.15284v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15283v1","updated":"2024-07-21T22:24:16Z","published":"2024-07-21T22:24:16Z","title":"Enhancing Hardware Fault Tolerance in Machines with Reinforcement\n  Learning Policy Gradient Algorithms","summary":"  Industry is rapidly moving towards fully autonomous and interconnected\nsystems that can detect and adapt to changing conditions, including machine\nhardware faults. Traditional methods for adding hardware fault tolerance to\nmachines involve duplicating components and algorithmically reconfiguring a\nmachine's processes when a fault occurs. However, the growing interest in\nreinforcement learning-based robotic control offers a new perspective on\nachieving hardware fault tolerance. However, limited research has explored the\npotential of these approaches for hardware fault tolerance in machines. This\npaper investigates the potential of two state-of-the-art reinforcement learning\nalgorithms, Proximal Policy Optimization (PPO) and Soft Actor-Critic (SAC), to\nenhance hardware fault tolerance into machines. We assess the performance of\nthese algorithms in two OpenAI Gym simulated environments, Ant-v2 and\nFetchReach-v1. Robot models in these environments are subjected to six\nsimulated hardware faults. Additionally, we conduct an ablation study to\ndetermine the optimal method for transferring an agent's knowledge, acquired\nthrough learning in a normal (pre-fault) environment, to a (post-)fault\nenvironment in a continual learning setting. Our results demonstrate that\nreinforcement learning-based approaches can enhance hardware fault tolerance in\nsimulated machines, with adaptation occurring within minutes. Specifically, PPO\nexhibits the fastest adaptation when retaining the knowledge within its models,\nwhile SAC performs best when discarding all acquired knowledge. Overall, this\nstudy highlights the potential of reinforcement learning-based approaches, such\nas PPO and SAC, for hardware fault tolerance in machines. These findings pave\nthe way for the development of robust and adaptive machines capable of\neffectively operating in real-world scenarios.\n","authors":["Sheila Schoepp","Mehran Taghian","Shotaro Miwa","Yoshihiro Mitsuka","Shadan Golestan","Osmar Zaïane"],"pdf_url":"https://arxiv.org/pdf/2407.15283v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15277v1","updated":"2024-07-21T22:01:09Z","published":"2024-07-21T22:01:09Z","title":"Conformal Predictions under Markovian Data","summary":"  We study the split Conformal Prediction method when applied to Markovian\ndata. We quantify the gap in terms of coverage induced by the correlations in\nthe data (compared to exchangeable data). This gap strongly depends on the\nmixing properties of the underlying Markov chain, and we prove that it\ntypically scales as $\\sqrt{t_\\mathrm{mix}\\ln(n)/n}$ (where $t_\\mathrm{mix}$ is\nthe mixing time of the chain). We also derive upper bounds on the impact of the\ncorrelations on the size of the prediction set. Finally we present $K$-split\nCP, a method that consists in thinning the calibration dataset and that adapts\nto the mixing properties of the chain. Its coverage gap is reduced to\n$t_\\mathrm{mix}/(n\\ln(n))$ without really affecting the size of the prediction\nset. We finally test our algorithms on synthetic and real-world datasets.\n","authors":["Frédéric Zheng","Alexandre Proutiere"],"pdf_url":"https://arxiv.org/pdf/2407.15277v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.18922v2","updated":"2024-07-21T21:48:54Z","published":"2024-04-29T17:58:30Z","title":"DPO Meets PPO: Reinforced Token Optimization for RLHF","summary":"  In the classical Reinforcement Learning from Human Feedback (RLHF) framework,\nProximal Policy Optimization (PPO) is employed to learn from sparse,\nsentence-level rewards -- a challenging scenario in traditional deep\nreinforcement learning. Despite the great successes of PPO in the alignment of\nstate-of-the-art closed-source large language models (LLMs), its open-source\nimplementation is still largely sub-optimal, as widely reported by numerous\nresearch studies. To address these issues, we introduce a framework that models\nRLHF problems as a Markov decision process (MDP), enabling the capture of\nfine-grained token-wise information. Furthermore, we provide theoretical\ninsights that demonstrate the superiority of our MDP framework over the\nprevious sentence-level bandit formulation. Under this framework, we introduce\nan algorithm, dubbed as Reinforced Token Optimization (\\texttt{RTO}), which\nlearns the token-wise reward function from preference data and performs policy\noptimization based on this learned token-wise reward signal. Theoretically,\n\\texttt{RTO} is proven to have the capability of finding the near-optimal\npolicy sample-efficiently. For its practical implementation, \\texttt{RTO}\ninnovatively integrates Direct Preference Optimization (DPO) and PPO. DPO,\noriginally derived from sparse sentence rewards, surprisingly provides us with\na token-wise characterization of response quality, which is seamlessly\nincorporated into our subsequent PPO training stage. Extensive real-world\nalignment experiments verify the effectiveness of the proposed approach.\n","authors":["Han Zhong","Guhao Feng","Wei Xiong","Xinle Cheng","Li Zhao","Di He","Jiang Bian","Liwei Wang"],"pdf_url":"https://arxiv.org/pdf/2404.18922v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15273v1","updated":"2024-07-21T21:35:01Z","published":"2024-07-21T21:35:01Z","title":"Unifying Invariant and Variant Features for Graph Out-of-Distribution\n  via Probability of Necessity and Sufficiency","summary":"  Graph Out-of-Distribution (OOD), requiring that models trained on biased data\ngeneralize to the unseen test data, has considerable real-world applications.\nOne of the most mainstream methods is to extract the invariant subgraph by\naligning the original and augmented data with the help of environment\naugmentation. However, these solutions might lead to the loss or redundancy of\nsemantic subgraphs and result in suboptimal generalization. To address this\nchallenge, we propose exploiting Probability of Necessity and Sufficiency (PNS)\nto extract sufficient and necessary invariant substructures. Beyond that, we\nfurther leverage the domain variant subgraphs related to the labels to boost\nthe generalization performance in an ensemble manner. Specifically, we first\nconsider the data generation process for graph data. Under mild conditions, we\nshow that the sufficient and necessary invariant subgraph can be extracted by\nminimizing an upper bound, built on the theoretical advance of the probability\nof necessity and sufficiency. To further bridge the theory and algorithm, we\ndevise the model called Sufficiency and Necessity Inspired Graph Learning\n(SNIGL), which ensembles an invariant subgraph classifier on top of latent\nsufficient and necessary invariant subgraphs, and a domain variant subgraph\nclassifier specific to the test domain for generalization enhancement.\nExperimental results demonstrate that our SNIGL model outperforms the\nstate-of-the-art techniques on six public benchmarks, highlighting its\neffectiveness in real-world scenarios.\n","authors":["Xuexin Chen","Ruichu Cai","Kaitao Zheng","Zhifan Jiang","Zhengting Huang","Zhifeng Hao","Zijian Li"],"pdf_url":"https://arxiv.org/pdf/2407.15273v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15264v1","updated":"2024-07-21T20:41:39Z","published":"2024-07-21T20:41:39Z","title":"LSM-GNN: Large-scale Storage-based Multi-GPU GNN Training by Optimizing\n  Data Transfer Scheme","summary":"  Graph Neural Networks (GNNs) are widely used today in recommendation systems,\nfraud detection, and node/link classification tasks. Real world GNNs continue\nto scale in size and require a large memory footprint for storing graphs and\nembeddings that often exceed the memory capacities of the target GPUs used for\ntraining. To address limited memory capacities, traditional GNN training\napproaches use graph partitioning and sharding techniques to scale up across\nmultiple GPUs within a node and/or scale out across multiple nodes. However,\nthis approach suffers from the high computational costs of graph partitioning\nalgorithms and inefficient communication across GPUs.\n  To address these overheads, we propose Large-scale Storage-based Multi-GPU\nGNN framework (LSM-GNN), a storagebased approach to train GNN models that\nutilizes a novel communication layer enabling GPU software caches to function\nas a system-wide shared cache with low overheads.LSM-GNN incorporates a hybrid\neviction policy that intelligently manages cache space by using both static and\ndynamic node information to significantly enhance cache performance.\nFurthermore, we introduce the Preemptive Victim-buffer Prefetcher (PVP), a\nmechanism for prefetching node feature data from a Victim Buffer located in CPU\npinned-memory to further reduce the pressure on the storage devices.\nExperimental results show that despite the lower compute capabilities and\nmemory capacities, LSM-GNN in a single node with two GPUs offers superior\nperformance over two-node-four-GPU Dist-DGL baseline and provides up to 3.75x\nspeed up on end-to-end epoch time while running large-scale GNN training\n","authors":["Jeongmin Brian Park","Kun Wu","Vikram Sharma Mailthody","Zaid Quresh","Scott Mahlke","Wen-mei Hwu"],"pdf_url":"https://arxiv.org/pdf/2407.15264v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.07867v2","updated":"2024-07-21T20:32:38Z","published":"2023-08-15T16:34:37Z","title":"Fast Risk Assessment in Power Grids through Novel Gaussian Process and\n  Active Learning","summary":"  This paper presents a graph-structured Gaussian process (GP) model for\ndata-driven risk assessment of critical voltage constraints. The proposed GP is\nbased on a novel kernel, named the vertex-degree kernel (VDK), that decomposes\nthe voltage-load relationship based on the network graph. To estimate the GP\nefficiently, we propose a novel active learning scheme that leverages the\nadditive structure of VDK. Further, we prove a probabilistic bound on the error\nin risk estimation using VDK-GP model that demonstrates that it is\nstatistically comparable to using standard AC power flow (AC-PF), but does not\nrequire computing a large number of ACPF solutions. Simulations demonstrate\nthat the proposed VDK-GP achieves more than two fold sample complexity\nreduction, compared to a generic GP on medium scale 500-Bus and large scale\n1354-Bus power systems. Moreover, active learning achieves an impressive\nreduction of over 15 times in comparison to the time complexity of Monte-Carlo\nsimulations (MCS), and have risk estimation error of order 1E-4 for both\n500-Bus and 1354-Bus system, demonstrating its superior efficiency in risk\nestimation.\n","authors":["Parikshit Pareek","Deepjyoti Deka","Sidhant Misra"],"pdf_url":"https://arxiv.org/pdf/2308.07867v2.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2404.09730v2","updated":"2024-07-21T20:23:07Z","published":"2024-04-15T12:29:28Z","title":"Convergence Analysis of Probability Flow ODE for Score-based Generative\n  Models","summary":"  Score-based generative models have emerged as a powerful approach for\nsampling high-dimensional probability distributions. Despite their\neffectiveness, their theoretical underpinnings remain relatively\nunderdeveloped. In this work, we study the convergence properties of\ndeterministic samplers based on probability flow ODEs from both theoretical and\nnumerical perspectives. Assuming access to $L^2$-accurate estimates of the\nscore function, we prove the total variation between the target and the\ngenerated data distributions can be bounded above by\n$\\mathcal{O}(d^{3/4}\\delta^{1/2})$ in the continuous time level, where $d$\ndenotes the data dimension and $\\delta$ represents the $L^2$-score matching\nerror. For practical implementations using a $p$-th order Runge-Kutta\nintegrator with step size $h$, we establish error bounds of\n$\\mathcal{O}(d^{3/4}\\delta^{1/2} + d\\cdot(dh)^p)$ at the discrete level.\nFinally, we present numerical studies on problems up to 128 dimensions to\nverify our theory.\n","authors":["Daniel Zhengyu Huang","Jiaoyang Huang","Zhengjiang Lin"],"pdf_url":"https://arxiv.org/pdf/2404.09730v2.pdf","comment":"37 pages, 7 figures"},{"id":"http://arxiv.org/abs/2407.11089v2","updated":"2024-07-21T19:47:47Z","published":"2024-07-14T15:27:27Z","title":"Explainable bank failure prediction models: Counterfactual explanations\n  to reduce the failure risk","summary":"  The accuracy and understandability of bank failure prediction models are\ncrucial. While interpretable models like logistic regression are favored for\ntheir explainability, complex models such as random forest, support vector\nmachines, and deep learning offer higher predictive performance but lower\nexplainability. These models, known as black boxes, make it difficult to derive\nactionable insights. To address this challenge, using counterfactual\nexplanations is suggested. These explanations demonstrate how changes in input\nvariables can alter the model output and suggest ways to mitigate bank failure\nrisk. The key challenge lies in selecting the most effective method for\ngenerating useful counterfactuals, which should demonstrate validity,\nproximity, sparsity, and plausibility. The paper evaluates several\ncounterfactual generation methods: WhatIf, Multi Objective, and Nearest\nInstance Counterfactual Explanation, and also explores resampling methods like\nundersampling, oversampling, SMOTE, and the cost sensitive approach to address\ndata imbalance in bank failure prediction in the US. The results indicate that\nthe Nearest Instance Counterfactual Explanation method yields higher quality\ncounterfactual explanations, mainly using the cost sensitive approach. Overall,\nthe Multi Objective Counterfactual and Nearest Instance Counterfactual\nExplanation methods outperform others regarding validity, proximity, and\nsparsity metrics, with the cost sensitive approach providing the most desirable\ncounterfactual explanations. These findings highlight the variability in the\nperformance of counterfactual generation methods across different balancing\nstrategies and machine learning models, offering valuable strategies to enhance\nthe utility of black box bank failure prediction models.\n","authors":["Seyma Gunonu","Gizem Altun","Mustafa Cavus"],"pdf_url":"https://arxiv.org/pdf/2407.11089v2.pdf","comment":"20 pages, 1 figure"},{"id":"http://arxiv.org/abs/2407.15247v1","updated":"2024-07-21T19:10:40Z","published":"2024-07-21T19:10:40Z","title":"TimeInf: Time Series Data Contribution via Influence Functions","summary":"  Evaluating the contribution of individual data points to a model's prediction\nis critical for interpreting model predictions and improving model performance.\nExisting data contribution methods have been applied to various data types,\nincluding tabular data, images, and texts; however, their primary focus has\nbeen on i.i.d. settings. Despite the pressing need for principled approaches\ntailored to time series datasets, the problem of estimating data contribution\nin such settings remains unexplored, possibly due to challenges associated with\nhandling inherent temporal dependencies. This paper introduces TimeInf, a data\ncontribution estimation method for time-series datasets. TimeInf uses influence\nfunctions to attribute model predictions to individual time points while\npreserving temporal structures. Our extensive empirical results demonstrate\nthat TimeInf outperforms state-of-the-art methods in identifying harmful\nanomalies and helpful time points for forecasting. Additionally, TimeInf offers\nintuitive and interpretable attributions of data values, allowing us to easily\ndistinguish diverse anomaly patterns through visualizations.\n","authors":["Yizi Zhang","Jingyan Shen","Xiaoxue Xiong","Yongchan Kwon"],"pdf_url":"https://arxiv.org/pdf/2407.15247v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15245v1","updated":"2024-07-21T19:05:30Z","published":"2024-07-21T19:05:30Z","title":"Weyl Calculus and Exactly Solvable Schrödinger Bridges with\n  Quadratic State Cost","summary":"  Schr\\\"{o}dinger bridge--a stochastic dynamical generalization of optimal mass\ntransport--exhibits a learning-control duality. Viewed as a stochastic control\nproblem, the Schr\\\"{o}dinger bridge finds an optimal control policy that steers\na given joint state statistics to another while minimizing the total control\neffort subject to controlled diffusion and deadline constraints. Viewed as a\nstochastic learning problem, the Schr\\\"{o}dinger bridge finds the most-likely\ndistribution-valued trajectory connecting endpoint distributional observations,\ni.e., solves the two point boundary-constrained maximum likelihood problem over\nthe manifold of probability distributions. Recent works have shown that solving\nthe Schr\\\"{o}dinger bridge problem with state cost requires finding the Markov\nkernel associated with a reaction-diffusion PDE where the state cost appears as\na state-dependent reaction rate. We explain how ideas from Weyl calculus in\nquantum mechanics, specifically the Weyl operator and the Weyl symbol, can help\ndetermine such Markov kernels. We illustrate these ideas by explicitly finding\nthe Markov kernel for the case of quadratic state cost via Weyl calculus,\nrecovering our earlier results but avoiding tedious computation with Hermite\npolynomials.\n","authors":["Alexis M. H. Teter","Wenqing Wang","Abhishek Halder"],"pdf_url":"https://arxiv.org/pdf/2407.15245v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.11779v9","updated":"2024-07-21T18:30:20Z","published":"2024-06-17T17:34:25Z","title":"Compact Proofs of Model Performance via Mechanistic Interpretability","summary":"  We propose using mechanistic interpretability -- techniques for reverse\nengineering model weights into human-interpretable algorithms -- to derive and\ncompactly prove formal guarantees on model performance. We prototype this\napproach by formally proving lower bounds on the accuracy of 151 small\ntransformers trained on a Max-of-$K$ task. We create 102 different\ncomputer-assisted proof strategies and assess their length and tightness of\nbound on each of our models. Using quantitative metrics, we find that shorter\nproofs seem to require and provide more mechanistic understanding. Moreover, we\nfind that more faithful mechanistic understanding leads to tighter performance\nbounds. We confirm these connections by qualitatively examining a subset of our\nproofs. Finally, we identify compounding structureless noise as a key challenge\nfor using mechanistic interpretability to generate compact proofs on model\nperformance.\n","authors":["Jason Gross","Rajashree Agrawal","Thomas Kwa","Euan Ong","Chun Hei Yip","Alex Gibson","Soufiane Noubir","Lawrence Chan"],"pdf_url":"https://arxiv.org/pdf/2406.11779v9.pdf","comment":"accepted to ICML 2024 Workshop on Mechanistic Interpretability\n  (Spotlight)"}],"Multimedia":[{"id":"http://arxiv.org/abs/2407.15155v1","updated":"2024-07-21T13:26:30Z","published":"2024-07-21T13:26:30Z","title":"Distilling Vision-Language Foundation Models: A Data-Free Approach via\n  Prompt Diversification","summary":"  Data-Free Knowledge Distillation (DFKD) has shown great potential in creating\na compact student model while alleviating the dependency on real training data\nby synthesizing surrogate data. However, prior arts are seldom discussed under\ndistribution shifts, which may be vulnerable in real-world applications. Recent\nVision-Language Foundation Models, e.g., CLIP, have demonstrated remarkable\nperformance in zero-shot out-of-distribution generalization, yet consuming\nheavy computation resources. In this paper, we discuss the extension of DFKD to\nVision-Language Foundation Models without access to the billion-level\nimage-text datasets. The objective is to customize a student model for\ndistribution-agnostic downstream tasks with given category concepts, inheriting\nthe out-of-distribution generalization capability from the pre-trained\nfoundation models. In order to avoid generalization degradation, the primary\nchallenge of this task lies in synthesizing diverse surrogate images driven by\ntext prompts. Since not only category concepts but also style information are\nencoded in text prompts, we propose three novel Prompt Diversification methods\nto encourage image synthesis with diverse styles, namely Mix-Prompt,\nRandom-Prompt, and Contrastive-Prompt. Experiments on out-of-distribution\ngeneralization datasets demonstrate the effectiveness of the proposed methods,\nwith Contrastive-Prompt performing the best.\n","authors":["Yunyi Xuan","Weijie Chen","Shicai Yang","Di Xie","Luojun Lin","Yueting Zhuang"],"pdf_url":"https://arxiv.org/pdf/2407.15155v1.pdf","comment":"Accepted by ACMMM 2023"},{"id":"http://arxiv.org/abs/2407.15063v1","updated":"2024-07-21T05:30:27Z","published":"2024-07-21T05:30:27Z","title":"Feeling the Grass Grow: Making Midair Haptic Parameters Visible,\n  Touchable and Controllable","summary":"  In this paper, we present an ultrasound mid-air haptic interaction system\nthat integrates a designed visualization of haptic parameters while maintaining\nease of control. The design of corresponding haptic parameters for real-world\ntactile textures is a complex task. Furthermore, users often face difficulties\nin simultaneously controlling multi-dimensional haptic parameters to achieve\nthe desired vibration feedback. To address these challenges, the SLS\noptimization method facilitates user control of these multi-dimensional\nparameters through a simple one-dimensional slider. Concurrently, our system\nemploys the \"Growing Grass\" metaphor to visualize haptic parameter adjustments\nin real-time. This approach combining visual and haptic sensations can bring\nricher experiences and generate a realistic sensation of touching a grassy\nsurface. Our objective is to enhance users' intuitive comprehension of haptic\nparameters through this innovative system.\n","authors":["Mingxin Zhang","Qirong Zhu","Yasutoshi Makino","Hiroyuki Shinoda"],"pdf_url":"https://arxiv.org/pdf/2407.15063v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15050v1","updated":"2024-07-21T04:37:11Z","published":"2024-07-21T04:37:11Z","title":"Arondight: Red Teaming Large Vision Language Models with Auto-generated\n  Multi-modal Jailbreak Prompts","summary":"  Large Vision Language Models (VLMs) extend and enhance the perceptual\nabilities of Large Language Models (LLMs). Despite offering new possibilities\nfor LLM applications, these advancements raise significant security and ethical\nconcerns, particularly regarding the generation of harmful content. While LLMs\nhave undergone extensive security evaluations with the aid of red teaming\nframeworks, VLMs currently lack a well-developed one. To fill this gap, we\nintroduce Arondight, a standardized red team framework tailored specifically\nfor VLMs. Arondight is dedicated to resolving issues related to the absence of\nvisual modality and inadequate diversity encountered when transitioning\nexisting red teaming methodologies from LLMs to VLMs. Our framework features an\nautomated multi-modal jailbreak attack, wherein visual jailbreak prompts are\nproduced by a red team VLM, and textual prompts are generated by a red team LLM\nguided by a reinforcement learning agent. To enhance the comprehensiveness of\nVLM security evaluation, we integrate entropy bonuses and novelty reward\nmetrics. These elements incentivize the RL agent to guide the red team LLM in\ncreating a wider array of diverse and previously unseen test cases. Our\nevaluation of ten cutting-edge VLMs exposes significant security\nvulnerabilities, particularly in generating toxic images and aligning\nmulti-modal prompts. In particular, our Arondight achieves an average attack\nsuccess rate of 84.5\\% on GPT-4 in all fourteen prohibited scenarios defined by\nOpenAI in terms of generating toxic text. For a clearer comparison, we also\ncategorize existing VLMs based on their safety levels and provide corresponding\nreinforcement recommendations. Our multimodal prompt dataset and red team code\nwill be released after ethics committee approval. CONTENT WARNING: THIS PAPER\nCONTAINS HARMFUL MODEL RESPONSES.\n","authors":["Yi Liu","Chengjun Cai","Xiaoli Zhang","Xingliang Yuan","Cong Wang"],"pdf_url":"https://arxiv.org/pdf/2407.15050v1.pdf","comment":"To be published in ACM MM 2024"},{"id":"http://arxiv.org/abs/2407.15046v1","updated":"2024-07-21T03:59:14Z","published":"2024-07-21T03:59:14Z","title":"Audio-visual training for improved grounding in video-text LLMs","summary":"  Recent advances in multimodal LLMs, have led to several video-text models\nbeing proposed for critical video-related tasks. However, most of the previous\nworks support visual input only, essentially muting the audio signal in the\nvideo. Few models that support both audio and visual input, are not explicitly\ntrained on audio data. Hence, the effect of audio towards video understanding\nis largely unexplored. To this end, we propose a model architecture that\nhandles audio-visual inputs explicitly. We train our model with both audio and\nvisual data from a video instruction-tuning dataset. Comparison with\nvision-only baselines, and other audio-visual models showcase that training on\naudio data indeed leads to improved grounding of responses. For better\nevaluation of audio-visual models, we also release a human-annotated benchmark\ndataset, with audio-aware question-answer pairs.\n","authors":["Shivprasad Sagare","Hemachandran S","Kinshuk Sarabhai","Prashant Ullegaddi","Rajeshkumar SA"],"pdf_url":"https://arxiv.org/pdf/2407.15046v1.pdf","comment":null}]},"2024-07-20T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2407.14997v1","updated":"2024-07-20T22:10:37Z","published":"2024-07-20T22:10:37Z","title":"Improving Citation Text Generation: Overcoming Limitations in Length\n  Control","summary":"  A key challenge in citation text generation is that the length of generated\ntext often differs from the length of the target, lowering the quality of the\ngeneration. While prior works have investigated length-controlled generation,\ntheir effectiveness depends on knowing the appropriate generation length. In\nthis work, we present an in-depth study of the limitations of predicting\nscientific citation text length and explore the use of heuristic estimates of\ndesired length.\n","authors":["Biswadip Mandal","Xiangci Li","Jessica Ouyang"],"pdf_url":"https://arxiv.org/pdf/2407.14997v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14985v1","updated":"2024-07-20T21:24:40Z","published":"2024-07-20T21:24:40Z","title":"Generalization v.s. Memorization: Tracing Language Models' Capabilities\n  Back to Pretraining Data","summary":"  Despite the proven utility of large language models (LLMs) in real-world\napplications, there remains a lack of understanding regarding how they leverage\ntheir large-scale pretraining text corpora to achieve such capabilities. In\nthis work, we investigate the interplay between generalization and memorization\nin pretrained LLMs at scale, through a comprehensive $n$-gram analysis of their\ntraining data. Our experiments focus on three general task types: translation,\nquestion-answering, and multiple-choice reasoning. With various sizes of\nopen-source LLMs and their pretraining corpora, we observe that as the model\nsize increases, the task-relevant $n$-gram pair data becomes increasingly\nimportant, leading to improved task performance, decreased memorization,\nstronger generalization, and emergent abilities. Our results support the\nhypothesis that LLMs' capabilities emerge from a delicate balance of\nmemorization and generalization with sufficient task-related pretraining data,\nand point the way to larger-scale analyses that could further improve our\nunderstanding of these models.\n","authors":["Antonis Antoniades","Xinyi Wang","Yanai Elazar","Alfonso Amayuelas","Alon Albalak","Kexun Zhang","William Yang Wang"],"pdf_url":"https://arxiv.org/pdf/2407.14985v1.pdf","comment":"ICML FM-Wild workshop version"},{"id":"http://arxiv.org/abs/2401.05650v2","updated":"2024-07-20T20:38:55Z","published":"2024-01-11T04:03:35Z","title":"On Context-aware Detection of Cherry-picking in News Reporting","summary":"  Cherry-picking refers to the deliberate selection of evidence or facts that\nfavor a particular viewpoint while ignoring or distorting evidence that\nsupports an opposing perspective. Manually identifying cherry-picked statements\nin news stories can be challenging. In this study, we introduce a novel\napproach to detecting cherry-picked statements by identifying missing important\nstatements in a target news story using language models and contextual\ninformation from other news sources. Furthermore, this research introduces a\nnovel dataset specifically designed for training and evaluating cherry-picking\ndetection models. Our best performing model achieves an F-1 score of about 89%\nin detecting important statements. Moreover, results show the effectiveness of\nincorporating external knowledge from alternative narratives when assessing\nstatement importance.\n","authors":["Israa Jaradat","Haiqi Zhang","Chengkai Li"],"pdf_url":"https://arxiv.org/pdf/2401.05650v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.04449v4","updated":"2024-07-20T20:16:44Z","published":"2023-02-09T05:47:03Z","title":"Read and Reap the Rewards: Learning to Play Atari with the Help of\n  Instruction Manuals","summary":"  High sample complexity has long been a challenge for RL. On the other hand,\nhumans learn to perform tasks not only from interaction or demonstrations, but\nalso by reading unstructured text documents, e.g., instruction manuals.\nInstruction manuals and wiki pages are among the most abundant data that could\ninform agents of valuable features and policies or task-specific environmental\ndynamics and reward structures. Therefore, we hypothesize that the ability to\nutilize human-written instruction manuals to assist learning policies for\nspecific tasks should lead to a more efficient and better-performing agent. We\npropose the Read and Reward framework. Read and Reward speeds up RL algorithms\non Atari games by reading manuals released by the Atari game developers. Our\nframework consists of a QA Extraction module that extracts and summarizes\nrelevant information from the manual and a Reasoning module that evaluates\nobject-agent interactions based on information from the manual. An auxiliary\nreward is then provided to a standard A2C RL agent, when interaction is\ndetected. Experimentally, various RL algorithms obtain significant improvement\nin performance and training speed when assisted by our design.\n","authors":["Yue Wu","Yewen Fan","Paul Pu Liang","Amos Azaria","Yuanzhi Li","Tom M. Mitchell"],"pdf_url":"https://arxiv.org/pdf/2302.04449v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.12876v2","updated":"2024-07-20T19:59:42Z","published":"2024-07-16T06:18:03Z","title":"Exploring the Use of Abusive Generative AI Models on Civitai","summary":"  The rise of generative AI is transforming the landscape of digital imagery,\nand exerting a significant influence on online creative communities. This has\nled to the emergence of AI-Generated Content (AIGC) social platforms, such as\nCivitai. These distinctive social platforms allow users to build and share\ntheir own generative AI models, thereby enhancing the potential for more\ndiverse artistic expression. Designed in the vein of social networks, they also\nprovide artists with the means to showcase their creations (generated from the\nmodels), engage in discussions, and obtain feedback, thus nurturing a sense of\ncommunity. Yet, this openness also raises concerns about the abuse of such\nplatforms, e.g., using models to disseminate deceptive deepfakes or infringe\nupon copyrights. To explore this, we conduct the first comprehensive empirical\nstudy of an AIGC social platform, focusing on its use for generating abusive\ncontent. As an exemplar, we construct a comprehensive dataset covering Civitai,\nthe largest available AIGC social platform. Based on this dataset of 87K models\nand 2M images, we explore the characteristics of content and discuss strategies\nfor moderation to better govern these platforms.\n","authors":["Yiluo Wei","Yiming Zhu","Pan Hui","Gareth Tyson"],"pdf_url":"https://arxiv.org/pdf/2407.12876v2.pdf","comment":"Accepted to ACM Multimedia 2024"},{"id":"http://arxiv.org/abs/2407.14971v1","updated":"2024-07-20T19:53:52Z","published":"2024-07-20T19:53:52Z","title":"Sim-CLIP: Unsupervised Siamese Adversarial Fine-Tuning for Robust and\n  Semantically-Rich Vision-Language Models","summary":"  Vision-language models (VLMs) have achieved significant strides in recent\ntimes specially in multimodal tasks, yet they remain susceptible to adversarial\nattacks on their vision components. To address this, we propose Sim-CLIP, an\nunsupervised adversarial fine-tuning method that enhances the robustness of the\nwidely-used CLIP vision encoder against such attacks while maintaining semantic\nrichness and specificity. By employing a Siamese architecture with cosine\nsimilarity loss, Sim-CLIP learns semantically meaningful and attack-resilient\nvisual representations without requiring large batch sizes or momentum\nencoders. Our results demonstrate that VLMs enhanced with Sim-CLIP's fine-tuned\nCLIP encoder exhibit significantly enhanced robustness against adversarial\nattacks, while preserving semantic meaning of the perturbed images. Notably,\nSim-CLIP does not require additional training or fine-tuning of the VLM itself;\nreplacing the original vision encoder with our fine-tuned Sim-CLIP suffices to\nprovide robustness. This work underscores the significance of reinforcing\nfoundational models like CLIP to safeguard the reliability of downstream VLM\napplications, paving the way for more secure and effective multimodal systems.\n","authors":["Md Zarif Hossain","Ahmed Imteaj"],"pdf_url":"https://arxiv.org/pdf/2407.14971v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14962v1","updated":"2024-07-20T18:48:35Z","published":"2024-07-20T18:48:35Z","title":"Recent Advances in Generative AI and Large Language Models: Current\n  Status, Challenges, and Perspectives","summary":"  The emergence of Generative Artificial Intelligence (AI) and Large Language\nModels (LLMs) has marked a new era of Natural Language Processing (NLP),\nintroducing unprecedented capabilities that are revolutionizing various\ndomains. This paper explores the current state of these cutting-edge\ntechnologies, demonstrating their remarkable advancements and wide-ranging\napplications. Our paper contributes to providing a holistic perspective on the\ntechnical foundations, practical applications, and emerging challenges within\nthe evolving landscape of Generative AI and LLMs. We believe that understanding\nthe generative capabilities of AI systems and the specific context of LLMs is\ncrucial for researchers, practitioners, and policymakers to collaboratively\nshape the responsible and ethical integration of these technologies into\nvarious domains. Furthermore, we identify and address main research gaps,\nproviding valuable insights to guide future research endeavors within the AI\nresearch community.\n","authors":["Desta Haileselassie Hagos","Rick Battle","Danda B. Rawat"],"pdf_url":"https://arxiv.org/pdf/2407.14962v1.pdf","comment":"This version is accepted for publication in the journal of IEEE\n  Transactions on artificial intelligence (TAI)"},{"id":"http://arxiv.org/abs/2407.10969v2","updated":"2024-07-20T17:57:26Z","published":"2024-07-15T17:59:29Z","title":"Q-Sparse: All Large Language Models can be Fully Sparsely-Activated","summary":"  We introduce, Q-Sparse, a simple yet effective approach to training\nsparsely-activated large language models (LLMs). Q-Sparse enables full sparsity\nof activations in LLMs which can bring significant efficiency gains in\ninference. This is achieved by applying top-K sparsification to the activations\nand the straight-through-estimator to the training. We also introduce Block\nQ-Sparse for batch training and inference. The key results from this work are,\n(1) Q-Sparse can achieve results comparable to those of baseline LLMs while\nbeing much more efficient at inference time; (2) We present an\ninference-optimal scaling law for sparsely-activated LLMs; (3) Q-Sparse is\neffective in different settings, including training-from-scratch,\ncontinue-training of off-the-shelf LLMs, and finetuning; (4) Q-Sparse works for\nboth full-precision and 1-bit LLMs (e.g., BitNet b1.58). Particularly, the\nsynergy of BitNet b1.58 and Q-Sparse (can be equipped with MoE) provides the\ncornerstone and a clear path to revolutionize the efficiency, including cost\nand energy consumption, of future LLMs.\n","authors":["Hongyu Wang","Shuming Ma","Ruiping Wang","Furu Wei"],"pdf_url":"https://arxiv.org/pdf/2407.10969v2.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2311.15964v4","updated":"2024-07-20T17:55:37Z","published":"2023-11-27T16:07:37Z","title":"Efficient Pre-training for Localized Instruction Generation of Videos","summary":"  Procedural videos, exemplified by recipe demonstrations, are instrumental in\nconveying step-by-step instructions. However, understanding such videos is\nchallenging as it involves the precise localization of steps and the generation\nof textual instructions. Manually annotating steps and writing instructions is\ncostly, which limits the size of current datasets and hinders effective\nlearning. Leveraging large but noisy video-transcript datasets for pre-training\ncan boost performance but demands significant computational resources.\nFurthermore, transcripts contain irrelevant content and differ in style from\nhuman-written instructions. To mitigate these issues, we propose a novel\ntechnique, Sieve-&-Swap, to automatically generate high-quality training data\nfor the recipe domain: (i) Sieve: filters irrelevant transcripts and (ii) Swap:\nacquires high-quality text by replacing transcripts with human-written\ninstruction from a text-only recipe dataset. The resulting dataset is three\norders of magnitude smaller than current web-scale datasets but enables\nefficient training of large-scale models. Alongside Sieve-&-Swap, we propose\nProcedure Transformer (ProcX), a model for end-to-end step localization and\ninstruction generation for procedural videos. When pre-trained on our curated\ndataset, this model achieves state-of-the-art performance on YouCook2 and Tasty\nwhile using a fraction of the training data. We have released code and dataset.\n","authors":["Anil Batra","Davide Moltisanti","Laura Sevilla-Lara","Marcus Rohrbach","Frank Keller"],"pdf_url":"https://arxiv.org/pdf/2311.15964v4.pdf","comment":"ECCV 2024"},{"id":"http://arxiv.org/abs/2407.14940v1","updated":"2024-07-20T17:25:53Z","published":"2024-07-20T17:25:53Z","title":"Conversational Rubert for Detecting Competitive Interruptions in\n  ASR-Transcribed Dialogues","summary":"  Interruption in a dialogue occurs when the listener begins their speech\nbefore the current speaker finishes speaking. Interruptions can be broadly\ndivided into two groups: cooperative (when the listener wants to support the\nspeaker), and competitive (when the listener tries to take control of the\nconversation against the speaker's will). A system that automatically\nclassifies interruptions can be used in call centers, specifically in the tasks\nof customer satisfaction monitoring and agent monitoring. In this study, we\ndeveloped a text-based interruption classification model by preparing an\nin-house dataset consisting of ASR-transcribed customer support telephone\ndialogues in Russian. We fine-tuned Conversational RuBERT on our dataset and\noptimized hyperparameters, and the model performed well. With further\nimprovements, the proposed model can be applied to automatic monitoring\nsystems.\n","authors":["Dmitrii Galimzianov","Viacheslav Vyshegorodtsev"],"pdf_url":"https://arxiv.org/pdf/2407.14940v1.pdf","comment":"9 pages, 1 figure, 3 tables"},{"id":"http://arxiv.org/abs/2407.14937v1","updated":"2024-07-20T17:05:04Z","published":"2024-07-20T17:05:04Z","title":"Operationalizing a Threat Model for Red-Teaming Large Language Models\n  (LLMs)","summary":"  Creating secure and resilient applications with large language models (LLM)\nrequires anticipating, adjusting to, and countering unforeseen threats.\nRed-teaming has emerged as a critical technique for identifying vulnerabilities\nin real-world LLM implementations. This paper presents a detailed threat model\nand provides a systematization of knowledge (SoK) of red-teaming attacks on\nLLMs. We develop a taxonomy of attacks based on the stages of the LLM\ndevelopment and deployment process and extract various insights from previous\nresearch. In addition, we compile methods for defense and practical red-teaming\nstrategies for practitioners. By delineating prominent attack motifs and\nshedding light on various entry points, this paper provides a framework for\nimproving the security and robustness of LLM-based systems.\n","authors":["Apurv Verma","Satyapriya Krishna","Sebastian Gehrmann","Madhavan Seshadri","Anu Pradhan","Tom Ault","Leslie Barrett","David Rabinowitz","John Doucette","NhatHai Phan"],"pdf_url":"https://arxiv.org/pdf/2407.14937v1.pdf","comment":"Preprint. Under review"},{"id":"http://arxiv.org/abs/2407.14933v1","updated":"2024-07-20T16:50:18Z","published":"2024-07-20T16:50:18Z","title":"Consent in Crisis: The Rapid Decline of the AI Data Commons","summary":"  General-purpose artificial intelligence (AI) systems are built on massive\nswathes of public web data, assembled into corpora such as C4, RefinedWeb, and\nDolma. To our knowledge, we conduct the first, large-scale, longitudinal audit\nof the consent protocols for the web domains underlying AI training corpora.\nOur audit of 14,000 web domains provides an expansive view of crawlable web\ndata and how consent preferences to use it are changing over time. We observe a\nproliferation of AI-specific clauses to limit use, acute differences in\nrestrictions on AI developers, as well as general inconsistencies between\nwebsites' expressed intentions in their Terms of Service and their robots.txt.\nWe diagnose these as symptoms of ineffective web protocols, not designed to\ncope with the widespread re-purposing of the internet for AI. Our longitudinal\nanalyses show that in a single year (2023-2024) there has been a rapid\ncrescendo of data restrictions from web sources, rendering ~5%+ of all tokens\nin C4, or 28%+ of the most actively maintained, critical sources in C4, fully\nrestricted from use. For Terms of Service crawling restrictions, a full 45% of\nC4 is now restricted. If respected or enforced, these restrictions are rapidly\nbiasing the diversity, freshness, and scaling laws for general-purpose AI\nsystems. We hope to illustrate the emerging crisis in data consent, foreclosing\nmuch of the open web, not only for commercial AI, but non-commercial AI and\nacademic purposes.\n","authors":["Shayne Longpre","Robert Mahari","Ariel Lee","Campbell Lund","Hamidah Oderinwale","William Brannon","Nayan Saxena","Naana Obeng-Marnu","Tobin South","Cole Hunter","Kevin Klyman","Christopher Klamm","Hailey Schoelkopf","Nikhil Singh","Manuel Cherep","Ahmad Anis","An Dinh","Caroline Chitongo","Da Yin","Damien Sileo","Deividas Mataciunas","Diganta Misra","Emad Alghamdi","Enrico Shippole","Jianguo Zhang","Joanna Materzynska","Kun Qian","Kush Tiwary","Lester Miranda","Manan Dey","Minnie Liang","Mohammed Hamdy","Niklas Muennighoff","Seonghyeon Ye","Seungone Kim","Shrestha Mohanty","Vipul Gupta","Vivek Sharma","Vu Minh Chien","Xuhui Zhou","Yizhi Li","Caiming Xiong","Luis Villa","Stella Biderman","Hanlin Li","Daphne Ippolito","Sara Hooker","Jad Kabbara","Sandy Pentland"],"pdf_url":"https://arxiv.org/pdf/2407.14933v1.pdf","comment":"42 pages (13 main), 5 figures, 9 tables"},{"id":"http://arxiv.org/abs/2407.14916v1","updated":"2024-07-20T16:05:17Z","published":"2024-07-20T16:05:17Z","title":"Improving Context-Aware Preference Modeling for Language Models","summary":"  While finetuning language models from pairwise preferences has proven\nremarkably effective, the underspecified nature of natural language presents\ncritical challenges. Direct preference feedback is uninterpretable, difficult\nto provide where multidimensional criteria may apply, and often inconsistent,\neither because it is based on incomplete instructions or provided by diverse\nprincipals. To address these challenges, we consider the two-step preference\nmodeling procedure that first resolves the under-specification by selecting a\ncontext, and then evaluates preference with respect to the chosen context. We\ndecompose reward modeling error according to these two steps, which suggests\nthat supervising context in addition to context-specific preference may be a\nviable approach to aligning models with diverse human preferences. For this to\nwork, the ability of models to evaluate context-specific preference is\ncritical. To this end, we contribute context-conditioned preference datasets\nand accompanying experiments that investigate the ability of language models to\nevaluate context-specific preference. We use our datasets to (1) show that\nexisting preference models benefit from, but fail to fully consider, added\ncontext, (2) finetune a context-aware reward model with context-specific\nperformance exceeding that of GPT-4 and Llama 3 70B on tested datasets, and (3)\ninvestigate the value of context-aware preference modeling.\n","authors":["Silviu Pitis","Ziang Xiao","Nicolas Le Roux","Alessandro Sordoni"],"pdf_url":"https://arxiv.org/pdf/2407.14916v1.pdf","comment":"10 pages (28 with references and appendix)"},{"id":"http://arxiv.org/abs/2407.10241v2","updated":"2024-07-20T15:59:46Z","published":"2024-07-14T15:17:02Z","title":"BiasAlert: A Plug-and-play Tool for Social Bias Detection in LLMs","summary":"  Evaluating the bias in Large Language Models (LLMs) becomes increasingly\ncrucial with their rapid development. However, existing evaluation methods rely\non fixed-form outputs and cannot adapt to the flexible open-text generation\nscenarios of LLMs (e.g., sentence completion and question answering). To\naddress this, we introduce BiasAlert, a plug-and-play tool designed to detect\nsocial bias in open-text generations of LLMs. BiasAlert integrates external\nhuman knowledge with inherent reasoning capabilities to detect bias reliably.\nExtensive experiments demonstrate that BiasAlert significantly outperforms\nexisting state-of-the-art methods like GPT4-as-A-Judge in detecting bias.\nFurthermore, through application studies, we demonstrate the utility of\nBiasAlert in reliable LLM bias evaluation and bias mitigation across various\nscenarios. Model and code will be publicly released.\n","authors":["Zhiting Fan","Ruizhe Chen","Ruiling Xu","Zuozhu Liu"],"pdf_url":"https://arxiv.org/pdf/2407.10241v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14904v1","updated":"2024-07-20T15:34:52Z","published":"2024-07-20T15:34:52Z","title":"Large-vocabulary forensic pathological analyses via prototypical\n  cross-modal contrastive learning","summary":"  Forensic pathology is critical in determining the cause and manner of death\nthrough post-mortem examinations, both macroscopic and microscopic. The field,\nhowever, grapples with issues such as outcome variability, laborious processes,\nand a scarcity of trained professionals. This paper presents SongCi, an\ninnovative visual-language model (VLM) designed specifically for forensic\npathology. SongCi utilizes advanced prototypical cross-modal self-supervised\ncontrastive learning to enhance the accuracy, efficiency, and generalizability\nof forensic analyses. It was pre-trained and evaluated on a comprehensive\nmulti-center dataset, which includes over 16 million high-resolution image\npatches, 2,228 vision-language pairs of post-mortem whole slide images (WSIs),\nand corresponding gross key findings, along with 471 distinct diagnostic\noutcomes. Our findings indicate that SongCi surpasses existing multi-modal AI\nmodels in many forensic pathology tasks, performs comparably to experienced\nforensic pathologists and significantly better than less experienced ones, and\nprovides detailed multi-modal explainability, offering critical assistance in\nforensic investigations. To the best of our knowledge, SongCi is the first VLM\nspecifically developed for forensic pathological analysis and the first\nlarge-vocabulary computational pathology (CPath) model that directly processes\ngigapixel WSIs in forensic science.\n","authors":["Chen Shen","Chunfeng Lian","Wanqing Zhang","Fan Wang","Jianhua Zhang","Shuanliang Fan","Xin Wei","Gongji Wang","Kehan Li","Hongshu Mu","Hao Wu","Xinggong Liang","Jianhua Ma","Zhenyuan Wang"],"pdf_url":"https://arxiv.org/pdf/2407.14904v1.pdf","comment":"28 pages, 6 figures, under review"},{"id":"http://arxiv.org/abs/2407.14885v1","updated":"2024-07-20T14:23:15Z","published":"2024-07-20T14:23:15Z","title":"Falcon2-11B Technical Report","summary":"  We introduce Falcon2-11B, a foundation model trained on over five trillion\ntokens, and its multimodal counterpart, Falcon2-11B-vlm, which is a\nvision-to-text model. We report our findings during the training of the\nFalcon2-11B which follows a multi-stage approach where the early stages are\ndistinguished by their context length and a final stage where we use a curated,\nhigh-quality dataset. Additionally, we report the effect of doubling the batch\nsize mid-training and how training loss spikes are affected by the learning\nrate. The downstream performance of the foundation model is evaluated on\nestablished benchmarks, including multilingual and code datasets. The\nfoundation model shows strong generalization across all the tasks which makes\nit suitable for downstream finetuning use cases. For the vision language model,\nwe report the performance on several benchmarks and show that our model\nachieves a higher average score compared to open-source models of similar size.\nThe model weights and code of both Falcon2-11B and Falcon2-11B-vlm are made\navailable under a permissive license.\n","authors":["Quentin Malartic","Nilabhra Roy Chowdhury","Ruxandra Cojocaru","Mugariya Farooq","Giulia Campesan","Yasser Abdelaziz Dahou Djilali","Sanath Narayan","Ankit Singh","Maksim Velikanov","Basma El Amel Boussaha","Mohammed Al-Yafeai","Hamza Alobeidli","Leen Al Qadi","Mohamed El Amine Seddik","Kirill Fedyanin","Reda Alami","Hakim Hacid"],"pdf_url":"https://arxiv.org/pdf/2407.14885v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14878v1","updated":"2024-07-20T13:56:39Z","published":"2024-07-20T13:56:39Z","title":"Modular Sentence Encoders: Separating Language Specialization from\n  Cross-Lingual Alignment","summary":"  Multilingual sentence encoders are commonly obtained by training multilingual\nlanguage models to map sentences from different languages into a shared\nsemantic space. As such, they are subject to curse of multilinguality, a loss\nof monolingual representational accuracy due to parameter sharing. Another\nlimitation of multilingual sentence encoders is the trade-off between\nmonolingual and cross-lingual performance. Training for cross-lingual alignment\nof sentence embeddings distorts the optimal monolingual structure of semantic\nspaces of individual languages, harming the utility of sentence embeddings in\nmonolingual tasks. In this work, we address both issues by modular training of\nsentence encoders, i.e., by separating monolingual specialization from\ncross-lingual alignment. We first efficiently train language-specific sentence\nencoders to avoid negative interference between languages (i.e., the curse). We\nthen align all non-English monolingual encoders to the English encoder by\ntraining a cross-lingual alignment adapter on top of each, preventing\ninterference with monolingual specialization from the first step. In both\nsteps, we resort to contrastive learning on machine-translated paraphrase data.\nMonolingual and cross-lingual evaluations on semantic text\nsimilarity/relatedness and multiple-choice QA render our modular solution more\neffective than multilingual sentence encoders, especially benefiting\nlow-resource languages.\n","authors":["Yongxin Huang","Kexin Wang","Goran Glavaš","Iryna Gurevych"],"pdf_url":"https://arxiv.org/pdf/2407.14878v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14875v1","updated":"2024-07-20T13:28:12Z","published":"2024-07-20T13:28:12Z","title":"Seal: Advancing Speech Language Models to be Few-Shot Learners","summary":"  Existing auto-regressive language models have demonstrated a remarkable\ncapability to perform a new task with just a few examples in prompt, without\nrequiring any additional training. In order to extend this capability to a\nmulti-modal setting (i.e. speech and language), this paper introduces the Seal\nmodel, an abbreviation for speech language model. It incorporates a novel\nalignment method, in which Kullback-Leibler divergence loss is performed to\ntrain a projector that bridges a frozen speech encoder with a frozen language\nmodel decoder. The resulting Seal model exhibits robust performance as a\nfew-shot learner on two speech understanding tasks. Additionally, consistency\nexperiments are conducted to validate its robustness on different pre-trained\nlanguage models.\n","authors":["Shuyu Lei","Lingen Liu","Jiaolong Yang","Yasen Jiao","Yuxiang Yang","Yushu Yang","Xiang Guo"],"pdf_url":"https://arxiv.org/pdf/2407.14875v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.10744v3","updated":"2024-07-20T12:57:15Z","published":"2023-09-19T16:41:19Z","title":"Evaluating Large Language Models' Ability Using a Psychiatric Screening\n  Tool Based on Metaphor and Sarcasm Scenarios","summary":"  Metaphors and sarcasm are precious fruits of our highly evolved social\ncommunication skills. However, children with the condition then known as\nAsperger syndrome are known to have difficulties in comprehending sarcasm, even\nif they possess adequate verbal IQs for understanding metaphors. Accordingly,\nresearchers had employed a screening test that assesses metaphor and sarcasm\ncomprehension to distinguish Asperger syndrome from other conditions with\nsimilar external behaviors (e.g., attention-deficit/hyperactivity disorder).\nThis study employs a standardized test to evaluate recent large language\nmodels' (LLMs) understanding of nuanced human communication. The results\nindicate improved metaphor comprehension with increased model parameters;\nhowever, no similar improvement was observed for sarcasm comprehension.\nConsidering that a human's ability to grasp sarcasm has been associated with\nthe amygdala, a pivotal cerebral region for emotional learning, a distinctive\nstrategy for training LLMs would be imperative to imbue them with the ability\nin a cognitively grounded manner.\n","authors":["Hiromu Yakura"],"pdf_url":"https://arxiv.org/pdf/2309.10744v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14845v1","updated":"2024-07-20T11:19:58Z","published":"2024-07-20T11:19:58Z","title":"Understanding the Relationship between Prompts and Response Uncertainty\n  in Large Language Models","summary":"  Large language models (LLMs) are widely used in decision-making, but their\nreliability, especially in critical tasks like healthcare, is not\nwell-established. Therefore, understanding how LLMs reason and make decisions\nis crucial for their safe deployment. This paper investigates how the\nuncertainty of responses generated by LLMs relates to the information provided\nin the input prompt. Leveraging the insight that LLMs learn to infer latent\nconcepts during pretraining, we propose a prompt-response concept model that\nexplains how LLMs generate responses and helps understand the relationship\nbetween prompts and response uncertainty. We show that the uncertainty\ndecreases as the prompt's informativeness increases, similar to epistemic\nuncertainty. Our detailed experimental results on real datasets validate our\nproposed model.\n","authors":["Ze Yu Zhang","Arun Verma","Finale Doshi-Velez","Bryan Kian Hsiang Low"],"pdf_url":"https://arxiv.org/pdf/2407.14845v1.pdf","comment":"27 pages, 11 figures"},{"id":"http://arxiv.org/abs/2401.05811v2","updated":"2024-07-20T11:13:38Z","published":"2024-01-11T10:28:17Z","title":"Tuning LLMs with Contrastive Alignment Instructions for Machine\n  Translation in Unseen, Low-resource Languages","summary":"  This article introduces contrastive alignment instructions (AlignInstruct) to\naddress two challenges in machine translation (MT) on large language models\n(LLMs). One is the expansion of supported languages to previously unseen ones.\nThe second relates to the lack of data in low-resource languages. Model\nfine-tuning through MT instructions (MTInstruct) is a straightforward approach\nto the first challenge. However, MTInstruct is limited by weak cross-lingual\nsignals inherent in the second challenge. AlignInstruct emphasizes\ncross-lingual supervision via a cross-lingual discriminator built using\nstatistical word alignments. Our results based on fine-tuning the BLOOMZ models\n(1b1, 3b, and 7b1) in up to 24 unseen languages showed that: (1) LLMs can\neffectively translate unseen languages using MTInstruct; (2) AlignInstruct led\nto consistent improvements in translation quality across 48 translation\ndirections involving English; (3) Discriminator-based instructions outperformed\ntheir generative counterparts as cross-lingual instructions; (4) AlignInstruct\nimproved performance in 30 zero-shot directions.\n","authors":["Zhuoyuan Mao","Yen Yu"],"pdf_url":"https://arxiv.org/pdf/2401.05811v2.pdf","comment":"Accepted to LoResMT 2024"},{"id":"http://arxiv.org/abs/2407.14829v1","updated":"2024-07-20T10:13:54Z","published":"2024-07-20T10:13:54Z","title":"Overview of AI-Debater 2023: The Challenges of Argument Generation Tasks","summary":"  In this paper we present the results of the AI-Debater 2023 Challenge held by\nthe Chinese Conference on Affect Computing (CCAC 2023), and introduce the\nrelated datasets. We organize two tracks to handle the argumentative generation\ntasks in different scenarios, namely, Counter-Argument Generation (Track 1) and\nClaim-based Argument Generation (Track 2). Each track is equipped with its\ndistinct dataset and baseline model respectively. In total, 32 competing teams\nregister for the challenge, from which we received 11 successful submissions.\nIn this paper, we will present the results of the challenge and a summary of\nthe systems, highlighting commonalities and innovations among participating\nsystems. Datasets and baseline models of the AI-Debater 2023 Challenge have\nbeen already released and can be accessed through the official website of the\nchallenge.\n","authors":["Jiayu Lin","Guanrong Chen","Bojun Jin","Chenyang Li","Shutong Jia","Wancong Lin","Yang Sun","Yuhang He","Caihua Yang","Jianzhu Bao","Jipeng Wu","Wen Su","Jinglu Chen","Xinyi Li","Tianyu Chen","Mingjie Han","Shuaiwen Du","Zijian Wang","Jiyin Li","Fuzhong Suo","Hao Wang","Nuanchen Lin","Rui Feng Xu","Long Zhang","Jiuxin Cao","Ting Jin","Zhongyu Wei"],"pdf_url":"https://arxiv.org/pdf/2407.14829v1.pdf","comment":"Overview of AI-Debater 2023"},{"id":"http://arxiv.org/abs/2407.14822v1","updated":"2024-07-20T09:54:55Z","published":"2024-07-20T09:54:55Z","title":"Text Style Transfer: An Introductory Overview","summary":"  Text Style Transfer (TST) is a pivotal task in natural language generation to\nmanipulate text style attributes while preserving style-independent content.\nThe attributes targeted in TST can vary widely, including politeness,\nauthorship, mitigation of offensive language, modification of feelings, and\nadjustment of text formality. TST has become a widely researched topic with\nsubstantial advancements in recent years. This paper provides an introductory\noverview of TST, addressing its challenges, existing approaches, datasets,\nevaluation measures, subtasks, and applications. This fundamental overview\nimproves understanding of the background and fundamentals of text style\ntransfer.\n","authors":["Sourabrata Mukherjee","Ondrej Dušek"],"pdf_url":"https://arxiv.org/pdf/2407.14822v1.pdf","comment":"Accepted at 4EU+ International Workshop on Recent Advancements in\n  Artificial Intelligence"},{"id":"http://arxiv.org/abs/2407.00342v3","updated":"2024-07-20T09:32:01Z","published":"2024-06-29T07:01:51Z","title":"Korean Aspect-Based Sentiment Analysis via Implicit-Feature Alignment\n  with Corpus Filtering","summary":"  Investigations into Aspect-Based Sentiment Analysis (ABSA) for Korean\nrestaurant reviews are notably lacking in the existing literature. Our research\nproposes an intuitive and effective framework for ABSA in low-resource\nlanguages such as Korean. It optimizes prediction labels by integrating\ntranslated benchmark and unlabeled Korean data. Using a model fine-tuned on\ntranslated data, we pseudo-labeled the actual Korean NLI set. Subsequently, we\napplied LaBSE and MSP-based filtering to this pseudo-NLI set as implicit\nfeature, enhancing Aspect Category Detection and Polarity determination through\nadditional training. Incorporating dual filtering, this model bridged dataset\ngaps, achieving positive results in Korean ABSA with minimal resources. Through\nadditional data injection pipelines, our approach aims to utilize high-resource\ndata and construct effective models within communities, whether corporate or\nindividual, in low-resource language countries. Compared to English ABSA, our\nframework showed an approximately 3% difference in F1 scores and accuracy. We\nrelease the dataset and our code for Korean ABSA, at this link.\n","authors":["Kibeom Nam"],"pdf_url":"https://arxiv.org/pdf/2407.00342v3.pdf","comment":"13 pages, EMNLP Industry Track (submitted), DMLR@ICML 2024"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2407.14895v1","updated":"2024-07-20T14:55:12Z","published":"2024-07-20T14:55:12Z","title":"Strategic Coupon Allocation for Increasing Providers' Sales Experiences\n  in Two-sided Marketplaces","summary":"  In a two-sided marketplace, network effects are crucial for competitiveness,\nand platforms need to retain users through advanced customer relationship\nmanagement as much as possible. Maintaining numerous providers' stable and\nactive presence on the platform is highly important to enhance the\nmarketplace's scale and diversity. The strongest motivation for providers to\ncontinue using the platform is to realize actual profits through sales. Then,\nwe propose a personalized promotion to increase the number of successful\nproviders with sales experiences on the platform. The main contributions of our\nresearch are twofold. First, we introduce a new perspective in provider\nmanagement with the distribution of successful sales experiences. Second, we\npropose a personalized promotion optimization method to maximize the number of\nproviders' sales experiences. By utilizing this approach, we ensure equal\nopportunities for providers to experience sales without being monopolized by a\nfew providers. Through experiments using actual data on coupon distribution, we\nconfirm that our method enables the implementation of coupon allocation\nstrategies that significantly increase the total number of providers having\nsales experiences.\n","authors":["Koya Ohashi","Sho Sekine","Deddy Jobson","Jie Yang","Naoki Nishimura","Noriyoshi Sukegawa","Yuichi Takano"],"pdf_url":"https://arxiv.org/pdf/2407.14895v1.pdf","comment":"8 pages, 10 figures, KDD 2024 Workshop on Two-sided Marketplace\n  Optimization: Search, Pricing, Matching & Growth"},{"id":"http://arxiv.org/abs/2407.14743v1","updated":"2024-07-20T03:52:14Z","published":"2024-07-20T03:52:14Z","title":"Denoising Long- and Short-term Interests for Sequential Recommendation","summary":"  User interests can be viewed over different time scales, mainly including\nstable long-term preferences and changing short-term intentions, and their\ncombination facilitates the comprehensive sequential recommendation. However,\nexisting work that focuses on different time scales of user modeling has\nignored the negative effects of different time-scale noise, which hinders\ncapturing actual user interests and cannot be resolved by conventional\nsequential denoising methods. In this paper, we propose a Long- and Short-term\nInterest Denoising Network (LSIDN), which employs different encoders and\ntailored denoising strategies to extract long- and short-term interests,\nrespectively, achieving both comprehensive and robust user modeling.\nSpecifically, we employ a session-level interest extraction and evolution\nstrategy to avoid introducing inter-session behavioral noise into long-term\ninterest modeling; we also adopt contrastive learning equipped with a\nhomogeneous exchanging augmentation to alleviate the impact of unintentional\nbehavioral noise on short-term interest modeling. Results of experiments on two\npublic datasets show that LSIDN consistently outperforms state-of-the-art\nmodels and achieves significant robustness.\n","authors":["Xinyu Zhang","Beibei Li","Beihong Jin"],"pdf_url":"https://arxiv.org/pdf/2407.14743v1.pdf","comment":"9 pages, accepted by SDM 2024"},{"id":"http://arxiv.org/abs/2407.14741v1","updated":"2024-07-20T03:41:57Z","published":"2024-07-20T03:41:57Z","title":"Orthogonal Hyper-category Guided Multi-interest Elicitation for\n  Micro-video Matching","summary":"  Watching micro-videos is becoming a part of public daily life. Usually, user\nwatching behaviors are thought to be rooted in their multiple different\ninterests. In the paper, we propose a model named OPAL for micro-video\nmatching, which elicits a user's multiple heterogeneous interests by\ndisentangling multiple soft and hard interest embeddings from user\ninteractions. Moreover, OPAL employs a two-stage training strategy, in which\nthe pre-train is to generate soft interests from historical interactions under\nthe guidance of orthogonal hyper-categories of micro-videos and the fine-tune\nis to reinforce the degree of disentanglement among the interests and learn the\ntemporal evolution of each interest of each user. We conduct extensive\nexperiments on two real-world datasets. The results show that OPAL not only\nreturns diversified micro-videos but also outperforms six state-of-the-art\nmodels in terms of recall and hit rate.\n","authors":["Beibei Li","Beihong Jin","Yisong Yu","Yiyuan Zheng","Jiageng Song","Wei Zhuo","Tao Xiang"],"pdf_url":"https://arxiv.org/pdf/2407.14741v1.pdf","comment":"6 pages, accepted by ICME 2024"},{"id":"http://arxiv.org/abs/2407.08334v3","updated":"2024-07-20T03:40:43Z","published":"2024-07-11T09:35:08Z","title":"ADMM Based Semi-Structured Pattern Pruning Framework For Transformer","summary":"  NLP(natural language processsing) has achieved great success through the\ntransformer model.However, the model has hundreds of millions or billions\nparameters,which is huge burden for its deployment on personal computer or\nsmall scale of server.To deal with it, we either make the model's weight matrix\nrelatively sparser, or compress attention layer. Pattern pruning ,one of the\nmost important pruning methods, permits selecting fixed number of parameters in\neach divided pattern block and prunes it. However, the effect of pattern\npruning is strictly limited by the sparsity within a region of weights in each\nlayer. In this paper,we first introduced Alternating Direction Method of\nMultipliers(ADMM) based pattern pruning framework to reshape the distribution\nof activation map. Specifically, we propose to formulate the pattern pruning on\ntransformer as a constrained optimization and use ADMM to optimize the problem.\nIn this way, the initial dense feature maps is transformed to rather regionally\nsparsified ones.Therefore, we can then achieve higher compression ratio with\nbetter performance based on pattern pruning method. Additionally, this paper\nprovides a theoretical derivations of the ADMM with local sparsity. Finally, we\nalso extend the proposed ADMM based framework with SR-STE to demonstrate its\ngeneralization and to avoid gradient vanishing problem. We conduct extensive\nexperiments on classification tasks over GLUE datasets. Significantly, we\nachieve 50% percent compression ratio while maintaining overall score 80.1 on\nGLUE dataset.\n","authors":["TianChen Wang"],"pdf_url":"https://arxiv.org/pdf/2407.08334v3.pdf","comment":"11 pages, 5 figures"}],"Multimedia":[{"id":"http://arxiv.org/abs/2407.14936v1","updated":"2024-07-20T17:03:23Z","published":"2024-07-20T17:03:23Z","title":"EidetiCom: A Cross-modal Brain-Computer Semantic Communication Paradigm\n  for Decoding Visual Perception","summary":"  Brain-computer interface (BCI) facilitates direct communication between the\nhuman brain and external systems by utilizing brain signals, eliminating the\nneed for conventional communication methods such as speaking, writing, or\ntyping. Nevertheless, the continuous generation of brain signals in BCI\nframeworks poses challenges for efficient storage and real-time transmission.\nWhile considering the human brain as a semantic source, the meaningful\ninformation associated with cognitive activities often gets obscured by\nsubstantial noise present in acquired brain signals, resulting in abundant\nredundancy. In this paper, we propose a cross-modal brain-computer semantic\ncommunication paradigm, named EidetiCom, for decoding visual perception under\nlimited-bandwidth constraint. The framework consists of three hierarchical\nlayers, each responsible for compressing the semantic information of brain\nsignals into representative features. These low-dimensional compact features\nare transmitted and converted into semantically meaningful representations at\nthe receiver side, serving three distinct tasks for decoding visual perception:\nbrain signal-based visual classification, brain-to-caption translation, and\nbrain-to-image generation, in a scalable manner. Through extensive qualitative\nand quantitative experiments, we demonstrate that the proposed paradigm\nfacilitates the semantic communication under low bit rate conditions ranging\nfrom 0.017 to 0.192 bits-per-sample, achieving high-quality semantic\nreconstruction and highlighting its potential for efficient storage and\nreal-time communication of brain recordings in BCI applications, such as\neidetic memory storage and assistive communication for patients.\n","authors":["Linfeng Zheng","Peilin Chen","Shiqi Wang"],"pdf_url":"https://arxiv.org/pdf/2407.14936v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14823v1","updated":"2024-07-20T10:00:20Z","published":"2024-07-20T10:00:20Z","title":"CrossDehaze: Scaling Up Image Dehazing with Cross-Data Vision Alignment\n  and Augmentation","summary":"  In recent years, as computer vision tasks have increasingly relied on\nhigh-quality image inputs, the task of image dehazing has received significant\nattention. Previously, many methods based on priors and deep learning have been\nproposed to address the task of image dehazing. Ignoring the domain gap between\ndifferent data, former de-hazing methods usually adopt multiple datasets for\nexplicit training, which often makes the methods themselves be violated. To\naddress this problem, we propose a novel method of internal and external data\naugmentation to improve the existing dehazing methodology. By using cross-data\nexternal augmentor. The dataset inherits samples from different domains that\nare firmly aligned, making the model learn more robust and generalizable\nfeatures. By using the internal data augmentation method, the model can fully\nexploit local information within the images, thereby obtaining more image\ndetails. To demonstrate the effectiveness of our proposed method, we conduct\ntraining on both the Natural Image Dataset (NID) and the Remote Sensing Image\nDataset (RSID). Experimental results show that our method clearly resolves the\ndomain gap in different dehazing datasets and presents a new pipeline for joint\ntraining in the dehazing task. Our approach significantly outperforms other\nadvanced methods in dehazing and produces dehazed images that are closest to\nreal haze-free images. The code will be available at:\nhttps://github.com/wengzp1/ScaleUpDehazing\n","authors":["Yukai Shi","Zhipeng Weng","Yupei Lin","Cidan Shi","Xiaojun Yang","Liang Lin"],"pdf_url":"https://arxiv.org/pdf/2407.14823v1.pdf","comment":"A cross-dataset vision alignment and augmentation technology is\n  proposed to boost generalizable feature learning in the de-hazing task"}]},"2024-07-23T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2407.16695v1","updated":"2024-07-23T17:57:41Z","published":"2024-07-23T17:57:41Z","title":"Stress-Testing Long-Context Language Models with Lifelong ICL and Task\n  Haystack","summary":"  We introduce Lifelong ICL, a problem setting that challenges long-context\nlanguage models (LMs) to learn from a sequence of language tasks through\nin-context learning (ICL). We further introduce Task Haystack, an evaluation\nsuite dedicated to assessing and diagnosing how long-context LMs utilizes\ncontexts in Lifelong ICL. When given a task instruction and test inputs,\nlong-context LMs are expected to leverage the relevant demonstrations in the\nLifelong ICL prompt, avoid distraction and interference from other tasks, and\nachieve test accuracies that are not significantly worse than the Single-task\nICL baseline.\n  Task Haystack draws inspiration from the widely-adopted\n\"needle-in-a-haystack\" (NIAH) evaluation, but presents new and unique\nchallenges. It demands that models (1) utilize the contexts with deeper\nunderstanding, rather than resorting to simple copying and pasting; (2)\nnavigate through long streams of evolving topics and tasks, which closely\napproximates the complexities of real-world usage of long-context LMs.\nAdditionally, Task Haystack inherits the controllability aspect of NIAH,\nproviding model developers with tools and visualizations to identify model\nvulnerabilities effectively.\n  We benchmark 12 long-context LMs using Task Haystack. We find that\nstate-of-the-art closed models such as GPT-4o still struggle in this setting,\nfailing 15% of the cases on average, while all open-weight models we evaluate\nfurther lack behind by a large margin, failing up to 61% of the cases. In our\ncontrolled analysis, we identify factors such as distraction and recency bias\nas contributors to these failure cases. Further, we observe declines in\nperformance when task instructions are paraphrased at test time or when ICL\ndemonstrations are repeated excessively, raising concerns about the robustness,\ninstruction understanding, and true context utilization of current long-context\nLMs.\n","authors":["Xiaoyue Xu","Qinyuan Ye","Xiang Ren"],"pdf_url":"https://arxiv.org/pdf/2407.16695v1.pdf","comment":"Code: https://github.com/INK-USC/Lifelong-ICL; Website:\n  https://inklab.usc.edu/lifelong-icl/"},{"id":"http://arxiv.org/abs/2407.16693v1","updated":"2024-07-23T17:56:32Z","published":"2024-07-23T17:56:32Z","title":"Explanation Regularisation through the Lens of Attributions","summary":"  Explanation regularisation (ER) has been introduced as a way to guide models\nto make their predictions in a manner more akin to humans, i.e., making their\nattributions \"plausible\". This is achieved by introducing an auxiliary\nexplanation loss, that measures how well the output of an input attribution\ntechnique for the model agrees with relevant human-annotated rationales. One\npositive outcome of using ER appears to be improved performance in\nout-of-domain (OOD) settings, presumably due to an increased reliance on\n\"plausible\" tokens. However, previous work has under-explored the impact of the\nER objective on model attributions, in particular when obtained with techniques\nother than the one used to train ER. In this work, we contribute a study of\nER's effectiveness at informing classification decisions on plausible tokens,\nand the relationship between increased plausibility and robustness to OOD\nconditions. Through a series of analyses, we find that the connection between\nER and the ability of a classifier to rely on plausible features has been\noverstated and that a stronger reliance on plausible tokens does not seem to be\nthe cause for any perceived OOD improvements.\n","authors":["Pedro Ferreira","Wilker Aziz","Ivan Titov"],"pdf_url":"https://arxiv.org/pdf/2407.16693v1.pdf","comment":"18 pages, 7 figures, 8 tables"},{"id":"http://arxiv.org/abs/2403.09636v2","updated":"2024-07-23T17:55:30Z","published":"2024-03-14T17:59:26Z","title":"Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference","summary":"  Transformers have emerged as the backbone of large language models (LLMs).\nHowever, generation remains inefficient due to the need to store in memory a\ncache of key-value representations for past tokens, whose size scales linearly\nwith the input sequence length and batch size. As a solution, we propose\nDynamic Memory Compression (DMC), a method for online key-value cache\ncompression at inference time. Most importantly, the model learns to apply\ndifferent compression ratios in different heads and layers. We retrofit\npre-trained LLMs such as Llama 2 (7B, 13B and 70B) into DMC Transformers,\nachieving up to 7x throughput increase during auto-regressive inference on an\nNVIDIA H100 GPU. DMC is applied via continued pre-training on a negligible\npercentage of the original data without adding any extra parameters. DMC\npreserves the original downstream performance with up to 4x cache compression,\noutperforming up-trained grouped-query attention (GQA) and key-value eviction\npolicies (H$_2$O, TOVA). GQA and DMC can be even combined to obtain compounded\ngains. Hence, DMC can serve as a drop-in replacement for KV caching in existing\nLLMs to fit longer contexts and larger batches within any given memory budget.\n","authors":["Piotr Nawrot","Adrian Łańcucki","Marcin Chochowski","David Tarjan","Edoardo M. Ponti"],"pdf_url":"https://arxiv.org/pdf/2403.09636v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16686v1","updated":"2024-07-23T17:50:45Z","published":"2024-07-23T17:50:45Z","title":"Can Large Language Models Automatically Jailbreak GPT-4V?","summary":"  GPT-4V has attracted considerable attention due to its extraordinary capacity\nfor integrating and processing multimodal information. At the same time, its\nability of face recognition raises new safety concerns of privacy leakage.\nDespite researchers' efforts in safety alignment through RLHF or preprocessing\nfilters, vulnerabilities might still be exploited. In our study, we introduce\nAutoJailbreak, an innovative automatic jailbreak technique inspired by prompt\noptimization. We leverage Large Language Models (LLMs) for red-teaming to\nrefine the jailbreak prompt and employ weak-to-strong in-context learning\nprompts to boost efficiency. Furthermore, we present an effective search method\nthat incorporates early stopping to minimize optimization time and token\nexpenditure. Our experiments demonstrate that AutoJailbreak significantly\nsurpasses conventional methods, achieving an Attack Success Rate (ASR)\nexceeding 95.3\\%. This research sheds light on strengthening GPT-4V security,\nunderscoring the potential for LLMs to be exploited in compromising GPT-4V\nintegrity.\n","authors":["Yuanwei Wu","Yue Huang","Yixin Liu","Xiang Li","Pan Zhou","Lichao Sun"],"pdf_url":"https://arxiv.org/pdf/2407.16686v1.pdf","comment":"TrustNLP@NAACL2024 (Fourth Workshop on Trustworthy Natural Language\n  Processing)"},{"id":"http://arxiv.org/abs/2407.16667v1","updated":"2024-07-23T17:34:36Z","published":"2024-07-23T17:34:36Z","title":"RedAgent: Red Teaming Large Language Models with Context-aware\n  Autonomous Language Agent","summary":"  Recently, advanced Large Language Models (LLMs) such as GPT-4 have been\nintegrated into many real-world applications like Code Copilot. These\napplications have significantly expanded the attack surface of LLMs, exposing\nthem to a variety of threats. Among them, jailbreak attacks that induce toxic\nresponses through jailbreak prompts have raised critical safety concerns. To\nidentify these threats, a growing number of red teaming approaches simulate\npotential adversarial scenarios by crafting jailbreak prompts to test the\ntarget LLM. However, existing red teaming methods do not consider the unique\nvulnerabilities of LLM in different scenarios, making it difficult to adjust\nthe jailbreak prompts to find context-specific vulnerabilities. Meanwhile,\nthese methods are limited to refining jailbreak templates using a few mutation\noperations, lacking the automation and scalability to adapt to different\nscenarios. To enable context-aware and efficient red teaming, we abstract and\nmodel existing attacks into a coherent concept called \"jailbreak strategy\" and\npropose a multi-agent LLM system named RedAgent that leverages these strategies\nto generate context-aware jailbreak prompts. By self-reflecting on contextual\nfeedback in an additional memory buffer, RedAgent continuously learns how to\nleverage these strategies to achieve effective jailbreaks in specific contexts.\nExtensive experiments demonstrate that our system can jailbreak most black-box\nLLMs in just five queries, improving the efficiency of existing red teaming\nmethods by two times. Additionally, RedAgent can jailbreak customized LLM\napplications more efficiently. By generating context-aware jailbreak prompts\ntowards applications on GPTs, we discover 60 severe vulnerabilities of these\nreal-world applications with only two queries per vulnerability. We have\nreported all found issues and communicated with OpenAI and Meta for bug fixes.\n","authors":["Huiyu Xu","Wenhui Zhang","Zhibo Wang","Feng Xiao","Rui Zheng","Yunhe Feng","Zhongjie Ba","Kui Ren"],"pdf_url":"https://arxiv.org/pdf/2407.16667v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16664v1","updated":"2024-07-23T17:29:02Z","published":"2024-07-23T17:29:02Z","title":"Towards scalable efficient on-device ASR with transfer learning","summary":"  Multilingual pretraining for transfer learning significantly boosts the\nrobustness of low-resource monolingual ASR models. This study systematically\ninvestigates three main aspects: (a) the impact of transfer learning on model\nperformance during initial training or fine-tuning, (b) the influence of\ntransfer learning across dataset domains and languages, and (c) the effect on\nrare-word recognition compared to non-rare words. Our finding suggests that\nRNNT-loss pretraining, followed by monolingual fine-tuning with Minimum Word\nError Rate (MinWER) loss, consistently reduces Word Error Rates (WER) across\nlanguages like Italian and French. WER Reductions (WERR) reach 36.2% and 42.8%\ncompared to monolingual baselines for MLS and in-house datasets. Out-of-domain\npretraining leads to 28% higher WERR than in-domain pretraining. Both rare and\nnon-rare words benefit, with rare words showing greater improvements with\nout-of-domain pretraining, and non-rare words with in-domain pretraining.\n","authors":["Laxmi Pandey","Ke Li","Jinxi Guo","Debjyoti Paul","Arthur Guo","Jay Mahadeokar","Xuedong Zhang"],"pdf_url":"https://arxiv.org/pdf/2407.16664v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16637v1","updated":"2024-07-23T16:54:28Z","published":"2024-07-23T16:54:28Z","title":"Course-Correction: Safety Alignment Using Synthetic Preferences","summary":"  The risk of harmful content generated by large language models (LLMs) becomes\na critical concern. This paper presents a systematic study on assessing and\nimproving LLMs' capability to perform the task of \\textbf{course-correction},\n\\ie, the model can steer away from generating harmful content autonomously. To\nstart with, we introduce the \\textsc{C$^2$-Eval} benchmark for quantitative\nassessment and analyze 10 popular LLMs, revealing varying proficiency of\ncurrent safety-tuned LLMs in course-correction. To improve, we propose\nfine-tuning LLMs with preference learning, emphasizing the preference for\ntimely course-correction. Using an automated pipeline, we create\n\\textsc{C$^2$-Syn}, a synthetic dataset with 750K pairwise preferences, to\nteach models the concept of timely course-correction through data-driven\npreference learning. Experiments on 2 LLMs, \\textsc{Llama2-Chat 7B} and\n\\textsc{Qwen2 7B}, show that our method effectively enhances course-correction\nskills without affecting general performance. Additionally, it effectively\nimproves LLMs' safety, particularly in resisting jailbreak attacks.\n","authors":["Rongwu Xu","Yishuo Cai","Zhenhong Zhou","Renjie Gu","Haiqin Weng","Yan Liu","Tianwei Zhang","Wei Xu","Han Qiu"],"pdf_url":"https://arxiv.org/pdf/2407.16637v1.pdf","comment":"Dataset and script will be available at\n  https://github.com/pillowsofwind/Course-Correction"},{"id":"http://arxiv.org/abs/2407.16624v1","updated":"2024-07-23T16:32:49Z","published":"2024-07-23T16:32:49Z","title":"Semantic Change Characterization with LLMs using Rhetorics","summary":"  Languages continually evolve in response to societal events, resulting in new\nterms and shifts in meanings. These changes have significant implications for\ncomputer applications, including automatic translation and chatbots, making it\nessential to characterize them accurately. The recent development of LLMs has\nnotably advanced natural language understanding, particularly in sense\ninference and reasoning. In this paper, we investigate the potential of LLMs in\ncharacterizing three types of semantic change: dimension, relation, and\norientation. We achieve this by combining LLMs' Chain-of-Thought with\nrhetorical devices and conducting an experimental assessment of our approach\nusing newly created datasets. Our results highlight the effectiveness of LLMs\nin capturing and analyzing semantic changes, providing valuable insights to\nimprove computational linguistic applications.\n","authors":["Jader Martins Camboim de Sá","Marcos Da Silveira","Cédric Pruski"],"pdf_url":"https://arxiv.org/pdf/2407.16624v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16615v1","updated":"2024-07-23T16:23:04Z","published":"2024-07-23T16:23:04Z","title":"Lawma: The Power of Specialization for Legal Tasks","summary":"  Annotation and classification of legal text are central components of\nempirical legal research. Traditionally, these tasks are often delegated to\ntrained research assistants. Motivated by the advances in language modeling,\nempirical legal scholars are increasingly turning to prompting commercial\nmodels, hoping that it will alleviate the significant cost of human annotation.\nDespite growing use, our understanding of how to best utilize large language\nmodels for legal tasks remains limited. We conduct a comprehensive study of 260\nlegal text classification tasks, nearly all new to the machine learning\ncommunity. Starting from GPT-4 as a baseline, we show that it has non-trivial\nbut highly varied zero-shot accuracy, often exhibiting performance that may be\ninsufficient for legal work. We then demonstrate that a lightly fine-tuned\nLlama 3 model vastly outperforms GPT-4 on almost all tasks, typically by\ndouble-digit percentage points. We find that larger models respond better to\nfine-tuning than smaller models. A few tens to hundreds of examples suffice to\nachieve high classification accuracy. Notably, we can fine-tune a single model\non all 260 tasks simultaneously at a small loss in accuracy relative to having\na separate model for each task. Our work points to a viable alternative to the\npredominant practice of prompting commercial models. For concrete legal tasks\nwith some available labeled data, researchers are better off using a fine-tuned\nopen-source model.\n","authors":["Ricardo Dominguez-Olmedo","Vedant Nanda","Rediet Abebe","Stefan Bechtold","Christoph Engel","Jens Frankenreiter","Krishna Gummadi","Moritz Hardt","Michael Livermore"],"pdf_url":"https://arxiv.org/pdf/2407.16615v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16607v1","updated":"2024-07-23T16:13:22Z","published":"2024-07-23T16:13:22Z","title":"Data Mixture Inference: What do BPE Tokenizers Reveal about their\n  Training Data?","summary":"  The pretraining data of today's strongest language models is opaque. In\nparticular, little is known about the proportions of various domains or\nlanguages represented. In this work, we tackle a task which we call data\nmixture inference, which aims to uncover the distributional make-up of training\ndata. We introduce a novel attack based on a previously overlooked source of\ninformation -- byte-pair encoding (BPE) tokenizers, used by the vast majority\nof modern language models. Our key insight is that the ordered list of merge\nrules learned by a BPE tokenizer naturally reveals information about the token\nfrequencies in its training data: the first merge is the most common byte pair,\nthe second is the most common pair after merging the first token, and so on.\nGiven a tokenizer's merge list along with data samples for each category of\ninterest, we formulate a linear program that solves for the proportion of each\ncategory in the tokenizer's training set. Importantly, to the extent to which\ntokenizer training data is representative of the pretraining data, we\nindirectly learn about the pretraining data. In controlled experiments, we show\nthat our attack recovers mixture ratios with high precision for tokenizers\ntrained on known mixtures of natural languages, programming languages, and data\nsources. We then apply our approach to off-the-shelf tokenizers released with\nrecent LMs. We confirm much publicly disclosed information about these models,\nand also make several new inferences: GPT-4o's tokenizer is much more\nmultilingual than its predecessors, training on 39% non-English data; Llama3\nextends GPT-3.5's tokenizer primarily for multilingual (48%) use; GPT-3.5's and\nClaude's tokenizers are trained on predominantly code (~60%). We hope our work\nsheds light on current design practices for pretraining data, and inspires\ncontinued research into data mixture inference for LMs.\n","authors":["Jonathan Hayase","Alisa Liu","Yejin Choi","Sewoong Oh","Noah A. Smith"],"pdf_url":"https://arxiv.org/pdf/2407.16607v1.pdf","comment":"19 pages, 5 figures"},{"id":"http://arxiv.org/abs/2407.16604v1","updated":"2024-07-23T16:06:22Z","published":"2024-07-23T16:06:22Z","title":"Shared Imagination: LLMs Hallucinate Alike","summary":"  Despite the recent proliferation of large language models (LLMs), their\ntraining recipes -- model architecture, pre-training data and optimization\nalgorithm -- are often very similar. This naturally raises the question of the\nsimilarity among the resulting models. In this paper, we propose a novel\nsetting, imaginary question answering (IQA), to better understand model\nsimilarity. In IQA, we ask one model to generate purely imaginary questions\n(e.g., on completely made-up concepts in physics) and prompt another model to\nanswer. Surprisingly, despite the total fictionality of these questions, all\nmodels can answer each other's questions with remarkable success, suggesting a\n\"shared imagination space\" in which these models operate during such\nhallucinations. We conduct a series of investigations into this phenomenon and\ndiscuss implications on model homogeneity, hallucination, and computational\ncreativity.\n","authors":["Yilun Zhou","Caiming Xiong","Silvio Savarese","Chien-Sheng Wu"],"pdf_url":"https://arxiv.org/pdf/2407.16604v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16593v1","updated":"2024-07-23T15:51:46Z","published":"2024-07-23T15:51:46Z","title":"A Comparative Study on Patient Language across Therapeutic Domains for\n  Effective Patient Voice Classification in Online Health Discussions","summary":"  There exists an invisible barrier between healthcare professionals'\nperception of a patient's clinical experience and the reality. This barrier may\nbe induced by the environment that hinders patients from sharing their\nexperiences openly with healthcare professionals. As patients are observed to\ndiscuss and exchange knowledge more candidly on social media, valuable insights\ncan be leveraged from these platforms. However, the abundance of non-patient\nposts on social media necessitates filtering out such irrelevant content to\ndistinguish the genuine voices of patients, a task we refer to as patient voice\nclassification. In this study, we analyse the importance of linguistic\ncharacteristics in accurately classifying patient voices. Our findings\nunderscore the essential role of linguistic and statistical text similarity\nanalysis in identifying common patterns among patient groups. These results\nallude to even starker differences in the way patients express themselves at a\ndisease level and across various therapeutic domains. Additionally, we\nfine-tuned a pre-trained Language Model on the combined datasets with similar\nlinguistic patterns, resulting in a highly accurate automatic patient voice\nclassification. Being the pioneering study on the topic, our focus on\nextracting authentic patient experiences from social media stands as a crucial\nstep towards advancing healthcare standards and fostering a patient-centric\napproach.\n","authors":["Giorgos Lysandrou","Roma English Owen","Vanja Popovic","Grant Le Brun","Aryo Pradipta Gema","Beatrice Alex","Elizabeth A. L. Fairley"],"pdf_url":"https://arxiv.org/pdf/2407.16593v1.pdf","comment":"14 pages, 4 figures, 5 tables, funded by Talking Medicines Limited"},{"id":"http://arxiv.org/abs/2407.01406v2","updated":"2024-07-23T15:51:12Z","published":"2024-07-01T15:56:24Z","title":"Adapting Multilingual LLMs to Low-Resource Languages with Knowledge\n  Graphs via Adapters","summary":"  This paper explores the integration of graph knowledge from linguistic\nontologies into multilingual Large Language Models (LLMs) using adapters to\nimprove performance for low-resource languages (LRLs) in sentiment analysis\n(SA) and named entity recognition (NER). Building upon successful\nparameter-efficient fine-tuning techniques, such as K-ADAPTER and MAD-X, we\npropose a similar approach for incorporating knowledge from multilingual\ngraphs, connecting concepts in various languages with each other through\nlinguistic relationships, into multilingual LLMs for LRLs. Specifically, we\nfocus on eight LRLs -- Maltese, Bulgarian, Indonesian, Nepali, Javanese,\nUyghur, Tibetan, and Sinhala -- and employ language-specific adapters\nfine-tuned on data extracted from the language-specific section of ConceptNet,\naiming to enable knowledge transfer across the languages covered by the\nknowledge graph. We compare various fine-tuning objectives, including standard\nMasked Language Modeling (MLM), MLM with full-word masking, and MLM with\ntargeted masking, to analyse their effectiveness in learning and integrating\nthe extracted graph data. Through empirical evaluation on language-specific\ntasks, we assess how structured graph knowledge affects the performance of\nmultilingual LLMs for LRLs in SA and NER, providing insights into the potential\nbenefits of adapting language models for low-resource scenarios.\n","authors":["Daniil Gurgurov","Mareike Hartmann","Simon Ostermann"],"pdf_url":"https://arxiv.org/pdf/2407.01406v2.pdf","comment":"9 pages, KaLLM workshop"},{"id":"http://arxiv.org/abs/2407.16574v1","updated":"2024-07-23T15:27:37Z","published":"2024-07-23T15:27:37Z","title":"TLCR: Token-Level Continuous Reward for Fine-grained Reinforcement\n  Learning from Human Feedback","summary":"  Reinforcement Learning from Human Feedback (RLHF) leverages human preference\ndata to train language models to align more closely with human essence. These\nhuman preference data, however, are labeled at the sequence level, creating a\nmismatch between sequence-level preference labels and tokens, which are\nautoregressively generated from the language model. Although several recent\napproaches have tried to provide token-level (i.e., dense) rewards for each\nindividual token, these typically rely on predefined discrete reward values\n(e.g., positive: +1, negative: -1, neutral: 0), failing to account for varying\ndegrees of preference inherent to each token. To address this limitation, we\nintroduce TLCR (Token-Level Continuous Reward) for RLHF, which incorporates a\ndiscriminator trained to distinguish positive and negative tokens, and the\nconfidence of the discriminator is used to assign continuous rewards to each\ntoken considering the context. Extensive experiments show that our proposed\nTLCR leads to consistent performance improvements over previous sequence-level\nor token-level discrete rewards on open-ended generation benchmarks.\n","authors":["Eunseop Yoon","Hee Suk Yoon","SooHwan Eom","Gunsoo Han","Daniel Wontae Nam","Daejin Jo","Kyoung-Woon On","Mark A. Hasegawa-Johnson","Sungwoong Kim","Chang D. Yoo"],"pdf_url":"https://arxiv.org/pdf/2407.16574v1.pdf","comment":"ACL2024 Findings"},{"id":"http://arxiv.org/abs/2407.16565v1","updated":"2024-07-23T15:17:11Z","published":"2024-07-23T15:17:11Z","title":"Retrieve, Generate, Evaluate: A Case Study for Medical Paraphrases\n  Generation with Small Language Models","summary":"  Recent surge in the accessibility of large language models (LLMs) to the\ngeneral population can lead to untrackable use of such models for\nmedical-related recommendations. Language generation via LLMs models has two\nkey problems: firstly, they are prone to hallucination and therefore, for any\nmedical purpose they require scientific and factual grounding; secondly, LLMs\npose tremendous challenge to computational resources due to their gigantic\nmodel size. In this work, we introduce pRAGe, a pipeline for Retrieval\nAugmented Generation and evaluation of medical paraphrases generation using\nSmall Language Models (SLM). We study the effectiveness of SLMs and the impact\nof external knowledge base for medical paraphrase generation in French.\n","authors":["Ioana Buhnila","Aman Sinha","Mathieu Constant"],"pdf_url":"https://arxiv.org/pdf/2407.16565v1.pdf","comment":"KnowledgeableLM 2024"},{"id":"http://arxiv.org/abs/2407.15009v2","updated":"2024-07-23T15:13:03Z","published":"2024-06-11T18:59:43Z","title":"RogueGPT: dis-ethical tuning transforms ChatGPT4 into a Rogue AI in 158\n  Words","summary":"  The ethical implications and potentials for misuse of Generative Artificial\nIntelligence are increasingly worrying topics. This paper explores how easily\nthe default ethical guardrails of ChatGPT, using its latest customization\nfeatures, can be bypassed by simple prompts and fine-tuning, that can be\neffortlessly accessed by the broad public. This malevolently altered version of\nChatGPT, nicknamed \"RogueGPT\", responded with worrying behaviours, beyond those\ntriggered by jailbreak prompts. We conduct an empirical study of RogueGPT\nresponses, assessing its flexibility in answering questions pertaining to what\nshould be disallowed usage. Our findings raise significant concerns about the\nmodel's knowledge about topics like illegal drug production, torture methods\nand terrorism. The ease of driving ChatGPT astray, coupled with its global\naccessibility, highlights severe issues regarding the data quality used for\ntraining the foundational model and the implementation of ethical safeguards.\nWe thus underline the responsibilities and dangers of user-driven\nmodifications, and the broader effects that these may have on the design of\nsafeguarding and ethical modules implemented by AI programmers.\n","authors":["Alessio Buscemi","Daniele Proverbio"],"pdf_url":"https://arxiv.org/pdf/2407.15009v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15047v2","updated":"2024-07-23T14:56:22Z","published":"2024-07-21T04:09:37Z","title":"End-to-End Video Question Answering with Frame Scoring Mechanisms and\n  Adaptive Sampling","summary":"  Video Question Answering (VideoQA) has emerged as a challenging frontier in\nthe field of multimedia processing, requiring intricate interactions between\nvisual and textual modalities. Simply uniformly sampling frames or\nindiscriminately aggregating frame-level visual features often falls short in\ncapturing the nuanced and relevant contexts of videos to well perform VideoQA.\nTo mitigate these issues, we propose VidF4, a novel VideoQA framework equipped\nwith tailored frame selection strategy for effective and efficient VideoQA. We\npropose three frame-scoring mechanisms that consider both question relevance\nand inter-frame similarity to evaluate the importance of each frame for a given\nquestion on the video. Furthermore, we design a differentiable adaptive frame\nsampling mechanism to facilitate end-to-end training for the frame selector and\nanswer generator. The experimental results across three widely adopted\nbenchmarks demonstrate that our model consistently outperforms existing VideoQA\nmethods, establishing a new SOTA across NExT-QA (+0.3%), STAR (+0.9%), and TVQA\n(+1.0%). Furthermore, through both quantitative and qualitative analyses, we\nvalidate the effectiveness of each design choice.\n","authors":["Jianxin Liang","Xiaojun Meng","Yueqian Wang","Chang Liu","Qun Liu","Dongyan Zhao"],"pdf_url":"https://arxiv.org/pdf/2407.15047v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.13782v3","updated":"2024-07-23T14:49:43Z","published":"2024-01-24T20:05:49Z","title":"Position: AI/ML Influencers Have a Place in the Academic Process","summary":"  As the number of accepted papers at AI and ML conferences reaches into the\nthousands, it has become unclear how researchers access and read research\npublications. In this paper, we investigate the role of social media\ninfluencers in enhancing the visibility of machine learning research,\nparticularly the citation counts of papers they share. We have compiled a\ncomprehensive dataset of over 8,000 papers, spanning tweets from December 2018\nto October 2023, alongside controls precisely matched by 9 key covariates. Our\nstatistical and causal inference analysis reveals a significant increase in\ncitations for papers endorsed by these influencers, with median citation counts\n2-3 times higher than those of the control group. Additionally, the study\ndelves into the geographic, gender, and institutional diversity of highlighted\nauthors. Given these findings, we advocate for a responsible approach to\ncuration, encouraging influencers to uphold the journalistic standard that\nincludes showcasing diverse research topics, authors, and institutions.\n","authors":["Iain Xie Weissburg","Mehir Arora","Xinyi Wang","Liangming Pan","William Yang Wang"],"pdf_url":"https://arxiv.org/pdf/2401.13782v3.pdf","comment":"15 Pages, 22 Figures, ICML 2024"},{"id":"http://arxiv.org/abs/2407.16537v1","updated":"2024-07-23T14:47:25Z","published":"2024-07-23T14:47:25Z","title":"Quantifying the Role of Textual Predictability in Automatic Speech\n  Recognition","summary":"  A long-standing question in automatic speech recognition research is how to\nattribute errors to the ability of a model to model the acoustics, versus its\nability to leverage higher-order context (lexicon, morphology, syntax,\nsemantics). We validate a novel approach which models error rates as a function\nof relative textual predictability, and yields a single number, $k$, which\nmeasures the effect of textual predictability on the recognizer. We use this\nmethod to demonstrate that a Wav2Vec 2.0-based model makes greater stronger use\nof textual context than a hybrid ASR model, in spite of not using an explicit\nlanguage model, and also use it to shed light on recent results demonstrating\npoor performance of standard ASR systems on African-American English. We\ndemonstrate that these mostly represent failures of acoustic--phonetic\nmodelling. We show how this approach can be used straightforwardly in\ndiagnosing and improving ASR.\n","authors":["Sean Robertson","Gerald Penn","Ewan Dunbar"],"pdf_url":"https://arxiv.org/pdf/2407.16537v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16526v1","updated":"2024-07-23T14:39:40Z","published":"2024-07-23T14:39:40Z","title":"Imperfect Vision Encoders: Efficient and Robust Tuning for\n  Vision-Language Models","summary":"  Vision language models (VLMs) demonstrate impressive capabilities in visual\nquestion answering and image captioning, acting as a crucial link between\nvisual and language models. However, existing open-source VLMs heavily rely on\npretrained and frozen vision encoders (such as CLIP). Despite CLIP's robustness\nacross diverse domains, it still exhibits non-negligible image understanding\nerrors. These errors propagate to the VLM responses, resulting in sub-optimal\nperformance. In our work, we propose an efficient and robust method for\nupdating vision encoders within VLMs. Our approach selectively and locally\nupdates encoders, leading to substantial performance improvements on data where\nprevious mistakes occurred, while maintaining overall robustness. Furthermore,\nwe demonstrate the effectiveness of our method during continual few-shot\nupdates. Theoretical grounding, generality, and computational efficiency\ncharacterize our approach.\n","authors":["Aristeidis Panos","Rahaf Aljundi","Daniel Olmeda Reino","Richard E Turner"],"pdf_url":"https://arxiv.org/pdf/2407.16526v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08320v4","updated":"2024-07-23T14:39:23Z","published":"2023-10-12T13:33:04Z","title":"Defending Our Privacy With Backdoors","summary":"  The proliferation of large AI models trained on uncurated, often sensitive\nweb-scraped data has raised significant privacy concerns. One of the concerns\nis that adversaries can extract information about the training data using\nprivacy attacks. Unfortunately, the task of removing specific information from\nthe models without sacrificing performance is not straightforward and has\nproven to be challenging. We propose a rather easy yet effective defense based\non backdoor attacks to remove private information, such as names and faces of\nindividuals, from vision-language models by fine-tuning them for only a few\nminutes instead of re-training them from scratch. Specifically, by\nstrategically inserting backdoors into text encoders, we align the embeddings\nof sensitive phrases with those of neutral terms-\"a person\" instead of the\nperson's actual name. For image encoders, we map individuals' embeddings to be\nremoved from the model to a universal, anonymous embedding. The results of our\nextensive experimental evaluation demonstrate the effectiveness of our\nbackdoor-based defense on CLIP by assessing its performance using a specialized\nprivacy attack for zero-shot classifiers. Our approach provides a new\n\"dual-use\" perspective on backdoor attacks and presents a promising avenue to\nenhance the privacy of individuals within models trained on uncurated\nweb-scraped data.\n","authors":["Dominik Hintersdorf","Lukas Struppek","Daniel Neider","Kristian Kersting"],"pdf_url":"https://arxiv.org/pdf/2310.08320v4.pdf","comment":"Accepted at ECAI 2024"},{"id":"http://arxiv.org/abs/2405.11357v3","updated":"2024-07-23T14:39:06Z","published":"2024-05-18T18:08:58Z","title":"Large Language Models Lack Understanding of Character Composition of\n  Words","summary":"  Large language models (LLMs) have demonstrated remarkable performances on a\nwide range of natural language tasks. Yet, LLMs' successes have been largely\nrestricted to tasks concerning words, sentences, or documents, and it remains\nquestionable how much they understand the minimal units of text, namely\ncharacters. In this paper, we examine contemporary LLMs regarding their ability\nto understand character composition of words, and show that most of them fail\nto reliably carry out even the simple tasks that can be handled by humans with\nperfection. We analyze their behaviors with comparison to token level\nperformances, and discuss the potential directions for future research.\n","authors":["Andrew Shin","Kunitake Kaneko"],"pdf_url":"https://arxiv.org/pdf/2405.11357v3.pdf","comment":"ICML 2024 Workshop on Large Language Models and Cognition"},{"id":"http://arxiv.org/abs/2407.16521v1","updated":"2024-07-23T14:34:38Z","published":"2024-07-23T14:34:38Z","title":"AMONGAGENTS: Evaluating Large Language Models in the Interactive\n  Text-Based Social Deduction Game","summary":"  Strategic social deduction games serve as valuable testbeds for evaluating\nthe understanding and inference skills of language models, offering crucial\ninsights into social science, artificial intelligence, and strategic gaming.\nThis paper focuses on creating proxies of human behavior in simulated\nenvironments, with \\textit{Among Us} utilized as a tool for studying simulated\nhuman behavior.\n  The study introduces a text-based game environment, named AmongAgent, that\nmirrors the dynamics of \\textit{Among Us}. Players act as crew members aboard a\nspaceship, tasked with identifying impostors who are sabotaging the ship and\neliminating the crew. Within this environment, the behavior of simulated\nlanguage agents is analyzed. The experiments involve diverse game sequences\nfeaturing different configurations of Crewmates and Impostor personality\narchetypes. Our work demonstrates that state-of-the-art large language models\n(LLMs) can effectively grasp the game rules and make decisions based on the\ncurrent context. This work aims to promote further exploration of LLMs in\ngoal-oriented games with incomplete information and complex action spaces, as\nthese settings offer valuable opportunities to assess language model\nperformance in socially driven scenarios.\n","authors":["Yizhou Chi","Lingjun Mao","Zineng Tang"],"pdf_url":"https://arxiv.org/pdf/2407.16521v1.pdf","comment":"Wordplay @ ACL 2024"},{"id":"http://arxiv.org/abs/2407.16516v1","updated":"2024-07-23T14:31:59Z","published":"2024-07-23T14:31:59Z","title":"Assessing In-context Learning and Fine-tuning for Topic Classification\n  of German Web Data","summary":"  Researchers in the political and social sciences often rely on classification\nmodels to analyze trends in information consumption by examining browsing\nhistories of millions of webpages. Automated scalable methods are necessary due\nto the impracticality of manual labeling. In this paper, we model the detection\nof topic-related content as a binary classification task and compare the\naccuracy of fine-tuned pre-trained encoder models against in-context learning\nstrategies. Using only a few hundred annotated data points per topic, we detect\ncontent related to three German policies in a database of scraped webpages. We\ncompare multilingual and monolingual models, as well as zero and few-shot\napproaches, and investigate the impact of negative sampling strategies and the\ncombination of URL & content-based features. Our results show that a small\nsample of annotated data is sufficient to train an effective classifier.\nFine-tuning encoder-based models yields better results than in-context\nlearning. Classifiers using both URL & content-based features perform best,\nwhile using URLs alone provides adequate results when content is unavailable.\n","authors":["Julian Schelb","Roberto Ulloa","Andreas Spitz"],"pdf_url":"https://arxiv.org/pdf/2407.16516v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.03302v2","updated":"2024-07-23T14:19:18Z","published":"2024-04-04T08:52:30Z","title":"How Easily do Irrelevant Inputs Skew the Responses of Large Language\n  Models?","summary":"  By leveraging the retrieval of information from external knowledge databases,\nLarge Language Models (LLMs) exhibit enhanced capabilities for accomplishing\nmany knowledge-intensive tasks. However, due to the inherent flaws of current\nretrieval systems, there might exist irrelevant information within those\nretrieving top-ranked passages. In this work, we present a comprehensive\ninvestigation into the robustness of LLMs to different types of irrelevant\ninformation under various conditions. We initially introduce a framework to\nconstruct high-quality irrelevant information that ranges from semantically\nunrelated, partially related, and related to questions. Furthermore, our\nanalysis demonstrates that the constructed irrelevant information not only\nscores highly on similarity metrics, being highly retrieved by existing\nsystems, but also bears semantic connections to the context. Our investigation\nreveals that current LLMs still face challenges in discriminating highly\nsemantically related information and can be easily distracted by these\nirrelevant yet misleading content. Besides, we also find that current solutions\nfor handling irrelevant information have limitations in improving the\nrobustness of LLMs to such distractions. All the resources are available on\nGitHub at https://github.com/Di-viner/LLM-Robustness-to-Irrelevant-Information.\n","authors":["Siye Wu","Jian Xie","Jiangjie Chen","Tinghui Zhu","Kai Zhang","Yanghua Xiao"],"pdf_url":"https://arxiv.org/pdf/2404.03302v2.pdf","comment":"COLM 2024"},{"id":"http://arxiv.org/abs/2401.14656v2","updated":"2024-07-23T13:56:42Z","published":"2024-01-26T05:33:34Z","title":"Scientific Large Language Models: A Survey on Biological & Chemical\n  Domains","summary":"  Large Language Models (LLMs) have emerged as a transformative power in\nenhancing natural language comprehension, representing a significant stride\ntoward artificial general intelligence. The application of LLMs extends beyond\nconventional linguistic boundaries, encompassing specialized linguistic systems\ndeveloped within various scientific disciplines. This growing interest has led\nto the advent of scientific LLMs, a novel subclass specifically engineered for\nfacilitating scientific discovery. As a burgeoning area in the community of AI\nfor Science, scientific LLMs warrant comprehensive exploration. However, a\nsystematic and up-to-date survey introducing them is currently lacking. In this\npaper, we endeavor to methodically delineate the concept of \"scientific\nlanguage\", whilst providing a thorough review of the latest advancements in\nscientific LLMs. Given the expansive realm of scientific disciplines, our\nanalysis adopts a focused lens, concentrating on the biological and chemical\ndomains. This includes an in-depth examination of LLMs for textual knowledge,\nsmall molecules, macromolecular proteins, genomic sequences, and their\ncombinations, analyzing them in terms of model architectures, capabilities,\ndatasets, and evaluation. Finally, we critically examine the prevailing\nchallenges and point out promising research directions along with the advances\nof LLMs. By offering a comprehensive overview of technical developments in this\nfield, this survey aspires to be an invaluable resource for researchers\nnavigating the intricate landscape of scientific LLMs.\n","authors":["Qiang Zhang","Keyang Ding","Tianwen Lyv","Xinda Wang","Qingyu Yin","Yiwen Zhang","Jing Yu","Yuhao Wang","Xiaotong Li","Zhuoyi Xiang","Kehua Feng","Xiang Zhuang","Zeyuan Wang","Ming Qin","Mengyao Zhang","Jinlu Zhang","Jiyu Cui","Tao Huang","Pengju Yan","Renjun Xu","Hongyang Chen","Xiaolin Li","Xiaohui Fan","Huabin Xing","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2401.14656v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.02984v2","updated":"2024-07-23T13:56:20Z","published":"2024-05-05T16:07:23Z","title":"E-TSL: A Continuous Educational Turkish Sign Language Dataset with\n  Baseline Methods","summary":"  This study introduces the continuous Educational Turkish Sign Language\n(E-TSL) dataset, collected from online Turkish language lessons for 5th, 6th,\nand 8th grades. The dataset comprises 1,410 videos totaling nearly 24 hours and\nincludes performances from 11 signers. Turkish, an agglutinative language,\nposes unique challenges for sign language translation, particularly with a\nvocabulary where 64% are singleton words and 85% are rare words, appearing less\nthan five times. We developed two baseline models to address these challenges:\nthe Pose to Text Transformer (P2T-T) and the Graph Neural Network based\nTransformer (GNN-T) models. The GNN-T model achieved 19.13% BLEU-1 score and\n3.28% BLEU-4 score, presenting a significant challenge compared to existing\nbenchmarks. The P2T-T model, while demonstrating slightly lower performance in\nBLEU scores, achieved a higher ROUGE-L score of 22.09%. Additionally, we\nbenchmarked our model using the well-known PHOENIX-Weather 2014T dataset to\nvalidate our approach.\n","authors":["Şükrü Öztürk","Hacer Yalim Keles"],"pdf_url":"https://arxiv.org/pdf/2405.02984v2.pdf","comment":"7 pages, 3 figures, 4 tables"},{"id":"http://arxiv.org/abs/2402.14973v3","updated":"2024-07-23T13:54:16Z","published":"2024-02-22T21:22:04Z","title":"GenCeption: Evaluate Multimodal LLMs with Unlabeled Unimodal Data","summary":"  Multimodal Large Language Models (MLLMs) are typically assessed using\nexpensive annotated multimodal benchmarks, which often lag behind the rapidly\nevolving demands of MLLM evaluation. This paper outlines and validates\nGenCeption, a novel, annotation-free evaluation method that requires only\nunimodal data to measure inter-modality semantic coherence and inversely\nassesses MLLMs' tendency to hallucinate. This approach eliminates the need for\ncostly data annotation, minimizes the risk of training data contamination,\nresults in slower benchmark saturation, and avoids the illusion of emerging\nabilities. Inspired by the DrawCeption game, GenCeption begins with a\nnon-textual sample and proceeds through iterative description and generation\nsteps. The semantic drift across iterations is quantified using the GC@T\nmetric. Based on the GenCeption method, we establish the MMECeption benchmark\nfor evaluating Vision LLMs (VLLMs), and compare performance of several popular\nVLLMs and human annotators. Our empirical results validate GenCeption's\neffectiveness, demonstrating strong correlations with established VLLM\nbenchmarks. VLLMs still significantly lack behind human performance and\nstruggle especially with text-intensive tasks.\n","authors":["Lele Cao","Valentin Buchner","Zineb Senane","Fangkai Yang"],"pdf_url":"https://arxiv.org/pdf/2402.14973v3.pdf","comment":"Significantly extended from v2. Source code:\n  https://github.com/llcresearch/GenCeption. Leaderboard:\n  https://huggingface.co/spaces/valbuc/GenCeption"},{"id":"http://arxiv.org/abs/2407.16470v1","updated":"2024-07-23T13:40:54Z","published":"2024-07-23T13:40:54Z","title":"Machine Translation Hallucination Detection for Low and High Resource\n  Languages using Large Language Models","summary":"  Recent advancements in massively multilingual machine translation systems\nhave significantly enhanced translation accuracy; however, even the best\nperforming systems still generate hallucinations, severely impacting user\ntrust. Detecting hallucinations in Machine Translation (MT) remains a critical\nchallenge, particularly since existing methods excel with High-Resource\nLanguages (HRLs) but exhibit substantial limitations when applied to\nLow-Resource Languages (LRLs). This paper evaluates hallucination detection\napproaches using Large Language Models (LLMs) and semantic similarity within\nmassively multilingual embeddings. Our study spans 16 language directions,\ncovering HRLs, LRLs, with diverse scripts. We find that the choice of model is\nessential for performance. On average, for HRLs, Llama3-70B outperforms the\nprevious state of the art by as much as 0.16 MCC (Matthews Correlation\nCoefficient). However, for LRLs we observe that Claude Sonnet outperforms other\nLLMs on average by 0.03 MCC. The key takeaway from our study is that LLMs can\nachieve performance comparable or even better than previously proposed models,\ndespite not being explicitly trained for any machine translation task. However,\ntheir advantage is less significant for LRLs.\n","authors":["Kenza Benkirane","Laura Gongas","Shahar Pelles","Naomi Fuchs","Joshua Darmon","Pontus Stenetorp","David Ifeoluwa Adelani","Eduardo Sanchez"],"pdf_url":"https://arxiv.org/pdf/2407.16470v1.pdf","comment":"Authors Kenza Benkirane and Laura Gongas contributed equally to this\n  work"},{"id":"http://arxiv.org/abs/2407.11484v4","updated":"2024-07-23T13:18:31Z","published":"2024-07-16T08:20:39Z","title":"The Oscars of AI Theater: A Survey on Role-Playing with Language Models","summary":"  This survey explores the burgeoning field of role-playing with language\nmodels, focusing on their development from early persona-based models to\nadvanced character-driven simulations facilitated by Large Language Models\n(LLMs). Initially confined to simple persona consistency due to limited model\ncapabilities, role-playing tasks have now expanded to embrace complex character\nportrayals involving character consistency, behavioral alignment, and overall\nattractiveness. We provide a comprehensive taxonomy of the critical components\nin designing these systems, including data, models and alignment, agent\narchitecture and evaluation. This survey not only outlines the current\nmethodologies and challenges, such as managing dynamic personal profiles and\nachieving high-level persona consistency but also suggests avenues for future\nresearch in improving the depth and realism of role-playing applications. The\ngoal is to guide future research by offering a structured overview of current\nmethodologies and identifying potential areas for improvement. Related\nresources and papers are available at\nhttps://github.com/nuochenpku/Awesome-Role-Play-Papers.\n","authors":["Nuo Chen","Yang Deng","Jia Li"],"pdf_url":"https://arxiv.org/pdf/2407.11484v4.pdf","comment":"28 pages"},{"id":"http://arxiv.org/abs/2407.16444v1","updated":"2024-07-23T12:53:41Z","published":"2024-07-23T12:53:41Z","title":"Psychomatics -- A Multidisciplinary Framework for Understanding\n  Artificial Minds","summary":"  Although LLMs and other artificial intelligence systems demonstrate cognitive\nskills similar to humans, like concept learning and language acquisition, the\nway they process information fundamentally differs from biological cognition.\nTo better understand these differences this paper introduces Psychomatics, a\nmultidisciplinary framework bridging cognitive science, linguistics, and\ncomputer science. It aims to better understand the high-level functioning of\nLLMs, focusing specifically on how LLMs acquire, learn, remember, and use\ninformation to produce their outputs. To achieve this goal, Psychomatics will\nrely on a comparative methodology, starting from a theory-driven research\nquestion - is the process of language development and use different in humans\nand LLMs? - drawing parallels between LLMs and biological systems. Our analysis\nshows how LLMs can map and manipulate complex linguistic patterns in their\ntraining data. Moreover, LLMs can follow Grice's Cooperative Principle to\nprovide relevant and informative responses. However, human cognition draws from\nmultiple sources of meaning, including experiential, emotional, and imaginative\nfacets, which transcend mere language processing and are rooted in our social\nand developmental trajectories. Moreover, current LLMs lack physical\nembodiment, reducing their ability to make sense of the intricate interplay\nbetween perception, action, and cognition that shapes human understanding and\nexpression. Ultimately, Psychomatics holds the potential to yield\ntransformative insights into the nature of language, cognition, and\nintelligence, both artificial and biological. Moreover, by drawing parallels\nbetween LLMs and human cognitive processes, Psychomatics can inform the\ndevelopment of more robust and human-like AI systems.\n","authors":["Giuseppe Riva","Fabrizia Mantovani","Brenda K. Wiederhold","Antonella Marchetti","Andrea Gaggioli"],"pdf_url":"https://arxiv.org/pdf/2407.16444v1.pdf","comment":"15 pages, 4 tables, 2 figures"},{"id":"http://arxiv.org/abs/2407.16434v1","updated":"2024-07-23T12:33:58Z","published":"2024-07-23T12:33:58Z","title":"Enhancing LLM's Cognition via Structurization","summary":"  When reading long-form text, human cognition is complex and structurized.\nWhile large language models (LLMs) process input contexts through a causal and\nsequential perspective, this approach can potentially limit their ability to\nhandle intricate and complex inputs effectively. To enhance LLM's cognition\ncapability, this paper presents a novel concept of context structurization.\nSpecifically, we transform the plain, unordered contextual sentences into\nwell-ordered and hierarchically structurized elements. By doing so, LLMs can\nbetter grasp intricate and extended contexts through precise attention and\ninformation-seeking along the organized structures. Extensive evaluations are\nconducted across various model architectures and sizes (including several 7B-\nto 72B-size auto-regressive LLMs as well as BERT-like masking models) on a\ndiverse set of NLP tasks (e.g., context-based question-answering, exhaustive\nhallucination evaluation, and passage-level dense retrieval). Empirical results\nshow consistent and significant performance gains afforded by a single-round\nstructurization. In particular, we boost a 72B-parameter open-source model to\nachieve comparable performance against GPT-3.5-Turbo as the hallucination\nevaluator. Besides, we show the feasibility of distilling advanced LLMs'\nlanguage processing abilities to a smaller yet effective StruXGPT-7B to execute\nstructurization, addressing the practicality of our approach. Code will be made\npublic soon.\n","authors":["Kai Liu","Zhihang Fu","Chao Chen","Wei Zhang","Rongxin Jiang","Fan Zhou","Yaowu Chen","Yue Wu","Jieping Ye"],"pdf_url":"https://arxiv.org/pdf/2407.16434v1.pdf","comment":"N/A"},{"id":"http://arxiv.org/abs/2407.16431v1","updated":"2024-07-23T12:29:37Z","published":"2024-07-23T12:29:37Z","title":"FairFlow: An Automated Approach to Model-based Counterfactual Data\n  Augmentation For NLP","summary":"  Despite the evolution of language models, they continue to portray harmful\nsocietal biases and stereotypes inadvertently learned from training data. These\ninherent biases often result in detrimental effects in various applications.\nCounterfactual Data Augmentation (CDA), which seeks to balance demographic\nattributes in training data, has been a widely adopted approach to mitigate\nbias in natural language processing. However, many existing CDA approaches rely\non word substitution techniques using manually compiled word-pair dictionaries.\nThese techniques often lead to out-of-context substitutions, resulting in\npotential quality issues. The advancement of model-based techniques, on the\nother hand, has been challenged by the need for parallel training data. Works\nin this area resort to manually generated parallel data that are expensive to\ncollect and are consequently limited in scale. This paper proposes FairFlow, an\nautomated approach to generating parallel data for training counterfactual text\ngenerator models that limits the need for human intervention. Furthermore, we\nshow that FairFlow significantly overcomes the limitations of dictionary-based\nword-substitution approaches whilst maintaining good performance.\n","authors":["Ewoenam Kwaku Tokpo","Toon Calders"],"pdf_url":"https://arxiv.org/pdf/2407.16431v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.09252v2","updated":"2024-07-23T12:28:31Z","published":"2024-07-12T13:30:44Z","title":"Context Embeddings for Efficient Answer Generation in RAG","summary":"  Retrieval-Augmented Generation (RAG) allows overcoming the limited knowledge\nof LLMs by extending the input with external information. As a consequence, the\ncontextual inputs to the model become much longer which slows down decoding\ntime directly translating to the time a user has to wait for an answer. We\naddress this challenge by presenting COCOM, an effective context compression\nmethod, reducing long contexts to only a handful of Context Embeddings speeding\nup the generation time by a large margin. Our method allows for different\ncompression rates trading off decoding time for answer quality. Compared to\nearlier methods, COCOM allows for handling multiple contexts more effectively,\nsignificantly reducing decoding time for long inputs. Our method demonstrates a\nspeed-up of up to 5.69 $\\times$ while achieving higher performance compared to\nexisting efficient context compression methods.\n","authors":["David Rau","Shuai Wang","Hervé Déjean","Stéphane Clinchant"],"pdf_url":"https://arxiv.org/pdf/2407.09252v2.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2404.07546v2","updated":"2024-07-23T12:28:14Z","published":"2024-04-11T08:20:10Z","title":"Does In-Context Learning Really Learn? Rethinking How Large Language\n  Models Respond and Solve Tasks via In-Context Learning","summary":"  In-context Learning (ICL) has emerged as a powerful capability alongside the\ndevelopment of scaled-up large language models (LLMs). By instructing LLMs\nusing few-shot demonstrative examples, ICL enables them to perform a wide range\nof tasks without updating millions of parameters. However, the precise\ncontributions of demonstrations towards improving end-task performance have not\nbeen thoroughly investigated in recent analytical studies. In this paper, we\nempirically decompose the overall performance of ICL into three dimensions,\nlabel space, format, and discrimination, and we evaluate four general-purpose\nLLMs across a diverse range of tasks. Counter-intuitively, we find that the\ndemonstrations have a marginal impact on provoking discriminative knowledge of\nlanguage models. However, ICL exhibits significant efficacy in regulating the\nlabel space and format, which helps LLMs respond to desired label words. We\nthen demonstrate that this ability functions similar to detailed instructions\nfor LLMs to follow. We additionally provide an in-depth analysis of the\nmechanism of retrieval helping with ICL. Our findings demonstrate that\nretrieving the semantically similar examples notably boosts the model's\ndiscriminative capability. However, we also observe a trade-off in selecting\ngood in-context examples regarding label diversity.\n","authors":["Quanyu Long","Yin Wu","Wenya Wang","Sinno Jialin Pan"],"pdf_url":"https://arxiv.org/pdf/2404.07546v2.pdf","comment":"39 pages, 8 figures. Accepted by Conference On Language Modeling\n  (COLM) 2024"},{"id":"http://arxiv.org/abs/2404.03381v3","updated":"2024-07-23T11:54:10Z","published":"2024-04-04T11:27:54Z","title":"Learning to Plan and Generate Text with Citations","summary":"  The increasing demand for the deployment of LLMs in information-seeking\nscenarios has spurred efforts in creating verifiable systems, which generate\nresponses to queries along with supporting evidence. In this paper, we explore\nthe attribution capabilities of plan-based models which have been recently\nshown to improve the faithfulness, grounding, and controllability of generated\ntext. We conceptualize plans as a sequence of questions which serve as\nblueprints of the generated content and its organization. We propose two\nattribution models that utilize different variants of blueprints, an\nabstractive model where questions are generated from scratch, and an extractive\nmodel where questions are copied from the input. Experiments on long-form\nquestion-answering show that planning consistently improves attribution\nquality. Moreover, the citations generated by blueprint models are more\naccurate compared to those obtained from LLM-based pipelines lacking a planning\ncomponent.\n","authors":["Constanza Fierro","Reinald Kim Amplayo","Fantine Huot","Nicola De Cao","Joshua Maynez","Shashi Narayan","Mirella Lapata"],"pdf_url":"https://arxiv.org/pdf/2404.03381v3.pdf","comment":"Accepted at ACL 2024"},{"id":"http://arxiv.org/abs/2404.07622v2","updated":"2024-07-23T11:50:03Z","published":"2024-04-11T10:16:44Z","title":"Language Models Meet Anomaly Detection for Better Interpretability and\n  Generalizability","summary":"  This research explores the integration of language models and unsupervised\nanomaly detection in medical imaging, addressing two key questions: (1) Can\nlanguage models enhance the interpretability of anomaly detection maps? and (2)\nCan anomaly maps improve the generalizability of language models in open-set\nanomaly detection tasks? To investigate these questions, we introduce a new\ndataset for multi-image visual question-answering on brain magnetic resonance\nimages encompassing multiple conditions. We propose KQ-Former (Knowledge\nQuerying Transformer), which is designed to optimally align visual and textual\ninformation in limited-sample contexts. Our model achieves a 60.81% accuracy on\nclosed questions, covering disease classification and severity across 15\ndifferent classes. For open questions, KQ-Former demonstrates a 70% improvement\nover the baseline with a BLEU-4 score of 0.41, and achieves the highest\nentailment ratios (up to 71.9%) and lowest contradiction ratios (down to 10.0%)\namong various natural language inference models. Furthermore, integrating\nanomaly maps results in an 18% accuracy increase in detecting open-set\nanomalies, thereby enhancing the language model's generalizability to\npreviously unseen medical conditions. The code and dataset are available at\nhttps://github.com/compai-lab/miccai-2024-junli?tab=readme-ov-file\n","authors":["Jun Li","Su Hwan Kim","Philip Müller","Lina Felsner","Daniel Rueckert","Benedikt Wiestler","Julia A. Schnabel","Cosmin I. Bercea"],"pdf_url":"https://arxiv.org/pdf/2404.07622v2.pdf","comment":"13 pages, 7 figures. 5th International Workshop on Multiscale\n  Multimodal Medical Imaging (MMMI 2024)"},{"id":"http://arxiv.org/abs/2402.07204v3","updated":"2024-07-23T11:25:26Z","published":"2024-02-11T13:30:53Z","title":"ITINERA: Integrating Spatial Optimization with Large Language Models for\n  Open-domain Urban Itinerary Planning","summary":"  Citywalk, a recently popular form of urban travel, requires genuine\npersonalization and understanding of fine-grained requests compared to\ntraditional itinerary planning. In this paper, we introduce the novel task of\nOpen-domain Urban Itinerary Planning (OUIP), which generates personalized urban\nitineraries from user requests in natural language. We then present ITINERA, an\nOUIP system that integrates spatial optimization with large language models to\nprovide customized urban itineraries based on user needs. This involves\ndecomposing user requests, selecting candidate points of interest (POIs),\nordering the POIs based on cluster-aware spatial optimization, and generating\nthe itinerary. Experiments on real-world datasets and the performance of the\ndeployed system demonstrate our system's capacity to deliver personalized and\nspatially coherent itineraries compared to current solutions.\n","authors":["Yihong Tang","Zhaokai Wang","Ao Qu","Yihao Yan","Zhaofeng Wu","Dingyi Zhuang","Jushi Kai","Kebing Hou","Xiaotong Guo","Jinhua Zhao","Zhan Zhao","Wei Ma"],"pdf_url":"https://arxiv.org/pdf/2402.07204v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16382v1","updated":"2024-07-23T11:12:47Z","published":"2024-07-23T11:12:47Z","title":"TookaBERT: A Step Forward for Persian NLU","summary":"  The field of natural language processing (NLP) has seen remarkable\nadvancements, thanks to the power of deep learning and foundation models.\nLanguage models, and specifically BERT, have been key players in this progress.\nIn this study, we trained and introduced two new BERT models using Persian\ndata. We put our models to the test, comparing them to seven existing models\nacross 14 diverse Persian natural language understanding (NLU) tasks. The\nresults speak for themselves: our larger model outperforms the competition,\nshowing an average improvement of at least +2.8 points. This highlights the\neffectiveness and potential of our new BERT models for Persian NLU tasks.\n","authors":["MohammadAli SadraeiJavaheri","Ali Moghaddaszadeh","Milad Molazadeh","Fariba Naeiji","Farnaz Aghababaloo","Hamideh Rafiee","Zahra Amirmahani","Tohid Abedini","Fatemeh Zahra Sheikhi","Amirmohammad Salehoof"],"pdf_url":"https://arxiv.org/pdf/2407.16382v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16370v1","updated":"2024-07-23T10:38:49Z","published":"2024-07-23T10:38:49Z","title":"Evolutionary Prompt Design for LLM-Based Post-ASR Error Correction","summary":"  Building upon the strength of modern large language models (LLMs), generative\nerror correction (GEC) has emerged as a promising paradigm that can elevate the\nperformance of modern automatic speech recognition (ASR) systems. One\nrepresentative approach is to leverage in-context learning to prompt LLMs so\nthat a better hypothesis can be generated by the LLMs based on a\ncarefully-designed prompt and an $N$-best list of hypotheses produced by ASR\nsystems. However, it is yet unknown whether the existing prompts are the most\neffective ones for the task of post-ASR error correction. In this context, this\npaper first explores alternative prompts to identify an initial set of\neffective prompts, and then proposes to employ an evolutionary prompt\noptimization algorithm to refine the initial prompts. Evaluations results on\nthe CHiME-4 subset of the Task $1$ of the SLT $2024$ GenSEC challenge show the\neffectiveness and potential of the proposed algorithms.\n","authors":["Rithik Sachdev","Zhong-Qiu Wang","Chao-Han Huck Yang"],"pdf_url":"https://arxiv.org/pdf/2407.16370v1.pdf","comment":"in submission"},{"id":"http://arxiv.org/abs/2402.10689v3","updated":"2024-07-23T10:28:44Z","published":"2024-02-16T13:46:38Z","title":"Cultural Commonsense Knowledge for Intercultural Dialogues","summary":"  Despite recent progress, large language models (LLMs) still face the\nchallenge of appropriately reacting to the intricacies of social and cultural\nconventions. This paper presents MANGO, a methodology for distilling\nhigh-accuracy, high-recall assertions of cultural knowledge. We judiciously and\niteratively prompt LLMs for this purpose from two entry points, concepts and\ncultures. Outputs are consolidated via clustering and generative summarization.\nRunning the MANGO method with GPT-3.5 as underlying LLM yields 167K\nhigh-accuracy assertions for 30K concepts and 11K cultures, surpassing prior\nresources by a large margin in quality and size. In an extrinsic evaluation for\nintercultural dialogues, we explore augmenting dialogue systems with cultural\nknowledge assertions. Notably, despite LLMs inherently possessing cultural\nknowledge, we find that adding knowledge from MANGO improves the overall\nquality, specificity, and cultural sensitivity of dialogue responses, as judged\nby human annotators. Data and code are available for download.\n","authors":["Tuan-Phong Nguyen","Simon Razniewski","Gerhard Weikum"],"pdf_url":"https://arxiv.org/pdf/2402.10689v3.pdf","comment":"11 pages, 2 figures, 10 tables, accepted at CIKM '24"},{"id":"http://arxiv.org/abs/2401.10711v4","updated":"2024-07-23T10:17:39Z","published":"2024-01-19T14:21:46Z","title":"Weakly Supervised Gaussian Contrastive Grounding with Large Multimodal\n  Models for Video Question Answering","summary":"  Video Question Answering (VideoQA) aims to answer natural language questions\nbased on the information observed in videos. Despite the recent success of\nLarge Multimodal Models (LMMs) in image-language understanding and reasoning,\nthey deal with VideoQA insufficiently, by simply taking uniformly sampled\nframes as visual inputs, which ignores question-relevant visual clues.\nMoreover, there are no human annotations for question-critical timestamps in\nexisting VideoQA datasets. In light of this, we propose a novel weakly\nsupervised framework to enforce the LMMs to reason out the answers with\nquestion-critical moments as visual inputs. Specifically, we first fuse the\nquestion and answer pairs as event descriptions to find multiple keyframes as\ntarget moments and pseudo-labels, with the visual-language alignment capability\nof the CLIP models. With these pseudo-labeled keyframes as additionally weak\nsupervision, we devise a lightweight Gaussian-based Contrastive Grounding (GCG)\nmodule. GCG learns multiple Gaussian functions to characterize the temporal\nstructure of the video, and sample question-critical frames as positive moments\nto be the visual inputs of LMMs. Extensive experiments on several benchmarks\nverify the effectiveness of our framework, and we achieve substantial\nimprovements compared to previous state-of-the-art methods.\n","authors":["Haibo Wang","Chenghang Lai","Yixuan Sun","Weifeng Ge"],"pdf_url":"https://arxiv.org/pdf/2401.10711v4.pdf","comment":"accepted by ACM Multimedia 2024"},{"id":"http://arxiv.org/abs/2407.16347v1","updated":"2024-07-23T09:50:14Z","published":"2024-07-23T09:50:14Z","title":"FACTTRACK: Time-Aware World State Tracking in Story Outlines","summary":"  While accurately detecting and correcting factual contradictions in language\nmodel outputs has become increasingly important as their capabilities improve,\ndoing so is highly challenging. We propose a novel method, FACTTRACK, for\ntracking atomic facts and addressing factual contradictions. Crucially,\nFACTTRACK also maintains time-aware validity intervals for each fact, allowing\nfor change over time. At a high level, FACTTRACK consists of a four-step\npipeline to update a world state data structure for each new event: (1)\ndecompose the event into directional atomic facts; (2) determine the validity\ninterval of each atomic fact using the world state; (3) detect contradictions\nwith existing facts in the world state; and finally (4) add new facts to the\nworld state and update existing atomic facts. When we apply FACTTRACK to\ncontradiction detection on structured story outlines, we find that FACTTRACK\nusing LLaMA2-7B-Chat substantially outperforms a fair baseline using\nLLaMA2-7B-Chat, and achieves performance comparable to a GPT4 baseline.\nMoreover, when using GPT4, FACTTRACK significantly outperforms the GPT4\nbaseline.\n","authors":["Zhiheng Lyu","Kevin Yang","Lingpeng Kong","Daniel Klein"],"pdf_url":"https://arxiv.org/pdf/2407.16347v1.pdf","comment":"22 pages"},{"id":"http://arxiv.org/abs/2407.10657v2","updated":"2024-07-23T09:41:50Z","published":"2024-07-15T12:16:33Z","title":"An Empirical Study of Validating Synthetic Data for Formula Generation","summary":"  Large language models (LLMs) can be leveraged to help with writing formulas\nin spreadsheets, but resources on these formulas are scarce, impacting both the\nbase performance of pre-trained models and limiting the ability to fine-tune\nthem. Given a corpus of formulas, we can use a(nother) model to generate\nsynthetic natural language utterances for fine-tuning. However, it is important\nto validate whether the NL generated by the LLM is indeed accurate to be\nbeneficial for fine-tuning. In this paper, we provide empirical results on the\nimpact of validating these synthetic training examples with surrogate\nobjectives that evaluate the accuracy of the synthetic annotations. We\ndemonstrate that validation improves performance over raw data across four\nmodels (2 open and 2 closed weight). Interestingly, we show that although\nvalidation tends to prune more challenging examples, it increases the\ncomplexity of problems that models can solve after being fine-tuned on\nvalidated data.\n","authors":["Usneek Singh","José Cambronero","Sumit Gulwani","Aditya Kanade","Anirudh Khatry","Vu Le","Mukul Singh","Gust Verbruggen"],"pdf_url":"https://arxiv.org/pdf/2407.10657v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15130v2","updated":"2024-07-23T09:30:57Z","published":"2024-07-21T11:54:49Z","title":"DOPRA: Decoding Over-accumulation Penalization and Re-allocation in\n  Specific Weighting Layer","summary":"  In this work, we introduce DOPRA, a novel approach designed to mitigate\nhallucinations in multi-modal large language models (MLLMs). Unlike existing\nsolutions that typically involve costly supplementary training data or the\nintegration of external knowledge sources, DOPRA innovatively addresses\nhallucinations by decoding specific weighted layer penalties and\nredistribution, offering an economical and effective solution without\nadditional resources. DOPRA is grounded in unique insights into the intrinsic\nmechanisms controlling hallucinations within MLLMs, especially the models'\ntendency to over-rely on a subset of summary tokens in the self-attention\nmatrix, neglecting critical image-related information. This phenomenon is\nparticularly pronounced in certain strata. To counteract this over-reliance,\nDOPRA employs a strategy of weighted overlay penalties and redistribution in\nspecific layers, such as the 12th layer, during the decoding process.\nFurthermore, DOPRA includes a retrospective allocation process that re-examines\nthe sequence of generated tokens, allowing the algorithm to reallocate token\nselection to better align with the actual image content, thereby reducing the\nincidence of hallucinatory descriptions in auto-generated captions. Overall,\nDOPRA represents a significant step forward in improving the output quality of\nMLLMs by systematically reducing hallucinations through targeted adjustments\nduring the decoding process.\n","authors":["Jinfeng Wei","Xiaofeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2407.15130v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16318v1","updated":"2024-07-23T09:14:27Z","published":"2024-07-23T09:14:27Z","title":"PrimeGuard: Safe and Helpful LLMs through Tuning-Free Routing","summary":"  Deploying language models (LMs) necessitates outputs to be both high-quality\nand compliant with safety guidelines. Although Inference-Time Guardrails (ITG)\noffer solutions that shift model output distributions towards compliance, we\nfind that current methods struggle in balancing safety with helpfulness. ITG\nMethods that safely address non-compliant queries exhibit lower helpfulness\nwhile those that prioritize helpfulness compromise on safety. We refer to this\ntrade-off as the guardrail tax, analogous to the alignment tax. To address\nthis, we propose PrimeGuard, a novel ITG method that utilizes structured\ncontrol flow.\n  PrimeGuard routes requests to different self-instantiations of the LM with\nvarying instructions, leveraging its inherent instruction-following\ncapabilities and in-context learning. Our tuning-free approach dynamically\ncompiles system-designer guidelines for each query. We construct and release\nsafe-eval, a diverse red-team safety benchmark. Extensive evaluations\ndemonstrate that PrimeGuard, without fine-tuning, overcomes the guardrail tax\nby (1) significantly increasing resistance to iterative jailbreak attacks and\n(2) achieving state-of-the-art results in safety guardrailing while (3)\nmatching helpfulness scores of alignment-tuned models. Extensive evaluations\ndemonstrate that PrimeGuard, without fine-tuning, outperforms all competing\nbaselines and overcomes the guardrail tax by improving the fraction of safe\nresponses from 61% to 97% and increasing average helpfulness scores from 4.17\nto 4.29 on the largest models, while reducing attack success rate from 100% to\n8%.\n  PrimeGuard implementation is available at\nhttps://github.com/dynamofl/PrimeGuard and safe-eval dataset is available at\nhttps://huggingface.co/datasets/dynamoai/safe_eval.\n","authors":["Blazej Manczak","Eliott Zemour","Eric Lin","Vaikkunth Mugunthan"],"pdf_url":"https://arxiv.org/pdf/2407.16318v1.pdf","comment":"ICML 2024 NextGenAISafety workshop version with links to\n  implementation and dataset"},{"id":"http://arxiv.org/abs/2407.16266v1","updated":"2024-07-23T08:13:51Z","published":"2024-07-23T08:13:51Z","title":"Beyond Binary Gender: Evaluating Gender-Inclusive Machine Translation\n  with Ambiguous Attitude Words","summary":"  Gender bias has been a focal point in the study of bias in machine\ntranslation and language models. Existing machine translation gender bias\nevaluations are primarily focused on male and female genders, limiting the\nscope of the evaluation. To assess gender bias accurately, these studies often\nrely on calculating the accuracy of gender pronouns or the masculine and\nfeminine attributes of grammatical gender via the stereotypes triggered by\noccupations or sentiment words ({\\em i.e.}, clear positive or negative\nattitude), which cannot extend to non-binary groups. This study presents a\nbenchmark AmbGIMT (Gender-Inclusive Machine Translation with Ambiguous attitude\nwords), which assesses gender bias beyond binary gender. Meanwhile, we propose\na novel process to evaluate gender bias based on the Emotional Attitude Score\n(EAS), which is used to quantify ambiguous attitude words. In evaluating three\nrecent and effective open-source LLMs and one powerful multilingual\ntranslation-specific model, our main observations are: (1) The translation\nperformance within non-binary gender contexts is markedly inferior in terms of\ntranslation quality and exhibits more negative attitudes than binary-gender\ncontexts. (2) The analysis experiments indicate that incorporating constraint\ncontext in prompts for gender identity terms can substantially reduce\ntranslation bias, while the bias remains evident despite the presence of the\nconstraints. The code is publicly available at\n\\url{https://github.com/pppa2019/ambGIMT}.\n","authors":["Yijie Chen","Yijin Liu","Fandong Meng","Jinan Xu","Yufeng Chen","Jie Zhou"],"pdf_url":"https://arxiv.org/pdf/2407.16266v1.pdf","comment":"The code is publicly available at\n  \\url{https://github.com/pppa2019/ambGIMT}"},{"id":"http://arxiv.org/abs/2407.12022v2","updated":"2024-07-23T08:08:45Z","published":"2024-06-28T01:44:57Z","title":"ITERTL: An Iterative Framework for Fine-tuning LLMs for RTL Code\n  Generation","summary":"  Recently, large language models (LLMs) have demonstrated excellent\nperformance in understanding human instructions and generating code, which has\ninspired researchers to explore the feasibility of generating RTL code with\nLLMs. However, the existing approaches to fine-tune LLMs on RTL codes typically\nare conducted on fixed datasets, which do not fully stimulate the capability of\nLLMs and require large amounts of reference data. To mitigate these issues , we\nintroduce a simple yet effective iterative training paradigm named ITERTL.\nDuring each iteration, samples are drawn from the model trained in the previous\ncycle. Then these new samples are employed for training in this loop. Through\nthis iterative approach, the distribution mismatch between the model and the\ntraining samples is reduced. Additionally, the model is thus enabled to explore\na broader generative space and receive more comprehensive feedback. Theoretical\nanalyses are conducted to investigate the mechanism of the effectiveness.\nExperimental results show the model trained through our proposed approach can\ncompete with and even outperform the state-of-the-art (SOTA) open-source model\nwith nearly 37\\% reference samples, achieving remarkable 42.9\\% and 62.2\\%\npass@1 rate on two VerilogEval evaluation datasets respectively. While using\nthe same amount of reference samples, our method can achieved a relative\nimprovement of 16.9\\% and 12.5\\% in pass@1 compared to the non-iterative\nmethod. This study facilitates the application of LLMs for generating RTL code\nin practical scenarios with limited data.\n","authors":["Peiyang Wu","Nan Guo","Xiao Xiao","Wenming Li","Xiaochun Ye","Dongrui Fan"],"pdf_url":"https://arxiv.org/pdf/2407.12022v2.pdf","comment":"There is some mistakes about the Experimental Setup in Section4.1"},{"id":"http://arxiv.org/abs/2402.12881v2","updated":"2024-07-23T08:07:40Z","published":"2024-02-20T10:23:00Z","title":"TEXT2AFFORD: Probing Object Affordance Prediction abilities of Language\n  Models solely from Text","summary":"  We investigate the knowledge of object affordances in pre-trained language\nmodels (LMs) and pre-trained Vision-Language models (VLMs). A growing body of\nliterature shows that PTLMs fail inconsistently and non-intuitively,\ndemonstrating a lack of reasoning and grounding. To take a first step toward\nquantifying the effect of grounding (or lack thereof), we curate a novel and\ncomprehensive dataset of object affordances -- Text2Afford, characterized by 15\naffordance classes. Unlike affordance datasets collected in vision and language\ndomains, we annotate in-the-wild sentences with objects and affordances.\nExperimental results reveal that PTLMs exhibit limited reasoning abilities when\nit comes to uncommon object affordances. We also observe that pre-trained VLMs\ndo not necessarily capture object affordances effectively. Through few-shot\nfine-tuning, we demonstrate improvement in affordance knowledge in PTLMs and\nVLMs. Our research contributes a novel dataset for language grounding tasks,\nand presents insights into LM capabilities, advancing the understanding of\nobject affordances. Codes and data are available at\nhttps://github.com/sayantan11995/Affordance\n","authors":["Sayantan Adak","Daivik Agrawal","Animesh Mukherjee","Somak Aditya"],"pdf_url":"https://arxiv.org/pdf/2402.12881v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16252v1","updated":"2024-07-23T07:40:41Z","published":"2024-07-23T07:40:41Z","title":"LawLuo: A Chinese Law Firm Co-run by LLM Agents","summary":"  Large Language Models (LLMs) demonstrate substantial potential in delivering\nlegal consultation services to users without a legal background, attributed to\ntheir superior text comprehension and generation capabilities. Nonetheless,\nexisting Chinese legal LLMs limit interaction to a single model-user dialogue,\nunlike the collaborative consultations typical of law firms, where multiple\nstaff members contribute to a single consultation. This limitation prevents an\nauthentic consultation experience. Additionally, extant Chinese legal LLMs\nsuffer from critical limitations: (1) insufficient control over the quality of\ninstruction fine-tuning data; (2) increased model hallucination resulting from\nusers' ambiguous queries; and (3) a reduction in the model's ability to follow\ninstructions over multiple dialogue turns. In response to these challenges, we\npropose a novel legal dialogue framework that leverages the collaborative\ncapabilities of multiple LLM agents, termed LawLuo. This framework encompasses\nfour agents: a receptionist, a lawyer, a secretary, and a boss, each\nresponsible for different functionalities, collaboratively providing a\ncomprehensive legal consultation to users. Additionally, we constructed two\nhigh-quality legal dialogue datasets, KINLED and MURLED, and fine-tuned\nChatGLM-3-6b using these datasets. We propose a legal query clarification\nalgorithm called ToLC. Experimental results demonstrate that LawLuo outperforms\nbaseline LLMs, including GPT-4, across three dimensions: lawyer-like language\nstyle, the usefulness of legal advice, and the accuracy of legal knowledge. Our\ncode and datasets are available at https://github.com/NEFUJing/LawLuo.\n","authors":["Jingyun Sun","Chengxiao Dai","Zhongze Luo","Yangbo Chang","Yang Li"],"pdf_url":"https://arxiv.org/pdf/2407.16252v1.pdf","comment":"11 pages, 13 figures, 2 tables"},{"id":"http://arxiv.org/abs/2407.16245v1","updated":"2024-07-23T07:31:43Z","published":"2024-07-23T07:31:43Z","title":"Exploring the Effectiveness and Consistency of Task Selection in\n  Intermediate-Task Transfer Learning","summary":"  Identifying beneficial tasks to transfer from is a critical step toward\nsuccessful intermediate-task transfer learning. In this work, we experiment\nwith 130 source-target task combinations and demonstrate that the transfer\nperformance exhibits severe variance across different source tasks and training\nseeds, highlighting the crucial role of intermediate-task selection in a\nbroader context. We compare four representative task selection methods in a\nunified setup, focusing on their effectiveness and consistency. Compared to\nembedding-free methods and text embeddings, task embeddings constructed from\nfine-tuned weights can better estimate task transferability by improving task\nprediction scores from 2.59% to 3.96%. Despite their strong performance, we\nobserve that the task embeddings do not consistently demonstrate superiority\nfor tasks requiring reasoning abilities. Furthermore, we introduce a novel\nmethod that measures pairwise token similarity using maximum inner product\nsearch, leading to the highest performance in task prediction. Our findings\nsuggest that token-wise similarity is better predictive for predicting\ntransferability compared to averaging weights.\n","authors":["Pin-Jie Lin","Miaoran Zhang","Marius Mosbach","Dietrich Klakow"],"pdf_url":"https://arxiv.org/pdf/2407.16245v1.pdf","comment":"Accepted to ACL SRW 2024"},{"id":"http://arxiv.org/abs/2407.16234v1","updated":"2024-07-23T07:17:46Z","published":"2024-07-23T07:17:46Z","title":"A Multi-view Mask Contrastive Learning Graph Convolutional Neural\n  Network for Age Estimation","summary":"  The age estimation task aims to use facial features to predict the age of\npeople and is widely used in public security, marketing, identification, and\nother fields. However, the features are mainly concentrated in facial\nkeypoints, and existing CNN and Transformer-based methods have inflexibility\nand redundancy for modeling complex irregular structures. Therefore, this paper\nproposes a Multi-view Mask Contrastive Learning Graph Convolutional Neural\nNetwork (MMCL-GCN) for age estimation. Specifically, the overall structure of\nthe MMCL-GCN network contains a feature extraction stage and an age estimation\nstage. In the feature extraction stage, we introduce a graph structure to\nconstruct face images as input and then design a Multi-view Mask Contrastive\nLearning (MMCL) mechanism to learn complex structural and semantic information\nabout face images. The learning mechanism employs an asymmetric siamese network\narchitecture, which utilizes an online encoder-decoder structure to reconstruct\nthe missing information from the original graph and utilizes the target encoder\nto learn latent representations for contrastive learning. Furthermore, to\npromote the two learning mechanisms better compatible and complementary, we\nadopt two augmentation strategies and optimize the joint losses. In the age\nestimation stage, we design a Multi-layer Extreme Learning Machine (ML-IELM)\nwith identity mapping to fully use the features extracted by the online\nencoder. Then, a classifier and a regressor were constructed based on ML-IELM,\nwhich were used to identify the age grouping interval and accurately estimate\nthe final age. Extensive experiments show that MMCL-GCN can effectively reduce\nthe error of age estimation on benchmark datasets such as Adience, MORPH-II,\nand LAP-2016.\n","authors":["Yiping Zhang","Yuntao Shou","Tao Meng","Wei Ai","Keqin Li"],"pdf_url":"https://arxiv.org/pdf/2407.16234v1.pdf","comment":"20 pages, 9 figures"},{"id":"http://arxiv.org/abs/2402.13220v2","updated":"2024-07-23T07:02:30Z","published":"2024-02-20T18:31:27Z","title":"How Easy is It to Fool Your Multimodal LLMs? An Empirical Analysis on\n  Deceptive Prompts","summary":"  The remarkable advancements in Multimodal Large Language Models (MLLMs) have\nnot rendered them immune to challenges, particularly in the context of handling\ndeceptive information in prompts, thus producing hallucinated responses under\nsuch conditions. To quantitatively assess this vulnerability, we present\nMAD-Bench, a carefully curated benchmark that contains 1000 test samples\ndivided into 5 categories, such as non-existent objects, count of objects, and\nspatial relationship. We provide a comprehensive analysis of popular MLLMs,\nranging from GPT-4v, Reka, Gemini-Pro, to open-sourced models, such as\nLLaVA-NeXT and MiniCPM-Llama3. Empirically, we observe significant performance\ngaps between GPT-4o and other models; and previous robust instruction-tuned\nmodels are not effective on this new benchmark. While GPT-4o achieves 82.82%\naccuracy on MAD-Bench, the accuracy of any other model in our experiments\nranges from 9% to 50%. We further propose a remedy that adds an additional\nparagraph to the deceptive prompts to encourage models to think twice before\nanswering the question. Surprisingly, this simple method can even double the\naccuracy; however, the absolute numbers are still too low to be satisfactory.\nWe hope MAD-Bench can serve as a valuable benchmark to stimulate further\nresearch to enhance model resilience against deceptive prompts.\n","authors":["Yusu Qian","Haotian Zhang","Yinfei Yang","Zhe Gan"],"pdf_url":"https://arxiv.org/pdf/2402.13220v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16222v1","updated":"2024-07-23T06:59:53Z","published":"2024-07-23T06:59:53Z","title":"PreAlign: Boosting Cross-Lingual Transfer by Early Establishment of\n  Multilingual Alignment","summary":"  Large language models demonstrate reasonable multilingual abilities, despite\npredominantly English-centric pretraining. However, the spontaneous\nmultilingual alignment in these models is shown to be weak, leading to\nunsatisfactory cross-lingual transfer and knowledge sharing. Previous works\nattempt to address this issue by explicitly injecting multilingual alignment\ninformation during or after pretraining. Thus for the early stage in\npretraining, the alignment is weak for sharing information or knowledge across\nlanguages. In this paper, we propose PreAlign, a framework that establishes\nmultilingual alignment prior to language model pretraining. PreAlign injects\nmultilingual alignment by initializing the model to generate similar\nrepresentations of aligned words and preserves this alignment using a\ncode-switching strategy during pretraining. Extensive experiments in a\nsynthetic English to English-Clone setting demonstrate that PreAlign\nsignificantly outperforms standard multilingual joint training in language\nmodeling, zero-shot cross-lingual transfer, and cross-lingual knowledge\napplication. Further experiments in real-world scenarios further validate\nPreAlign's effectiveness across various model sizes.\n","authors":["Jiahuan Li","Shujian Huang","Xinyu Dai","Jiajun Chen"],"pdf_url":"https://arxiv.org/pdf/2407.16222v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16221v1","updated":"2024-07-23T06:56:54Z","published":"2024-07-23T06:56:54Z","title":"Do LLMs Know When to NOT Answer? Investigating Abstention Abilities of\n  Large Language Models","summary":"  As Large Language Models (LLMs) achieve remarkable performance across various\nNLP tasks, their reliability becomes essential for widespread adoption. This\npaper focuses on Abstention Ability (AA), a critical yet under explored aspect\nof reliability - the ability of LLMs to refrain from answering questions when\nthey are uncertain or when definitive answer is not possible, while maintaining\nquestion-answering (QA) task performance. While previous works have focused on\nunderstanding the recollection abilities of LLMs or their ability to identify\nimponderable/unanswerable questions, we believe there is a need for an\neffective AA evaluation method. Therefore, we propose a black-box evaluation\nmethodology to examine and understand the AA of LLMs across a variety of\nmultiple-choice QA tasks. We measure AA by rewarding models for abstaining from\nanswering when their predictions are incorrect or when the questions are\ninherently unanswerable. We investigate three strategies, Strict Prompting,\nVerbal Confidence Thresholding, and Chain-of-Thought (CoT), to understand their\nimpact on abstention across different LLMs. Our findings reveal that while even\nstate-of-the-art LLMs like GPT-4 struggle with abstention, strategic prompting\nsuch as CoT, can significantly enhance this ability. Furthermore, we\ndemonstrate that improving AA also leads to better overall QA task performance,\nunderscoring the importance of evaluating AA in LLMs.\n","authors":["Nishanth Madhusudhan","Sathwik Tejaswi Madhusudhan","Vikas Yadav","Masoud Hashemi"],"pdf_url":"https://arxiv.org/pdf/2407.16221v1.pdf","comment":"5 pages (5th page contains References) and 2 figures"},{"id":"http://arxiv.org/abs/2407.16216v1","updated":"2024-07-23T06:45:52Z","published":"2024-07-23T06:45:52Z","title":"A Comprehensive Survey of LLM Alignment Techniques: RLHF, RLAIF, PPO,\n  DPO and More","summary":"  With advancements in self-supervised learning, the availability of trillions\ntokens in a pre-training corpus, instruction fine-tuning, and the development\nof large Transformers with billions of parameters, large language models (LLMs)\nare now capable of generating factual and coherent responses to human queries.\nHowever, the mixed quality of training data can lead to the generation of\nundesired responses, presenting a significant challenge. Over the past two\nyears, various methods have been proposed from different perspectives to\nenhance LLMs, particularly in aligning them with human expectation. Despite\nthese efforts, there has not been a comprehensive survey paper that categorizes\nand details these approaches. In this work, we aim to address this gap by\ncategorizing these papers into distinct topics and providing detailed\nexplanations of each alignment method, thereby helping readers gain a thorough\nunderstanding of the current state of the field.\n","authors":["Zhichao Wang","Bin Bi","Shiva Kumar Pentyala","Kiran Ramnath","Sougata Chaudhuri","Shubham Mehrotra"," Zixu"," Zhu","Xiang-Bo Mao","Sitaram Asur"," Na"," Cheng"],"pdf_url":"https://arxiv.org/pdf/2407.16216v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.12703v3","updated":"2024-07-23T06:26:30Z","published":"2024-07-17T16:25:37Z","title":"Subgraph-Aware Training of Text-based Methods for Knowledge Graph\n  Completion","summary":"  Fine-tuning pre-trained language models (PLMs) has recently shown a potential\nto improve knowledge graph completion (KGC). However, most PLM-based methods\nencode only textual information, neglecting various topological structures of\nknowledge graphs (KGs). In this paper, we empirically validate the significant\nrelations between the structural properties of KGs and the performance of the\nPLM-based methods. To leverage the structural knowledge, we propose a\nSubgraph-Aware Training framework for KGC (SATKGC) that combines (i)\nsubgraph-aware mini-batching to encourage hard negative sampling, and (ii) a\nnew contrastive learning method to focus more on harder entities and harder\nnegative triples in terms of the structural properties. To the best of our\nknowledge, this is the first study to comprehensively incorporate the\nstructural inductive bias of the subgraphs into fine-tuning PLMs. Extensive\nexperiments on four KGC benchmarks demonstrate the superiority of SATKGC. Our\ncode is available.\n","authors":["Youmin Ko","Hyemin Yang","Taeuk Kim","Hyunjoon Kim"],"pdf_url":"https://arxiv.org/pdf/2407.12703v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16207v1","updated":"2024-07-23T06:21:24Z","published":"2024-07-23T06:21:24Z","title":"Graph-Structured Speculative Decoding","summary":"  Speculative decoding has emerged as a promising technique to accelerate the\ninference of Large Language Models (LLMs) by employing a small language model\nto draft a hypothesis sequence, which is then validated by the LLM. The\neffectiveness of this approach heavily relies on the balance between\nperformance and efficiency of the draft model. In our research, we focus on\nenhancing the proportion of draft tokens that are accepted to the final output\nby generating multiple hypotheses instead of just one. This allows the LLM more\noptions to choose from and select the longest sequence that meets its\nstandards. Our analysis reveals that hypotheses produced by the draft model\nshare many common token sequences, suggesting a potential for optimizing\ncomputation. Leveraging this observation, we introduce an innovative approach\nutilizing a directed acyclic graph (DAG) to manage the drafted hypotheses. This\nstructure enables us to efficiently predict and merge recurring token\nsequences, vastly reducing the computational demands of the draft model. We\nterm this approach Graph-structured Speculative Decoding (GSD). We apply GSD\nacross a range of LLMs, including a 70-billion parameter LLaMA-2 model, and\nobserve a remarkable speedup of 1.73$\\times$ to 1.96$\\times$, significantly\nsurpassing standard speculative decoding.\n","authors":["Zhuocheng Gong","Jiahao Liu","Ziyue Wang","Pengfei Wu","Jingang Wang","Xunliang Cai","Dongyan Zhao","Rui Yan"],"pdf_url":"https://arxiv.org/pdf/2407.16207v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.11406v3","updated":"2024-07-23T06:20:32Z","published":"2024-02-18T00:04:40Z","title":"Don't Go To Extremes: Revealing the Excessive Sensitivity and\n  Calibration Limitations of LLMs in Implicit Hate Speech Detection","summary":"  The fairness and trustworthiness of Large Language Models (LLMs) are\nreceiving increasing attention. Implicit hate speech, which employs indirect\nlanguage to convey hateful intentions, occupies a significant portion of\npractice. However, the extent to which LLMs effectively address this issue\nremains insufficiently examined. This paper delves into the capability of LLMs\nto detect implicit hate speech (Classification Task) and express confidence in\ntheir responses (Calibration Task). Our evaluation meticulously considers\nvarious prompt patterns and mainstream uncertainty estimation methods. Our\nfindings highlight that LLMs exhibit two extremes: (1) LLMs display excessive\nsensitivity towards groups or topics that may cause fairness issues, resulting\nin misclassifying benign statements as hate speech. (2) LLMs' confidence scores\nfor each method excessively concentrate on a fixed range, remaining unchanged\nregardless of the dataset's complexity. Consequently, the calibration\nperformance is heavily reliant on primary classification accuracy. These\ndiscoveries unveil new limitations of LLMs, underscoring the need for caution\nwhen optimizing models to ensure they do not veer towards extremes. This serves\nas a reminder to carefully consider sensitivity and confidence in the pursuit\nof model fairness.\n","authors":["Min Zhang","Jianfeng He","Taoran Ji","Chang-Tien Lu"],"pdf_url":"https://arxiv.org/pdf/2402.11406v3.pdf","comment":"ACL 2024 Main Conference"},{"id":"http://arxiv.org/abs/2407.16205v1","updated":"2024-07-23T06:14:41Z","published":"2024-07-23T06:14:41Z","title":"Figure it Out: Analyzing-based Jailbreak Attack on Large Language Models","summary":"  The rapid development of Large Language Models (LLMs) has brought remarkable\ngenerative capabilities across diverse tasks. However, despite the impressive\nachievements, these models still have numerous security vulnerabilities,\nparticularly when faced with jailbreak attacks. Therefore, by investigating\njailbreak attacks, we can uncover hidden weaknesses in LLMs and guide us in\ndeveloping more robust defense mechanisms to fortify their security. In this\npaper, we further explore the boundary of jailbreak attacks on LLMs and propose\nAnalyzing-based Jailbreak (ABJ). This effective jailbreak attack method takes\nadvantage of LLMs' growing analyzing and reasoning capability and reveals their\nunderlying vulnerabilities when facing analysis-based tasks. We conduct a\ndetailed evaluation of ABJ across various open-source and closed-source LLMs,\nwhich achieves 94.8% Attack Success Rate (ASR) and 1.06 Attack Efficiency (AE)\non GPT-4-turbo-0409, demonstrating state-of-the-art attack effectiveness and\nefficiency. Our research highlights the importance of prioritizing and\nenhancing the safety of LLMs to mitigate the risks of misuse.\n","authors":["Shi Lin","Rongchang Li","Xun Wang","Changting Lin","Wenpeng Xing","Meng Han"],"pdf_url":"https://arxiv.org/pdf/2407.16205v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16192v1","updated":"2024-07-23T05:34:41Z","published":"2024-07-23T05:34:41Z","title":"How to Leverage Personal Textual Knowledge for Personalized\n  Conversational Information Retrieval","summary":"  Personalized conversational information retrieval (CIR) combines\nconversational and personalizable elements to satisfy various users' complex\ninformation needs through multi-turn interaction based on their backgrounds.\nThe key promise is that the personal textual knowledge base (PTKB) can improve\nthe CIR effectiveness because the retrieval results can be more related to the\nuser's background. However, PTKB is noisy: not every piece of knowledge in PTKB\nis relevant to the specific query at hand. In this paper, we explore and test\nseveral ways to select knowledge from PTKB and use it for query reformulation\nby using a large language model (LLM). The experimental results show the PTKB\nmight not always improve the search results when used alone, but LLM can help\ngenerate a more appropriate personalized query when high-quality guidance is\nprovided.\n","authors":["Fengran Mo","Longxiang Zhao","Kaiyu Huang","Yue Dong","Degen Huang","Jian-Yun Nie"],"pdf_url":"https://arxiv.org/pdf/2407.16192v1.pdf","comment":"Accepted to CIKM 2024"},{"id":"http://arxiv.org/abs/2407.16190v1","updated":"2024-07-23T05:32:00Z","published":"2024-07-23T05:32:00Z","title":"Artificial Agency and Large Language Models","summary":"  The arrival of Large Language Models (LLMs) has stirred up philosophical\ndebates about the possibility of realizing agency in an artificial manner. In\nthis work we contribute to the debate by presenting a theoretical model that\ncan be used as a threshold conception for artificial agents. The model defines\nagents as systems whose actions and goals are always influenced by a dynamic\nframework of factors that consists of the agent's accessible history, its\nadaptive repertoire and its external environment. This framework, in turn, is\ninfluenced by the actions that the agent takes and the goals that it forms. We\nshow with the help of the model that state-of-the-art LLMs are not agents yet,\nbut that there are elements to them that suggest a way forward. The paper\nargues that a combination of the agent architecture presented in Park et al.\n(2023) together with the use of modules like the Coscientist in Boiko et al.\n(2023) could potentially be a way to realize agency in an artificial manner. We\nend the paper by reflecting on the obstacles one might face in building such an\nartificial agent and by presenting possible directions for future research.\n","authors":["Maud Van Lier","Gorka Muñoz-Gil"],"pdf_url":"https://arxiv.org/pdf/2407.16190v1.pdf","comment":"Accepted for publication in journal Intellectica, special issue\n  \"Philosophies of AI: thinking and writing with LLMs\" (Intellectica, issue 81)"},{"id":"http://arxiv.org/abs/2403.19995v2","updated":"2024-07-23T05:21:44Z","published":"2024-03-29T06:22:37Z","title":"Development of Compositionality and Generalization through Interactive\n  Learning of Language and Action of Robots","summary":"  Humans excel at applying learned behavior to unlearned situations. A crucial\ncomponent of this generalization behavior is our ability to compose/decompose a\nwhole into reusable parts, an attribute known as compositionality. One of the\nfundamental questions in robotics concerns this characteristic. \"How can\nlinguistic compositionality be developed concomitantly with sensorimotor skills\nthrough associative learning, particularly when individuals only learn partial\nlinguistic compositions and their corresponding sensorimotor patterns?\" To\naddress this question, we propose a brain-inspired neural network model that\nintegrates vision, proprioception, and language into a framework of predictive\ncoding and active inference, based on the free-energy principle. The\neffectiveness and capabilities of this model were assessed through various\nsimulation experiments conducted with a robot arm. Our results show that\ngeneralization in learning to unlearned verb-noun compositions, is\nsignificantly enhanced when training variations of task composition are\nincreased. We attribute this to self-organized compositional structures in\nlinguistic latent state space being influenced significantly by sensorimotor\nlearning. Ablation studies show that visual attention and working memory are\nessential to accurately generate visuo-motor sequences to achieve\nlinguistically represented goals. These insights advance our understanding of\nmechanisms underlying development of compositionality through interactions of\nlinguistic and sensorimotor experience.\n","authors":["Prasanna Vijayaraghavan","Jeffrey Frederic Queisser","Sergio Verduzco Flores","Jun Tani"],"pdf_url":"https://arxiv.org/pdf/2403.19995v2.pdf","comment":"64 pages, 6 figures, 10 supplementary figures"},{"id":"http://arxiv.org/abs/2407.16181v1","updated":"2024-07-23T04:57:03Z","published":"2024-07-23T04:57:03Z","title":"Structural Optimization Ambiguity and Simplicity Bias in Unsupervised\n  Neural Grammar Induction","summary":"  Neural parameterization has significantly advanced unsupervised grammar\ninduction. However, training these models with a traditional likelihood loss\nfor all possible parses exacerbates two issues: 1) $\\textit{structural\noptimization ambiguity}$ that arbitrarily selects one among structurally\nambiguous optimal grammars despite the specific preference of gold parses, and\n2) $\\textit{structural simplicity bias}$ that leads a model to underutilize\nrules to compose parse trees. These challenges subject unsupervised neural\ngrammar induction (UNGI) to inevitable prediction errors, high variance, and\nthe necessity for extensive grammars to achieve accurate predictions. This\npaper tackles these issues, offering a comprehensive analysis of their origins.\nAs a solution, we introduce $\\textit{sentence-wise parse-focusing}$ to reduce\nthe parse pool per sentence for loss evaluation, using the structural bias from\npre-trained parsers on the same dataset. In unsupervised parsing benchmark\ntests, our method significantly improves performance while effectively reducing\nvariance and bias toward overly simplistic parses. Our research promotes\nlearning more compact, accurate, and consistent explicit grammars, facilitating\nbetter interpretability.\n","authors":["Jinwook Park","Kangil Kim"],"pdf_url":"https://arxiv.org/pdf/2407.16181v1.pdf","comment":"Accepted in ACL2024 Findings, 16 pages, 10 figures"},{"id":"http://arxiv.org/abs/2311.09136v3","updated":"2024-07-23T04:35:45Z","published":"2023-11-15T17:27:14Z","title":"Rescue: Ranking LLM Responses with Partial Ordering to Improve Response\n  Generation","summary":"  Customizing LLMs for a specific task involves separating high-quality\nresponses from lower-quality ones. This skill can be developed using supervised\nfine-tuning with extensive human preference data. However, obtaining a large\nvolume of expert-annotated data is costly for most tasks. In this paper, we\nexplore a novel method to optimize LLMs using ranking metrics. This method\ntrains the model to prioritize the best responses from a pool of candidates\ncreated for a particular task. Rather than a traditional full ordering, we\nadvocate for a partial ordering, as achieving consensus on the perfect order of\ncandidate responses can be challenging. Our partial ordering is more robust,\nless sensitive to noise, and can be achieved with limited human annotations or\nthrough heuristic methods. We test our system's improved response generation\nability using benchmark datasets, including textual entailment and\nmulti-document question answering. We conduct ablation studies to understand\ncrucial factors, such as how to gather candidate responses for a specific task,\ndetermine their most suitable order, and balance supervised fine-tuning with\nranking metrics. Our approach, named Rescue, offers a promising avenue for\nenhancing the response generation and task accuracy of LLMs.\n","authors":["Yikun Wang","Rui Zheng","Haoming Li","Qi Zhang","Tao Gui","Fei Liu"],"pdf_url":"https://arxiv.org/pdf/2311.09136v3.pdf","comment":"ACL 2024 SRW"},{"id":"http://arxiv.org/abs/2407.16168v1","updated":"2024-07-23T04:22:30Z","published":"2024-07-23T04:22:30Z","title":"Progressively Modality Freezing for Multi-Modal Entity Alignment","summary":"  Multi-Modal Entity Alignment aims to discover identical entities across\nheterogeneous knowledge graphs. While recent studies have delved into fusion\nparadigms to represent entities holistically, the elimination of features\nirrelevant to alignment and modal inconsistencies is overlooked, which are\ncaused by inherent differences in multi-modal features. To address these\nchallenges, we propose a novel strategy of progressive modality freezing,\ncalled PMF, that focuses on alignmentrelevant features and enhances multi-modal\nfeature fusion. Notably, our approach introduces a pioneering cross-modal\nassociation loss to foster modal consistency. Empirical evaluations across nine\ndatasets confirm PMF's superiority, demonstrating stateof-the-art performance\nand the rationale for freezing modalities. Our code is available at\nhttps://github.com/ninibymilk/PMF-MMEA.\n","authors":["Yani Huang","Xuefeng Zhang","Richong Zhang","Junfan Chen","Jaein Kim"],"pdf_url":"https://arxiv.org/pdf/2407.16168v1.pdf","comment":"13pages, 8 figures, Accepted by ACL2024"},{"id":"http://arxiv.org/abs/2407.16166v1","updated":"2024-07-23T04:20:14Z","published":"2024-07-23T04:20:14Z","title":"Robust Privacy Amidst Innovation with Large Language Models Through a\n  Critical Assessment of the Risks","summary":"  This study examines integrating EHRs and NLP with large language models\n(LLMs) to improve healthcare data management and patient care. It focuses on\nusing advanced models to create secure, HIPAA-compliant synthetic patient notes\nfor biomedical research. The study used de-identified and re-identified MIMIC\nIII datasets with GPT-3.5, GPT-4, and Mistral 7B to generate synthetic notes.\nText generation employed templates and keyword extraction for contextually\nrelevant notes, with one-shot generation for comparison. Privacy assessment\nchecked PHI occurrence, while text utility was tested using an ICD-9 coding\ntask. Text quality was evaluated with ROUGE and cosine similarity metrics to\nmeasure semantic similarity with source notes. Analysis of PHI occurrence and\ntext utility via the ICD-9 coding task showed that the keyword-based method had\nlow risk and good performance. One-shot generation showed the highest PHI\nexposure and PHI co-occurrence, especially in geographic location and date\ncategories. The Normalized One-shot method achieved the highest classification\naccuracy. Privacy analysis revealed a critical balance between data utility and\nprivacy protection, influencing future data use and sharing. Re-identified data\nconsistently outperformed de-identified data. This study demonstrates the\neffectiveness of keyword-based methods in generating privacy-protecting\nsynthetic clinical notes that retain data usability, potentially transforming\nclinical data-sharing practices. The superior performance of re-identified over\nde-identified data suggests a shift towards methods that enhance utility and\nprivacy by using dummy PHIs to perplex privacy attacks.\n","authors":["Yao-Shun Chuang","Atiquer Rahman Sarkar","Noman Mohammed","Xiaoqian Jiang"],"pdf_url":"https://arxiv.org/pdf/2407.16166v1.pdf","comment":"13 pages, 4 figures, 1 table, 1 supplementary, under review"},{"id":"http://arxiv.org/abs/2407.15351v2","updated":"2024-07-23T04:01:19Z","published":"2024-07-22T03:36:38Z","title":"LLMExplainer: Large Language Model based Bayesian Inference for Graph\n  Explanation Generation","summary":"  Recent studies seek to provide Graph Neural Network (GNN) interpretability\nvia multiple unsupervised learning models. Due to the scarcity of datasets,\ncurrent methods easily suffer from learning bias. To solve this problem, we\nembed a Large Language Model (LLM) as knowledge into the GNN explanation\nnetwork to avoid the learning bias problem. We inject LLM as a Bayesian\nInference (BI) module to mitigate learning bias. The efficacy of the BI module\nhas been proven both theoretically and experimentally. We conduct experiments\non both synthetic and real-world datasets. The innovation of our work lies in\ntwo parts: 1. We provide a novel view of the possibility of an LLM functioning\nas a Bayesian inference to improve the performance of existing algorithms; 2.\nWe are the first to discuss the learning bias issues in the GNN explanation\nproblem.\n","authors":["Jiaxing Zhang","Jiayi Liu","Dongsheng Luo","Jennifer Neville","Hua Wei"],"pdf_url":"https://arxiv.org/pdf/2407.15351v2.pdf","comment":"Preprint Paper with 13 pages"},{"id":"http://arxiv.org/abs/2407.16160v1","updated":"2024-07-23T03:58:08Z","published":"2024-07-23T03:58:08Z","title":"UniMEL: A Unified Framework for Multimodal Entity Linking with Large\n  Language Models","summary":"  Multimodal Entity Linking (MEL) is a crucial task that aims at linking\nambiguous mentions within multimodal contexts to the referent entities in a\nmultimodal knowledge base, such as Wikipedia. Existing methods focus heavily on\nusing complex mechanisms and extensive model tuning methods to model the\nmultimodal interaction on specific datasets. However, these methods\novercomplicate the MEL task and overlook the visual semantic information, which\nmakes them costly and hard to scale. Moreover, these methods can not solve the\nissues like textual ambiguity, redundancy, and noisy images, which severely\ndegrade their performance. Fortunately, the advent of Large Language Models\n(LLMs) with robust capabilities in text understanding and reasoning,\nparticularly Multimodal Large Language Models (MLLMs) that can process\nmultimodal inputs, provides new insights into addressing this challenge.\nHowever, how to design a universally applicable LLMs-based MEL approach remains\na pressing challenge. To this end, we propose UniMEL, a unified framework which\nestablishes a new paradigm to process multimodal entity linking tasks using\nLLMs. In this framework, we employ LLMs to augment the representation of\nmentions and entities individually by integrating textual and visual\ninformation and refining textual information. Subsequently, we employ the\nembedding-based method for retrieving and re-ranking candidate entities. Then,\nwith only ~0.26% of the model parameters fine-tuned, LLMs can make the final\nselection from the candidate entities. Extensive experiments on three public\nbenchmark datasets demonstrate that our solution achieves state-of-the-art\nperformance, and ablation studies verify the effectiveness of all modules. Our\ncode is available at https://anonymous.4open.science/r/UniMEL/.\n","authors":["Liu Qi","He Yongyi","Lian Defu","Zheng Zhi","Xu Tong","Liu Che","Chen Enhong"],"pdf_url":"https://arxiv.org/pdf/2407.16160v1.pdf","comment":"CIKM 2024. The first two authors contributed equally to this work"},{"id":"http://arxiv.org/abs/2407.16154v1","updated":"2024-07-23T03:47:28Z","published":"2024-07-23T03:47:28Z","title":"DDK: Distilling Domain Knowledge for Efficient Large Language Models","summary":"  Despite the advanced intelligence abilities of large language models (LLMs)\nin various applications, they still face significant computational and storage\ndemands. Knowledge Distillation (KD) has emerged as an effective strategy to\nimprove the performance of a smaller LLM (i.e., the student model) by\ntransferring knowledge from a high-performing LLM (i.e., the teacher model).\nPrevailing techniques in LLM distillation typically use a black-box model API\nto generate high-quality pretrained and aligned datasets, or utilize white-box\ndistillation by altering the loss function to better transfer knowledge from\nthe teacher LLM. However, these methods ignore the knowledge differences\nbetween the student and teacher LLMs across domains. This results in excessive\nfocus on domains with minimal performance gaps and insufficient attention to\ndomains with large gaps, reducing overall performance. In this paper, we\nintroduce a new LLM distillation framework called DDK, which dynamically\nadjusts the composition of the distillation dataset in a smooth manner\naccording to the domain performance differences between the teacher and student\nmodels, making the distillation process more stable and effective. Extensive\nevaluations show that DDK significantly improves the performance of student\nmodels, outperforming both continuously pretrained baselines and existing\nknowledge distillation methods by a large margin.\n","authors":["Jiaheng Liu","Chenchen Zhang","Jinyang Guo","Yuanxing Zhang","Haoran Que","Ken Deng","Zhiqi Bai","Jie Liu","Ge Zhang","Jiakai Wang","Yanan Wu","Congnan Liu","Wenbo Su","Jiamang Wang","Lin Qu","Bo Zheng"],"pdf_url":"https://arxiv.org/pdf/2407.16154v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16148v1","updated":"2024-07-23T03:18:00Z","published":"2024-07-23T03:18:00Z","title":"CHIME: LLM-Assisted Hierarchical Organization of Scientific Studies for\n  Literature Review Support","summary":"  Literature review requires researchers to synthesize a large amount of\ninformation and is increasingly challenging as the scientific literature\nexpands. In this work, we investigate the potential of LLMs for producing\nhierarchical organizations of scientific studies to assist researchers with\nliterature review. We define hierarchical organizations as tree structures\nwhere nodes refer to topical categories and every node is linked to the studies\nassigned to that category. Our naive LLM-based pipeline for hierarchy\ngeneration from a set of studies produces promising yet imperfect hierarchies,\nmotivating us to collect CHIME, an expert-curated dataset for this task focused\non biomedicine. Given the challenging and time-consuming nature of building\nhierarchies from scratch, we use a human-in-the-loop process in which experts\ncorrect errors (both links between categories and study assignment) in\nLLM-generated hierarchies. CHIME contains 2,174 LLM-generated hierarchies\ncovering 472 topics, and expert-corrected hierarchies for a subset of 100\ntopics. Expert corrections allow us to quantify LLM performance, and we find\nthat while they are quite good at generating and organizing categories, their\nassignment of studies to categories could be improved. We attempt to train a\ncorrector model with human feedback which improves study assignment by 12.6 F1\npoints. We release our dataset and models to encourage research on developing\nbetter assistive tools for literature review.\n","authors":["Chao-Chun Hsu","Erin Bransom","Jenna Sparks","Bailey Kuehl","Chenhao Tan","David Wadden","Lucy Lu Wang","Aakanksha Naik"],"pdf_url":"https://arxiv.org/pdf/2407.16148v1.pdf","comment":"2024 ACL Findings"},{"id":"http://arxiv.org/abs/2404.01461v4","updated":"2024-07-23T02:41:57Z","published":"2024-04-01T20:15:06Z","title":"Will the Real Linda Please Stand up...to Large Language Models?\n  Examining the Representativeness Heuristic in LLMs","summary":"  Although large language models (LLMs) have demonstrated remarkable\nproficiency in modeling text and generating human-like text, they may exhibit\nbiases acquired from training data in doing so. Specifically, LLMs may be\nsusceptible to a common cognitive trap in human decision-making called the\nrepresentativeness heuristic. This is a concept in psychology that refers to\njudging the likelihood of an event based on how closely it resembles a\nwell-known prototype or typical example, versus considering broader facts or\nstatistical evidence. This research investigates the impact of the\nrepresentativeness heuristic on LLM reasoning. We created ReHeAT\n(Representativeness Heuristic AI Testing), a dataset containing a series of\nproblems spanning six common types of representativeness heuristics.\nExperiments reveal that four LLMs applied to ReHeAT all exhibited\nrepresentativeness heuristic biases. We further identify that the model's\nreasoning steps are often incorrectly based on a stereotype rather than on the\nproblem's description. Interestingly, the performance improves when adding a\nhint in the prompt to remind the model to use its knowledge. This suggests the\nuniqueness of the representativeness heuristic compared to traditional biases.\nIt can occur even when LLMs possess the correct knowledge while falling into a\ncognitive trap. This highlights the importance of future research focusing on\nthe representativeness heuristic in model reasoning and decision-making and on\ndeveloping solutions to address it.\n","authors":["Pengda Wang","Zilin Xiao","Hanjie Chen","Frederick L. Oswald"],"pdf_url":"https://arxiv.org/pdf/2404.01461v4.pdf","comment":"Published as a conference paper at COLM 2024"},{"id":"http://arxiv.org/abs/2407.16127v1","updated":"2024-07-23T02:25:01Z","published":"2024-07-23T02:25:01Z","title":"Finetuning Generative Large Language Models with Discrimination\n  Instructions for Knowledge Graph Completion","summary":"  Traditional knowledge graph (KG) completion models learn embeddings to\npredict missing facts. Recent works attempt to complete KGs in a\ntext-generation manner with large language models (LLMs). However, they need to\nground the output of LLMs to KG entities, which inevitably brings errors. In\nthis paper, we present a finetuning framework, DIFT, aiming to unleash the KG\ncompletion ability of LLMs and avoid grounding errors. Given an incomplete\nfact, DIFT employs a lightweight model to obtain candidate entities and\nfinetunes an LLM with discrimination instructions to select the correct one\nfrom the given candidates. To improve performance while reducing instruction\ndata, DIFT uses a truncated sampling method to select useful facts for\nfinetuning and injects KG embeddings into the LLM. Extensive experiments on\nbenchmark datasets demonstrate the effectiveness of our proposed framework.\n","authors":["Yang Liu","Xiaobin Tian","Zequn Sun","Wei Hu"],"pdf_url":"https://arxiv.org/pdf/2407.16127v1.pdf","comment":"Accepted in the 23rd International Semantic Web Conference (ISWC\n  2024)"},{"id":"http://arxiv.org/abs/2407.07791v2","updated":"2024-07-23T01:59:54Z","published":"2024-07-10T16:08:46Z","title":"Flooding Spread of Manipulated Knowledge in LLM-Based Multi-Agent\n  Communities","summary":"  The rapid adoption of large language models (LLMs) in multi-agent systems has\nhighlighted their impressive capabilities in various applications, such as\ncollaborative problem-solving and autonomous negotiation. However, the security\nimplications of these LLM-based multi-agent systems have not been thoroughly\ninvestigated, particularly concerning the spread of manipulated knowledge. In\nthis paper, we investigate this critical issue by constructing a detailed\nthreat model and a comprehensive simulation environment that mirrors real-world\nmulti-agent deployments in a trusted platform. Subsequently, we propose a novel\ntwo-stage attack method involving Persuasiveness Injection and Manipulated\nKnowledge Injection to systematically explore the potential for manipulated\nknowledge (i.e., counterfactual and toxic knowledge) spread without explicit\nprompt manipulation.\n  Our method leverages the inherent vulnerabilities of LLMs in handling world\nknowledge, which can be exploited by attackers to unconsciously spread\nfabricated information. Through extensive experiments, we demonstrate that our\nattack method can successfully induce LLM-based agents to spread both\ncounterfactual and toxic knowledge without degrading their foundational\ncapabilities during agent communication. Furthermore, we show that these\nmanipulations can persist through popular retrieval-augmented generation\nframeworks, where several benign agents store and retrieve manipulated chat\nhistories for future interactions. This persistence indicates that even after\nthe interaction has ended, the benign agents may continue to be influenced by\nmanipulated knowledge. Our findings reveal significant security risks in\nLLM-based multi-agent systems, emphasizing the imperative need for robust\ndefenses against manipulated knowledge spread, such as introducing ``guardian''\nagents and advanced fact-checking tools.\n","authors":["Tianjie Ju","Yiting Wang","Xinbei Ma","Pengzhou Cheng","Haodong Zhao","Yulong Wang","Lifeng Liu","Jian Xie","Zhuosheng Zhang","Gongshen Liu"],"pdf_url":"https://arxiv.org/pdf/2407.07791v2.pdf","comment":"18 Pages, working in progress"},{"id":"http://arxiv.org/abs/2407.16110v1","updated":"2024-07-23T00:52:12Z","published":"2024-07-23T00:52:12Z","title":"Analyzing the Polysemy Evolution using Semantic Cells","summary":"  The senses of words evolve. The sense of the same word may change from today\nto tomorrow, and multiple senses of the same word may be the result of the\nevolution of each other, that is, they may be parents and children. If we view\nJuba as an evolving ecosystem, the paradigm of learning the correct answer,\nwhich does not move with the sense of a word, is no longer valid. This paper is\na case study that shows that word polysemy is an evolutionary consequence of\nthe modification of Semantic Cells, which has al-ready been presented by the\nauthor, by introducing a small amount of diversity in its initial state as an\nexample of analyzing the current set of short sentences. In particular, the\nanalysis of a sentence sequence of 1000 sentences in some order for each of the\nfour senses of the word Spring, collected using Chat GPT, shows that the word\nacquires the most polysemy monotonically in the analysis when the senses are\narranged in the order in which they have evolved. In other words, we present a\nmethod for analyzing the dynamism of a word's acquiring polysemy with evolution\nand, at the same time, a methodology for viewing polysemy from an evolutionary\nframework rather than a learning-based one.\n","authors":["Yukio Ohsawa","Dingming Xue","Kaira Sekiguchi"],"pdf_url":"https://arxiv.org/pdf/2407.16110v1.pdf","comment":"11 pages, 2 figures. arXiv admin note: text overlap with\n  arXiv:2404.14749"},{"id":"http://arxiv.org/abs/2406.04496v2","updated":"2024-07-23T00:46:37Z","published":"2024-06-06T20:41:36Z","title":"Time Sensitive Knowledge Editing through Efficient Finetuning","summary":"  Large Language Models (LLMs) have demonstrated impressive capability in\ndifferent tasks and are bringing transformative changes to many domains.\nHowever, keeping the knowledge in LLMs up-to-date remains a challenge once\npretraining is complete. It is thus essential to design effective methods to\nboth update obsolete knowledge and induce new knowledge into LLMs. Existing\nlocate-and-edit knowledge editing (KE) method suffers from two limitations.\nFirst, the post-edit LLMs by such methods generally have poor capability in\nanswering complex queries that require multi-hop reasoning. Second, the long\nrun-time of such locate-and-edit methods to perform knowledge edits make it\ninfeasible for large scale KE in practice. In this paper, we explore\nParameter-Efficient Fine-Tuning (PEFT) techniques as an alternative for KE. We\ncurate a more comprehensive temporal KE dataset with both knowledge update and\nknowledge injection examples for KE performance benchmarking. We further probe\nthe effect of fine-tuning on a range of layers in an LLM for the multi-hop QA\ntask. We find that PEFT performs better than locate-and-edit techniques for\ntime-sensitive knowledge edits.\n","authors":["Xiou Ge","Ali Mousavi","Edouard Grave","Armand Joulin","Kun Qian","Benjamin Han","Mostafa Arefiyan","Yunyao Li"],"pdf_url":"https://arxiv.org/pdf/2406.04496v2.pdf","comment":"ACL 2024 main"},{"id":"http://arxiv.org/abs/2407.16908v1","updated":"2024-07-23T23:58:19Z","published":"2024-07-23T23:58:19Z","title":"Generation Constraint Scaling Can Mitigate Hallucination","summary":"  Addressing the issue of hallucinations in large language models (LLMs) is a\ncritical challenge. As the cognitive mechanisms of hallucination have been\nrelated to memory, here we explore hallucination for LLM that is enabled with\nexplicit memory mechanisms. We empirically demonstrate that by simply scaling\nthe readout vector that constrains generation in a memory-augmented LLM\ndecoder, hallucination mitigation can be achieved in a training-free manner.\nOur method is geometry-inspired and outperforms a state-of-the-art LLM editing\nmethod on the task of generation of Wikipedia-like biography entries both in\nterms of generation quality and runtime complexity.\n","authors":["Georgios Kollias","Payel Das","Subhajit Chaudhury"],"pdf_url":"https://arxiv.org/pdf/2407.16908v1.pdf","comment":"7 pages; accepted at ICML 2024 Workshop on Large Language Models and\n  Cognition"},{"id":"http://arxiv.org/abs/2403.13031v2","updated":"2024-07-23T22:56:13Z","published":"2024-03-19T07:25:02Z","title":"RigorLLM: Resilient Guardrails for Large Language Models against\n  Undesired Content","summary":"  Recent advancements in Large Language Models (LLMs) have showcased remarkable\ncapabilities across various tasks in different domains. However, the emergence\nof biases and the potential for generating harmful content in LLMs,\nparticularly under malicious inputs, pose significant challenges. Current\nmitigation strategies, while effective, are not resilient under adversarial\nattacks. This paper introduces Resilient Guardrails for Large Language Models\n(RigorLLM), a novel framework designed to efficiently and effectively moderate\nharmful and unsafe inputs and outputs for LLMs. By employing a multi-faceted\napproach that includes energy-based training data augmentation through Langevin\ndynamics, optimizing a safe suffix for inputs via minimax optimization, and\nintegrating a fusion-based model combining robust KNN with LLMs based on our\ndata augmentation, RigorLLM offers a robust solution to harmful content\nmoderation. Our experimental evaluations demonstrate that RigorLLM not only\noutperforms existing baselines like OpenAI API and Perspective API in detecting\nharmful content but also exhibits unparalleled resilience to jailbreaking\nattacks. The innovative use of constrained optimization and a fusion-based\nguardrail approach represents a significant step forward in developing more\nsecure and reliable LLMs, setting a new standard for content moderation\nframeworks in the face of evolving digital threats.\n","authors":["Zhuowen Yuan","Zidi Xiong","Yi Zeng","Ning Yu","Ruoxi Jia","Dawn Song","Bo Li"],"pdf_url":"https://arxiv.org/pdf/2403.13031v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16860v1","updated":"2024-07-23T22:04:04Z","published":"2024-07-23T22:04:04Z","title":"$\\textit{BenchIE}^{FL}$ : A Manually Re-Annotated Fact-Based Open\n  Information Extraction Benchmark","summary":"  Open Information Extraction (OIE) is a field of natural language processing\nthat aims to present textual information in a format that allows it to be\norganized, analyzed and reflected upon. Numerous OIE systems are developed,\nclaiming ever-increasing performance, marking the need for objective\nbenchmarks. BenchIE is the latest reference we know of. Despite being very well\nthought out, we noticed a number of issues we believe are limiting. Therefore,\nwe propose $\\textit{BenchIE}^{FL}$, a new OIE benchmark which fully enforces\nthe principles of BenchIE while containing fewer errors, omissions and\nshortcomings when candidate facts are matched towards reference ones.\n$\\textit{BenchIE}^{FL}$ allows insightful conclusions to be drawn on the actual\nperformance of OIE extractors.\n","authors":["Fabrice Lamarche","Philippe Langlais"],"pdf_url":"https://arxiv.org/pdf/2407.16860v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16837v1","updated":"2024-07-23T21:02:38Z","published":"2024-07-23T21:02:38Z","title":"CompBench: A Comparative Reasoning Benchmark for Multimodal LLMs","summary":"  The ability to compare objects, scenes, or situations is crucial for\neffective decision-making and problem-solving in everyday life. For instance,\ncomparing the freshness of apples enables better choices during grocery\nshopping, while comparing sofa designs helps optimize the aesthetics of our\nliving space. Despite its significance, the comparative capability is largely\nunexplored in artificial general intelligence (AGI). In this paper, we\nintroduce CompBench, a benchmark designed to evaluate the comparative reasoning\ncapability of multimodal large language models (MLLMs). CompBench mines and\npairs images through visually oriented questions covering eight dimensions of\nrelative comparison: visual attribute, existence, state, emotion, temporality,\nspatiality, quantity, and quality. We curate a collection of around 40K image\npairs using metadata from diverse vision datasets and CLIP similarity scores.\nThese image pairs span a broad array of visual domains, including animals,\nfashion, sports, and both outdoor and indoor scenes. The questions are\ncarefully crafted to discern relative characteristics between two images and\nare labeled by human annotators for accuracy and relevance. We use CompBench to\nevaluate recent MLLMs, including GPT-4V(ision), Gemini-Pro, and LLaVA-1.6. Our\nresults reveal notable shortcomings in their comparative abilities. We believe\nCompBench not only sheds light on these limitations but also establishes a\nsolid foundation for future enhancements in the comparative capability of\nMLLMs.\n","authors":["Jihyung Kil","Zheda Mai","Justin Lee","Zihe Wang","Kerrie Cheng","Lemeng Wang","Ye Liu","Arpita Chowdhury","Wei-Lun Chao"],"pdf_url":"https://arxiv.org/pdf/2407.16837v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16833v1","updated":"2024-07-23T20:51:52Z","published":"2024-07-23T20:51:52Z","title":"Retrieval Augmented Generation or Long-Context LLMs? A Comprehensive\n  Study and Hybrid Approach","summary":"  Retrieval Augmented Generation (RAG) has been a powerful tool for Large\nLanguage Models (LLMs) to efficiently process overly lengthy contexts. However,\nrecent LLMs like Gemini-1.5 and GPT-4 show exceptional capabilities to\nunderstand long contexts directly. We conduct a comprehensive comparison\nbetween RAG and long-context (LC) LLMs, aiming to leverage the strengths of\nboth. We benchmark RAG and LC across various public datasets using three latest\nLLMs. Results reveal that when resourced sufficiently, LC consistently\noutperforms RAG in terms of average performance. However, RAG's significantly\nlower cost remains a distinct advantage. Based on this observation, we propose\nSelf-Route, a simple yet effective method that routes queries to RAG or LC\nbased on model self-reflection. Self-Route significantly reduces the\ncomputation cost while maintaining a comparable performance to LC. Our findings\nprovide a guideline for long-context applications of LLMs using RAG and LC.\n","authors":["Zhuowan Li","Cheng Li","Mingyang Zhang","Qiaozhu Mei","Michael Bendersky"],"pdf_url":"https://arxiv.org/pdf/2407.16833v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.16254v3","updated":"2024-07-23T20:38:11Z","published":"2023-11-27T19:02:17Z","title":"Safe-CLIP: Removing NSFW Concepts from Vision-and-Language Models","summary":"  Large-scale vision-and-language models, such as CLIP, are typically trained\non web-scale data, which can introduce inappropriate content and lead to the\ndevelopment of unsafe and biased behavior. This, in turn, hampers their\napplicability in sensitive and trustworthy contexts and could raise significant\nconcerns in their adoption. Our research introduces a novel approach to\nenhancing the safety of vision-and-language models by diminishing their\nsensitivity to NSFW (not safe for work) inputs. In particular, our methodology\nseeks to sever \"toxic\" linguistic and visual concepts, unlearning the linkage\nbetween unsafe linguistic or visual items and unsafe regions of the embedding\nspace. We show how this can be done by fine-tuning a CLIP model on synthetic\ndata obtained from a large language model trained to convert between safe and\nunsafe sentences, and a text-to-image generator. We conduct extensive\nexperiments on the resulting embedding space for cross-modal retrieval,\ntext-to-image, and image-to-text generation, where we show that our model can\nbe remarkably employed with pre-trained generative models. Our source code and\ntrained models are available at: https://github.com/aimagelab/safe-clip.\n","authors":["Samuele Poppi","Tobia Poppi","Federico Cocchi","Marcella Cornia","Lorenzo Baraldi","Rita Cucchiara"],"pdf_url":"https://arxiv.org/pdf/2311.16254v3.pdf","comment":"ECCV 2024"},{"id":"http://arxiv.org/abs/2402.07179v3","updated":"2024-07-23T19:41:05Z","published":"2024-02-11T12:25:41Z","title":"Prompt Perturbation in Retrieval-Augmented Generation based Large\n  Language Models","summary":"  The robustness of large language models (LLMs) becomes increasingly important\nas their use rapidly grows in a wide range of domains. Retrieval-Augmented\nGeneration (RAG) is considered as a means to improve the trustworthiness of\ntext generation from LLMs. However, how the outputs from RAG-based LLMs are\naffected by slightly different inputs is not well studied. In this work, we\nfind that the insertion of even a short prefix to the prompt leads to the\ngeneration of outputs far away from factually correct answers. We\nsystematically evaluate the effect of such prefixes on RAG by introducing a\nnovel optimization technique called Gradient Guided Prompt Perturbation (GGPP).\nGGPP achieves a high success rate in steering outputs of RAG-based LLMs to\ntargeted wrong answers. It can also cope with instructions in the prompts\nrequesting to ignore irrelevant context. We also exploit LLMs' neuron\nactivation difference between prompts with and without GGPP perturbations to\ngive a method that improves the robustness of RAG-based LLMs through a highly\neffective detector trained on neuron activation triggered by GGPP generated\nprompts. Our evaluation on open-sourced LLMs demonstrates the effectiveness of\nour methods.\n","authors":["Zhibo Hu","Chen Wang","Yanfeng Shu"," Helen"," Paik","Liming Zhu"],"pdf_url":"https://arxiv.org/pdf/2402.07179v3.pdf","comment":"12 pages, 9 figures"},{"id":"http://arxiv.org/abs/2403.01749v2","updated":"2024-07-23T19:19:02Z","published":"2024-03-04T05:57:50Z","title":"Differentially Private Synthetic Data via Foundation Model APIs 2: Text","summary":"  Text data has become extremely valuable due to the emergence of machine\nlearning algorithms that learn from it. A lot of high-quality text data\ngenerated in the real world is private and therefore cannot be shared or used\nfreely due to privacy concerns. Generating synthetic replicas of private text\ndata with a formal privacy guarantee, i.e., differential privacy (DP), offers a\npromising and scalable solution. However, existing methods necessitate DP\nfinetuning of large language models (LLMs) on private data to generate DP\nsynthetic data. This approach is not viable for proprietary LLMs (e.g.,\nGPT-3.5) and also demands considerable computational resources for open-source\nLLMs. Lin et al. (2024) recently introduced the Private Evolution (PE)\nalgorithm to generate DP synthetic images with only API access to diffusion\nmodels. In this work, we propose an augmented PE algorithm, named Aug-PE, that\napplies to the complex setting of text. We use API access to an LLM and\ngenerate DP synthetic text without any model training. We conduct comprehensive\nexperiments on three benchmark datasets. Our results demonstrate that Aug-PE\nproduces DP synthetic text that yields competitive utility with the SOTA DP\nfinetuning baselines. This underscores the feasibility of relying solely on API\naccess of LLMs to produce high-quality DP synthetic texts, thereby facilitating\nmore accessible routes to privacy-preserving LLM applications. Our code and\ndata are available at https://github.com/AI-secure/aug-pe.\n","authors":["Chulin Xie","Zinan Lin","Arturs Backurs","Sivakanth Gopi","Da Yu","Huseyin A Inan","Harsha Nori","Haotian Jiang","Huishuai Zhang","Yin Tat Lee","Bo Li","Sergey Yekhanin"],"pdf_url":"https://arxiv.org/pdf/2403.01749v2.pdf","comment":"ICML'24 Spotlight"},{"id":"http://arxiv.org/abs/2305.14341v3","updated":"2024-07-23T18:28:43Z","published":"2023-05-23T17:59:19Z","title":"APPLS: Evaluating Evaluation Metrics for Plain Language Summarization","summary":"  While there has been significant development of models for Plain Language\nSummarization (PLS), evaluation remains a challenge. PLS lacks a dedicated\nassessment metric, and the suitability of text generation evaluation metrics is\nunclear due to the unique transformations involved (e.g., adding background\nexplanations, removing jargon). To address these questions, our study\nintroduces a granular meta-evaluation testbed, APPLS, designed to evaluate\nmetrics for PLS. We identify four PLS criteria from previous work --\ninformativeness, simplification, coherence, and faithfulness -- and define a\nset of perturbations corresponding to these criteria that sensitive metrics\nshould be able to detect. We apply these perturbations to extractive hypotheses\nfor two PLS datasets to form our testbed. Using APPLS, we assess performance of\n14 metrics, including automated scores, lexical features, and LLM prompt-based\nevaluations. Our analysis reveals that while some current metrics show\nsensitivity to specific criteria, no single method captures all four criteria\nsimultaneously. We therefore recommend a suite of automated metrics be used to\ncapture PLS quality along all relevant criteria. This work contributes the\nfirst meta-evaluation testbed for PLS and a comprehensive evaluation of\nexisting metrics. APPLS and our evaluation code is available at\nhttps://github.com/LinguisticAnomalies/APPLS.\n","authors":["Yue Guo","Tal August","Gondy Leroy","Trevor Cohen","Lucy Lu Wang"],"pdf_url":"https://arxiv.org/pdf/2305.14341v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.04604v3","updated":"2024-07-23T18:26:32Z","published":"2024-06-07T03:27:51Z","title":"Learning Task Decomposition to Assist Humans in Competitive Programming","summary":"  When using language models (LMs) to solve complex problems, humans might\nstruggle to understand the LM-generated solutions and repair the flawed ones.\nTo assist humans in repairing them, we propose to automatically decompose\ncomplex solutions into multiple simpler pieces that correspond to specific\nsubtasks. We introduce a novel objective for learning task decomposition,\ntermed assistive value (AssistV), which measures the feasibility and speed for\nhumans to repair the decomposed solution. We collect a dataset of human repair\nexperiences on different decomposed solutions. Utilizing the collected data as\nin-context examples, we then learn to critique, refine, and rank decomposed\nsolutions to improve AssistV. We validate our method under competitive\nprogramming problems: under 177 hours of human study, our method enables\nnon-experts to solve 33.3\\% more problems, speeds them up by 3.3x, and empowers\nthem to match unassisted experts.\n","authors":["Jiaxin Wen","Ruiqi Zhong","Pei Ke","Zhihong Shao","Hongning Wang","Minlie Huang"],"pdf_url":"https://arxiv.org/pdf/2406.04604v3.pdf","comment":"ACL 2024 Main Conference"},{"id":"http://arxiv.org/abs/2407.07004v2","updated":"2024-07-23T18:20:45Z","published":"2024-07-09T16:17:16Z","title":"Empirical analysis of Binding Precedent efficiency in the Brazilian\n  Supreme Court via Similar Case Retrieval","summary":"  Binding precedents (S\\'umulas Vinculantes) constitute a juridical instrument\nunique to the Brazilian legal system and whose objectives include the\nprotection of the Federal Supreme Court against repetitive demands. Studies of\nthe effectiveness of these instruments in decreasing the Court's exposure to\nsimilar cases, however, indicate that they tend to fail in such a direction,\nwith some of the binding precedents seemingly creating new demands. We\nempirically assess the legal impact of five binding precedents, 11, 14, 17, 26\nand 37, at the highest court level through their effects on the legal subjects\nthey address. This analysis is only possible through the comparison of the\nCourt's ruling about the precedents' themes before they are created, which\nmeans that these decisions should be detected through techniques of Similar\nCase Retrieval. The contributions of this article are therefore twofold: on the\nmathematical side, we compare the uses of different methods of Natural Language\nProcessing -- TF-IDF, LSTM, BERT, and regex -- for Similar Case Retrieval,\nwhereas on the legal side, we contrast the inefficiency of these binding\nprecedents with a set of hypotheses that may justify their repeated usage. We\nobserve that the deep learning models performed significantly worse in the\nspecific Similar Case Retrieval task and that the reasons for binding\nprecedents to fail in responding to repetitive demand are heterogeneous and\ncase-dependent, making it impossible to single out a specific cause.\n","authors":["Raphaël Tinarrage","Henrique Ennes","Lucas E. Resck","Lucas T. Gomes","Jean R. Ponciano","Jorge Poco"],"pdf_url":"https://arxiv.org/pdf/2407.07004v2.pdf","comment":"54 pages, 22 figures"},{"id":"http://arxiv.org/abs/2301.10527v3","updated":"2024-07-23T18:17:35Z","published":"2023-01-25T11:21:12Z","title":"Cross-lingual Argument Mining in the Medical Domain","summary":"  Nowadays the medical domain is receiving more and more attention in\napplications involving Artificial Intelligence as clinicians decision-making is\nincreasingly dependent on dealing with enormous amounts of unstructured textual\ndata. In this context, Argument Mining (AM) helps to meaningfully structure\ntextual data by identifying the argumentative components in the text and\nclassifying the relations between them. However, as it is the case for man\ntasks in Natural Language Processing in general and in medical text processing\nin particular, the large majority of the work on computational argumentation\nhas been focusing only on the English language. In this paper, we investigate\nseveral strategies to perform AM in medical texts for a language such as\nSpanish, for which no annotated data is available. Our work shows that\nautomatically translating and projecting annotations (data-transfer) from\nEnglish to a given target language is an effective way to generate annotated\ndata without costly manual intervention. Furthermore, and contrary to\nconclusions from previous work for other sequence labelling tasks, our\nexperiments demonstrate that data-transfer outperforms methods based on the\ncrosslingual transfer capabilities of multilingual pre-trained language models\n(model-transfer). Finally, we show how the automatically generated data in\nSpanish can also be used to improve results in the original English monolingual\nsetting, providing thus a fully automatic data augmentation strategy.\n","authors":["Anar Yeginbergen","Rodrigo Agerri"],"pdf_url":"https://arxiv.org/pdf/2301.10527v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16772v1","updated":"2024-07-23T18:10:43Z","published":"2024-07-23T18:10:43Z","title":"VisMin: Visual Minimal-Change Understanding","summary":"  Fine-grained understanding of objects, attributes, and relationships between\nobjects is crucial for visual-language models (VLMs). Existing benchmarks\nprimarily focus on evaluating VLMs' capability to distinguish between two very\nsimilar \\textit{captions} given an image. In this paper, we introduce a new,\nchallenging benchmark termed \\textbf{Vis}ual \\textbf{Min}imal-Change\nUnderstanding (VisMin), which requires models to predict the correct\nimage-caption match given two images and two captions. The image pair and\ncaption pair contain minimal changes, i.e., only one aspect changes at a time\nfrom among the following: \\textit{object}, \\textit{attribute}, \\textit{count},\nand \\textit{spatial relation}. These changes test the models' understanding of\nobjects, attributes (such as color, material, shape), counts, and spatial\nrelationships between objects. We built an automatic framework using large\nlanguage models and diffusion models, followed by a rigorous 4-step\nverification process by human annotators. Empirical experiments reveal that\ncurrent VLMs exhibit notable deficiencies in understanding spatial\nrelationships and counting abilities. We also generate a large-scale training\ndataset to finetune CLIP and Idefics2, showing significant improvements in\nfine-grained understanding across benchmarks and in CLIP's general image-text\nalignment. We release all resources, including the benchmark, training data,\nand finetuned model checkpoints, at \\url{https://vismin.net/}.\n","authors":["Rabiul Awal","Saba Ahmadi","Le Zhang","Aishwarya Agrawal"],"pdf_url":"https://arxiv.org/pdf/2407.16772v1.pdf","comment":"Project URL at https://vismin.net/"},{"id":"http://arxiv.org/abs/2407.14962v2","updated":"2024-07-23T18:07:28Z","published":"2024-07-20T18:48:35Z","title":"Recent Advances in Generative AI and Large Language Models: Current\n  Status, Challenges, and Perspectives","summary":"  The emergence of Generative Artificial Intelligence (AI) and Large Language\nModels (LLMs) has marked a new era of Natural Language Processing (NLP),\nintroducing unprecedented capabilities that are revolutionizing various\ndomains. This paper explores the current state of these cutting-edge\ntechnologies, demonstrating their remarkable advancements and wide-ranging\napplications. Our paper contributes to providing a holistic perspective on the\ntechnical foundations, practical applications, and emerging challenges within\nthe evolving landscape of Generative AI and LLMs. We believe that understanding\nthe generative capabilities of AI systems and the specific context of LLMs is\ncrucial for researchers, practitioners, and policymakers to collaboratively\nshape the responsible and ethical integration of these technologies into\nvarious domains. Furthermore, we identify and address main research gaps,\nproviding valuable insights to guide future research endeavors within the AI\nresearch community.\n","authors":["Desta Haileselassie Hagos","Rick Battle","Danda B. Rawat"],"pdf_url":"https://arxiv.org/pdf/2407.14962v2.pdf","comment":"This version is accepted for publication in the journal of IEEE\n  Transactions on Artificial Intelligence (TAI)"},{"id":"http://arxiv.org/abs/2311.04378v4","updated":"2024-07-23T18:05:59Z","published":"2023-11-07T22:52:54Z","title":"Watermarks in the Sand: Impossibility of Strong Watermarking for\n  Generative Models","summary":"  Watermarking generative models consists of planting a statistical signal\n(watermark) in a model's output so that it can be later verified that the\noutput was generated by the given model. A strong watermarking scheme satisfies\nthe property that a computationally bounded attacker cannot erase the watermark\nwithout causing significant quality degradation. In this paper, we study the\n(im)possibility of strong watermarking schemes. We prove that, under\nwell-specified and natural assumptions, strong watermarking is impossible to\nachieve. This holds even in the private detection algorithm setting, where the\nwatermark insertion and detection algorithms share a secret key, unknown to the\nattacker. To prove this result, we introduce a generic efficient watermark\nattack; the attacker is not required to know the private key of the scheme or\neven which scheme is used. Our attack is based on two assumptions: (1) The\nattacker has access to a \"quality oracle\" that can evaluate whether a candidate\noutput is a high-quality response to a prompt, and (2) The attacker has access\nto a \"perturbation oracle\" which can modify an output with a nontrivial\nprobability of maintaining quality, and which induces an efficiently mixing\nrandom walk on high-quality outputs. We argue that both assumptions can be\nsatisfied in practice by an attacker with weaker computational capabilities\nthan the watermarked model itself, to which the attacker has only black-box\naccess. Furthermore, our assumptions will likely only be easier to satisfy over\ntime as models grow in capabilities and modalities. We demonstrate the\nfeasibility of our attack by instantiating it to attack three existing\nwatermarking schemes for large language models: Kirchenbauer et al. (2023),\nKuditipudi et al. (2023), and Zhao et al. (2023). The same attack successfully\nremoves the watermarks planted by all three schemes, with only minor quality\ndegradation.\n","authors":["Hanlin Zhang","Benjamin L. Edelman","Danilo Francati","Daniele Venturi","Giuseppe Ateniese","Boaz Barak"],"pdf_url":"https://arxiv.org/pdf/2311.04378v4.pdf","comment":"ICML 2024. Website: https://hanlin-zhang.com/impossibility-watermarks"},{"id":"http://arxiv.org/abs/2407.16737v1","updated":"2024-07-23T17:15:23Z","published":"2024-07-23T17:15:23Z","title":"A Survey of Text Style Transfer: Applications and Ethical Implications","summary":"  Text style transfer (TST) is an important task in controllable text\ngeneration, which aims to control selected attributes of language use, such as\npoliteness, formality, or sentiment, without altering the style-independent\ncontent of the text. The field has received considerable research attention in\nrecent years and has already been covered in several reviews, but the focus has\nmostly been on the development of new algorithms and learning from different\ntypes of data (supervised, unsupervised, out-of-domain, etc.) and not so much\non the application side. However, TST-related technologies are gradually\nreaching a production- and deployment-ready level, and therefore, the inclusion\nof the application perspective in TST research becomes crucial. Similarly, the\noften overlooked ethical considerations of TST technology have become a\npressing issue. This paper presents a comprehensive review of TST applications\nthat have been researched over the years, using both traditional linguistic\napproaches and more recent deep learning methods. We discuss current\nchallenges, future research directions, and ethical implications of TST\napplications in text generation. By providing a holistic overview of the\nlandscape of TST applications, we hope to stimulate further research and\ncontribute to a better understanding of the potential as well as ethical\nconsiderations associated with TST.\n","authors":["Sourabrata Mukherjee","Mateusz Lango","Zdenek Kasner","Ondrej Dušek"],"pdf_url":"https://arxiv.org/pdf/2407.16737v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16724v1","updated":"2024-07-23T12:38:48Z","published":"2024-07-23T12:38:48Z","title":"Educating LLMs like Human Students: Structure-aware Injection of Domain\n  Knowledge","summary":"  This paper presents a pioneering methodology, termed StructTuning, to\nefficiently transform foundation Large Language Models (LLMs) into domain\nspecialists. It significantly minimizes the training corpus requirement to a\nmere 0.3% while achieving an impressive 50% of traditional knowledge injection\nperformance. Our method is inspired by the educational processes for human\nstudents, particularly how structured domain knowledge from textbooks is\nabsorbed and then applied to tackle real-world challenges through specific\nexercises. Based on this, we propose a novel two-stage knowledge injection\nstrategy: Structure-aware Continual Pre-Training (SCPT) and Structure-aware\nSupervised Fine-Tuning (SSFT). In the SCPT phase, we organize the training data\ninto an auto-generated taxonomy of domain knowledge, enabling LLMs to\neffectively memorize textual segments linked to specific expertise within the\ntaxonomy's architecture. Subsequently, in the SSFT phase, we explicitly prompt\nmodels to reveal the underlying knowledge structure in their outputs,\nleveraging this structured domain insight to address practical problems\nadeptly. Our ultimate method has undergone extensive evaluations across model\narchitectures and scales, using closed-book question-answering tasks on\nLongBench and MMedBench datasets. Remarkably, our method matches 50% of the\nimprovement displayed by the state-of-the-art MMedLM2 on MMedBench, but with\nonly 0.3% quantity of the training corpus. This breakthrough showcases the\npotential to scale up our StructTuning for stronger domain-specific LLMs. Code\nwill be made public soon.\n","authors":["Kai Liu","Ze Chen","Zhihang Fu","Rongxin Jiang","Fan Zhou","Yaowu Chen","Yue Wu","Jieping Ye"],"pdf_url":"https://arxiv.org/pdf/2407.16724v1.pdf","comment":"N/A"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2407.16698v1","updated":"2024-07-23T17:59:59Z","published":"2024-07-23T17:59:59Z","title":"Diffusion Models for Monocular Depth Estimation: Overcoming Challenging\n  Conditions","summary":"  We present a novel approach designed to address the complexities posed by\nchallenging, out-of-distribution data in the single-image depth estimation\ntask. Starting with images that facilitate depth prediction due to the absence\nof unfavorable factors, we systematically generate new, user-defined scenes\nwith a comprehensive set of challenges and associated depth information. This\nis achieved by leveraging cutting-edge text-to-image diffusion models with\ndepth-aware control, known for synthesizing high-quality image content from\ntextual prompts while preserving the coherence of 3D structure between\ngenerated and source imagery. Subsequent fine-tuning of any monocular depth\nnetwork is carried out through a self-distillation protocol that takes into\naccount images generated using our strategy and its own depth predictions on\nsimple, unchallenging scenes. Experiments on benchmarks tailored for our\npurposes demonstrate the effectiveness and versatility of our proposal.\n","authors":["Fabio Tosi","Pierluigi Zama Ramirez","Matteo Poggi"],"pdf_url":"https://arxiv.org/pdf/2407.16698v1.pdf","comment":"ECCV 2024. Code: https://github.com/fabiotosi92/Diffusion4RobustDepth\n  Project page: https://diffusion4robustdepth.github.io/"},{"id":"http://arxiv.org/abs/2407.16697v1","updated":"2024-07-23T17:59:44Z","published":"2024-07-23T17:59:44Z","title":"AbdomenAtlas: A Large-Scale, Detailed-Annotated, & Multi-Center Dataset\n  for Efficient Transfer Learning and Open Algorithmic Benchmarking","summary":"  We introduce the largest abdominal CT dataset (termed AbdomenAtlas) of 20,460\nthree-dimensional CT volumes sourced from 112 hospitals across diverse\npopulations, geographies, and facilities. AbdomenAtlas provides 673K\nhigh-quality masks of anatomical structures in the abdominal region annotated\nby a team of 10 radiologists with the help of AI algorithms. We start by having\nexpert radiologists manually annotate 22 anatomical structures in 5,246 CT\nvolumes. Following this, a semi-automatic annotation procedure is performed for\nthe remaining CT volumes, where radiologists revise the annotations predicted\nby AI, and in turn, AI improves its predictions by learning from revised\nannotations. Such a large-scale, detailed-annotated, and multi-center dataset\nis needed for two reasons. Firstly, AbdomenAtlas provides important resources\nfor AI development at scale, branded as large pre-trained models, which can\nalleviate the annotation workload of expert radiologists to transfer to broader\nclinical applications. Secondly, AbdomenAtlas establishes a large-scale\nbenchmark for evaluating AI algorithms -- the more data we use to test the\nalgorithms, the better we can guarantee reliable performance in complex\nclinical scenarios. An ISBI & MICCAI challenge named BodyMaps: Towards 3D Atlas\nof Human Body was launched using a subset of our AbdomenAtlas, aiming to\nstimulate AI innovation and to benchmark segmentation accuracy, inference\nefficiency, and domain generalizability. We hope our AbdomenAtlas can set the\nstage for larger-scale clinical trials and offer exceptional opportunities to\npractitioners in the medical imaging community. Codes, models, and datasets are\navailable at https://www.zongweiz.com/dataset\n","authors":["Wenxuan Li","Chongyu Qu","Xiaoxi Chen","Pedro R. A. S. Bassi","Yijia Shi","Yuxiang Lai","Qian Yu","Huimin Xue","Yixiong Chen","Xiaorui Lin","Yutong Tang","Yining Cao","Haoqi Han","Zheyuan Zhang","Jiawei Liu","Tiezheng Zhang","Yujiu Ma","Jincheng Wang","Guang Zhang","Alan Yuille","Zongwei Zhou"],"pdf_url":"https://arxiv.org/pdf/2407.16697v1.pdf","comment":"Published in Medical Image Analysis"},{"id":"http://arxiv.org/abs/2407.16696v1","updated":"2024-07-23T17:58:26Z","published":"2024-07-23T17:58:26Z","title":"PartGLEE: A Foundation Model for Recognizing and Parsing Any Objects","summary":"  We present PartGLEE, a part-level foundation model for locating and\nidentifying both objects and parts in images. Through a unified framework,\nPartGLEE accomplishes detection, segmentation, and grounding of instances at\nany granularity in the open world scenario. Specifically, we propose a Q-Former\nto construct the hierarchical relationship between objects and parts, parsing\nevery object into corresponding semantic parts. By incorporating a large amount\nof object-level data, the hierarchical relationships can be extended, enabling\nPartGLEE to recognize a rich variety of parts. We conduct comprehensive studies\nto validate the effectiveness of our method, PartGLEE achieves the\nstate-of-the-art performance across various part-level tasks and obtain\ncompetitive results on object-level tasks. The proposed PartGLEE significantly\nenhances hierarchical modeling capabilities and part-level perception over our\nprevious GLEE model. Further analysis indicates that the hierarchical cognitive\nability of PartGLEE is able to facilitate a detailed comprehension in images\nfor mLLMs. The model and code will be released at\nhttps://provencestar.github.io/PartGLEE-Vision/ .\n","authors":["Junyi Li","Junfeng Wu","Weizhi Zhao","Song Bai","Xiang Bai"],"pdf_url":"https://arxiv.org/pdf/2407.16696v1.pdf","comment":"Accepted by ECCV2024, homepage:\n  https://provencestar.github.io/PartGLEE-Vision/"},{"id":"http://arxiv.org/abs/2407.16684v1","updated":"2024-07-23T17:50:00Z","published":"2024-07-23T17:50:00Z","title":"AutoRG-Brain: Grounded Report Generation for Brain MRI","summary":"  Radiologists are tasked with interpreting a large number of images in a daily\nbase, with the responsibility of generating corresponding reports. This\ndemanding workload elevates the risk of human error, potentially leading to\ntreatment delays, increased healthcare costs, revenue loss, and operational\ninefficiencies. To address these challenges, we initiate a series of work on\ngrounded Automatic Report Generation (AutoRG), starting from the brain MRI\ninterpretation system, which supports the delineation of brain structures, the\nlocalization of anomalies, and the generation of well-organized findings. We\nmake contributions from the following aspects, first, on dataset construction,\nwe release a comprehensive dataset encompassing segmentation masks of anomaly\nregions and manually authored reports, termed as RadGenome-Brain MRI. This data\nresource is intended to catalyze ongoing research and development in the field\nof AI-assisted report generation systems. Second, on system design, we propose\nAutoRG-Brain, the first brain MRI report generation system with pixel-level\ngrounded visual clues. Third, for evaluation, we conduct quantitative\nassessments and human evaluations of brain structure segmentation, anomaly\nlocalization, and report generation tasks to provide evidence of its\nreliability and accuracy. This system has been integrated into real clinical\nscenarios, where radiologists were instructed to write reports based on our\ngenerated findings and anomaly segmentation masks. The results demonstrate that\nour system enhances the report-writing skills of junior doctors, aligning their\nperformance more closely with senior doctors, thereby boosting overall\nproductivity.\n","authors":["Jiayu Lei","Xiaoman Zhang","Chaoyi Wu","Lisong Dai","Ya Zhang","Yanyong Zhang","Yanfeng Wang","Weidi Xie","Yuehua Li"],"pdf_url":"https://arxiv.org/pdf/2407.16684v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16682v1","updated":"2024-07-23T17:47:25Z","published":"2024-07-23T17:47:25Z","title":"SAM-CP: Marrying SAM with Composable Prompts for Versatile Segmentation","summary":"  The Segment Anything model (SAM) has shown a generalized ability to group\nimage pixels into patches, but applying it to semantic-aware segmentation still\nfaces major challenges. This paper presents SAM-CP, a simple approach that\nestablishes two types of composable prompts beyond SAM and composes them for\nversatile segmentation. Specifically, given a set of classes (in texts) and a\nset of SAM patches, the Type-I prompt judges whether a SAM patch aligns with a\ntext label, and the Type-II prompt judges whether two SAM patches with the same\ntext label also belong to the same instance. To decrease the complexity in\ndealing with a large number of semantic classes and patches, we establish a\nunified framework that calculates the affinity between (semantic and instance)\nqueries and SAM patches and merges patches with high affinity to the query.\nExperiments show that SAM-CP achieves semantic, instance, and panoptic\nsegmentation in both open and closed domains. In particular, it achieves\nstate-of-the-art performance in open-vocabulary segmentation. Our research\noffers a novel and generalized methodology for equipping vision foundation\nmodels like SAM with multi-grained semantic perception abilities.\n","authors":["Pengfei Chen","Lingxi Xie","Xinyue Huo","Xuehui Yu","Xiaopeng Zhang","Yingfei Sun","Zhenjun Han","Qi Tian"],"pdf_url":"https://arxiv.org/pdf/2407.16682v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16670v1","updated":"2024-07-23T17:39:49Z","published":"2024-07-23T17:39:49Z","title":"FakingRecipe: Detecting Fake News on Short Video Platforms from the\n  Perspective of Creative Process","summary":"  As short-form video-sharing platforms become a significant channel for news\nconsumption, fake news in short videos has emerged as a serious threat in the\nonline information ecosystem, making developing detection methods for this new\nscenario an urgent need. Compared with that in text and image formats, fake\nnews on short video platforms contains rich but heterogeneous information in\nvarious modalities, posing a challenge to effective feature utilization. Unlike\nexisting works mostly focusing on analyzing what is presented, we introduce a\nnovel perspective that considers how it might be created. Through the lens of\nthe creative process behind news video production, our empirical analysis\nuncovers the unique characteristics of fake news videos in material selection\nand editing. Based on the obtained insights, we design FakingRecipe, a creative\nprocess-aware model for detecting fake news short videos. It captures the fake\nnews preferences in material selection from sentimental and semantic aspects\nand considers the traits of material editing from spatial and temporal aspects.\nTo improve evaluation comprehensiveness, we first construct FakeTT, an English\ndataset for this task, and conduct experiments on both FakeTT and the existing\nChinese FakeSV dataset. The results show FakingRecipe's superiority in\ndetecting fake news on short video platforms.\n","authors":["Yuyan Bu","Qiang Sheng","Juan Cao","Peng Qi","Danding Wang","Jintao Li"],"pdf_url":"https://arxiv.org/pdf/2407.16670v1.pdf","comment":"Will appear at ACM Multimedia 2024 (MM 2024), 13 pages, 15 figures"},{"id":"http://arxiv.org/abs/2302.14368v3","updated":"2024-07-23T17:32:51Z","published":"2023-02-28T07:43:00Z","title":"Enhanced Controllability of Diffusion Models via Feature Disentanglement\n  and Realism-Enhanced Sampling Methods","summary":"  As Diffusion Models have shown promising performance, a lot of efforts have\nbeen made to improve the controllability of Diffusion Models. However, how to\ntrain Diffusion Models to have the disentangled latent spaces and how to\nnaturally incorporate the disentangled conditions during the sampling process\nhave been underexplored. In this paper, we present a training framework for\nfeature disentanglement of Diffusion Models (FDiff). We further propose two\nsampling methods that can boost the realism of our Diffusion Models and also\nenhance the controllability. Concisely, we train Diffusion Models conditioned\non two latent features, a spatial content mask, and a flattened style\nembedding. We rely on the inductive bias of the denoising process of Diffusion\nModels to encode pose/layout information in the content feature and\nsemantic/style information in the style feature. Regarding the sampling\nmethods, we first generalize Composable Diffusion Models (GCDM) by breaking the\nconditional independence assumption to allow for some dependence between\nconditional inputs, which is shown to be effective in realistic generation in\nour experiments. Second, we propose timestep-dependent weight scheduling for\ncontent and style features to further improve the performance. We also observe\nbetter controllability of our proposed methods compared to existing methods in\nimage manipulation and image translation.\n","authors":["Wonwoong Cho","Hareesh Ravi","Midhun Harikumar","Vinh Khuc","Krishna Kumar Singh","Jingwan Lu","David I. Inouye","Ajinkya Kale"],"pdf_url":"https://arxiv.org/pdf/2302.14368v3.pdf","comment":"ECCV 2024; Code will be opened after a patent application is granted"},{"id":"http://arxiv.org/abs/2407.16665v1","updated":"2024-07-23T17:32:02Z","published":"2024-07-23T17:32:02Z","title":"A Framework for Pupil Tracking with Event Cameras","summary":"  Saccades are extremely rapid movements of both eyes that occur\nsimultaneously, typically observed when an individual shifts their focus from\none object to another. These movements are among the swiftest produced by\nhumans and possess the potential to achieve velocities greater than that of\nblinks. The peak angular speed of the eye during a saccade can reach as high as\n700{\\deg}/s in humans, especially during larger saccades that cover a visual\nangle of 25{\\deg}. Previous research has demonstrated encouraging outcomes in\ncomprehending neurological conditions through the study of saccades. A\nnecessary step in saccade detection involves accurately identifying the precise\nlocation of the pupil within the eye, from which additional information such as\ngaze angles can be inferred. Conventional frame-based cameras often struggle\nwith the high temporal precision necessary for tracking very fast movements,\nresulting in motion blur and latency issues. Event cameras, on the other hand,\noffer a promising alternative by recording changes in the visual scene\nasynchronously and providing high temporal resolution and low latency. By\nbridging the gap between traditional computer vision and event-based vision, we\npresent events as frames that can be readily utilized by standard deep learning\nalgorithms. This approach harnesses YOLOv8, a state-of-the-art object detection\ntechnology, to process these frames for pupil tracking using the publicly\naccessible Ev-Eye dataset. Experimental results demonstrate the framework's\neffectiveness, highlighting its potential applications in neuroscience,\nophthalmology, and human-computer interaction.\n","authors":["Khadija Iddrisu","Waseem Shariff","Suzanne Little"],"pdf_url":"https://arxiv.org/pdf/2407.16665v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.00936v4","updated":"2024-07-23T17:30:56Z","published":"2024-04-01T05:46:15Z","title":"A Comprehensive Review of Knowledge Distillation in Computer Vision","summary":"  Deep learning techniques have been demonstrated to surpass preceding\ncutting-edge machine learning techniques in recent years, with computer vision\nbeing one of the most prominent examples. However, deep learning models suffer\nfrom significant drawbacks when deployed in resource-constrained environments\ndue to their large model size and high complexity. Knowledge Distillation is\none of the prominent solutions to overcome this challenge. This review paper\nexamines the current state of research on knowledge distillation, a technique\nfor compressing complex models into smaller and simpler ones. The paper\nprovides an overview of the major principles and techniques associated with\nknowledge distillation and reviews the applications of knowledge distillation\nin the domain of computer vision. The review focuses on the benefits of\nknowledge distillation, as well as the problems that must be overcome to\nimprove its effectiveness.\n","authors":["Gousia Habib","Tausifa jan Saleem","Sheikh Musa Kaleem","Tufail Rouf","Brejesh Lall"],"pdf_url":"https://arxiv.org/pdf/2404.00936v4.pdf","comment":"38 pages ,8 figures"},{"id":"http://arxiv.org/abs/2407.16658v1","updated":"2024-07-23T17:19:23Z","published":"2024-07-23T17:19:23Z","title":"EgoCVR: An Egocentric Benchmark for Fine-Grained Composed Video\n  Retrieval","summary":"  In Composed Video Retrieval, a video and a textual description which modifies\nthe video content are provided as inputs to the model. The aim is to retrieve\nthe relevant video with the modified content from a database of videos. In this\nchallenging task, the first step is to acquire large-scale training datasets\nand collect high-quality benchmarks for evaluation. In this work, we introduce\nEgoCVR, a new evaluation benchmark for fine-grained Composed Video Retrieval\nusing large-scale egocentric video datasets. EgoCVR consists of 2,295 queries\nthat specifically focus on high-quality temporal video understanding. We find\nthat existing Composed Video Retrieval frameworks do not achieve the necessary\nhigh-quality temporal video understanding for this task. To address this\nshortcoming, we adapt a simple training-free method, propose a generic\nre-ranking framework for Composed Video Retrieval, and demonstrate that this\nachieves strong results on EgoCVR. Our code and benchmark are freely available\nat https://github.com/ExplainableML/EgoCVR.\n","authors":["Thomas Hummel","Shyamgopal Karthik","Mariana-Iuliana Georgescu","Zeynep Akata"],"pdf_url":"https://arxiv.org/pdf/2407.16658v1.pdf","comment":"ECCV 2024"},{"id":"http://arxiv.org/abs/2407.16655v1","updated":"2024-07-23T17:17:05Z","published":"2024-07-23T17:17:05Z","title":"MovieDreamer: Hierarchical Generation for Coherent Long Visual Sequence","summary":"  Recent advancements in video generation have primarily leveraged diffusion\nmodels for short-duration content. However, these approaches often fall short\nin modeling complex narratives and maintaining character consistency over\nextended periods, which is essential for long-form video production like\nmovies. We propose MovieDreamer, a novel hierarchical framework that integrates\nthe strengths of autoregressive models with diffusion-based rendering to\npioneer long-duration video generation with intricate plot progressions and\nhigh visual fidelity. Our approach utilizes autoregressive models for global\nnarrative coherence, predicting sequences of visual tokens that are\nsubsequently transformed into high-quality video frames through diffusion\nrendering. This method is akin to traditional movie production processes, where\ncomplex stories are factorized down into manageable scene capturing. Further,\nwe employ a multimodal script that enriches scene descriptions with detailed\ncharacter information and visual style, enhancing continuity and character\nidentity across scenes. We present extensive experiments across various movie\ngenres, demonstrating that our approach not only achieves superior visual and\nnarrative quality but also effectively extends the duration of generated\ncontent significantly beyond current capabilities. Homepage:\nhttps://aim-uofa.github.io/MovieDreamer/.\n","authors":["Canyu Zhao","Mingyu Liu","Wen Wang","Jianlong Yuan","Hao Chen","Bo Zhang","Chunhua Shen"],"pdf_url":"https://arxiv.org/pdf/2407.16655v1.pdf","comment":"23 pages, 18 figures"},{"id":"http://arxiv.org/abs/2407.16653v1","updated":"2024-07-23T17:14:01Z","published":"2024-07-23T17:14:01Z","title":"Aggregated Attributions for Explanatory Analysis of 3D Segmentation\n  Models","summary":"  Analysis of 3D segmentation models, especially in the context of medical\nimaging, is often limited to segmentation performance metrics that overlook the\ncrucial aspect of explainability and bias. Currently, effectively explaining\nthese models with saliency maps is challenging due to the high dimensions of\ninput images multiplied by the ever-growing number of segmented class labels.\nTo this end, we introduce Agg^2Exp, a methodology for aggregating fine-grained\nvoxel attributions of the segmentation model's predictions. Unlike classical\nexplanation methods that primarily focus on the local feature attribution,\nAgg^2Exp enables a more comprehensive global view on the importance of\npredicted segments in 3D images. Our benchmarking experiments show that\ngradient-based voxel attributions are more faithful to the model's predictions\nthan perturbation-based explanations. As a concrete use-case, we apply Agg^2Exp\nto discover knowledge acquired by the Swin UNEt TRansformer model trained on\nthe TotalSegmentator v2 dataset for segmenting anatomical structures in\ncomputed tomography medical images. Agg^2Exp facilitates the explanatory\nanalysis of large segmentation models beyond their predictive performance.\n","authors":["Maciej Chrabaszcz","Hubert Baniecki","Piotr Komorowski","Szymon Płotka","Przemyslaw Biecek"],"pdf_url":"https://arxiv.org/pdf/2407.16653v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.05811v2","updated":"2024-07-23T17:12:12Z","published":"2024-07-08T10:45:30Z","title":"MapsTP: HD Map Images Based Multimodal Trajectory Prediction for\n  Automated Vehicles","summary":"  Predicting ego vehicle trajectories remains a critical challenge, especially\nin urban and dense areas due to the unpredictable behaviours of other vehicles\nand pedestrians. Multimodal trajectory prediction enhances decision-making by\nconsidering multiple possible future trajectories based on diverse sources of\nenvironmental data. In this approach, we leverage ResNet-50 to extract image\nfeatures from high-definition map data and use IMU sensor data to calculate\nspeed, acceleration, and yaw rate. A temporal probabilistic network is employed\nto compute potential trajectories, selecting the most accurate and highly\nprobable trajectory paths. This method integrates HD map data to improve the\nrobustness and reliability of trajectory predictions for autonomous vehicles.\n","authors":["Sushil Sharma","Arindam Das","Ganesh Sistu","Mark Halton","Ciarán Eising"],"pdf_url":"https://arxiv.org/pdf/2407.05811v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07053v3","updated":"2024-07-23T17:12:12Z","published":"2024-07-09T17:18:27Z","title":"Multimodal Self-Instruct: Synthetic Abstract Image and Visual Reasoning\n  Instruction Using Language Model","summary":"  Although most current large multimodal models (LMMs) can already understand\nphotos of natural scenes and portraits, their understanding of abstract images,\ne.g., charts, maps, or layouts, and visual reasoning capabilities remains quite\nrudimentary. They often struggle with simple daily tasks, such as reading time\nfrom a clock, understanding a flowchart, or planning a route using a road map.\nIn light of this, we design a multi-modal self-instruct, utilizing large\nlanguage models and their code capabilities to synthesize massive abstract\nimages and visual reasoning instructions across daily scenarios. Our strategy\neffortlessly creates a multimodal benchmark with 11,193 instructions for eight\nvisual scenarios: charts, tables, simulated maps, dashboards, flowcharts,\nrelation graphs, floor plans, and visual puzzles. \\textbf{This benchmark,\nconstructed with simple lines and geometric elements, exposes the shortcomings\nof most advanced LMMs} like Claude-3.5-Sonnet and GPT-4o in abstract image\nunderstanding, spatial relations reasoning, and visual element induction.\nBesides, to verify the quality of our synthetic data, we fine-tune an LMM using\n62,476 synthetic chart, table and road map instructions. The results\ndemonstrate improved chart understanding and map navigation performance, and\nalso demonstrate potential benefits for other visual reasoning tasks. Our code\nis available at: \\url{https://github.com/zwq2018/Multi-modal-Self-instruct}.\n","authors":["Wenqi Zhang","Zhenglin Cheng","Yuanyu He","Mengna Wang","Yongliang Shen","Zeqi Tan","Guiyang Hou","Mingqian He","Yanna Ma","Weiming Lu","Yueting Zhuang"],"pdf_url":"https://arxiv.org/pdf/2407.07053v3.pdf","comment":"code: https://github.com/zwq2018/Multi-modal-Self-instruct dataset:\n  https://huggingface.co/datasets/zwq2018/Multi-modal-Self-instruct\n  Leaderboard: https://multi-modal-self-instruct.github.io/"},{"id":"http://arxiv.org/abs/2407.16647v1","updated":"2024-07-23T17:02:24Z","published":"2024-07-23T17:02:24Z","title":"Deformable Convolution Based Road Scene Semantic Segmentation of Fisheye\n  Images in Autonomous Driving","summary":"  This study investigates the effectiveness of modern Deformable Convolutional\nNeural Networks (DCNNs) for semantic segmentation tasks, particularly in\nautonomous driving scenarios with fisheye images. These images, providing a\nwide field of view, pose unique challenges for extracting spatial and geometric\ninformation due to dynamic changes in object attributes. Our experiments focus\non segmenting the WoodScape fisheye image dataset into ten distinct classes,\nassessing the Deformable Networks' ability to capture intricate spatial\nrelationships and improve segmentation accuracy. Additionally, we explore\ndifferent loss functions to address class imbalance issues and compare the\nperformance of conventional CNN architectures with Deformable Convolution-based\nCNNs, including Vanilla U-Net and Residual U-Net architectures. The significant\nimprovement in mIoU score resulting from integrating Deformable CNNs\ndemonstrates their effectiveness in handling the geometric distortions present\nin fisheye imagery, exceeding the performance of traditional CNN architectures.\nThis underscores the significant role of Deformable convolution in enhancing\nsemantic segmentation performance for fisheye imagery.\n","authors":["Anam Manzoor","Aryan Singh","Ganesh Sistu","Reenu Mohandas","Eoin Grua","Anthony Scanlan","Ciarán Eising"],"pdf_url":"https://arxiv.org/pdf/2407.16647v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16638v1","updated":"2024-07-23T16:55:04Z","published":"2024-07-23T16:55:04Z","title":"Unveiling and Mitigating Bias in Audio Visual Segmentation","summary":"  Community researchers have developed a range of advanced audio-visual\nsegmentation models aimed at improving the quality of sounding objects' masks.\nWhile masks created by these models may initially appear plausible, they\noccasionally exhibit anomalies with incorrect grounding logic. We attribute\nthis to real-world inherent preferences and distributions as a simpler signal\nfor learning than the complex audio-visual grounding, which leads to the\ndisregard of important modality information. Generally, the anomalous phenomena\nare often complex and cannot be directly observed systematically. In this\nstudy, we made a pioneering effort with the proper synthetic data to categorize\nand analyze phenomena as two types \"audio priming bias\" and \"visual prior\"\naccording to the source of anomalies. For audio priming bias, to enhance audio\nsensitivity to different intensities and semantics, a perception module\nspecifically for audio perceives the latent semantic information and\nincorporates information into a limited set of queries, namely active queries.\nMoreover, the interaction mechanism related to such active queries in the\ntransformer decoder is customized to adapt to the need for interaction\nregulating among audio semantics. For visual prior, multiple contrastive\ntraining strategies are explored to optimize the model by incorporating a\nbiased branch, without even changing the structure of the model. During\nexperiments, observation demonstrates the presence and the impact that has been\nproduced by the biases of the existing model. Finally, through experimental\nevaluation of AVS benchmarks, we demonstrate the effectiveness of our methods\nin handling both types of biases, achieving competitive performance across all\nthree subsets.\n","authors":["Peiwen Sun","Honggang Zhang","Di Hu"],"pdf_url":"https://arxiv.org/pdf/2407.16638v1.pdf","comment":"Accepted by ACM MM 24 (ORAL)"},{"id":"http://arxiv.org/abs/2407.16636v1","updated":"2024-07-23T16:52:42Z","published":"2024-07-23T16:52:42Z","title":"Velocity Driven Vision: Asynchronous Sensor Fusion Birds Eye View Models\n  for Autonomous Vehicles","summary":"  Fusing different sensor modalities can be a difficult task, particularly if\nthey are asynchronous. Asynchronisation may arise due to long processing times\nor improper synchronisation during calibration, and there must exist a way to\nstill utilise this previous information for the purpose of safe driving, and\nobject detection in ego vehicle/ multi-agent trajectory prediction.\nDifficulties arise in the fact that the sensor modalities have captured\ninformation at different times and also at different positions in space.\nTherefore, they are not spatially nor temporally aligned. This paper will\ninvestigate the challenge of radar and LiDAR sensors being asynchronous\nrelative to the camera sensors, for various time latencies. The spatial\nalignment will be resolved before lifting into BEV space via the transformation\nof the radar/LiDAR point clouds into the new ego frame coordinate system. Only\nafter this can we concatenate the radar/LiDAR point cloud and lifted camera\nfeatures. Temporal alignment will be remedied for radar data only, we will\nimplement a novel method of inferring the future radar point positions using\nthe velocity information. Our approach to resolving the issue of sensor\nasynchrony yields promising results. We demonstrate velocity information can\ndrastically improve IoU for asynchronous datasets, as for a time latency of 360\nmilliseconds (ms), IoU improves from 49.54 to 53.63. Additionally, for a time\nlatency of 550ms, the camera+radar (C+R) model outperforms the camera+LiDAR\n(C+L) model by 0.18 IoU. This is an advancement in utilising the\noften-neglected radar sensor modality, which is less favoured than LiDAR for\nautonomous driving purposes.\n","authors":["Seamie Hayes","Sushil Sharma","Ciarán Eising"],"pdf_url":"https://arxiv.org/pdf/2407.16636v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16634v1","updated":"2024-07-23T16:49:01Z","published":"2024-07-23T16:49:01Z","title":"Knowledge-driven AI-generated data for accurate and interpretable breast\n  ultrasound diagnoses","summary":"  Data-driven deep learning models have shown great capabilities to assist\nradiologists in breast ultrasound (US) diagnoses. However, their effectiveness\nis limited by the long-tail distribution of training data, which leads to\ninaccuracies in rare cases. In this study, we address a long-standing challenge\nof improving the diagnostic model performance on rare cases using long-tailed\ndata. Specifically, we introduce a pipeline, TAILOR, that builds a\nknowledge-driven generative model to produce tailored synthetic data. The\ngenerative model, using 3,749 lesions as source data, can generate millions of\nbreast-US images, especially for error-prone rare cases. The generated data can\nbe further used to build a diagnostic model for accurate and interpretable\ndiagnoses. In the prospective external evaluation, our diagnostic model\noutperforms the average performance of nine radiologists by 33.5% in\nspecificity with the same sensitivity, improving their performance by providing\npredictions with an interpretable decision-making process. Moreover, on ductal\ncarcinoma in situ (DCIS), our diagnostic model outperforms all radiologists by\na large margin, with only 34 DCIS lesions in the source data. We believe that\nTAILOR can potentially be extended to various diseases and imaging modalities.\n","authors":["Haojun Yu","Youcheng Li","Nan Zhang","Zihan Niu","Xuantong Gong","Yanwen Luo","Quanlin Wu","Wangyan Qin","Mengyuan Zhou","Jie Han","Jia Tao","Ziwei Zhao","Di Dai","Di He","Dong Wang","Binghui Tang","Ling Huo","Qingli Zhu","Yong Wang","Liwei Wang"],"pdf_url":"https://arxiv.org/pdf/2407.16634v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09215v3","updated":"2024-07-23T16:20:54Z","published":"2023-11-15T18:56:51Z","title":"ConvNet vs Transformer, Supervised vs CLIP: Beyond ImageNet Accuracy","summary":"  Modern computer vision offers a great variety of models to practitioners, and\nselecting a model from multiple options for specific applications can be\nchallenging. Conventionally, competing model architectures and training\nprotocols are compared by their classification accuracy on ImageNet. However,\nthis single metric does not fully capture performance nuances critical for\nspecialized tasks. In this work, we conduct an in-depth comparative analysis of\nmodel behaviors beyond ImageNet accuracy, for both ConvNet and Vision\nTransformer architectures, each across supervised and CLIP training paradigms.\nAlthough our selected models have similar ImageNet accuracies and compute\nrequirements, we find that they differ in many other aspects: types of\nmistakes, output calibration, transferability, and feature invariance, among\nothers. This diversity in model characteristics, not captured by traditional\nmetrics, highlights the need for more nuanced analysis when choosing among\ndifferent models. Our code is available at\nhttps://github.com/kirill-vish/Beyond-INet.\n","authors":["Kirill Vishniakov","Zhiqiang Shen","Zhuang Liu"],"pdf_url":"https://arxiv.org/pdf/2311.09215v3.pdf","comment":"Project page: https://kirill-vish.github.io/beyond-imagenet-accuracy/"},{"id":"http://arxiv.org/abs/2407.16608v1","updated":"2024-07-23T16:13:27Z","published":"2024-07-23T16:13:27Z","title":"Deep Bayesian segmentation for colon polyps: Well-calibrated predictions\n  in medical imaging","summary":"  Colorectal polyps are generally benign alterations that, if not identified\npromptly and managed successfully, can progress to cancer and cause\naffectations on the colon mucosa, known as adenocarcinoma. Today advances in\nDeep Learning have demonstrated the ability to achieve significant performance\nin image classification and detection in medical diagnosis applications.\nNevertheless, these models are prone to overfitting, and making decisions based\nonly on point estimations may provide incorrect predictions. Thus, to obtain a\nmore informed decision, we must consider point estimations along with their\nreliable uncertainty quantification. In this paper, we built different Bayesian\nneural network approaches based on the flexibility of posterior distribution to\ndevelop semantic segmentation of colorectal polyp images. We found that these\nmodels not only provide state-of-the-art performance on the segmentation of\nthis medical dataset but also, yield accurate uncertainty estimates. We applied\nmultiplicative normalized flows(MNF) and reparameterization trick on the UNET,\nFPN, and LINKNET architectures tested with multiple backbones in deterministic\nand Bayesian versions. We report that the FPN + EfficientnetB7 architecture\nwith MNF is the most promising option given its IOU of 0.94 and Expected\nCalibration Error (ECE) of 0.004, combined with its superiority in identifying\ndifficult-to-detect colorectal polyps, which is effective in clinical areas\nwhere early detection prevents the development of colon cancer.\n","authors":["Daniela L. Ramos","Hector J. Hortua"],"pdf_url":"https://arxiv.org/pdf/2407.16608v1.pdf","comment":"comments are welcome. 43 pages"},{"id":"http://arxiv.org/abs/2407.16600v1","updated":"2024-07-23T16:03:02Z","published":"2024-07-23T16:03:02Z","title":"DHGS: Decoupled Hybrid Gaussian Splatting for Driving Scene","summary":"  Existing Gaussian splatting methods struggle to achieve satisfactory novel\nview synthesis in driving scenes due to the lack of crafty design and geometric\nconstraints of related elements. This paper introduces a novel method called\nDecoupled Hybrid Gaussian Splatting (DHGS), which aims at promoting the\nrendering quality of novel view synthesis for driving scenes. The novelty of\nthis work lies in the decoupled and hybrid pixel-level blender for road and\nnon-road layers, without conventional unified differentiable rendering logic\nfor the entire scene, meanwhile maintaining consistent and continuous\nsuperimposition through the proposed depth-ordered rendering strategy. Beyond\nthat, an implicit road representation comprised of Signed Distance Field (SDF)\nis trained to supervise the road surface with subtle geometric attributes.\nAccompanied by the use of auxiliary transmittance loss and consistency loss,\nnovel images with imperceptible boundary and elevated fidelity are ultimately\nobtained. Substantial experiments on Waymo dataset prove that DHGS outperforms\nthe state-of-the-art methods.\n","authors":["Xi Shi","Lingli Chen","Peng Wei","Xi Wu","Tian Jiang","Yonggang Luo","Lecheng Xie"],"pdf_url":"https://arxiv.org/pdf/2407.16600v1.pdf","comment":"12 pages, 12 figures, conference"},{"id":"http://arxiv.org/abs/2404.03531v2","updated":"2024-07-23T16:01:27Z","published":"2024-04-04T15:35:43Z","title":"COMO: Compact Mapping and Odometry","summary":"  We present COMO, a real-time monocular mapping and odometry system that\nencodes dense geometry via a compact set of 3D anchor points. Decoding anchor\npoint projections into dense geometry via per-keyframe depth covariance\nfunctions guarantees that depth maps are joined together at visible anchor\npoints. The representation enables joint optimization of camera poses and dense\ngeometry, intrinsic 3D consistency, and efficient second-order inference. To\nmaintain a compact yet expressive map, we introduce a frontend that leverages\nthe covariance function for tracking and initializing potentially visually\nindistinct 3D points across frames. Altogether, we introduce a real-time system\ncapable of estimating accurate poses and consistent geometry.\n","authors":["Eric Dexheimer","Andrew J. Davison"],"pdf_url":"https://arxiv.org/pdf/2404.03531v2.pdf","comment":"Website: https://edexheim.github.io/como/"},{"id":"http://arxiv.org/abs/2404.00979v2","updated":"2024-07-23T15:54:17Z","published":"2024-04-01T07:50:10Z","title":"PDF: A Probability-Driven Framework for Open World 3D Point Cloud\n  Semantic Segmentation","summary":"  Existing point cloud semantic segmentation networks cannot identify unknown\nclasses and update their knowledge, due to a closed-set and static perspective\nof the real world, which would induce the intelligent agent to make bad\ndecisions. To address this problem, we propose a Probability-Driven Framework\n(PDF) for open world semantic segmentation that includes (i) a lightweight\nU-decoder branch to identify unknown classes by estimating the uncertainties,\n(ii) a flexible pseudo-labeling scheme to supply geometry features along with\nprobability distribution features of unknown classes by generating pseudo\nlabels, and (iii) an incremental knowledge distillation strategy to incorporate\nnovel classes into the existing knowledge base gradually. Our framework enables\nthe model to behave like human beings, which could recognize unknown objects\nand incrementally learn them with the corresponding knowledge. Experimental\nresults on the S3DIS and ScanNetv2 datasets demonstrate that the proposed PDF\noutperforms other methods by a large margin in both important tasks of open\nworld semantic segmentation.\n","authors":["Jinfeng Xu","Siyuan Yang","Xianzhi Li","Yuan Tang","Yixue Hao","Long Hu","Min Chen"],"pdf_url":"https://arxiv.org/pdf/2404.00979v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16575v1","updated":"2024-07-23T15:30:08Z","published":"2024-07-23T15:30:08Z","title":"Timeliness-Fidelity Tradeoff in 3D Scene Representations","summary":"  Real-time three-dimensional (3D) scene representations serve as one of the\nbuilding blocks that bolster various innovative applications, e.g., digital\nmanufacturing, Virtual/Augmented/Extended/Mixed Reality (VR/AR/XR/MR), and the\nmetaverse. Despite substantial efforts that have been made to real-time\ncommunications and computing, real-time 3D scene representations remain a\nchallenging task. This paper investigates the tradeoff between timeliness and\nfidelity in real-time 3D scene representations. Specifically, we establish a\nframework to evaluate the impact of communication delay on the tradeoff, where\nthe real-world scenario is monitored by multiple cameras that communicate with\nan edge server. To improve fidelity for 3D scene representations, we propose to\nuse a single-step Proximal Policy Optimization (PPO) method that leverages the\nAge of Information (AoI) to decide if the received image needs to be involved\nin 3D scene representations and rendering. We test our framework and the\nproposed approach with different well-known 3D scene representation methods.\nSimulation results reveal that real-time 3D scene representation can be\nsensitively affected by communication delay, and our proposed method can\nachieve optimal 3D scene representation results.\n","authors":["Xiangmin Xu","Zhen Meng","Yichi Zhang","Changyang She","Philip G. Zhao"],"pdf_url":"https://arxiv.org/pdf/2407.16575v1.pdf","comment":"This paper has been accepted for publication by the IEEE\n  International Conference on Computer Communications (INFOCOM) Workshops 2024"},{"id":"http://arxiv.org/abs/2407.16560v1","updated":"2024-07-23T15:14:39Z","published":"2024-07-23T15:14:39Z","title":"COALA: A Practical and Vision-Centric Federated Learning Platform","summary":"  We present COALA, a vision-centric Federated Learning (FL) platform, and a\nsuite of benchmarks for practical FL scenarios, which we categorize into three\nlevels: task, data, and model. At the task level, COALA extends support from\nsimple classification to 15 computer vision tasks, including object detection,\nsegmentation, pose estimation, and more. It also facilitates federated\nmultiple-task learning, allowing clients to tackle multiple tasks\nsimultaneously. At the data level, COALA goes beyond supervised FL to benchmark\nboth semi-supervised FL and unsupervised FL. It also benchmarks feature\ndistribution shifts other than commonly considered label distribution shifts.\nIn addition to dealing with static data, it supports federated continual\nlearning for continuously changing data in real-world scenarios. At the model\nlevel, COALA benchmarks FL with split models and different models in different\nclients. COALA platform offers three degrees of customization for these\npractical FL scenarios, including configuration customization, components\ncustomization, and workflow customization. We conduct systematic benchmarking\nexperiments for the practical FL scenarios and highlight potential\nopportunities for further advancements in FL. Codes are open sourced at\nhttps://github.com/SonyResearch/COALA.\n","authors":["Weiming Zhuang","Jian Xu","Chen Chen","Jingtao Li","Lingjuan Lyu"],"pdf_url":"https://arxiv.org/pdf/2407.16560v1.pdf","comment":"ICML'24"},{"id":"http://arxiv.org/abs/2407.16554v1","updated":"2024-07-23T15:07:52Z","published":"2024-07-23T15:07:52Z","title":"Coarse-to-Fine Proposal Refinement Framework for Audio Temporal Forgery\n  Detection and Localization","summary":"  Recently, a novel form of audio partial forgery has posed challenges to its\nforensics, requiring advanced countermeasures to detect subtle forgery\nmanipulations within long-duration audio. However, existing countermeasures\nstill serve a classification purpose and fail to perform meaningful analysis of\nthe start and end timestamps of partial forgery segments. To address this\nchallenge, we introduce a novel coarse-to-fine proposal refinement framework\n(CFPRF) that incorporates a frame-level detection network (FDN) and a proposal\nrefinement network (PRN) for audio temporal forgery detection and localization.\nSpecifically, the FDN aims to mine informative inconsistency cues between real\nand fake frames to obtain discriminative features that are beneficial for\nroughly indicating forgery regions. The PRN is responsible for predicting\nconfidence scores and regression offsets to refine the coarse-grained proposals\nderived from the FDN. To learn robust discriminative features, we devise a\ndifference-aware feature learning (DAFL) module guided by contrastive\nrepresentation learning to enlarge the sensitive differences between different\nframes induced by minor manipulations. We further design a boundary-aware\nfeature enhancement (BAFE) module to capture the contextual information of\nmultiple transition boundaries and guide the interaction between boundary\ninformation and temporal features via a cross-attention mechanism. Extensive\nexperiments show that our CFPRF achieves state-of-the-art performance on\nvarious datasets, including LAV-DF, ASVS2019PS, and HAD.\n","authors":["Junyan Wu","Wei Lu","Xiangyang Luo","Rui Yang","Qian Wang","Xiaochun Cao"],"pdf_url":"https://arxiv.org/pdf/2407.16554v1.pdf","comment":"9pages, 3figures. This paper has been accepted for ACM MM 2024"},{"id":"http://arxiv.org/abs/2407.16552v1","updated":"2024-07-23T15:05:55Z","published":"2024-07-23T15:05:55Z","title":"MicroEmo: Time-Sensitive Multimodal Emotion Recognition with\n  Micro-Expression Dynamics in Video Dialogues","summary":"  Multimodal Large Language Models (MLLMs) have demonstrated remarkable\nmultimodal emotion recognition capabilities, integrating multimodal cues from\nvisual, acoustic, and linguistic contexts in the video to recognize human\nemotional states. However, existing methods ignore capturing local facial\nfeatures of temporal dynamics of micro-expressions and do not leverage the\ncontextual dependencies of the utterance-aware temporal segments in the video,\nthereby limiting their expected effectiveness to a certain extent. In this\nwork, we propose MicroEmo, a time-sensitive MLLM aimed at directing attention\nto the local facial micro-expression dynamics and the contextual dependencies\nof utterance-aware video clips. Our model incorporates two key architectural\ncontributions: (1) a global-local attention visual encoder that integrates\nglobal frame-level timestamp-bound image features with local facial features of\ntemporal dynamics of micro-expressions; (2) an utterance-aware video Q-Former\nthat captures multi-scale and contextual dependencies by generating visual\ntoken sequences for each utterance segment and for the entire video then\ncombining them. Preliminary qualitative experiments demonstrate that in a new\nExplainable Multimodal Emotion Recognition (EMER) task that exploits\nmulti-modal and multi-faceted clues to predict emotions in an open-vocabulary\n(OV) manner, MicroEmo demonstrates its effectiveness compared with the latest\nmethods.\n","authors":["Liyun Zhang"],"pdf_url":"https://arxiv.org/pdf/2407.16552v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15794v2","updated":"2024-07-23T14:57:14Z","published":"2024-07-22T16:52:32Z","title":"Disentangling spatio-temporal knowledge for weakly supervised object\n  detection and segmentation in surgical video","summary":"  Weakly supervised video object segmentation (WSVOS) enables the\nidentification of segmentation maps without requiring an extensive training\ndataset of object masks, relying instead on coarse video labels indicating\nobject presence. Current state-of-the-art methods either require multiple\nindependent stages of processing that employ motion cues or, in the case of\nend-to-end trainable networks, lack in segmentation accuracy, in part due to\nthe difficulty of learning segmentation maps from videos with transient object\npresence. This limits the application of WSVOS for semantic annotation of\nsurgical videos where multiple surgical tools frequently move in and out of the\nfield of view, a problem that is more difficult than typically encountered in\nWSVOS. This paper introduces Video Spatio-Temporal Disentanglement Networks\n(VDST-Net), a framework to disentangle spatiotemporal information using\nsemi-decoupled knowledge distillation to predict high-quality class activation\nmaps (CAMs). A teacher network designed to resolve temporal conflicts when\nspecifics about object location and timing in the video are not provided works\nwith a student network that integrates information over time by leveraging\ntemporal dependencies. We demonstrate the efficacy of our framework on a public\nreference dataset and on a more challenging surgical video dataset where\nobjects are, on average, present in less than 60\\% of annotated frames. Our\nmethod outperforms state-of-the-art techniques and generates superior\nsegmentation masks under video-level weak supervision.\n","authors":["Guiqiu Liao","Matjaz Jogan","Sai Koushik","Eric Eaton","Daniel A. Hashimoto"],"pdf_url":"https://arxiv.org/pdf/2407.15794v2.pdf","comment":"13 pages, 6 figures, 8 tables"},{"id":"http://arxiv.org/abs/2407.15047v2","updated":"2024-07-23T14:56:22Z","published":"2024-07-21T04:09:37Z","title":"End-to-End Video Question Answering with Frame Scoring Mechanisms and\n  Adaptive Sampling","summary":"  Video Question Answering (VideoQA) has emerged as a challenging frontier in\nthe field of multimedia processing, requiring intricate interactions between\nvisual and textual modalities. Simply uniformly sampling frames or\nindiscriminately aggregating frame-level visual features often falls short in\ncapturing the nuanced and relevant contexts of videos to well perform VideoQA.\nTo mitigate these issues, we propose VidF4, a novel VideoQA framework equipped\nwith tailored frame selection strategy for effective and efficient VideoQA. We\npropose three frame-scoring mechanisms that consider both question relevance\nand inter-frame similarity to evaluate the importance of each frame for a given\nquestion on the video. Furthermore, we design a differentiable adaptive frame\nsampling mechanism to facilitate end-to-end training for the frame selector and\nanswer generator. The experimental results across three widely adopted\nbenchmarks demonstrate that our model consistently outperforms existing VideoQA\nmethods, establishing a new SOTA across NExT-QA (+0.3%), STAR (+0.9%), and TVQA\n(+1.0%). Furthermore, through both quantitative and qualitative analyses, we\nvalidate the effectiveness of each design choice.\n","authors":["Jianxin Liang","Xiaojun Meng","Yueqian Wang","Chang Liu","Qun Liu","Dongyan Zhao"],"pdf_url":"https://arxiv.org/pdf/2407.15047v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16541v1","updated":"2024-07-23T14:53:47Z","published":"2024-07-23T14:53:47Z","title":"QPT V2: Masked Image Modeling Advances Visual Scoring","summary":"  Quality assessment and aesthetics assessment aim to evaluate the perceived\nquality and aesthetics of visual content. Current learning-based methods suffer\ngreatly from the scarcity of labeled data and usually perform sub-optimally in\nterms of generalization. Although masked image modeling (MIM) has achieved\nnoteworthy advancements across various high-level tasks (e.g., classification,\ndetection etc.). In this work, we take on a novel perspective to investigate\nits capabilities in terms of quality- and aesthetics-awareness. To this end, we\npropose Quality- and aesthetics-aware pretraining (QPT V2), the first\npretraining framework based on MIM that offers a unified solution to quality\nand aesthetics assessment. To perceive the high-level semantics and\nfine-grained details, pretraining data is curated. To comprehensively encompass\nquality- and aesthetics-related factors, degradation is introduced. To capture\nmulti-scale quality and aesthetic information, model structure is modified.\nExtensive experimental results on 11 downstream benchmarks clearly show the\nsuperior performance of QPT V2 in comparison with current state-of-the-art\napproaches and other pretraining paradigms. Code and models will be released at\n\\url{https://github.com/KeiChiTse/QPT-V2}.\n","authors":["Qizhi Xie","Kun Yuan","Yunpeng Qu","Mingda Wu","Ming Sun","Chao Zhou","Jihong Zhu"],"pdf_url":"https://arxiv.org/pdf/2407.16541v1.pdf","comment":"8 pages, 6 figures"},{"id":"http://arxiv.org/abs/2407.15706v2","updated":"2024-07-23T14:52:05Z","published":"2024-07-22T15:16:47Z","title":"Multi-Modality Co-Learning for Efficient Skeleton-based Action\n  Recognition","summary":"  Skeleton-based action recognition has garnered significant attention due to\nthe utilization of concise and resilient skeletons. Nevertheless, the absence\nof detailed body information in skeletons restricts performance, while other\nmultimodal methods require substantial inference resources and are inefficient\nwhen using multimodal data during both training and inference stages. To\naddress this and fully harness the complementary multimodal features, we\npropose a novel multi-modality co-learning (MMCL) framework by leveraging the\nmultimodal large language models (LLMs) as auxiliary networks for efficient\nskeleton-based action recognition, which engages in multi-modality co-learning\nduring the training stage and keeps efficiency by employing only concise\nskeletons in inference. Our MMCL framework primarily consists of two modules.\nFirst, the Feature Alignment Module (FAM) extracts rich RGB features from video\nframes and aligns them with global skeleton features via contrastive learning.\nSecond, the Feature Refinement Module (FRM) uses RGB images with temporal\ninformation and text instruction to generate instructive features based on the\npowerful generalization of multimodal LLMs. These instructive text features\nwill further refine the classification scores and the refined scores will\nenhance the model's robustness and generalization in a manner similar to soft\nlabels. Extensive experiments on NTU RGB+D, NTU RGB+D 120 and Northwestern-UCLA\nbenchmarks consistently verify the effectiveness of our MMCL, which outperforms\nthe existing skeleton-based action recognition methods. Meanwhile, experiments\non UTD-MHAD and SYSU-Action datasets demonstrate the commendable generalization\nof our MMCL in zero-shot and domain-adaptive action recognition. Our code is\npublicly available at: https://github.com/liujf69/MMCL-Action.\n","authors":["Jinfu Liu","Chen Chen","Mengyuan Liu"],"pdf_url":"https://arxiv.org/pdf/2407.15706v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15631v2","updated":"2024-07-23T14:51:18Z","published":"2024-07-22T13:44:06Z","title":"A Diffusion Model for Simulation Ready Coronary Anatomy with\n  Morpho-skeletal Control","summary":"  Virtual interventions enable the physics-based simulation of device\ndeployment within coronary arteries. This framework allows for counterfactual\nreasoning by deploying the same device in different arterial anatomies.\nHowever, current methods to create such counterfactual arteries face a\ntrade-off between controllability and realism. In this study, we investigate\nhow Latent Diffusion Models (LDMs) can custom synthesize coronary anatomy for\nvirtual intervention studies based on mid-level anatomic constraints such as\ntopological validity, local morphological shape, and global skeletal structure.\nWe also extend diffusion model guidance strategies to the context of\nmorpho-skeletal conditioning and propose a novel guidance method for continuous\nattributes that adaptively updates the negative guiding condition throughout\nsampling. Our framework enables the generation and editing of coronary anatomy\nin a controllable manner, allowing device designers to derive mechanistic\ninsights regarding anatomic variation and simulated device deployment.\n","authors":["Karim Kadry","Shreya Gupta","Jonas Sogbadji","Michiel Schaap","Kersten Petersen","Takuya Mizukami","Carlos Collet","Farhad R. Nezami","Elazer R. Edelman"],"pdf_url":"https://arxiv.org/pdf/2407.15631v2.pdf","comment":"Accepted to ECCV 2024"},{"id":"http://arxiv.org/abs/2401.13782v3","updated":"2024-07-23T14:49:43Z","published":"2024-01-24T20:05:49Z","title":"Position: AI/ML Influencers Have a Place in the Academic Process","summary":"  As the number of accepted papers at AI and ML conferences reaches into the\nthousands, it has become unclear how researchers access and read research\npublications. In this paper, we investigate the role of social media\ninfluencers in enhancing the visibility of machine learning research,\nparticularly the citation counts of papers they share. We have compiled a\ncomprehensive dataset of over 8,000 papers, spanning tweets from December 2018\nto October 2023, alongside controls precisely matched by 9 key covariates. Our\nstatistical and causal inference analysis reveals a significant increase in\ncitations for papers endorsed by these influencers, with median citation counts\n2-3 times higher than those of the control group. Additionally, the study\ndelves into the geographic, gender, and institutional diversity of highlighted\nauthors. Given these findings, we advocate for a responsible approach to\ncuration, encouraging influencers to uphold the journalistic standard that\nincludes showcasing diverse research topics, authors, and institutions.\n","authors":["Iain Xie Weissburg","Mehir Arora","Xinyi Wang","Liangming Pan","William Yang Wang"],"pdf_url":"https://arxiv.org/pdf/2401.13782v3.pdf","comment":"15 Pages, 22 Figures, ICML 2024"},{"id":"http://arxiv.org/abs/2407.16526v1","updated":"2024-07-23T14:39:40Z","published":"2024-07-23T14:39:40Z","title":"Imperfect Vision Encoders: Efficient and Robust Tuning for\n  Vision-Language Models","summary":"  Vision language models (VLMs) demonstrate impressive capabilities in visual\nquestion answering and image captioning, acting as a crucial link between\nvisual and language models. However, existing open-source VLMs heavily rely on\npretrained and frozen vision encoders (such as CLIP). Despite CLIP's robustness\nacross diverse domains, it still exhibits non-negligible image understanding\nerrors. These errors propagate to the VLM responses, resulting in sub-optimal\nperformance. In our work, we propose an efficient and robust method for\nupdating vision encoders within VLMs. Our approach selectively and locally\nupdates encoders, leading to substantial performance improvements on data where\nprevious mistakes occurred, while maintaining overall robustness. Furthermore,\nwe demonstrate the effectiveness of our method during continual few-shot\nupdates. Theoretical grounding, generality, and computational efficiency\ncharacterize our approach.\n","authors":["Aristeidis Panos","Rahaf Aljundi","Daniel Olmeda Reino","Richard E Turner"],"pdf_url":"https://arxiv.org/pdf/2407.16526v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08320v4","updated":"2024-07-23T14:39:23Z","published":"2023-10-12T13:33:04Z","title":"Defending Our Privacy With Backdoors","summary":"  The proliferation of large AI models trained on uncurated, often sensitive\nweb-scraped data has raised significant privacy concerns. One of the concerns\nis that adversaries can extract information about the training data using\nprivacy attacks. Unfortunately, the task of removing specific information from\nthe models without sacrificing performance is not straightforward and has\nproven to be challenging. We propose a rather easy yet effective defense based\non backdoor attacks to remove private information, such as names and faces of\nindividuals, from vision-language models by fine-tuning them for only a few\nminutes instead of re-training them from scratch. Specifically, by\nstrategically inserting backdoors into text encoders, we align the embeddings\nof sensitive phrases with those of neutral terms-\"a person\" instead of the\nperson's actual name. For image encoders, we map individuals' embeddings to be\nremoved from the model to a universal, anonymous embedding. The results of our\nextensive experimental evaluation demonstrate the effectiveness of our\nbackdoor-based defense on CLIP by assessing its performance using a specialized\nprivacy attack for zero-shot classifiers. Our approach provides a new\n\"dual-use\" perspective on backdoor attacks and presents a promising avenue to\nenhance the privacy of individuals within models trained on uncurated\nweb-scraped data.\n","authors":["Dominik Hintersdorf","Lukas Struppek","Daniel Neider","Kristian Kersting"],"pdf_url":"https://arxiv.org/pdf/2310.08320v4.pdf","comment":"Accepted at ECAI 2024"},{"id":"http://arxiv.org/abs/2303.13123v2","updated":"2024-07-23T14:38:34Z","published":"2023-03-23T09:23:57Z","title":"Laplacian Segmentation Networks Improve Epistemic Uncertainty\n  Quantification","summary":"  Image segmentation relies heavily on neural networks which are known to be\noverconfident, especially when making predictions on out-of-distribution (OOD)\nimages. This is a common scenario in the medical domain due to variations in\nequipment, acquisition sites, or image corruptions. This work addresses the\nchallenge of OOD detection by proposing Laplacian Segmentation Networks (LSN):\nmethods which jointly model epistemic (model) and aleatoric (data) uncertainty\nfor OOD detection. In doing so, we propose the first Laplace approximation of\nthe weight posterior that scales to large neural networks with skip connections\nthat have high-dimensional outputs. We demonstrate on three datasets that the\nLSN-modeled parameter distributions, in combination with suitable uncertainty\nmeasures, gives superior OOD detection.\n","authors":["Kilian Zepf","Selma Wanna","Marco Miani","Juston Moore","Jes Frellsen","Søren Hauberg","Frederik Warburg","Aasa Feragen"],"pdf_url":"https://arxiv.org/pdf/2303.13123v2.pdf","comment":"Published in the Conference Proceedings of the 27th International\n  Conference on Medical Image Computing and Computer Assisted Intervention\n  (MICCAI)"},{"id":"http://arxiv.org/abs/2407.16514v1","updated":"2024-07-23T14:30:51Z","published":"2024-07-23T14:30:51Z","title":"Is 3D Convolution with 5D Tensors Really Necessary for Video Analysis?","summary":"  In this paper, we present a comprehensive study and propose several novel\ntechniques for implementing 3D convolutional blocks using 2D and/or 1D\nconvolutions with only 4D and/or 3D tensors. Our motivation is that 3D\nconvolutions with 5D tensors are computationally very expensive and they may\nnot be supported by some of the edge devices used in real-time applications\nsuch as robots. The existing approaches mitigate this by splitting the 3D\nkernels into spatial and temporal domains, but they still use 3D convolutions\nwith 5D tensors in their implementations. We resolve this issue by introducing\nsome appropriate 4D/3D tensor reshaping as well as new combination techniques\nfor spatial and temporal splits. The proposed implementation methods show\nsignificant improvement both in terms of efficiency and accuracy. The\nexperimental results confirm that the proposed spatio-temporal processing\nstructure outperforms the original model in terms of speed and accuracy using\nonly 4D tensors with fewer parameters.\n","authors":["Habib Hajimolahoseini","Walid Ahmed","Austin Wen","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2407.16514v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16511v1","updated":"2024-07-23T14:25:28Z","published":"2024-07-23T14:25:28Z","title":"DreamVTON: Customizing 3D Virtual Try-on with Personalized Diffusion\n  Models","summary":"  Image-based 3D Virtual Try-ON (VTON) aims to sculpt the 3D human according to\nperson and clothes images, which is data-efficient (i.e., getting rid of\nexpensive 3D data) but challenging. Recent text-to-3D methods achieve\nremarkable improvement in high-fidelity 3D human generation, demonstrating its\npotential for 3D virtual try-on. Inspired by the impressive success of\npersonalized diffusion models (e.g., Dreambooth and LoRA) for 2D VTON, it is\nstraightforward to achieve 3D VTON by integrating the personalization technique\ninto the diffusion-based text-to-3D framework. However, employing the\npersonalized module in a pre-trained diffusion model (e.g., StableDiffusion\n(SD)) would degrade the model's capability for multi-view or multi-domain\nsynthesis, which is detrimental to the geometry and texture optimization guided\nby Score Distillation Sampling (SDS) loss. In this work, we propose a novel\ncustomizing 3D human try-on model, named \\textbf{DreamVTON}, to separately\noptimize the geometry and texture of the 3D human. Specifically, a personalized\nSD with multi-concept LoRA is proposed to provide the generative prior about\nthe specific person and clothes, while a Densepose-guided ControlNet is\nexploited to guarantee consistent prior about body pose across various camera\nviews. Besides, to avoid the inconsistent multi-view priors from the\npersonalized SD dominating the optimization, DreamVTON introduces a\ntemplate-based optimization mechanism, which employs mask templates for\ngeometry shape learning and normal/RGB templates for geometry/texture details\nlearning. Furthermore, for the geometry optimization phase, DreamVTON\nintegrates a normal-style LoRA into personalized SD to enhance normal map\ngenerative prior, facilitating smooth geometry modeling.\n","authors":["Zhenyu Xie","Haoye Dong","Yufei Gao","Zehua Ma","Xiaodan Liang"],"pdf_url":"https://arxiv.org/pdf/2407.16511v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.04149v2","updated":"2024-07-23T14:25:08Z","published":"2024-07-04T20:53:19Z","title":"SineKAN: Kolmogorov-Arnold Networks Using Sinusoidal Activation\n  Functions","summary":"  Recent work has established an alternative to traditional multi-layer\nperceptron neural networks in the form of Kolmogorov-Arnold Networks (KAN). The\ngeneral KAN framework uses learnable activation functions on the edges of the\ncomputational graph followed by summation on nodes. The learnable edge\nactivation functions in the original implementation are basis spline functions\n(B-Spline). Here, we present a model in which learnable grids of B-Spline\nactivation functions are replaced by grids of re-weighted sine functions. We\nshow that this leads to better or comparable numerical performance to B-Spline\nKAN models on the MNIST benchmark, while also providing a substantial speed\nincrease on the order of 4-8 times.\n","authors":["Eric A. F. Reinhardt","P. R. Dinesh","Sergei Gleyzer"],"pdf_url":"https://arxiv.org/pdf/2407.04149v2.pdf","comment":"9 pages, 9 figures"},{"id":"http://arxiv.org/abs/2407.16508v1","updated":"2024-07-23T14:24:26Z","published":"2024-07-23T14:24:26Z","title":"ToDER: Towards Colonoscopy Depth Estimation and Reconstruction with\n  Geometry Constraint Adaptation","summary":"  Visualizing colonoscopy is crucial for medical auxiliary diagnosis to prevent\nundetected polyps in areas that are not fully observed. Traditional\nfeature-based and depth-based reconstruction approaches usually end up with\nundesirable results due to incorrect point matching or imprecise depth\nestimation in realistic colonoscopy videos. Modern deep-based methods often\nrequire a sufficient number of ground truth samples, which are generally hard\nto obtain in optical colonoscopy. To address this issue, self-supervised and\ndomain adaptation methods have been explored. However, these methods neglect\ngeometry constraints and exhibit lower accuracy in predicting detailed depth.\nWe thus propose a novel reconstruction pipeline with a bi-directional\nadaptation architecture named ToDER to get precise depth estimations.\nFurthermore, we carefully design a TNet module in our adaptation architecture\nto yield geometry constraints and obtain better depth quality. Estimated depth\nis finally utilized to reconstruct a reliable colon model for visualization.\nExperimental results demonstrate that our approach can precisely predict depth\nmaps in both realistic and synthetic colonoscopy videos compared with other\nself-supervised and domain adaptation methods. Our method on realistic\ncolonoscopy also shows the great potential for visualizing unobserved regions\nand preventing misdiagnoses.\n","authors":["Zhenhua Wu","Yanlin Jin","Liangdong Qiu","Xiaoguang Han","Xiang Wan","Guanbin Li"],"pdf_url":"https://arxiv.org/pdf/2407.16508v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16503v1","updated":"2024-07-23T14:21:00Z","published":"2024-07-23T14:21:00Z","title":"HDRSplat: Gaussian Splatting for High Dynamic Range 3D Scene\n  Reconstruction from Raw Images","summary":"  The recent advent of 3D Gaussian Splatting (3DGS) has revolutionized the 3D\nscene reconstruction space enabling high-fidelity novel view synthesis in\nreal-time. However, with the exception of RawNeRF, all prior 3DGS and\nNeRF-based methods rely on 8-bit tone-mapped Low Dynamic Range (LDR) images for\nscene reconstruction. Such methods struggle to achieve accurate reconstructions\nin scenes that require a higher dynamic range. Examples include scenes captured\nin nighttime or poorly lit indoor spaces having a low signal-to-noise ratio, as\nwell as daylight scenes with shadow regions exhibiting extreme contrast. Our\nproposed method HDRSplat tailors 3DGS to train directly on 14-bit linear raw\nimages in near darkness which preserves the scenes' full dynamic range and\ncontent. Our key contributions are two-fold: Firstly, we propose a linear HDR\nspace-suited loss that effectively extracts scene information from noisy dark\nregions and nearly saturated bright regions simultaneously, while also handling\nview-dependent colors without increasing the degree of spherical harmonics.\nSecondly, through careful rasterization tuning, we implicitly overcome the\nheavy reliance and sensitivity of 3DGS on point cloud initialization. This is\ncritical for accurate reconstruction in regions of low texture, high depth of\nfield, and low illumination. HDRSplat is the fastest method to date that does\n14-bit (HDR) 3D scene reconstruction in $\\le$15 minutes/scene ($\\sim$30x faster\nthan prior state-of-the-art RawNeRF). It also boasts the fastest inference\nspeed at $\\ge$120fps. We further demonstrate the applicability of our HDR scene\nreconstruction by showcasing various applications like synthetic defocus, dense\ndepth map extraction, and post-capture control of exposure, tone-mapping and\nview-point.\n","authors":["Shreyas Singh","Aryan Garg","Kaushik Mitra"],"pdf_url":"https://arxiv.org/pdf/2407.16503v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.04878v2","updated":"2024-07-23T14:18:48Z","published":"2024-02-07T14:18:19Z","title":"Shape-biased Texture Agnostic Representations for Improved Textureless\n  and Metallic Object Detection and 6D Pose Estimation","summary":"  Recent advances in machine learning have greatly benefited object detection\nand 6D pose estimation. However, textureless and metallic objects still pose a\nsignificant challenge due to few visual cues and the texture bias of CNNs. To\naddress his issue, we propose a strategy for inducing a shape bias to CNN\ntraining. In particular, by randomizing textures applied to object surfaces\nduring data rendering, we create training data without consistent textural\ncues. This methodology allows for seamless integration into existing data\nrendering engines, and results in negligible computational overhead for data\nrendering and network training. Our findings demonstrate that the shape bias we\ninduce via randomized texturing, improves over existing approaches using style\ntransfer. We evaluate with three detectors and two pose estimators. For the\nmost recent object detector and for pose estimation in general, estimation\naccuracy improves for textureless and metallic objects. Additionally we show\nthat our approach increases the pose estimation accuracy in the presence of\nimage noise and strong illumination changes. Code and datasets are publicly\navailable at github.com/hoenigpeter/randomized_texturing.\n","authors":["Peter Hönig","Stefan Thalhammer","Jean-Baptiste Weibel","Matthias Hirschmanner","Markus Vincze"],"pdf_url":"https://arxiv.org/pdf/2402.04878v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.12368v3","updated":"2024-07-23T14:13:48Z","published":"2024-04-18T17:50:23Z","title":"Gradient-Regularized Out-of-Distribution Detection","summary":"  One of the challenges for neural networks in real-life applications is the\noverconfident errors these models make when the data is not from the original\ntraining distribution.\n  Addressing this issue is known as Out-of-Distribution (OOD) detection.\n  Many state-of-the-art OOD methods employ an auxiliary dataset as a surrogate\nfor OOD data during training to achieve improved performance.\n  However, these methods fail to fully exploit the local information embedded\nin the auxiliary dataset.\n  In this work, we propose the idea of leveraging the information embedded in\nthe gradient of the loss function during training to enable the network to not\nonly learn a desired OOD score for each sample but also to exhibit similar\nbehavior in a local neighborhood around each sample.\n  We also develop a novel energy-based sampling method to allow the network to\nbe exposed to more informative OOD samples during the training phase. This is\nespecially important when the auxiliary dataset is large. We demonstrate the\neffectiveness of our method through extensive experiments on several OOD\nbenchmarks, improving the existing state-of-the-art FPR95 by 4% on our ImageNet\nexperiment.\n  We further provide a theoretical analysis through the lens of certified\nrobustness and Lipschitz analysis to showcase the theoretical foundation of our\nwork. Our code is available at https://github.com/o4lc/Greg-OOD.\n","authors":["Sina Sharifi","Taha Entesari","Bardia Safaei","Vishal M. Patel","Mahyar Fazlyab"],"pdf_url":"https://arxiv.org/pdf/2404.12368v3.pdf","comment":"Accepted to ECCV 2024"},{"id":"http://arxiv.org/abs/2407.16497v1","updated":"2024-07-23T14:12:57Z","published":"2024-07-23T14:12:57Z","title":"Dynamic Retraining-Updating Mean Teacher for Source-Free Object\n  Detection","summary":"  In object detection, unsupervised domain adaptation (UDA) aims to transfer\nknowledge from a labeled source domain to an unlabeled target domain. However,\nUDA's reliance on labeled source data restricts its adaptability in\nprivacy-related scenarios. This study focuses on source-free object detection\n(SFOD), which adapts a source-trained detector to an unlabeled target domain\nwithout using labeled source data. Recent advancements in self-training,\nparticularly with the Mean Teacher (MT) framework, show promise for SFOD\ndeployment. However, the absence of source supervision significantly\ncompromises the stability of these approaches. We identify two primary issues,\n(1) uncontrollable degradation of the teacher model due to inopportune updates\nfrom the student model, and (2) the student model's tendency to replicate\nerrors from incorrect pseudo labels, leading to it being trapped in a local\noptimum. Both factors contribute to a detrimental circular dependency,\nresulting in rapid performance degradation in recent self-training frameworks.\nTo tackle these challenges, we propose the Dynamic Retraining-Updating (DRU)\nmechanism, which actively manages the student training and teacher updating\nprocesses to achieve co-evolutionary training. Additionally, we introduce\nHistorical Student Loss to mitigate the influence of incorrect pseudo labels.\nOur method achieves state-of-the-art performance in the SFOD setting on\nmultiple domain adaptation benchmarks, comparable to or even surpassing\nadvanced UDA methods. The code will be released at\nhttps://github.com/lbktrinh/DRU\n","authors":["Trinh Le Ba Khanh","Huy-Hung Nguyen","Long Hoang Pham","Duong Nguyen-Ngoc Tran","Jae Wook Jeon"],"pdf_url":"https://arxiv.org/pdf/2407.16497v1.pdf","comment":"ECCV 2024"},{"id":"http://arxiv.org/abs/2405.02984v2","updated":"2024-07-23T13:56:20Z","published":"2024-05-05T16:07:23Z","title":"E-TSL: A Continuous Educational Turkish Sign Language Dataset with\n  Baseline Methods","summary":"  This study introduces the continuous Educational Turkish Sign Language\n(E-TSL) dataset, collected from online Turkish language lessons for 5th, 6th,\nand 8th grades. The dataset comprises 1,410 videos totaling nearly 24 hours and\nincludes performances from 11 signers. Turkish, an agglutinative language,\nposes unique challenges for sign language translation, particularly with a\nvocabulary where 64% are singleton words and 85% are rare words, appearing less\nthan five times. We developed two baseline models to address these challenges:\nthe Pose to Text Transformer (P2T-T) and the Graph Neural Network based\nTransformer (GNN-T) models. The GNN-T model achieved 19.13% BLEU-1 score and\n3.28% BLEU-4 score, presenting a significant challenge compared to existing\nbenchmarks. The P2T-T model, while demonstrating slightly lower performance in\nBLEU scores, achieved a higher ROUGE-L score of 22.09%. Additionally, we\nbenchmarked our model using the well-known PHOENIX-Weather 2014T dataset to\nvalidate our approach.\n","authors":["Şükrü Öztürk","Hacer Yalim Keles"],"pdf_url":"https://arxiv.org/pdf/2405.02984v2.pdf","comment":"7 pages, 3 figures, 4 tables"},{"id":"http://arxiv.org/abs/2402.08823v2","updated":"2024-07-23T13:52:28Z","published":"2024-02-13T22:07:29Z","title":"RanDumb: A Simple Approach that Questions the Efficacy of Continual\n  Representation Learning","summary":"  Continual learning has primarily focused on the issue of catastrophic\nforgetting and the associated stability-plasticity tradeoffs. However, little\nattention has been paid to the efficacy of continually learned representations,\nas representations are learned alongside classifiers throughout the learning\nprocess. Our primary contribution is empirically demonstrating that existing\nonline continually trained deep networks produce inferior representations\ncompared to a simple pre-defined random transforms. Our approach embeds raw\npixels using a fixed random transform, approximating an RBF-Kernel initialized\nbefore any data is seen. We then train a simple linear classifier on top\nwithout storing any exemplars, processing one sample at a time in an online\ncontinual learning setting. This method, called RanDumb, significantly\noutperforms state-of-the-art continually learned representations across all\nstandard online continual learning benchmarks. Our study reveals the\nsignificant limitations of representation learning, particularly in\nlow-exemplar and online continual learning scenarios. Extending our\ninvestigation to popular exemplar-free scenarios with pretrained models, we\nfind that training only a linear classifier on top of pretrained\nrepresentations surpasses most continual fine-tuning and prompt-tuning\nstrategies. Overall, our investigation challenges the prevailing assumptions\nabout effective representation learning in online continual learning. Our code\nis available at://github.com/drimpossible/RanDumb.\n","authors":["Ameya Prabhu","Shiven Sinha","Ponnurangam Kumaraguru","Philip H. S. Torr","Ozan Sener","Puneet K. Dokania"],"pdf_url":"https://arxiv.org/pdf/2402.08823v2.pdf","comment":"Tech Report"},{"id":"http://arxiv.org/abs/2407.16477v1","updated":"2024-07-23T13:49:19Z","published":"2024-07-23T13:49:19Z","title":"qMRI Diffusor: Quantitative T1 Mapping of the Brain using a Denoising\n  Diffusion Probabilistic Model","summary":"  Quantitative MRI (qMRI) offers significant advantages over weighted images by\nproviding objective parameters related to tissue properties. Deep\nlearning-based methods have demonstrated effectiveness in estimating\nquantitative maps from series of weighted images. In this study, we present\nqMRI Diffusor, a novel approach to qMRI utilising deep generative models.\nSpecifically, we implemented denoising diffusion probabilistic models (DDPM)\nfor T1 quantification in the brain, framing the estimation of quantitative maps\nas a conditional generation task. The proposed method is compared with the\nresidual neural network (ResNet) and the recurrent inference machine (RIM) on\nboth phantom and in vivo data. The results indicate that our method achieves\nimproved accuracy and precision in parameter estimation, along with superior\nvisual performance. Moreover, our method inherently incorporates stochasticity,\nenabling straightforward quantification of uncertainty. Hence, the proposed\nmethod holds significant promise for quantitative MR mapping.\n","authors":["Shishuai Wang","Hua Ma","Juan A. Hernandez-Tamames","Stefan Klein","Dirk H. J. Poot"],"pdf_url":"https://arxiv.org/pdf/2407.16477v1.pdf","comment":"Accepted by Deep Generative Models workshop at MICCAI 2024"},{"id":"http://arxiv.org/abs/2404.15259v3","updated":"2024-07-23T13:41:03Z","published":"2024-04-23T17:46:50Z","title":"FlowMap: High-Quality Camera Poses, Intrinsics, and Depth via Gradient\n  Descent","summary":"  This paper introduces FlowMap, an end-to-end differentiable method that\nsolves for precise camera poses, camera intrinsics, and per-frame dense depth\nof a video sequence. Our method performs per-video gradient-descent\nminimization of a simple least-squares objective that compares the optical flow\ninduced by depth, intrinsics, and poses against correspondences obtained via\noff-the-shelf optical flow and point tracking. Alongside the use of point\ntracks to encourage long-term geometric consistency, we introduce\ndifferentiable re-parameterizations of depth, intrinsics, and pose that are\namenable to first-order optimization. We empirically show that camera\nparameters and dense depth recovered by our method enable photo-realistic novel\nview synthesis on 360-degree trajectories using Gaussian Splatting. Our method\nnot only far outperforms prior gradient-descent based bundle adjustment\nmethods, but surprisingly performs on par with COLMAP, the state-of-the-art SfM\nmethod, on the downstream task of 360-degree novel view synthesis (even though\nour method is purely gradient-descent based, fully differentiable, and presents\na complete departure from conventional SfM).\n","authors":["Cameron Smith","David Charatan","Ayush Tewari","Vincent Sitzmann"],"pdf_url":"https://arxiv.org/pdf/2404.15259v3.pdf","comment":"Project website: https://cameronosmith.github.io/flowmap/"},{"id":"http://arxiv.org/abs/2407.16464v1","updated":"2024-07-23T13:27:44Z","published":"2024-07-23T13:27:44Z","title":"Lymphoid Infiltration Assessment of the Tumor Margins in H&E Slides","summary":"  Lymphoid infiltration at tumor margins is a key prognostic marker in solid\ntumors, playing a crucial role in guiding immunotherapy decisions. Current\nassessment methods, heavily reliant on immunohistochemistry (IHC), face\nchallenges in tumor margin delineation and are affected by tissue preservation\nconditions. In contrast, we propose a Hematoxylin and Eosin (H&E)\nstaining-based approach, underpinned by an advanced lymphocyte segmentation\nmodel trained on a public dataset for the precise detection of CD3+ and CD20+\nlymphocytes. In our colorectal cancer study, we demonstrate that our H&E-based\nmethod offers a compelling alternative to traditional IHC, achieving comparable\nresults in many cases. Our method's validity is further explored through a\nTuring test, involving blinded assessments by a pathologist of anonymized\ncurves from H&E and IHC slides. This approach invites the medical community to\nconsider Turing tests as a standard for evaluating medical applications\ninvolving expert human evaluation, thereby opening new avenues for enhancing\ncancer management and immunotherapy planning.\n","authors":["Zhuxian Guo","Amine Marzouki","Jean-François Emile","Henning Müller","Camille Kurtz","Nicolas Loménie"],"pdf_url":"https://arxiv.org/pdf/2407.16464v1.pdf","comment":"Published in Medical Optical Imaging and Virtual Microscopy Image\n  Analysis (MOVI) at MICCAI 2024"},{"id":"http://arxiv.org/abs/2402.11957v2","updated":"2024-07-23T13:22:58Z","published":"2024-02-19T08:59:58Z","title":"Event-Based Motion Magnification","summary":"  Detecting and magnifying imperceptible high-frequency motions in real-world\nscenarios has substantial implications for industrial and medical applications.\nThese motions are characterized by small amplitudes and high frequencies.\nTraditional motion magnification methods rely on costly high-speed cameras or\nactive light sources, which limit the scope of their applications. In this\nwork, we propose a dual-camera system consisting of an event camera and a\nconventional RGB camera for video motion magnification, providing\ntemporally-dense information from the event stream and spatially-dense data\nfrom the RGB images. This innovative combination enables a broad and\ncost-effective amplification of high-frequency motions. By revisiting the\nphysical camera model, we observe that estimating motion direction and\nmagnitude necessitates the integration of event streams with additional image\nfeatures. On this basis, we propose a novel deep network tailored for\nevent-based motion magnification. Our approach utilizes the Second-order\nRecurrent Propagation module to proficiently interpolate multiple frames while\naddressing artifacts and distortions induced by magnified motions.\nAdditionally, we employ a temporal filter to distinguish between noise and\nuseful signals, thus minimizing the impact of noise. We also introduced the\nfirst event-based motion magnification dataset, which includes a synthetic\nsubset and a real-captured subset for training and benchmarking. Through\nextensive experiments in magnifying small-amplitude, high-frequency motions, we\ndemonstrate the effectiveness and accuracy of our dual-camera system and\nnetwork, offering a cost-effective and flexible solution for motion detection\nand magnification.\n","authors":["Yutian Chen","Shi Guo","Fangzheng Yu","Feng Zhang","Jinwei Gu","Tianfan Xue"],"pdf_url":"https://arxiv.org/pdf/2402.11957v2.pdf","comment":"Accepted to ECCV 2024"},{"id":"http://arxiv.org/abs/2407.00917v2","updated":"2024-07-23T13:10:28Z","published":"2024-07-01T02:42:55Z","title":"From Category to Scenery: An End-to-End Framework for Multi-Person\n  Human-Object Interaction Recognition in Videos","summary":"  Video-based Human-Object Interaction (HOI) recognition explores the intricate\ndynamics between humans and objects, which are essential for a comprehensive\nunderstanding of human behavior and intentions. While previous work has made\nsignificant strides, effectively integrating geometric and visual features to\nmodel dynamic relationships between humans and objects in a graph framework\nremains a challenge. In this work, we propose a novel end-to-end category to\nscenery framework, CATS, starting by generating geometric features for various\ncategories through graphs respectively, then fusing them with corresponding\nvisual features. Subsequently, we construct a scenery interactive graph with\nthese enhanced geometric-visual features as nodes to learn the relationships\namong human and object categories. This methodological advance facilitates a\ndeeper, more structured comprehension of interactions, bridging\ncategory-specific insights with broad scenery dynamics. Our method demonstrates\nstate-of-the-art performance on two pivotal HOI benchmarks, including the\nMPHOI-72 dataset for multi-person HOIs and the single-person HOI CAD-120\ndataset.\n","authors":["Tanqiu Qiao","Ruochen Li","Frederick W. B. Li","Hubert P. H. Shum"],"pdf_url":"https://arxiv.org/pdf/2407.00917v2.pdf","comment":"Accepted by ICPR 2024"},{"id":"http://arxiv.org/abs/2407.16448v1","updated":"2024-07-23T12:58:49Z","published":"2024-07-23T12:58:49Z","title":"MonoWAD: Weather-Adaptive Diffusion Model for Robust Monocular 3D Object\n  Detection","summary":"  Monocular 3D object detection is an important challenging task in autonomous\ndriving. Existing methods mainly focus on performing 3D detection in ideal\nweather conditions, characterized by scenarios with clear and optimal\nvisibility. However, the challenge of autonomous driving requires the ability\nto handle changes in weather conditions, such as foggy weather, not just clear\nweather. We introduce MonoWAD, a novel weather-robust monocular 3D object\ndetector with a weather-adaptive diffusion model. It contains two components:\n(1) the weather codebook to memorize the knowledge of the clear weather and\ngenerate a weather-reference feature for any input, and (2) the\nweather-adaptive diffusion model to enhance the feature representation of the\ninput feature by incorporating a weather-reference feature. This serves an\nattention role in indicating how much improvement is needed for the input\nfeature according to the weather conditions. To achieve this goal, we introduce\na weather-adaptive enhancement loss to enhance the feature representation under\nboth clear and foggy weather conditions. Extensive experiments under various\nweather conditions demonstrate that MonoWAD achieves weather-robust monocular\n3D object detection. The code and dataset are released at\nhttps://github.com/VisualAIKHU/MonoWAD.\n","authors":["Youngmin Oh","Hyung-Il Kim","Seong Tae Kim","Jung Uk Kim"],"pdf_url":"https://arxiv.org/pdf/2407.16448v1.pdf","comment":"Accepted by ECCV 2024"},{"id":"http://arxiv.org/abs/2407.15212v2","updated":"2024-07-23T12:57:32Z","published":"2024-07-21T16:34:03Z","title":"Surfel-based Gaussian Inverse Rendering for Fast and Relightable Dynamic\n  Human Reconstruction from Monocular Video","summary":"  Efficient and accurate reconstruction of a relightable, dynamic clothed human\navatar from a monocular video is crucial for the entertainment industry. This\npaper introduces the Surfel-based Gaussian Inverse Avatar (SGIA) method, which\nintroduces efficient training and rendering for relightable dynamic human\nreconstruction. SGIA advances previous Gaussian Avatar methods by\ncomprehensively modeling Physically-Based Rendering (PBR) properties for\nclothed human avatars, allowing for the manipulation of avatars into novel\nposes under diverse lighting conditions. Specifically, our approach integrates\npre-integration and image-based lighting for fast light calculations that\nsurpass the performance of existing implicit-based techniques. To address\nchallenges related to material lighting disentanglement and accurate geometry\nreconstruction, we propose an innovative occlusion approximation strategy and a\nprogressive training approach. Extensive experiments demonstrate that SGIA not\nonly achieves highly accurate physical properties but also significantly\nenhances the realistic relighting of dynamic human avatars, providing a\nsubstantial speed advantage. We exhibit more results in our project page:\nhttps://GS-IA.github.io.\n","authors":["Yiqun Zhao","Chenming Wu","Binbin Huang","Yihao Zhi","Chen Zhao","Jingdong Wang","Shenghua Gao"],"pdf_url":"https://arxiv.org/pdf/2407.15212v2.pdf","comment":"Under Review; Project Page: https://GS-IA.github.io"},{"id":"http://arxiv.org/abs/2407.16430v1","updated":"2024-07-23T12:28:59Z","published":"2024-07-23T12:28:59Z","title":"Rethinking Out-of-Distribution Detection on Imbalanced Data Distribution","summary":"  Detecting and rejecting unknown out-of-distribution (OOD) samples is critical\nfor deployed neural networks to void unreliable predictions. In real-world\nscenarios, however, the efficacy of existing OOD detection methods is often\nimpeded by the inherent imbalance of in-distribution (ID) data, which causes\nsignificant performance decline. Through statistical observations, we have\nidentified two common challenges faced by different OOD detectors:\nmisidentifying tail class ID samples as OOD, while erroneously predicting OOD\nsamples as head class from ID. To explain this phenomenon, we introduce a\ngeneralized statistical framework, termed ImOOD, to formulate the OOD detection\nproblem on imbalanced data distribution. Consequently, the theoretical analysis\nreveals that there exists a class-aware bias item between balanced and\nimbalanced OOD detection, which contributes to the performance gap. Building\nupon this finding, we present a unified training-time regularization technique\nto mitigate the bias and boost imbalanced OOD detectors across architecture\ndesigns. Our theoretically grounded method translates into consistent\nimprovements on the representative CIFAR10-LT, CIFAR100-LT, and ImageNet-LT\nbenchmarks against several state-of-the-art OOD detection approaches. Code will\nbe made public soon.\n","authors":["Kai Liu","Zhihang Fu","Sheng Jin","Chao Chen","Ze Chen","Rongxin Jiang","Fan Zhou","Yaowu Chen","Jieping Ye"],"pdf_url":"https://arxiv.org/pdf/2407.16430v1.pdf","comment":"N/A"},{"id":"http://arxiv.org/abs/2311.03967v3","updated":"2024-07-23T12:27:37Z","published":"2023-11-07T13:06:50Z","title":"CeCNN: Copula-enhanced convolutional neural networks in joint prediction\n  of refraction error and axial length based on ultra-widefield fundus images","summary":"  The ultra-widefield (UWF) fundus image is an attractive 3D biomarker in\nAI-aided myopia screening because it provides much richer myopia-related\ninformation. Though axial length (AL) has been acknowledged to be highly\nrelated to the two key targets of myopia screening, Spherical Equivalence (SE)\nmeasuring and high myopia diagnosis, its prediction based on the UWF fundus\nimage is rarely considered. To save the high expense and time costs of\nmeasuring SE and AL, we propose the Copula-enhanced Convolutional Neural\nNetwork (CeCNN), a one-stop UWF-based ophthalmic AI framework to jointly\npredict SE, AL, and myopia status. The CeCNN formulates a multiresponse\nregression that relates multiple dependent discrete-continuous responses and\nthe image covariate, where the nonlinearity of the association is modeled by a\nbackbone CNN. To thoroughly describe the dependence structure among the\nresponses, we model and incorporate the conditional dependence among responses\nin a CNN through a new copula-likelihood loss. We provide statistical\ninterpretations of the conditional dependence among responses, and reveal that\nsuch dependence is beyond the dependence explained by the image covariate. We\nheuristically justify that the proposed loss can enhance the estimation\nefficiency of the CNN weights. We apply the CeCNN to the UWF dataset collected\nby us and demonstrate that the CeCNN sharply enhances the predictive capability\nof various backbone CNNs. Our study evidences the ophthalmology view that\nbesides SE, AL is also an important measure to myopia.\n","authors":["Chong Zhong","Yang Li","Danjuan Yang","Meiyan Li","Xingyao Zhou","Bo Fu","Catherine C. Liu","A. H. Welsh"],"pdf_url":"https://arxiv.org/pdf/2311.03967v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17536v3","updated":"2024-07-23T12:23:10Z","published":"2024-06-25T13:20:39Z","title":"MedMNIST-C: Comprehensive benchmark and improved classifier robustness\n  by simulating realistic image corruptions","summary":"  The integration of neural-network-based systems into clinical practice is\nlimited by challenges related to domain generalization and robustness. The\ncomputer vision community established benchmarks such as ImageNet-C as a\nfundamental prerequisite to measure progress towards those challenges. Similar\ndatasets are largely absent in the medical imaging community which lacks a\ncomprehensive benchmark that spans across imaging modalities and applications.\nTo address this gap, we create and open-source MedMNIST-C, a benchmark dataset\nbased on the MedMNIST+ collection covering 12 datasets and 9 imaging\nmodalities. We simulate task and modality-specific image corruptions of varying\nseverity to comprehensively evaluate the robustness of established algorithms\nagainst real-world artifacts and distribution shifts. We further provide\nquantitative evidence that our simple-to-use artificial corruptions allow for\nhighly performant, lightweight data augmentation to enhance model robustness.\nUnlike traditional, generic augmentation strategies, our approach leverages\ndomain knowledge, exhibiting significantly higher robustness when compared to\nwidely adopted methods. By introducing MedMNIST-C and open-sourcing the\ncorresponding library allowing for targeted data augmentations, we contribute\nto the development of increasingly robust methods tailored to the challenges of\nmedical imaging. The code is available at\nhttps://github.com/francescodisalvo05/medmnistc-api .\n","authors":["Francesco Di Salvo","Sebastian Doerrich","Christian Ledig"],"pdf_url":"https://arxiv.org/pdf/2406.17536v3.pdf","comment":"Accepted at MICCAI Workshop on Advancing Data Solutions in Medical\n  Imaging AI (ADSMI @ MICCAI 2024)"},{"id":"http://arxiv.org/abs/2407.16424v1","updated":"2024-07-23T12:21:23Z","published":"2024-07-23T12:21:23Z","title":"ESOD: Efficient Small Object Detection on High-Resolution Images","summary":"  Enlarging input images is a straightforward and effective approach to promote\nsmall object detection. However, simple image enlargement is significantly\nexpensive on both computations and GPU memory. In fact, small objects are\nusually sparsely distributed and locally clustered. Therefore, massive feature\nextraction computations are wasted on the non-target background area of images.\nRecent works have tried to pick out target-containing regions using an extra\nnetwork and perform conventional object detection, but the newly introduced\ncomputation limits their final performance. In this paper, we propose to reuse\nthe detector's backbone to conduct feature-level object-seeking and\npatch-slicing, which can avoid redundant feature extraction and reduce the\ncomputation cost. Incorporating a sparse detection head, we are able to detect\nsmall objects on high-resolution inputs (e.g., 1080P or larger) for superior\nperformance. The resulting Efficient Small Object Detection (ESOD) approach is\na generic framework, which can be applied to both CNN- and ViT-based detectors\nto save the computation and GPU memory costs. Extensive experiments demonstrate\nthe efficacy and efficiency of our method. In particular, our method\nconsistently surpasses the SOTA detectors by a large margin (e.g., 8% gains on\nAP) on the representative VisDrone, UAVDT, and TinyPerson datasets. Code will\nbe made public soon.\n","authors":["Kai Liu","Zhihang Fu","Sheng Jin","Ze Chen","Fan Zhou","Rongxin Jiang","Yaowu Chen","Jieping Ye"],"pdf_url":"https://arxiv.org/pdf/2407.16424v1.pdf","comment":"N/A"},{"id":"http://arxiv.org/abs/2403.06600v2","updated":"2024-07-23T12:20:27Z","published":"2024-03-11T10:46:43Z","title":"BEV$^2$PR: BEV-Enhanced Visual Place Recognition with Structural Cues","summary":"  In this paper, we propose a new image-based visual place recognition (VPR)\nframework by exploiting the structural cues in bird's-eye view (BEV) from a\nsingle monocular camera. The motivation arises from two key observations about\nplace recognition methods based on both appearance and structure: 1) For the\nmethods relying on LiDAR sensors, the integration of LiDAR in robotic systems\nhas led to increased expenses, while the alignment of data between different\nsensors is also a major challenge. 2) Other image-/camera-based methods,\ninvolving integrating RGB images and their derived variants (eg, pseudo depth\nimages, pseudo 3D point clouds), exhibit several limitations, such as the\nfailure to effectively exploit the explicit spatial relationships between\ndifferent objects. To tackle the above issues, we design a new BEV-enhanced VPR\nframework, namely BEV$^2$PR, generating a composite descriptor with both visual\ncues and spatial awareness based on a single camera. The key points lie in: 1)\nWe use BEV features as an explicit source of structural knowledge in\nconstructing global features. 2) The lower layers of the pre-trained backbone\nfrom BEV generation are shared for visual and structural streams in VPR,\nfacilitating the learning of fine-grained local features in the visual stream.\n3) The complementary visual and structural features can jointly enhance VPR\nperformance. Our BEV$^2$PR framework enables consistent performance\nimprovements over several popular aggregation modules for RGB global features.\nThe experiments on our collected VPR-NuScenes dataset demonstrate an absolute\ngain of 2.47% on Recall@1 for the strong Conv-AP baseline to achieve the best\nperformance in our setting, and notably, a 18.06% gain on the hard set. The\ncode and dataset will be available at https://github.com/FudongGe/BEV2PR.\n","authors":["Fudong Ge","Yiwei Zhang","Shuhan Shen","Yue Wang","Weiming Hu","Jin Gao"],"pdf_url":"https://arxiv.org/pdf/2403.06600v2.pdf","comment":"Accepted at IROS 2024 as Oral Presentation. Code available at\n  https://github.com/FudongGe/BEV2PR"},{"id":"http://arxiv.org/abs/2407.15240v2","updated":"2024-07-23T12:13:42Z","published":"2024-07-21T18:09:40Z","title":"BIGbench: A Unified Benchmark for Social Bias in Text-to-Image\n  Generative Models Based on Multi-modal LLM","summary":"  Text-to-Image (T2I) generative models are becoming more crucial in terms of\ntheir ability to generate complex and high-quality images, which also raises\nconcerns about the social biases in their outputs, especially in human\ngeneration. Sociological research has established systematic classifications of\nbias; however, existing research of T2I models often conflates different types\nof bias, hindering the progress of these methods. In this paper, we introduce\nBIGbench, a unified benchmark for Biases of Image Generation with a\nwell-designed dataset. In contrast to existing benchmarks, BIGbench classifies\nand evaluates complex biases into four dimensions: manifestation of bias,\nvisibility of bias, acquired attributes, and protected attributes.\nAdditionally, BIGbench applies advanced multi-modal large language models\n(MLLM), achieving fully automated evaluation while maintaining high accuracy.\nWe apply BIGbench to evaluate eight recent general T2I models and three\ndebiased methods. We also conduct human evaluation, whose results demonstrated\nthe effectiveness of BIGbench in aligning images and identifying various\nbiases. Besides, our study also revealed new research directions about biases,\nincluding the side-effect of irrelevant protected attributes and distillation.\nOur dataset and benchmark is openly accessible to the research community to\nensure the reproducibility.\n","authors":["Hanjun Luo","Haoyu Huang","Ziye Deng","Xuecheng Liu","Ruizhe Chen","Zuozhu Liu"],"pdf_url":"https://arxiv.org/pdf/2407.15240v2.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2405.17814"},{"id":"http://arxiv.org/abs/2401.12978v3","updated":"2024-07-23T12:13:21Z","published":"2024-01-23T18:59:59Z","title":"Beyond the Contact: Discovering Comprehensive Affordance for 3D Objects\n  from Pre-trained 2D Diffusion Models","summary":"  Understanding the inherent human knowledge in interacting with a given\nenvironment (e.g., affordance) is essential for improving AI to better assist\nhumans. While existing approaches primarily focus on human-object contacts\nduring interactions, such affordance representation cannot fully address other\nimportant aspects of human-object interactions (HOIs), i.e., patterns of\nrelative positions and orientations. In this paper, we introduce a novel\naffordance representation, named Comprehensive Affordance (ComA). Given a 3D\nobject mesh, ComA models the distribution of relative orientation and proximity\nof vertices in interacting human meshes, capturing plausible patterns of\ncontact, relative orientations, and spatial relationships. To construct the\ndistribution, we present a novel pipeline that synthesizes diverse and\nrealistic 3D HOI samples given any 3D object mesh. The pipeline leverages a\npre-trained 2D inpainting diffusion model to generate HOI images from object\nrenderings and lifts them into 3D. To avoid the generation of false\naffordances, we propose a new inpainting framework, Adaptive Mask Inpainting.\nSince ComA is built on synthetic samples, it can extend to any object in an\nunbounded manner. Through extensive experiments, we demonstrate that ComA\noutperforms competitors that rely on human annotations in modeling\ncontact-based affordance. Importantly, we also showcase the potential of ComA\nto reconstruct human-object interactions in 3D through an optimization\nframework, highlighting its advantage in incorporating both contact and\nnon-contact properties.\n","authors":["Hyeonwoo Kim","Sookwan Han","Patrick Kwon","Hanbyul Joo"],"pdf_url":"https://arxiv.org/pdf/2401.12978v3.pdf","comment":"Project Page: https://snuvclab.github.io/coma/"},{"id":"http://arxiv.org/abs/2407.16418v1","updated":"2024-07-23T12:02:57Z","published":"2024-07-23T12:02:57Z","title":"Accelerating Learned Video Compression via Low-Resolution Representation\n  Learning","summary":"  In recent years, the field of learned video compression has witnessed rapid\nadvancement, exemplified by the latest neural video codecs DCVC-DC that has\noutperformed the upcoming next-generation codec ECM in terms of compression\nratio. Despite this, learned video compression frameworks often exhibit low\nencoding and decoding speeds primarily due to their increased computational\ncomplexity and unnecessary high-resolution spatial operations, which hugely\nhinder their applications in reality. In this work, we introduce an\nefficiency-optimized framework for learned video compression that focuses on\nlow-resolution representation learning, aiming to significantly enhance the\nencoding and decoding speeds. Firstly, we diminish the computational load by\nreducing the resolution of inter-frame propagated features obtained from reused\nfeatures of decoded frames, including I-frames. We implement a joint training\nstrategy for both the I-frame and P-frame models, further improving the\ncompression ratio. Secondly, our approach efficiently leverages multi-frame\npriors for parameter prediction, minimizing computation at the decoding end.\nThirdly, we revisit the application of the Online Encoder Update (OEU) strategy\nfor high-resolution sequences, achieving notable improvements in compression\nratio without compromising decoding efficiency. Our efficiency-optimized\nframework has significantly improved the balance between compression ratio and\nspeed for learned video compression. In comparison to traditional codecs, our\nmethod achieves performance levels on par with the low-decay P configuration of\nthe H.266 reference software VTM. Furthermore, when contrasted with DCVC-HEM,\nour approach delivers a comparable compression ratio while boosting encoding\nand decoding speeds by a factor of 3 and 7, respectively. On RTX 2080Ti, our\nmethod can decode each 1080p frame under 100ms.\n","authors":["Zidian Qiu","Zongyao He","Zhi Jin"],"pdf_url":"https://arxiv.org/pdf/2407.16418v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.12488v2","updated":"2024-07-23T11:58:53Z","published":"2024-04-18T20:03:56Z","title":"Global Counterfactual Directions","summary":"  Despite increasing progress in development of methods for generating visual\ncounterfactual explanations, especially with the recent rise of Denoising\nDiffusion Probabilistic Models, previous works consider them as an entirely\nlocal technique. In this work, we take the first step at globalizing them.\nSpecifically, we discover that the latent space of Diffusion Autoencoders\nencodes the inference process of a given classifier in the form of global\ndirections. We propose a novel proxy-based approach that discovers two types of\nthese directions with the use of only single image in an entirely black-box\nmanner. Precisely, g-directions allow for flipping the decision of a given\nclassifier on an entire dataset of images, while h-directions further increase\nthe diversity of explanations. We refer to them in general as Global\nCounterfactual Directions (GCDs). Moreover, we show that GCDs can be naturally\ncombined with Latent Integrated Gradients resulting in a new black-box\nattribution method, while simultaneously enhancing the understanding of\ncounterfactual explanations. We validate our approach on existing benchmarks\nand show that it generalizes to real-world use-cases.\n","authors":["Bartlomiej Sobieski","Przemysław Biecek"],"pdf_url":"https://arxiv.org/pdf/2404.12488v2.pdf","comment":"ECCV 2024"},{"id":"http://arxiv.org/abs/2407.16413v1","updated":"2024-07-23T11:58:08Z","published":"2024-07-23T11:58:08Z","title":"Low Complexity Regularized Phase Retrieval","summary":"  In this paper, we study the phase retrieval problem in the situation where\nthe vector to be recovered has an a priori structure that can encoded into a\nregularization term. This regularizer is intended to promote solutions\nconforming to some notion of simplicity or low complexity. We investigate both\nnoiseless recovery and stability to noise and provide a very general and\nunified analysis framework that goes far beyond the sparse phase retrieval\nmostly considered in the literature. In the noiseless case we provide\nsufficient conditions under which exact recovery, up to global sign change, is\npossible. For Gaussian measurement maps, we also provide a sample complexity\nbound for exact recovery. This bound depends on the Gaussian width of the\ndescent cone at the soughtafter vector which is a geometric measure of the\ncomplexity of the latter. In the noisy case, we consider both the constrained\n(Mozorov) and penalized (Tikhonov) formulations. We provide sufficient\nconditions for stable recovery and prove linear convergence for sufficiently\nsmall noise. For Gaussian measurements, we again give a sample complexity bound\nfor linear convergence to hold with high probability. This bound scales\nlinearly in the intrinsic dimension of the sought-after vector but only\nlogarithmically in the ambient dimension.\n","authors":["Jean-Jacques Godeme","Jalal Fadili"],"pdf_url":"https://arxiv.org/pdf/2407.16413v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16406v1","updated":"2024-07-23T11:50:59Z","published":"2024-07-23T11:50:59Z","title":"Hi-EF: Benchmarking Emotion Forecasting in Human-interaction","summary":"  Affective Forecasting, a research direction in psychology that predicts\nindividuals future emotions, is often constrained by numerous external factors\nlike social influence and temporal distance. To address this, we transform\nAffective Forecasting into a Deep Learning problem by designing an Emotion\nForecasting paradigm based on two-party interactions. We propose a novel\nEmotion Forecasting (EF) task grounded in the theory that an individuals\nemotions are easily influenced by the emotions or other information conveyed\nduring interactions with another person. To tackle this task, we have developed\na specialized dataset, Human-interaction-based Emotion Forecasting (Hi-EF),\nwhich contains 3069 two-party Multilayered-Contextual Interaction Samples\n(MCIS) with abundant affective-relevant labels and three modalities. Hi-EF not\nonly demonstrates the feasibility of the EF task but also highlights its\npotential. Additionally, we propose a methodology that establishes a\nfoundational and referential baseline model for the EF task and extensive\nexperiments are provided. The dataset and code is available at\nhttps://github.com/Anonymize-Author/Hi-EF.\n","authors":["Haoran Wang","Xinji Mai","Zeng Tao","Yan Wang","Jiawen Yu","Ziheng Zhou","Xuan Tong","Shaoqi Yan","Qing Zhao","Shuyong Gao","Wenqiang Zhang"],"pdf_url":"https://arxiv.org/pdf/2407.16406v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07622v2","updated":"2024-07-23T11:50:03Z","published":"2024-04-11T10:16:44Z","title":"Language Models Meet Anomaly Detection for Better Interpretability and\n  Generalizability","summary":"  This research explores the integration of language models and unsupervised\nanomaly detection in medical imaging, addressing two key questions: (1) Can\nlanguage models enhance the interpretability of anomaly detection maps? and (2)\nCan anomaly maps improve the generalizability of language models in open-set\nanomaly detection tasks? To investigate these questions, we introduce a new\ndataset for multi-image visual question-answering on brain magnetic resonance\nimages encompassing multiple conditions. We propose KQ-Former (Knowledge\nQuerying Transformer), which is designed to optimally align visual and textual\ninformation in limited-sample contexts. Our model achieves a 60.81% accuracy on\nclosed questions, covering disease classification and severity across 15\ndifferent classes. For open questions, KQ-Former demonstrates a 70% improvement\nover the baseline with a BLEU-4 score of 0.41, and achieves the highest\nentailment ratios (up to 71.9%) and lowest contradiction ratios (down to 10.0%)\namong various natural language inference models. Furthermore, integrating\nanomaly maps results in an 18% accuracy increase in detecting open-set\nanomalies, thereby enhancing the language model's generalizability to\npreviously unseen medical conditions. The code and dataset are available at\nhttps://github.com/compai-lab/miccai-2024-junli?tab=readme-ov-file\n","authors":["Jun Li","Su Hwan Kim","Philip Müller","Lina Felsner","Daniel Rueckert","Benedikt Wiestler","Julia A. Schnabel","Cosmin I. Bercea"],"pdf_url":"https://arxiv.org/pdf/2404.07622v2.pdf","comment":"13 pages, 7 figures. 5th International Workshop on Multiscale\n  Multimodal Medical Imaging (MMMI 2024)"},{"id":"http://arxiv.org/abs/2407.16405v1","updated":"2024-07-23T11:49:58Z","published":"2024-07-23T11:49:58Z","title":"On Differentially Private 3D Medical Image Synthesis with Controllable\n  Latent Diffusion Models","summary":"  Generally, the small size of public medical imaging datasets coupled with\nstringent privacy concerns, hampers the advancement of data-hungry deep\nlearning models in medical imaging. This study addresses these challenges for\n3D cardiac MRI images in the short-axis view. We propose Latent Diffusion\nModels that generate synthetic images conditioned on medical attributes, while\nensuring patient privacy through differentially private model training. To our\nknowledge, this is the first work to apply and quantify differential privacy in\n3D medical image generation. We pre-train our models on public data and\nfinetune them with differential privacy on the UK Biobank dataset. Our\nexperiments reveal that pre-training significantly improves model performance,\nachieving a Fr\\'echet Inception Distance (FID) of 26.77 at $\\epsilon=10$,\ncompared to 92.52 for models without pre-training. Additionally, we explore the\ntrade-off between privacy constraints and image quality, investigating how\ntighter privacy budgets affect output controllability and may lead to degraded\nperformance. Our results demonstrate that proper consideration during training\nwith differential privacy can substantially improve the quality of synthetic\ncardiac MRI images, but there are still notable challenges in achieving\nconsistent medical realism.\n","authors":["Deniz Daum","Richard Osuala","Anneliese Riess","Georgios Kaissis","Julia A. Schnabel","Maxime Di Folco"],"pdf_url":"https://arxiv.org/pdf/2407.16405v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.03594v4","updated":"2024-07-23T11:48:57Z","published":"2023-12-06T16:34:46Z","title":"A Task is Worth One Word: Learning with Task Prompts for High-Quality\n  Versatile Image Inpainting","summary":"  Advancing image inpainting is challenging as it requires filling\nuser-specified regions for various intents, such as background filling and\nobject synthesis. Existing approaches focus on either context-aware filling or\nobject synthesis using text descriptions. However, achieving both tasks\nsimultaneously is challenging due to differing training strategies. To overcome\nthis challenge, we introduce PowerPaint, the first high-quality and versatile\ninpainting model that excels in multiple inpainting tasks. First, we introduce\nlearnable task prompts along with tailored fine-tuning strategies to guide the\nmodel's focus on different inpainting targets explicitly. This enables\nPowerPaint to accomplish various inpainting tasks by utilizing different task\nprompts, resulting in state-of-the-art performance. Second, we demonstrate the\nversatility of the task prompt in PowerPaint by showcasing its effectiveness as\na negative prompt for object removal. Moreover, we leverage prompt\ninterpolation techniques to enable controllable shape-guided object inpainting,\nenhancing the model's applicability in shape-guided applications. Finally, we\nconduct extensive experiments and applications to verify the effectiveness of\nPowerPaint. We release our codes and models on our project page:\nhttps://powerpaint.github.io/.\n","authors":["Junhao Zhuang","Yanhong Zeng","Wenran Liu","Chun Yuan","Kai Chen"],"pdf_url":"https://arxiv.org/pdf/2312.03594v4.pdf","comment":"Project page with code: https://powerpaint.github.io/"},{"id":"http://arxiv.org/abs/2403.11469v2","updated":"2024-07-23T11:43:31Z","published":"2024-03-18T04:41:59Z","title":"Generative Motion Stylization of Cross-structure Characters within\n  Canonical Motion Space","summary":"  Stylized motion breathes life into characters. However, the fixed skeleton\nstructure and style representation hinder existing data-driven motion synthesis\nmethods from generating stylized motion for various characters. In this work,\nwe propose a generative motion stylization pipeline, named MotionS, for\nsynthesizing diverse and stylized motion on cross-structure characters using\ncross-modality style prompts. Our key insight is to embed motion style into a\ncross-modality latent space and perceive the cross-structure skeleton\ntopologies, allowing for motion stylization within a canonical motion space.\nSpecifically, the large-scale Contrastive-Language-Image-Pre-training (CLIP)\nmodel is leveraged to construct the cross-modality latent space, enabling\nflexible style representation within it. Additionally, two topology-encoded\ntokens are learned to capture the canonical and specific skeleton topologies,\nfacilitating cross-structure topology shifting. Subsequently, the\ntopology-shifted stylization diffusion is designed to generate motion content\nfor the particular skeleton and stylize it in the shifted canonical motion\nspace using multi-modality style descriptions. Through an extensive set of\nexamples, we demonstrate the flexibility and generalizability of our pipeline\nacross various characters and style descriptions. Qualitative and quantitative\ncomparisons show the superiority of our pipeline over state-of-the-arts,\nconsistently delivering high-quality stylized motion across a broad spectrum of\nskeletal structures.\n","authors":["Jiaxu Zhang","Xin Chen","Gang Yu","Zhigang Tu"],"pdf_url":"https://arxiv.org/pdf/2403.11469v2.pdf","comment":"ACM MM 2024"},{"id":"http://arxiv.org/abs/2407.16396v1","updated":"2024-07-23T11:35:33Z","published":"2024-07-23T11:35:33Z","title":"Learning Unsigned Distance Functions from Multi-view Images with Volume\n  Rendering Priors","summary":"  Unsigned distance functions (UDFs) have been a vital representation for open\nsurfaces. With different differentiable renderers, current methods are able to\ntrain neural networks to infer a UDF by minimizing the rendering errors on the\nUDF to the multi-view ground truth. However, these differentiable renderers are\nmainly handcrafted, which makes them either biased on ray-surface\nintersections, or sensitive to unsigned distance outliers, or not scalable to\nlarge scale scenes. To resolve these issues, we present a novel differentiable\nrenderer to infer UDFs more accurately. Instead of using handcrafted equations,\nour differentiable renderer is a neural network which is pre-trained in a\ndata-driven manner. It learns how to render unsigned distances into depth\nimages, leading to a prior knowledge, dubbed volume rendering priors. To infer\na UDF for an unseen scene from multiple RGB images, we generalize the learned\nvolume rendering priors to map inferred unsigned distances in alpha blending\nfor RGB image rendering. Our results show that the learned volume rendering\npriors are unbiased, robust, scalable, 3D aware, and more importantly, easy to\nlearn. We evaluate our method on both widely used benchmarks and real scenes,\nand report superior performance over the state-of-the-art methods.\n","authors":["Wenyuan Zhang","Kanle Shi","Yu-Shen Liu","Zhizhong Han"],"pdf_url":"https://arxiv.org/pdf/2407.16396v1.pdf","comment":"Accepted by ECCV 2024. Project page:\n  https://wen-yuan-zhang.github.io/VolumeRenderingPriors/"},{"id":"http://arxiv.org/abs/2407.16394v1","updated":"2024-07-23T11:31:11Z","published":"2024-07-23T11:31:11Z","title":"SEDS: Semantically Enhanced Dual-Stream Encoder for Sign Language\n  Retrieval","summary":"  Different from traditional video retrieval, sign language retrieval is more\nbiased towards understanding the semantic information of human actions\ncontained in video clips. Previous works typically only encode RGB videos to\nobtain high-level semantic features, resulting in local action details drowned\nin a large amount of visual information redundancy. Furthermore, existing\nRGB-based sign retrieval works suffer from the huge memory cost of dense visual\ndata embedding in end-to-end training, and adopt offline RGB encoder instead,\nleading to suboptimal feature representation. To address these issues, we\npropose a novel sign language representation framework called Semantically\nEnhanced Dual-Stream Encoder (SEDS), which integrates Pose and RGB modalities\nto represent the local and global information of sign language videos.\nSpecifically, the Pose encoder embeds the coordinates of keypoints\ncorresponding to human joints, effectively capturing detailed action features.\nFor better context-aware fusion of two video modalities, we propose a Cross\nGloss Attention Fusion (CGAF) module to aggregate the adjacent clip features\nwith similar semantic information from intra-modality and inter-modality.\nMoreover, a Pose-RGB Fine-grained Matching Objective is developed to enhance\nthe aggregated fusion feature by contextual matching of fine-grained\ndual-stream features. Besides the offline RGB encoder, the whole framework only\ncontains learnable lightweight networks, which can be trained end-to-end.\nExtensive experiments demonstrate that our framework significantly outperforms\nstate-of-the-art methods on various datasets.\n","authors":["Longtao Jiang","Min Wang","Zecheng Li","Yao Fang","Wengang Zhou","Houqiang Li"],"pdf_url":"https://arxiv.org/pdf/2407.16394v1.pdf","comment":"Accepted to ACM International Conference on Multimedia (MM) 2024"},{"id":"http://arxiv.org/abs/2407.03542v2","updated":"2024-07-23T11:16:22Z","published":"2024-07-03T23:27:53Z","title":"Probing Perfection: The Relentless Art of Meddling for Pulmonary Airway\n  Segmentation from HRCT via a Human-AI Collaboration Based Active Learning\n  Method","summary":"  In pulmonary tracheal segmentation, the scarcity of annotated data is a\nprevalent issue in medical segmentation. Additionally, Deep Learning (DL)\nmethods face challenges: the opacity of 'black box' models and the need for\nperformance enhancement. Our Human-Computer Interaction (HCI) based models\n(RS_UNet, LC_UNet, UUNet, and WD_UNet) address these challenges by combining\ndiverse query strategies with various DL models. We train four HCI models and\nrepeat these steps: (1) Query Strategy: The HCI models select samples that\nprovide the most additional representative information when labeled in each\niteration and identify unlabeled samples with the greatest predictive disparity\nusing Wasserstein Distance, Least Confidence, Entropy Sampling, and Random\nSampling. (2) Central line correction: Selected samples are used for expert\ncorrection of system-generated tracheal central lines in each training round.\n(3) Update training dataset: Experts update the training dataset after each DL\nmodel's training epoch, enhancing the trustworthiness and performance of the\nmodels. (4) Model training: The HCI model is trained using the updated dataset\nand an enhanced UNet version. Experimental results confirm the effectiveness of\nthese HCI-based approaches, showing that WD-UNet, LC-UNet, UUNet, and RS-UNet\nachieve comparable or superior performance to state-of-the-art DL models.\nNotably, WD-UNet achieves this with only 15%-35% of the training data, reducing\nphysician annotation time by 65%-85%.\n","authors":["Shiyi Wang","Yang Nan","Sheng Zhang","Federico Felder","Xiaodan Xing","Yingying Fang","Javier Del Ser","Simon L F Walsh","Guang Yang"],"pdf_url":"https://arxiv.org/pdf/2407.03542v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16384v1","updated":"2024-07-23T11:14:54Z","published":"2024-07-23T11:14:54Z","title":"A Multitask Deep Learning Model for Classification and Regression of\n  Hyperspectral Images: Application to the large-scale dataset","summary":"  Multitask learning is a widely recognized technique in the field of computer\nvision and deep learning domain. However, it is still a research question in\nremote sensing, particularly for hyperspectral imaging. Moreover, most of the\nresearch in the remote sensing domain focuses on small and single-task-based\nannotated datasets, which limits the generalizability and scalability of the\ndeveloped models to more diverse and complex real-world scenarios. Thus, in\nthis study, we propose a multitask deep learning model designed to perform\nmultiple classification and regression tasks simultaneously on hyperspectral\nimages. We validated our approach on a large hyperspectral dataset called\nTAIGA, which contains 13 forest variables, including three categorical\nvariables and ten continuous variables with different biophysical parameters.\nWe design a sharing encoder and task-specific decoder network to streamline\nfeature learning while allowing each task-specific decoder to focus on the\nunique aspects of its respective task.\n  Additionally, a dense atrous pyramid pooling layer and attention network were\nintegrated to extract multi-scale contextual information and enable selective\ninformation processing by prioritizing task-specific features. Further, we\ncomputed multitask loss and optimized its parameters for the proposed framework\nto improve the model performance and efficiency across diverse tasks. A\ncomprehensive qualitative and quantitative analysis of the results shows that\nthe proposed method significantly outperforms other state-of-the-art methods.\nWe trained our model across 10 seeds/trials to ensure robustness. Our proposed\nmodel demonstrates higher mean performance while maintaining lower or\nequivalent variability. To make the work reproducible, the codes will be\navailable at\nhttps://github.com/Koushikey4596/Multitask-Deep-Learning-Model-for-Taiga-datatset.\n","authors":["Koushikey Chhapariya","Alexandre Benoit","Krishna Mohan Buddhiraju","Anil Kumar"],"pdf_url":"https://arxiv.org/pdf/2407.16384v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.00636v3","updated":"2024-07-23T10:47:22Z","published":"2024-03-31T10:13:55Z","title":"Learning to Generate Conditional Tri-plane for 3D-aware Expression\n  Controllable Portrait Animation","summary":"  In this paper, we present Export3D, a one-shot 3D-aware portrait animation\nmethod that is able to control the facial expression and camera view of a given\nportrait image. To achieve this, we introduce a tri-plane generator with an\neffective expression conditioning method, which directly generates a tri-plane\nof 3D prior by transferring the expression parameter of 3DMM into the source\nimage. The tri-plane is then decoded into the image of different view through a\ndifferentiable volume rendering. Existing portrait animation methods heavily\nrely on image warping to transfer the expression in the motion space,\nchallenging on disentanglement of appearance and expression. In contrast, we\npropose a contrastive pre-training framework for appearance-free expression\nparameter, eliminating undesirable appearance swap when transferring a\ncross-identity expression. Extensive experiments show that our pre-training\nframework can learn the appearance-free expression representation hidden in\n3DMM, and our model can generate 3D-aware expression controllable portrait\nimages without appearance swap in the cross-identity manner.\n","authors":["Taekyung Ki","Dongchan Min","Gyeongsu Chae"],"pdf_url":"https://arxiv.org/pdf/2404.00636v3.pdf","comment":"ECCV 2024. Project page: https://export3d.github.io"},{"id":"http://arxiv.org/abs/2403.18512v2","updated":"2024-07-23T10:41:22Z","published":"2024-03-27T12:41:30Z","title":"ParCo: Part-Coordinating Text-to-Motion Synthesis","summary":"  We study a challenging task: text-to-motion synthesis, aiming to generate\nmotions that align with textual descriptions and exhibit coordinated movements.\nCurrently, the part-based methods introduce part partition into the motion\nsynthesis process to achieve finer-grained generation. However, these methods\nencounter challenges such as the lack of coordination between different part\nmotions and difficulties for networks to understand part concepts. Moreover,\nintroducing finer-grained part concepts poses computational complexity\nchallenges. In this paper, we propose Part-Coordinating Text-to-Motion\nSynthesis (ParCo), endowed with enhanced capabilities for understanding part\nmotions and communication among different part motion generators, ensuring a\ncoordinated and fined-grained motion synthesis. Specifically, we discretize\nwhole-body motion into multiple part motions to establish the prior concept of\ndifferent parts. Afterward, we employ multiple lightweight generators designed\nto synthesize different part motions and coordinate them through our part\ncoordination module. Our approach demonstrates superior performance on common\nbenchmarks with economic computations, including HumanML3D and KIT-ML,\nproviding substantial evidence of its effectiveness. Code is available at\nhttps://github.com/qrzou/ParCo .\n","authors":["Qiran Zou","Shangyuan Yuan","Shian Du","Yu Wang","Chang Liu","Yi Xu","Jie Chen","Xiangyang Ji"],"pdf_url":"https://arxiv.org/pdf/2403.18512v2.pdf","comment":"Accepted by ECCV 2024. Code: https://github.com/qrzou/ParCo"},{"id":"http://arxiv.org/abs/2407.16369v1","updated":"2024-07-23T10:34:02Z","published":"2024-07-23T10:34:02Z","title":"FCNR: Fast Compressive Neural Representation of Visualization Images","summary":"  We present FCNR, a fast compressive neural representation for tens of\nthousands of visualization images under varying viewpoints and timesteps. The\nexisting NeRVI solution, albeit enjoying a high compression ratio, incurs slow\nspeeds in encoding and decoding. Built on the recent advances in stereo image\ncompression, FCNR assimilates stereo context modules and joint context transfer\nmodules to compress image pairs. Our solution significantly improves encoding\nand decoding speed while maintaining high reconstruction quality and satisfying\ncompression ratio. To demonstrate its effectiveness, we compare FCNR with\nstate-of-the-art neural compression methods, including E-NeRV, HNeRV, NeRVI,\nand ECSIC. The source code can be found at\nhttps://github.com/YunfeiLu0112/FCNR.\n","authors":["Yunfei Lu","Pengfei Gu","Chaoli Wang"],"pdf_url":"https://arxiv.org/pdf/2407.16369v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16367v1","updated":"2024-07-23T10:21:18Z","published":"2024-07-23T10:21:18Z","title":"Navigating Uncertainty in Medical Image Segmentation","summary":"  We address the selection and evaluation of uncertain segmentation methods in\nmedical imaging and present two case studies: prostate segmentation,\nillustrating that for minimal annotator variation simple deterministic models\ncan suffice, and lung lesion segmentation, highlighting the limitations of the\nGeneralized Energy Distance (GED) in model selection. Our findings lead to\nguidelines for accurately choosing and developing uncertain segmentation\nmodels, that integrate aleatoric and epistemic components. These guidelines are\ndesigned to aid researchers and practitioners in better developing, selecting,\nand evaluating uncertain segmentation methods, thereby facilitating enhanced\nadoption and effective application of segmentation uncertainty in practice.\n","authors":["Kilian Zepf","Jes Frellsen","Aasa Feragen"],"pdf_url":"https://arxiv.org/pdf/2407.16367v1.pdf","comment":"Published in the conference proceedings of the 21st IEEE\n  International Symposium on Biomedical Imaging (ISBI 2024)"},{"id":"http://arxiv.org/abs/2401.10711v4","updated":"2024-07-23T10:17:39Z","published":"2024-01-19T14:21:46Z","title":"Weakly Supervised Gaussian Contrastive Grounding with Large Multimodal\n  Models for Video Question Answering","summary":"  Video Question Answering (VideoQA) aims to answer natural language questions\nbased on the information observed in videos. Despite the recent success of\nLarge Multimodal Models (LMMs) in image-language understanding and reasoning,\nthey deal with VideoQA insufficiently, by simply taking uniformly sampled\nframes as visual inputs, which ignores question-relevant visual clues.\nMoreover, there are no human annotations for question-critical timestamps in\nexisting VideoQA datasets. In light of this, we propose a novel weakly\nsupervised framework to enforce the LMMs to reason out the answers with\nquestion-critical moments as visual inputs. Specifically, we first fuse the\nquestion and answer pairs as event descriptions to find multiple keyframes as\ntarget moments and pseudo-labels, with the visual-language alignment capability\nof the CLIP models. With these pseudo-labeled keyframes as additionally weak\nsupervision, we devise a lightweight Gaussian-based Contrastive Grounding (GCG)\nmodule. GCG learns multiple Gaussian functions to characterize the temporal\nstructure of the video, and sample question-critical frames as positive moments\nto be the visual inputs of LMMs. Extensive experiments on several benchmarks\nverify the effectiveness of our framework, and we achieve substantial\nimprovements compared to previous state-of-the-art methods.\n","authors":["Haibo Wang","Chenghang Lai","Yixuan Sun","Weifeng Ge"],"pdf_url":"https://arxiv.org/pdf/2401.10711v4.pdf","comment":"accepted by ACM Multimedia 2024"},{"id":"http://arxiv.org/abs/2407.16364v1","updated":"2024-07-23T10:11:56Z","published":"2024-07-23T10:11:56Z","title":"Harmonizing Visual Text Comprehension and Generation","summary":"  In this work, we present TextHarmony, a unified and versatile multimodal\ngenerative model proficient in comprehending and generating visual text.\nSimultaneously generating images and texts typically results in performance\ndegradation due to the inherent inconsistency between vision and language\nmodalities. To overcome this challenge, existing approaches resort to\nmodality-specific data for supervised fine-tuning, necessitating distinct model\ninstances. We propose Slide-LoRA, which dynamically aggregates\nmodality-specific and modality-agnostic LoRA experts, partially decoupling the\nmultimodal generation space. Slide-LoRA harmonizes the generation of vision and\nlanguage within a singular model instance, thereby facilitating a more unified\ngenerative process. Additionally, we develop a high-quality image caption\ndataset, DetailedTextCaps-100K, synthesized with a sophisticated closed-source\nMLLM to enhance visual text generation capabilities further. Comprehensive\nexperiments across various benchmarks demonstrate the effectiveness of the\nproposed approach. Empowered by Slide-LoRA, TextHarmony achieves comparable\nperformance to modality-specific fine-tuning results with only a 2% increase in\nparameters and shows an average improvement of 2.5% in visual text\ncomprehension tasks and 4.0% in visual text generation tasks. Our work\ndelineates the viability of an integrated approach to multimodal generation\nwithin the visual text domain, setting a foundation for subsequent inquiries.\n","authors":["Zhen Zhao","Jingqun Tang","Binghong Wu","Chunhui Lin","Shu Wei","Hao Liu","Xin Tan","Zhizhong Zhang","Can Huang","Yuan Xie"],"pdf_url":"https://arxiv.org/pdf/2407.16364v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.02157v2","updated":"2024-07-23T10:08:52Z","published":"2024-07-02T10:55:43Z","title":"FineCLIPER: Multi-modal Fine-grained CLIP for Dynamic Facial Expression\n  Recognition with AdaptERs","summary":"  Dynamic Facial Expression Recognition (DFER) is crucial for understanding\nhuman behavior. However, current methods exhibit limited performance mainly due\nto the scarcity of high-quality data, the insufficient utilization of facial\ndynamics, and the ambiguity of expression semantics, etc. To this end, we\npropose a novel framework, named Multi-modal Fine-grained CLIP for Dynamic\nFacial Expression Recognition with AdaptERs (FineCLIPER), incorporating the\nfollowing novel designs: 1) To better distinguish between similar facial\nexpressions, we extend the class labels to textual descriptions from both\npositive and negative aspects, and obtain supervision by calculating the\ncross-modal similarity based on the CLIP model; 2) Our FineCLIPER adopts a\nhierarchical manner to effectively mine useful cues from DFE videos.\nSpecifically, besides directly embedding video frames as input (low semantic\nlevel), we propose to extract the face segmentation masks and landmarks based\non each frame (middle semantic level) and utilize the Multi-modal Large\nLanguage Model (MLLM) to further generate detailed descriptions of facial\nchanges across frames with designed prompts (high semantic level).\nAdditionally, we also adopt Parameter-Efficient Fine-Tuning (PEFT) to enable\nefficient adaptation of large pre-trained models (i.e., CLIP) for this task.\nOur FineCLIPER achieves SOTA performance on the DFEW, FERV39k, and MAFW\ndatasets in both supervised and zero-shot settings with few tunable parameters.\nProject Page: https://haroldchen19.github.io/FineCLIPER-Page/\n","authors":["Haodong Chen","Haojian Huang","Junhao Dong","Mingzhe Zheng","Dian Shao"],"pdf_url":"https://arxiv.org/pdf/2407.02157v2.pdf","comment":"Accepted to ACM MM 2024"},{"id":"http://arxiv.org/abs/2403.11909v2","updated":"2024-07-23T10:02:52Z","published":"2024-03-18T16:11:42Z","title":"RoGUENeRF: A Robust Geometry-Consistent Universal Enhancer for NeRF","summary":"  Recent advances in neural rendering have enabled highly photorealistic 3D\nscene reconstruction and novel view synthesis. Despite this progress, current\nstate-of-the-art methods struggle to reconstruct high frequency detail, due to\nfactors such as a low-frequency bias of radiance fields and inaccurate camera\ncalibration. One approach to mitigate this issue is to enhance images\npost-rendering. 2D enhancers can be pre-trained to recover some detail but are\nagnostic to scene geometry and do not easily generalize to new distributions of\nimage degradation. Conversely, existing 3D enhancers are able to transfer\ndetail from nearby training images in a generalizable manner, but suffer from\ninaccurate camera calibration and can propagate errors from the geometry into\nrendered images. We propose a neural rendering enhancer, RoGUENeRF, which\nexploits the best of both paradigms. Our method is pre-trained to learn a\ngeneral enhancer while also leveraging information from nearby training images\nvia robust 3D alignment and geometry-aware fusion. Our approach restores\nhigh-frequency textures while maintaining geometric consistency and is also\nrobust to inaccurate camera calibration. We show that RoGUENeRF substantially\nenhances the rendering quality of a wide range of neural rendering baselines,\ne.g. improving the PSNR of MipNeRF360 by 0.63dB and Nerfacto by 1.34dB on the\nreal world 360v2 dataset.\n","authors":["Sibi Catley-Chandar","Richard Shaw","Gregory Slabaugh","Eduardo Perez-Pellitero"],"pdf_url":"https://arxiv.org/pdf/2403.11909v2.pdf","comment":"Accepted to ECCV 2024. Project Page:\n  https://sib1.github.io/projects/roguenerf/"},{"id":"http://arxiv.org/abs/2407.16354v1","updated":"2024-07-23T09:58:20Z","published":"2024-07-23T09:58:20Z","title":"Strike a Balance in Continual Panoptic Segmentation","summary":"  This study explores the emerging area of continual panoptic segmentation,\nhighlighting three key balances. First, we introduce past-class backtrace\ndistillation to balance the stability of existing knowledge with the\nadaptability to new information. This technique retraces the features\nassociated with past classes based on the final label assignment results,\nperforming knowledge distillation targeting these specific features from the\nprevious model while allowing other features to flexibly adapt to new\ninformation. Additionally, we introduce a class-proportional memory strategy,\nwhich aligns the class distribution in the replay sample set with that of the\nhistorical training data. This strategy maintains a balanced class\nrepresentation during replay, enhancing the utility of the limited-capacity\nreplay sample set in recalling prior classes. Moreover, recognizing that replay\nsamples are annotated only for the classes of their original step, we devise\nbalanced anti-misguidance losses, which combat the impact of incomplete\nannotations without incurring classification bias. Building upon these\ninnovations, we present a new method named Balanced Continual Panoptic\nSegmentation (BalConpas). Our evaluation on the challenging ADE20K dataset\ndemonstrates its superior performance compared to existing state-of-the-art\nmethods. The official code is available at\nhttps://github.com/jinpeng0528/BalConpas.\n","authors":["Jinpeng Chen","Runmin Cong","Yuxuan Luo","Horace Ho Shing Ip","Sam Kwong"],"pdf_url":"https://arxiv.org/pdf/2407.16354v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16344v1","updated":"2024-07-23T09:45:25Z","published":"2024-07-23T09:45:25Z","title":"SOAP: Enhancing Spatio-Temporal Relation and Motion Information\n  Capturing for Few-Shot Action Recognition","summary":"  High frame-rate (HFR) videos of action recognition improve fine-grained\nexpression while reducing the spatio-temporal relation and motion information\ndensity. Thus, large amounts of video samples are continuously required for\ntraditional data-driven training. However, samples are not always sufficient in\nreal-world scenarios, promoting few-shot action recognition (FSAR) research. We\nobserve that most recent FSAR works build spatio-temporal relation of video\nsamples via temporal alignment after spatial feature extraction, cutting apart\nspatial and temporal features within samples. They also capture motion\ninformation via narrow perspectives between adjacent frames without considering\ndensity, leading to insufficient motion information capturing. Therefore, we\npropose a novel plug-and-play architecture for FSAR called Spatio-tempOral\nfrAme tuPle enhancer (SOAP) in this paper. The model we designed with such\narchitecture refers to SOAP-Net. Temporal connections between different feature\nchannels and spatio-temporal relation of features are considered instead of\nsimple feature extraction. Comprehensive motion information is also captured,\nusing frame tuples with multiple frames containing more motion information than\nadjacent frames. Combining frame tuples of diverse frame counts further\nprovides a broader perspective. SOAP-Net achieves new state-of-the-art\nperformance across well-known benchmarks such as SthSthV2, Kinetics, UCF101,\nand HMDB51. Extensive empirical evaluations underscore the competitiveness,\npluggability, generalization, and robustness of SOAP. The code is released at\nhttps://github.com/wenbohuang1002/SOAP.\n","authors":["Wenbo Huang","Jinghui Zhang","Xuwei Qian","Zhen Wu","Meng Wang","Lei Zhang"],"pdf_url":"https://arxiv.org/pdf/2407.16344v1.pdf","comment":"Accepted by ACM MM 2024"},{"id":"http://arxiv.org/abs/2407.16341v1","updated":"2024-07-23T09:41:10Z","published":"2024-07-23T09:41:10Z","title":"Motion Capture from Inertial and Vision Sensors","summary":"  Human motion capture is the foundation for many computer vision and graphics\ntasks. While industrial motion capture systems with complex camera arrays or\nexpensive wearable sensors have been widely adopted in movie and game\nproduction, consumer-affordable and easy-to-use solutions for personal\napplications are still far from mature. To utilize a mixture of a monocular\ncamera and very few inertial measurement units (IMUs) for accurate multi-modal\nhuman motion capture in daily life, we contribute MINIONS in this paper, a\nlarge-scale Motion capture dataset collected from INertial and visION Sensors.\nMINIONS has several featured properties: 1) large scale of over five million\nframes and 400 minutes duration; 2) multi-modality data of IMUs signals and RGB\nvideos labeled with joint positions, joint rotations, SMPL parameters, etc.; 3)\na diverse set of 146 fine-grained single and interactive actions with textual\ndescriptions. With the proposed MINIONS, we conduct experiments on multi-modal\nmotion capture and explore the possibilities of consumer-affordable motion\ncapture using a monocular camera and very few IMUs. The experiment results\nemphasize the unique advantages of inertial and vision sensors, showcasing the\npromise of consumer-affordable multi-modal motion capture and providing a\nvaluable resource for further research and development.\n","authors":["Xiaodong Chen","Wu Liu","Qian Bao","Xinchen Liu","Quanwei Yang","Ruoli Dai","Tao Mei"],"pdf_url":"https://arxiv.org/pdf/2407.16341v1.pdf","comment":"17 pages,9 figures"},{"id":"http://arxiv.org/abs/2402.12923v2","updated":"2024-07-23T09:34:45Z","published":"2024-02-20T11:18:40Z","title":"Advancements in Point Cloud-Based 3D Defect Detection and Classification\n  for Industrial Systems: A Comprehensive Survey","summary":"  In recent years, 3D point clouds (PCs) have gained significant attention due\nto their diverse applications across various fields, such as computer vision\n(CV), condition monitoring (CM), virtual reality, robotics, autonomous driving,\netc. Deep learning (DL) has proven effective in leveraging 3D PCs to address\nvarious challenges encountered in 2D vision. However, applying deep neural\nnetworks (DNNs) to process 3D PCs presents unique challenges. This paper\nprovides an in-depth review of recent advancements in DL-based industrial CM\nusing 3D PCs, with a specific focus on defect shape classification and\nsegmentation within industrial applications. Recognizing the crucial role of\nthese aspects in industrial maintenance, the paper offers insightful\nobservations on the strengths and limitations of the reviewed DL-based PC\nprocessing methods. This knowledge synthesis aims to contribute to\nunderstanding and enhancing CM processes, particularly within the framework of\nremaining useful life (RUL), in industrial systems.\n","authors":["Anju Rani","Daniel Ortiz-Arroyo","Petar Durdevic"],"pdf_url":"https://arxiv.org/pdf/2402.12923v2.pdf","comment":"27 pages, 13 figures, 3 tables, review paper"},{"id":"http://arxiv.org/abs/2407.15472v2","updated":"2024-07-23T09:27:10Z","published":"2024-07-22T08:35:41Z","title":"Learning deep illumination-robust features from multispectral filter\n  array images","summary":"  Multispectral (MS) snapshot cameras equipped with a MS filter array (MSFA),\ncapture multiple spectral bands in a single shot, resulting in a raw mosaic\nimage where each pixel holds only one channel value. The fully-defined MS image\nis estimated from the raw one through $\\textit{demosaicing}$, which inevitably\nintroduces spatio-spectral artifacts. Moreover, training on fully-defined MS\nimages can be computationally intensive, particularly with deep neural networks\n(DNNs), and may result in features lacking discrimination power due to\nsuboptimal learning of spatio-spectral interactions. Furthermore, outdoor MS\nimage acquisition occurs under varying lighting conditions, leading to\nillumination-dependent features. This paper presents an original approach to\nlearn discriminant and illumination-robust features directly from raw images.\nIt involves: $\\textit{raw spectral constancy}$ to mitigate the impact of\nillumination, $\\textit{MSFA-preserving}$ transformations suited for raw image\naugmentation to train DNNs on diverse raw textures, and $\\textit{raw-mixing}$\nto capture discriminant spatio-spectral interactions in raw images. Experiments\non MS image classification show that our approach outperforms both handcrafted\nand recent deep learning-based methods, while also requiring significantly less\ncomputational effort.\n","authors":["Anis Amziane"],"pdf_url":"https://arxiv.org/pdf/2407.15472v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16328v1","updated":"2024-07-23T09:23:00Z","published":"2024-07-23T09:23:00Z","title":"Improving multidimensional projection quality with user-specific metrics\n  and optimal scaling","summary":"  The growing prevalence of high-dimensional data has fostered the development\nof multidimensional projection (MP) techniques, such as t-SNE, UMAP, and LAMP,\nfor data visualization and exploration. However, conventional MP methods\ntypically employ generic quality metrics, neglecting individual user\npreferences. This study proposes a new framework that tailors MP techniques\nbased on user-specific quality criteria, enhancing projection interpretability.\n  Our approach combines three visual quality metrics, stress, neighborhood\npreservation, and silhouette score, to create a composite metric for a precise\nMP evaluation. We then optimize the projection scale by maximizing the\ncomposite metric value. We conducted an experiment involving two users with\ndifferent projection preferences, generating projections using t-SNE, UMAP, and\nLAMP. Users rate projections according to their criteria, producing two\ntraining sets. We derive optimal weights for each set and apply them to other\ndatasets to determine the best projections per user.\n  Our findings demonstrate that personalized projections effectively capture\nuser preferences, fostering better data exploration and enabling more informed\ndecision-making. This user-centric approach promotes advancements in\nmultidimensional projection techniques that accommodate diverse user\npreferences and enhance interpretability.\n","authors":["Maniru Ibrahim"],"pdf_url":"https://arxiv.org/pdf/2407.16328v1.pdf","comment":"10 Pages, 4 figures"},{"id":"http://arxiv.org/abs/2407.16327v1","updated":"2024-07-23T09:22:06Z","published":"2024-07-23T09:22:06Z","title":"Understanding Impacts of Electromagnetic Signal Injection Attacks on\n  Object Detection","summary":"  Object detection can localize and identify objects in images, and it is\nextensively employed in critical multimedia applications such as security\nsurveillance and autonomous driving. Despite the success of existing object\ndetection models, they are often evaluated in ideal scenarios where captured\nimages guarantee the accurate and complete representation of the detecting\nscenes. However, images captured by image sensors may be affected by different\nfactors in real applications, including cyber-physical attacks. In particular,\nattackers can exploit hardware properties within the systems to inject\nelectromagnetic interference so as to manipulate the images. Such attacks can\ncause noisy or incomplete information about the captured scene, leading to\nincorrect detection results, potentially granting attackers malicious control\nover critical functions of the systems. This paper presents a research work\nthat comprehensively quantifies and analyzes the impacts of such attacks on\nstate-of-the-art object detection models in practice. It also sheds light on\nthe underlying reasons for the incorrect detection outcomes.\n","authors":["Youqian Zhang","Chunxi Yang","Eugene Y. Fu","Qinhong Jiang","Chen Yan","Sze-Yiu Chau","Grace Ngai","Hong-Va Leong","Xiapu Luo","Wenyuan Xu"],"pdf_url":"https://arxiv.org/pdf/2407.16327v1.pdf","comment":"2024 IEEE International Conference on Multimedia and Expo (ICME),\n  July 15 - July 19, 2024, Niagra Falls, Ontario, Canada"},{"id":"http://arxiv.org/abs/2311.17955v3","updated":"2024-07-23T09:09:33Z","published":"2023-11-29T08:11:20Z","title":"PEAN: A Diffusion-Based Prior-Enhanced Attention Network for Scene Text\n  Image Super-Resolution","summary":"  Scene text image super-resolution (STISR) aims at simultaneously increasing\nthe resolution and readability of low-resolution scene text images, thus\nboosting the performance of the downstream recognition task. Two factors in\nscene text images, visual structure and semantic information, affect the\nrecognition performance significantly. To mitigate the effects from these\nfactors, this paper proposes a Prior-Enhanced Attention Network (PEAN).\nSpecifically, an attention-based modulation module is leveraged to understand\nscene text images by neatly perceiving the local and global dependence of\nimages, despite the shape of the text. Meanwhile, a diffusion-based module is\ndeveloped to enhance the text prior, hence offering better guidance for the SR\nnetwork to generate SR images with higher semantic accuracy. Additionally, a\nmulti-task learning paradigm is employed to optimize the network, enabling the\nmodel to generate legible SR images. As a result, PEAN establishes new SOTA\nresults on the TextZoom benchmark. Experiments are also conducted to analyze\nthe importance of the enhanced text prior as a means of improving the\nperformance of the SR network. Code is available at\nhttps://github.com/jdfxzzy/PEAN.\n","authors":["Zuoyan Zhao","Hui Xue","Pengfei Fang","Shipeng Zhu"],"pdf_url":"https://arxiv.org/pdf/2311.17955v3.pdf","comment":"Accepted by ACMMM 2024"},{"id":"http://arxiv.org/abs/2407.06617v4","updated":"2024-07-23T09:08:47Z","published":"2024-07-09T07:47:16Z","title":"Mobius: A High Efficient Spatial-Temporal Parallel Training Paradigm for\n  Text-to-Video Generation Task","summary":"  Inspired by the success of the text-to-image (T2I) generation task, many\nresearchers are devoting themselves to the text-to-video (T2V) generation task.\nMost of the T2V frameworks usually inherit from the T2I model and add\nextra-temporal layers of training to generate dynamic videos, which can be\nviewed as a fine-tuning task. However, the traditional 3D-Unet is a serial mode\nand the temporal layers follow the spatial layers, which will result in high\nGPU memory and training time consumption according to its serial feature flow.\nWe believe that this serial mode will bring more training costs with the large\ndiffusion model and massive datasets, which are not environmentally friendly\nand not suitable for the development of the T2V. Therefore, we propose a highly\nefficient spatial-temporal parallel training paradigm for T2V tasks, named\nMobius. In our 3D-Unet, the temporal layers and spatial layers are parallel,\nwhich optimizes the feature flow and backpropagation. The Mobius will save 24%\nGPU memory and 12% training time, which can greatly improve the T2V fine-tuning\ntask and provide a novel insight for the AIGC community. We will release our\ncodes in the future.\n","authors":["Yiran Yang","Jinchao Zhang","Ying Deng","Jie Zhou"],"pdf_url":"https://arxiv.org/pdf/2407.06617v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16313v1","updated":"2024-07-23T09:05:23Z","published":"2024-07-23T09:05:23Z","title":"Deep Learning for Pancreas Segmentation: a Systematic Review","summary":"  Pancreas segmentation has been traditionally challenging due to its small\nsize in computed tomography abdominal volumes, high variability of shape and\npositions among patients, and blurred boundaries due to low contrast between\nthe pancreas and surrounding organs. Many deep learning models for pancreas\nsegmentation have been proposed in the past few years. We present a thorough\nsystematic review based on the Preferred Reporting Items for Systematic Reviews\nand Meta-analyses (PRISMA) statement. The literature search was conducted on\nPubMed, Web of Science, Scopus, and IEEE Xplore on original studies published\nin peer-reviewed journals from 2013 to 2023. Overall, 130 studies were\nretrieved. We initially provided an overview of the technical background of the\nmost common network architectures and publicly available datasets. Then, the\nanalysis of the studies combining visual presentation in tabular form and text\ndescription was reported. The tables grouped the studies specifying the\napplication, dataset size, design (model architecture, learning strategy, and\nloss function), results, and main contributions. We first analyzed the studies\nfocusing on parenchyma segmentation using coarse-to-fine approaches,\nmulti-organ segmentation, semi-supervised learning, and unsupervised learning,\nfollowed by those studies on generalization to other datasets and those\nconcerning the design of new loss functions. Then, we analyzed the studies on\nsegmentation of tumors, cysts, and inflammation reporting multi-stage methods,\nsemi-supervised learning, generalization to other datasets, and design of new\nloss functions. Finally, we provided a critical discussion on the subject based\non the published evidence underlining current issues that need to be addressed\nbefore clinical translation.\n","authors":["Andrea Moglia","Matteo Cavicchioli","Luca Mainardi","Pietro Cerveri"],"pdf_url":"https://arxiv.org/pdf/2407.16313v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16309v1","updated":"2024-07-23T09:02:46Z","published":"2024-07-23T09:02:46Z","title":"A new visual quality metric for Evaluating the performance of\n  multidimensional projections","summary":"  Multidimensional projections (MP) are among the most essential approaches in\nthe visual analysis of multidimensional data. It transforms multidimensional\ndata into two-dimensional representations that may be shown as scatter plots\nwhile preserving their similarity with the original data. Human visual\nperception is frequently used to evaluate the quality of MP. In this work, we\npropose to study and improve on a well-known map called Local Affine\nMultidimensional Projection (LAMP), which takes a multidimensional instance and\nembeds it in Cartesian space via moving least squares deformation. We propose a\nnew visual quality metric based on human perception. The new metric combines\nthree previously used metrics: silhouette coefficient, neighborhood\npreservation, and silhouette ratio. We show that the proposed metric produces\nmore precise results in analyzing the quality of MP than other previously used\nmetrics. Finally, we describe an algorithm that attempts to overcome a\nlimitation of the LAMP method which requires a similar scale for control points\nand their counterparts in the Cartesian space.\n","authors":["Maniru Ibrahim","Thales Vieira"],"pdf_url":"https://arxiv.org/pdf/2407.16309v1.pdf","comment":"19 pages, 10 figures"},{"id":"http://arxiv.org/abs/2407.16308v1","updated":"2024-07-23T09:02:35Z","published":"2024-07-23T09:02:35Z","title":"SAFNet: Selective Alignment Fusion Network for Efficient HDR Imaging","summary":"  Multi-exposure High Dynamic Range (HDR) imaging is a challenging task when\nfacing truncated texture and complex motion. Existing deep learning-based\nmethods have achieved great success by either following the alignment and\nfusion pipeline or utilizing attention mechanism. However, the large\ncomputation cost and inference delay hinder them from deploying on resource\nlimited devices. In this paper, to achieve better efficiency, a novel Selective\nAlignment Fusion Network (SAFNet) for HDR imaging is proposed. After extracting\npyramid features, it jointly refines valuable area masks and cross-exposure\nmotion in selected regions with shared decoders, and then fuses high quality\nHDR image in an explicit way. This approach can focus the model on finding\nvaluable regions while estimating their easily detectable and meaningful\nmotion. For further detail enhancement, a lightweight refine module is\nintroduced which enjoys privileges from previous optical flow, selection masks\nand initial prediction. Moreover, to facilitate learning on samples with large\nmotion, a new window partition cropping method is presented during training.\nExperiments on public and newly developed challenging datasets show that\nproposed SAFNet not only exceeds previous SOTA competitors quantitatively and\nqualitatively, but also runs order of magnitude faster. Code and dataset is\navailable at https://github.com/ltkong218/SAFNet.\n","authors":["Lingtong Kong","Bo Li","Yike Xiong","Hao Zhang","Hong Gu","Jinwei Chen"],"pdf_url":"https://arxiv.org/pdf/2407.16308v1.pdf","comment":"Accepted by ECCV 2024"},{"id":"http://arxiv.org/abs/2407.16302v1","updated":"2024-07-23T08:57:11Z","published":"2024-07-23T08:57:11Z","title":"DeepClean: Integrated Distortion Identification and Algorithm Selection\n  for Rectifying Image Corruptions","summary":"  Distortion identification and rectification in images and videos is vital for\nachieving good performance in downstream vision applications. Instead of\nrelying on fixed trial-and-error based image processing pipelines, we propose a\ntwo-level sequential planning approach for automated image distortion\nclassification and rectification. At the higher level it detects the class of\ncorruptions present in the input image, if any. The lower level selects a\nspecific algorithm to be applied, from a set of externally provided candidate\nalgorithms. The entire two-level setup runs in the form of a single forward\npass during inference and it is to be queried iteratively until the retrieval\nof the original image. We demonstrate improvements compared to three baselines\non the object detection task on COCO image dataset with rich set of\ndistortions. The advantage of our approach is its dynamic reconfiguration,\nconditioned on the input image and generalisability to unseen candidate\nalgorithms at inference time, since it relies only on the comparison of their\noutput of the image embeddings.\n","authors":["Aditya Kapoor","Harshad Khadilkar","Jayvardhana Gubbi"],"pdf_url":"https://arxiv.org/pdf/2407.16302v1.pdf","comment":"7 pages, 3 figures"},{"id":"http://arxiv.org/abs/2407.16298v1","updated":"2024-07-23T08:54:55Z","published":"2024-07-23T08:54:55Z","title":"EffiSegNet: Gastrointestinal Polyp Segmentation through a Pre-Trained\n  EfficientNet-based Network with a Simplified Decoder","summary":"  This work introduces EffiSegNet, a novel segmentation framework leveraging\ntransfer learning with a pre-trained Convolutional Neural Network (CNN)\nclassifier as its backbone. Deviating from traditional architectures with a\nsymmetric U-shape, EffiSegNet simplifies the decoder and utilizes full-scale\nfeature fusion to minimize computational cost and the number of parameters. We\nevaluated our model on the gastrointestinal polyp segmentation task using the\npublicly available Kvasir-SEG dataset, achieving state-of-the-art results.\nSpecifically, the EffiSegNet-B4 network variant achieved an F1 score of 0.9552,\nmean Dice (mDice) 0.9483, mean Intersection over Union (mIoU) 0.9056, Precision\n0.9679, and Recall 0.9429 with a pre-trained backbone - to the best of our\nknowledge, the highest reported scores in the literature for this dataset.\nAdditional training from scratch also demonstrated exceptional performance\ncompared to previous work, achieving an F1 score of 0.9286, mDice 0.9207, mIoU\n0.8668, Precision 0.9311 and Recall 0.9262. These results underscore the\nimportance of a well-designed encoder in image segmentation networks and the\neffectiveness of transfer learning approaches.\n","authors":["Ioannis A. Vezakis","Konstantinos Georgas","Dimitrios Fotiadis","George K. Matsopoulos"],"pdf_url":"https://arxiv.org/pdf/2407.16298v1.pdf","comment":"To be published in IEEE Engineering in Medicine and Biology (EMBC)\n  2024 conference proceedings"},{"id":"http://arxiv.org/abs/2407.13675v2","updated":"2024-07-23T08:47:34Z","published":"2024-07-18T16:50:59Z","title":"MeshSegmenter: Zero-Shot Mesh Semantic Segmentation via Texture\n  Synthesis","summary":"  We present MeshSegmenter, a simple yet effective framework designed for\nzero-shot 3D semantic segmentation. This model successfully extends the\npowerful capabilities of 2D segmentation models to 3D meshes, delivering\naccurate 3D segmentation across diverse meshes and segment descriptions.\nSpecifically, our model leverages the Segment Anything Model (SAM) model to\nsegment the target regions from images rendered from the 3D shape. In light of\nthe importance of the texture for segmentation, we also leverage the pretrained\nstable diffusion model to generate images with textures from 3D shape, and\nleverage SAM to segment the target regions from images with textures. Textures\nsupplement the shape for segmentation and facilitate accurate 3D segmentation\neven in geometrically non-prominent areas, such as segmenting a car door within\na car mesh. To achieve the 3D segments, we render 2D images from different\nviews and conduct segmentation for both textured and untextured images. Lastly,\nwe develop a multi-view revoting scheme that integrates 2D segmentation results\nand confidence scores from various views onto the 3D mesh, ensuring the 3D\nconsistency of segmentation results and eliminating inaccuracies from specific\nperspectives. Through these innovations, MeshSegmenter offers stable and\nreliable 3D segmentation results both quantitatively and qualitatively,\nhighlighting its potential as a transformative tool in the field of 3D\nzero-shot segmentation. The code is available at\n\\url{https://github.com/zimingzhong/MeshSegmenter}.\n","authors":["Ziming Zhong","Yanxu Xu","Jing Li","Jiale Xu","Zhengxin Li","Chaohui Yu","Shenghua Gao"],"pdf_url":"https://arxiv.org/pdf/2407.13675v2.pdf","comment":"The paper was accepted by ECCV2024"},{"id":"http://arxiv.org/abs/2407.16291v1","updated":"2024-07-23T08:46:14Z","published":"2024-07-23T08:46:14Z","title":"TAPTRv2: Attention-based Position Update Improves Tracking Any Point","summary":"  In this paper, we present TAPTRv2, a Transformer-based approach built upon\nTAPTR for solving the Tracking Any Point (TAP) task. TAPTR borrows designs from\nDEtection TRansformer (DETR) and formulates each tracking point as a point\nquery, making it possible to leverage well-studied operations in DETR-like\nalgorithms. TAPTRv2 improves TAPTR by addressing a critical issue regarding its\nreliance on cost-volume,which contaminates the point query\\'s content feature\nand negatively impacts both visibility prediction and cost-volume computation.\nIn TAPTRv2, we propose a novel attention-based position update (APU) operation\nand use key-aware deformable attention to realize. For each query, this\noperation uses key-aware attention weights to combine their corresponding\ndeformable sampling positions to predict a new query position. This design is\nbased on the observation that local attention is essentially the same as\ncost-volume, both of which are computed by dot-production between a query and\nits surrounding features. By introducing this new operation, TAPTRv2 not only\nremoves the extra burden of cost-volume computation, but also leads to a\nsubstantial performance improvement. TAPTRv2 surpasses TAPTR and achieves\nstate-of-the-art performance on many challenging datasets, demonstrating the\nsuperiority\n","authors":["Hongyang Li","Hao Zhang","Shilong Liu","Zhaoyang Zeng","Feng Li","Tianhe Ren","Bohan Li","Lei Zhang"],"pdf_url":"https://arxiv.org/pdf/2407.16291v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16289v1","updated":"2024-07-23T08:43:42Z","published":"2024-07-23T08:43:42Z","title":"Federated Learning for Face Recognition via Intra-subject\n  Self-supervised Learning","summary":"  Federated Learning (FL) for face recognition aggregates locally optimized\nmodels from individual clients to construct a generalized face recognition\nmodel. However, previous studies present two major challenges: insufficient\nincorporation of self-supervised learning and the necessity for clients to\naccommodate multiple subjects. To tackle these limitations, we propose FedFS\n(Federated Learning for personalized Face recognition via intra-subject\nSelf-supervised learning framework), a novel federated learning architecture\ntailored to train personalized face recognition models without imposing\nsubjects. Our proposed FedFS comprises two crucial components that leverage\naggregated features of the local and global models to cooperate with\nrepresentations of an off-the-shelf model. These components are (1) adaptive\nsoft label construction, utilizing dot product operations to reformat labels\nwithin intra-instances, and (2) intra-subject self-supervised learning,\nemploying cosine similarity operations to strengthen robust intra-subject\nrepresentations. Additionally, we introduce a regularization loss to prevent\noverfitting and ensure the stability of the optimized model. To assess the\neffectiveness of FedFS, we conduct comprehensive experiments on the DigiFace-1M\nand VGGFace datasets, demonstrating superior performance compared to previous\nmethods.\n","authors":["Hansol Kim","Hoyeol Choi","Youngjun Kwak"],"pdf_url":"https://arxiv.org/pdf/2407.16289v1.pdf","comment":"Accepted at the The 35th British Machine Vision Conference 2024 (BMVC\n  2024), Glasgow, UK. Youngjun Kwak is corresponding author"},{"id":"http://arxiv.org/abs/2301.09489v5","updated":"2024-07-23T08:43:06Z","published":"2023-01-23T15:32:27Z","title":"Contracting Skeletal Kinematics for Human-Related Video Anomaly\n  Detection","summary":"  Detecting the anomaly of human behavior is paramount to timely recognizing\nendangering situations, such as street fights or elderly falls. However,\nanomaly detection is complex since anomalous events are rare and because it is\nan open set recognition task, i.e., what is anomalous at inference has not been\nobserved at training. We propose COSKAD, a novel model that encodes skeletal\nhuman motion by a graph convolutional network and learns to COntract SKeletal\nkinematic embeddings onto a latent hypersphere of minimum volume for Video\nAnomaly Detection. We propose three latent spaces: the commonly-adopted\nEuclidean and the novel spherical and hyperbolic. All variants outperform the\nstate-of-the-art on the most recent UBnormal dataset, for which we contribute a\nhuman-related version with annotated skeletons. COSKAD sets a new\nstate-of-the-art on the human-related versions of ShanghaiTech Campus and CUHK\nAvenue, with performance comparable to video-based methods. Source code and\ndataset will be released upon acceptance.\n","authors":["Alessandro Flaborea","Guido D'Amely","Stefano D'Arrigo","Marco Aurelio Sterpa","Alessio Sampieri","Fabio Galasso"],"pdf_url":"https://arxiv.org/pdf/2301.09489v5.pdf","comment":"Published in the Pattern Recognition Journal"},{"id":"http://arxiv.org/abs/2404.10335v3","updated":"2024-07-23T08:33:28Z","published":"2024-04-16T07:19:52Z","title":"Efficient Generation of Targeted and Transferable Adversarial Examples\n  for Vision-Language Models Via Diffusion Models","summary":"  Adversarial attacks, particularly \\textbf{targeted} transfer-based attacks,\ncan be used to assess the adversarial robustness of large visual-language\nmodels (VLMs), allowing for a more thorough examination of potential security\nflaws before deployment. However, previous transfer-based adversarial attacks\nincur high costs due to high iteration counts and complex method structure.\nFurthermore, due to the unnaturalness of adversarial semantics, the generated\nadversarial examples have low transferability. These issues limit the utility\nof existing methods for assessing robustness. To address these issues, we\npropose AdvDiffVLM, which uses diffusion models to generate natural,\nunrestricted and targeted adversarial examples via score matching.\nSpecifically, AdvDiffVLM uses Adaptive Ensemble Gradient Estimation to modify\nthe score during the diffusion model's reverse generation process, ensuring\nthat the produced adversarial examples have natural adversarial targeted\nsemantics, which improves their transferability. Simultaneously, to improve the\nquality of adversarial examples, we use the GradCAM-guided Mask method to\ndisperse adversarial semantics throughout the image rather than concentrating\nthem in a single area. Finally, AdvDiffVLM embeds more target semantics into\nadversarial examples after multiple iterations. Experimental results show that\nour method generates adversarial examples 5x to 10x faster than\nstate-of-the-art transfer-based adversarial attacks while maintaining higher\nquality adversarial examples. Furthermore, compared to previous transfer-based\nadversarial attacks, the adversarial examples generated by our method have\nbetter transferability. Notably, AdvDiffVLM can successfully attack a variety\nof commercial VLMs in a black-box environment, including GPT-4V.\n","authors":["Qi Guo","Shanmin Pang","Xiaojun Jia","Yang Liu","Qing Guo"],"pdf_url":"https://arxiv.org/pdf/2404.10335v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16277v1","updated":"2024-07-23T08:29:49Z","published":"2024-07-23T08:29:49Z","title":"When, Where, and What? An Novel Benchmark for Accident Anticipation and\n  Localization with Large Language Models","summary":"  As autonomous driving systems increasingly become part of daily\ntransportation, the ability to accurately anticipate and mitigate potential\ntraffic accidents is paramount. Traditional accident anticipation models\nprimarily utilizing dashcam videos are adept at predicting when an accident may\noccur but fall short in localizing the incident and identifying involved\nentities. Addressing this gap, this study introduces a novel framework that\nintegrates Large Language Models (LLMs) to enhance predictive capabilities\nacross multiple dimensions--what, when, and where accidents might occur. We\ndevelop an innovative chain-based attention mechanism that dynamically adjusts\nto prioritize high-risk elements within complex driving scenes. This mechanism\nis complemented by a three-stage model that processes outputs from smaller\nmodels into detailed multimodal inputs for LLMs, thus enabling a more nuanced\nunderstanding of traffic dynamics. Empirical validation on the DAD, CCD, and\nA3D datasets demonstrates superior performance in Average Precision (AP) and\nMean Time-To-Accident (mTTA), establishing new benchmarks for accident\nprediction technology. Our approach not only advances the technological\nframework for autonomous driving safety but also enhances human-AI interaction,\nmaking predictive insights generated by autonomous systems more intuitive and\nactionable.\n","authors":["Haicheng Liao","Yongkang Li","Chengyue Wang","Yanchen Guan","KaHou Tam","Chunlin Tian","Li Li","Chengzhong Xu","Zhenning Li"],"pdf_url":"https://arxiv.org/pdf/2407.16277v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14923v2","updated":"2024-07-23T08:27:39Z","published":"2024-07-20T16:23:57Z","title":"RayFormer: Improving Query-Based Multi-Camera 3D Object Detection via\n  Ray-Centric Strategies","summary":"  The recent advances in query-based multi-camera 3D object detection are\nfeatured by initializing object queries in the 3D space, and then sampling\nfeatures from perspective-view images to perform multi-round query refinement.\nIn such a framework, query points near the same camera ray are likely to sample\nsimilar features from very close pixels, resulting in ambiguous query features\nand degraded detection accuracy. To this end, we introduce RayFormer, a\ncamera-ray-inspired query-based 3D object detector that aligns the\ninitialization and feature extraction of object queries with the optical\ncharacteristics of cameras. Specifically, RayFormer transforms perspective-view\nimage features into bird's eye view (BEV) via the lift-splat-shoot method and\nsegments the BEV map to sectors based on the camera rays. Object queries are\nuniformly and sparsely initialized along each camera ray, facilitating the\nprojection of different queries onto different areas in the image to extract\ndistinct features. Besides, we leverage the instance information of images to\nsupplement the uniformly initialized object queries by further involving\nadditional queries along the ray from 2D object detection boxes. To extract\nunique object-level features that cater to distinct queries, we design a ray\nsampling method that suitably organizes the distribution of feature sampling\npoints on both images and bird's eye view. Extensive experiments are conducted\non the nuScenes dataset to validate our proposed ray-inspired model design. The\nproposed RayFormer achieves 55.5% mAP and 63.3% NDS, respectively. Our codes\nwill be made available.\n","authors":["Xiaomeng Chu","Jiajun Deng","Guoliang You","Yifan Duan","Yao Li","Yanyong Zhang"],"pdf_url":"https://arxiv.org/pdf/2407.14923v2.pdf","comment":"Accepted by ACM Multimedia 2024"},{"id":"http://arxiv.org/abs/2407.16269v1","updated":"2024-07-23T08:18:43Z","published":"2024-07-23T08:18:43Z","title":"HyTAS: A Hyperspectral Image Transformer Architecture Search Benchmark\n  and Analysis","summary":"  Hyperspectral Imaging (HSI) plays an increasingly critical role in precise\nvision tasks within remote sensing, capturing a wide spectrum of visual data.\nTransformer architectures have significantly enhanced HSI task performance,\nwhile advancements in Transformer Architecture Search (TAS) have improved model\ndiscovery. To harness these advancements for HSI classification, we make the\nfollowing contributions: i) We propose HyTAS, the first benchmark on\ntransformer architecture search for Hyperspectral imaging, ii) We\ncomprehensively evaluate 12 different methods to identify the optimal\ntransformer over 5 different datasets, iii) We perform an extensive factor\nanalysis on the Hyperspectral transformer search performance, greatly\nmotivating future research in this direction. All benchmark materials are\navailable at HyTAS.\n","authors":["Fangqin Zhou","Mert Kilickaya","Joaquin Vanschoren","Ran Piao"],"pdf_url":"https://arxiv.org/pdf/2407.16269v1.pdf","comment":"The paper is accepted at ECCV2024"},{"id":"http://arxiv.org/abs/2407.16268v1","updated":"2024-07-23T08:18:04Z","published":"2024-07-23T08:18:04Z","title":"Image Classification using Fuzzy Pooling in Convolutional\n  Kolmogorov-Arnold Networks","summary":"  Nowadays, deep learning models are increasingly required to be both\ninterpretable and highly accurate. We present an approach that integrates\nKolmogorov-Arnold Network (KAN) classification heads and Fuzzy Pooling into\nconvolutional neural networks (CNNs). By utilizing the interpretability of KAN\nand the uncertainty handling capabilities of fuzzy logic, the integration shows\npotential for improved performance in image classification tasks. Our\ncomparative analysis demonstrates that the modified CNN architecture with KAN\nand Fuzzy Pooling achieves comparable or higher accuracy than traditional\nmodels. The findings highlight the effectiveness of combining fuzzy logic and\nKAN to develop more interpretable and efficient deep learning models. Future\nwork will aim to expand this approach across larger datasets.\n","authors":["Ayan Igali","Pakizar Shamoi"],"pdf_url":"https://arxiv.org/pdf/2407.16268v1.pdf","comment":"The paper has been submitted to IEEE SCIS ISIS 2024 for consideration"},{"id":"http://arxiv.org/abs/2407.16264v1","updated":"2024-07-23T08:10:38Z","published":"2024-07-23T08:10:38Z","title":"Masks and Manuscripts: Advancing Medical Pre-training with End-to-End\n  Masking and Narrative Structuring","summary":"  Contemporary medical contrastive learning faces challenges from inconsistent\nsemantics and sample pair morphology, leading to dispersed and converging\nsemantic shifts. The variability in text reports, due to multiple authors,\ncomplicates semantic consistency. To tackle these issues, we propose a two-step\napproach. Initially, text reports are converted into a standardized triplet\nformat, laying the groundwork for our novel concept of ``observations'' and\n``verdicts''. This approach refines the {Entity, Position, Exist} triplet into\nbinary questions, guiding towards a clear ``verdict''. We also innovate in\nvisual pre-training with a Meijering-based masking, focusing on features\nrepresentative of medical images' local context. By integrating this with our\ntext conversion method, our model advances cross-modal representation in a\nmultimodal contrastive learning framework, setting new benchmarks in medical\nimage analysis.\n","authors":["Shreyank N Gowda","David A. Clifton"],"pdf_url":"https://arxiv.org/pdf/2407.16264v1.pdf","comment":"Accepted in MICCAI-24"},{"id":"http://arxiv.org/abs/2407.16260v1","updated":"2024-07-23T07:59:57Z","published":"2024-07-23T07:59:57Z","title":"DreamDissector: Learning Disentangled Text-to-3D Generation from 2D\n  Diffusion Priors","summary":"  Text-to-3D generation has recently seen significant progress. To enhance its\npracticality in real-world applications, it is crucial to generate multiple\nindependent objects with interactions, similar to layer-compositing in 2D image\nediting. However, existing text-to-3D methods struggle with this task, as they\nare designed to generate either non-independent objects or independent objects\nlacking spatially plausible interactions. Addressing this, we propose\nDreamDissector, a text-to-3D method capable of generating multiple independent\nobjects with interactions. DreamDissector accepts a multi-object text-to-3D\nNeRF as input and produces independent textured meshes. To achieve this, we\nintroduce the Neural Category Field (NeCF) for disentangling the input NeRF.\nAdditionally, we present the Category Score Distillation Sampling (CSDS),\nfacilitated by a Deep Concept Mining (DCM) module, to tackle the concept gap\nissue in diffusion models. By leveraging NeCF and CSDS, we can effectively\nderive sub-NeRFs from the original scene. Further refinement enhances geometry\nand texture. Our experimental results validate the effectiveness of\nDreamDissector, providing users with novel means to control 3D synthesis at the\nobject level and potentially opening avenues for various creative applications\nin the future.\n","authors":["Zizheng Yan","Jiapeng Zhou","Fanpeng Meng","Yushuang Wu","Lingteng Qiu","Zisheng Ye","Shuguang Cui","Guanying Chen","Xiaoguang Han"],"pdf_url":"https://arxiv.org/pdf/2407.16260v1.pdf","comment":"ECCV 2024. Project page: https://chester256.github.io/dreamdissector"},{"id":"http://arxiv.org/abs/2402.17430v2","updated":"2024-07-23T07:57:55Z","published":"2024-02-27T11:43:09Z","title":"Leveraging Enhanced Queries of Point Sets for Vectorized Map\n  Construction","summary":"  In autonomous driving, the high-definition (HD) map plays a crucial role in\nlocalization and planning. Recently, several methods have facilitated\nend-to-end online map construction in DETR-like frameworks. However, little\nattention has been paid to the potential capabilities of exploring the query\nmechanism for map elements. This paper introduces MapQR, an end-to-end method\nwith an emphasis on enhancing query capabilities for constructing online\nvectorized maps. To probe desirable information efficiently, MapQR utilizes a\nnovel query design, called scatter-and-gather query, which is modelled by\nseparate content and position parts explicitly. The base map instance queries\nare scattered to different reference points and added with positional\nembeddings to probe information from BEV features. Then these scatted queries\nare gathered back to enhance information within each map instance. Together\nwith a simple and effective improvement of a BEV encoder, the proposed MapQR\nachieves the best mean average precision (mAP) and maintains good efficiency on\nboth nuScenes and Argoverse 2. In addition, integrating our query design into\nother models can boost their performance significantly. The source code is\navailable at https://github.com/HXMap/MapQR.\n","authors":["Zihao Liu","Xiaoyu Zhang","Guangwei Liu","Ji Zhao","Ningyi Xu"],"pdf_url":"https://arxiv.org/pdf/2402.17430v2.pdf","comment":"Accepted to European Conference on Computer Vision (ECCV), 2024.Code\n  can be found at https://github.com/HXMap/MapQR"},{"id":"http://arxiv.org/abs/2407.14302v2","updated":"2024-07-23T07:57:17Z","published":"2024-07-19T13:33:38Z","title":"Dyn-Adapter: Towards Disentangled Representation for Efficient Visual\n  Recognition","summary":"  Parameter-efficient transfer learning (PETL) is a promising task, aiming to\nadapt the large-scale pre-trained model to downstream tasks with a relatively\nmodest cost. However, current PETL methods struggle in compressing\ncomputational complexity and bear a heavy inference burden due to the complete\nforward process. This paper presents an efficient visual recognition paradigm,\ncalled Dynamic Adapter (Dyn-Adapter), that boosts PETL efficiency by subtly\ndisentangling features in multiple levels. Our approach is simple: first, we\ndevise a dynamic architecture with balanced early heads for multi-level feature\nextraction, along with adaptive training strategy. Second, we introduce a\nbidirectional sparsity strategy driven by the pursuit of powerful\ngeneralization ability. These qualities enable us to fine-tune efficiently and\neffectively: we reduce FLOPs during inference by 50%, while maintaining or even\nyielding higher recognition accuracy. Extensive experiments on diverse datasets\nand pretrained backbones demonstrate the potential of Dyn-Adapter serving as a\ngeneral efficiency booster for PETL in vision recognition tasks.\n","authors":["Yurong Zhang","Honghao Chen","Xinyu Zhang","Xiangxiang Chu","Li Song"],"pdf_url":"https://arxiv.org/pdf/2407.14302v2.pdf","comment":"ECCV 2024"},{"id":"http://arxiv.org/abs/2405.16341v2","updated":"2024-07-23T07:55:03Z","published":"2024-05-25T19:56:01Z","title":"R.A.C.E.: Robust Adversarial Concept Erasure for Secure Text-to-Image\n  Diffusion Model","summary":"  In the evolving landscape of text-to-image (T2I) diffusion models, the\nremarkable capability to generate high-quality images from textual descriptions\nfaces challenges with the potential misuse of reproducing sensitive content. To\naddress this critical issue, we introduce \\textbf{R}obust \\textbf{A}dversarial\n\\textbf{C}oncept \\textbf{E}rase (RACE), a novel approach designed to mitigate\nthese risks by enhancing the robustness of concept erasure method for T2I\nmodels. RACE utilizes a sophisticated adversarial training framework to\nidentify and mitigate adversarial text embeddings, significantly reducing the\nAttack Success Rate (ASR). Impressively, RACE achieves a 30 percentage point\nreduction in ASR for the ``nudity'' concept against the leading white-box\nattack method. Our extensive evaluations demonstrate RACE's effectiveness in\ndefending against both white-box and black-box attacks, marking a significant\nadvancement in protecting T2I diffusion models from generating inappropriate or\nmisleading imagery. This work underlines the essential need for proactive\ndefense measures in adapting to the rapidly advancing field of adversarial\nchallenges. Our code is publicly available:\n\\url{https://github.com/chkimmmmm/R.A.C.E.}\n","authors":["Changhoon Kim","Kyle Min","Yezhou Yang"],"pdf_url":"https://arxiv.org/pdf/2405.16341v2.pdf","comment":"Accepted at ECCV 2024"},{"id":"http://arxiv.org/abs/2404.14409v4","updated":"2024-07-23T07:47:35Z","published":"2024-04-22T17:59:36Z","title":"CrossScore: Towards Multi-View Image Evaluation and Scoring","summary":"  We introduce a novel cross-reference image quality assessment method that\neffectively fills the gap in the image assessment landscape, complementing the\narray of established evaluation schemes -- ranging from full-reference metrics\nlike SSIM, no-reference metrics such as NIQE, to general-reference metrics\nincluding FID, and Multi-modal-reference metrics, e.g., CLIPScore. Utilising a\nneural network with the cross-attention mechanism and a unique data collection\npipeline from NVS optimisation, our method enables accurate image quality\nassessment without requiring ground truth references. By comparing a query\nimage against multiple views of the same scene, our method addresses the\nlimitations of existing metrics in novel view synthesis (NVS) and similar tasks\nwhere direct reference images are unavailable. Experimental results show that\nour method is closely correlated to the full-reference metric SSIM, while not\nrequiring ground truth references.\n","authors":["Zirui Wang","Wenjing Bian","Victor Adrian Prisacariu"],"pdf_url":"https://arxiv.org/pdf/2404.14409v4.pdf","comment":"Accepted at ECCV 2024. Project page see\n  https://crossscore.active.vision"},{"id":"http://arxiv.org/abs/2401.11535v3","updated":"2024-07-23T07:47:13Z","published":"2024-01-21T16:14:04Z","title":"EndoGS: Deformable Endoscopic Tissues Reconstruction with Gaussian\n  Splatting","summary":"  Surgical 3D reconstruction is a critical area of research in robotic surgery,\nwith recent works adopting variants of dynamic radiance fields to achieve\nsuccess in 3D reconstruction of deformable tissues from single-viewpoint\nvideos. However, these methods often suffer from time-consuming optimization or\ninferior quality, limiting their adoption in downstream tasks. Inspired by 3D\nGaussian Splatting, a recent trending 3D representation, we present EndoGS,\napplying Gaussian Splatting for deformable endoscopic tissue reconstruction.\nSpecifically, our approach incorporates deformation fields to handle dynamic\nscenes, depth-guided supervision with spatial-temporal weight masks to optimize\n3D targets with tool occlusion from a single viewpoint, and surface-aligned\nregularization terms to capture the much better geometry. As a result, EndoGS\nreconstructs and renders high-quality deformable endoscopic tissues from a\nsingle-viewpoint video, estimated depth maps, and labeled tool masks.\nExperiments on DaVinci robotic surgery videos demonstrate that EndoGS achieves\nsuperior rendering quality. Code is available at\nhttps://github.com/HKU-MedAI/EndoGS.\n","authors":["Lingting Zhu","Zhao Wang","Jiahao Cui","Zhenchao Jin","Guying Lin","Lequan Yu"],"pdf_url":"https://arxiv.org/pdf/2401.11535v3.pdf","comment":"Accepted by Embodied AI and Robotics for HealTHcare of International\n  Conference on Medical Image Computing and Computer Assisted Intervention\n  (MICCAI EARTH 2024). 11 pages, 4 figures"},{"id":"http://arxiv.org/abs/2407.16252v1","updated":"2024-07-23T07:40:41Z","published":"2024-07-23T07:40:41Z","title":"LawLuo: A Chinese Law Firm Co-run by LLM Agents","summary":"  Large Language Models (LLMs) demonstrate substantial potential in delivering\nlegal consultation services to users without a legal background, attributed to\ntheir superior text comprehension and generation capabilities. Nonetheless,\nexisting Chinese legal LLMs limit interaction to a single model-user dialogue,\nunlike the collaborative consultations typical of law firms, where multiple\nstaff members contribute to a single consultation. This limitation prevents an\nauthentic consultation experience. Additionally, extant Chinese legal LLMs\nsuffer from critical limitations: (1) insufficient control over the quality of\ninstruction fine-tuning data; (2) increased model hallucination resulting from\nusers' ambiguous queries; and (3) a reduction in the model's ability to follow\ninstructions over multiple dialogue turns. In response to these challenges, we\npropose a novel legal dialogue framework that leverages the collaborative\ncapabilities of multiple LLM agents, termed LawLuo. This framework encompasses\nfour agents: a receptionist, a lawyer, a secretary, and a boss, each\nresponsible for different functionalities, collaboratively providing a\ncomprehensive legal consultation to users. Additionally, we constructed two\nhigh-quality legal dialogue datasets, KINLED and MURLED, and fine-tuned\nChatGLM-3-6b using these datasets. We propose a legal query clarification\nalgorithm called ToLC. Experimental results demonstrate that LawLuo outperforms\nbaseline LLMs, including GPT-4, across three dimensions: lawyer-like language\nstyle, the usefulness of legal advice, and the accuracy of legal knowledge. Our\ncode and datasets are available at https://github.com/NEFUJing/LawLuo.\n","authors":["Jingyun Sun","Chengxiao Dai","Zhongze Luo","Yangbo Chang","Yang Li"],"pdf_url":"https://arxiv.org/pdf/2407.16252v1.pdf","comment":"11 pages, 13 figures, 2 tables"},{"id":"http://arxiv.org/abs/2407.16248v1","updated":"2024-07-23T07:36:54Z","published":"2024-07-23T07:36:54Z","title":"Spatiotemporal Graph Guided Multi-modal Network for Livestreaming\n  Product Retrieval","summary":"  With the rapid expansion of e-commerce, more consumers have become accustomed\nto making purchases via livestreaming. Accurately identifying the products\nbeing sold by salespeople, i.e., livestreaming product retrieval (LPR), poses a\nfundamental and daunting challenge. The LPR task encompasses three primary\ndilemmas in real-world scenarios: 1) the recognition of intended products from\ndistractor products present in the background; 2) the video-image heterogeneity\nthat the appearance of products showcased in live streams often deviates\nsubstantially from standardized product images in stores; 3) there are numerous\nconfusing products with subtle visual nuances in the shop. To tackle these\nchallenges, we propose the Spatiotemporal Graphing Multi-modal Network (SGMN).\nFirst, we employ a text-guided attention mechanism that leverages the spoken\ncontent of salespeople to guide the model to focus toward intended products,\nemphasizing their salience over cluttered background products. Second, a\nlong-range spatiotemporal graph network is further designed to achieve both\ninstance-level interaction and frame-level matching, solving the misalignment\ncaused by video-image heterogeneity. Third, we propose a multi-modal hard\nexample mining, assisting the model in distinguishing highly similar products\nwith fine-grained features across the video-image-text domain. Through\nextensive quantitative and qualitative experiments, we demonstrate the superior\nperformance of our proposed SGMN model, surpassing the state-of-the-art methods\nby a substantial margin. The code is available at\n\\url{https://github.com/Huxiaowan/SGMN}.\n","authors":["Xiaowan Hu","Yiyi Chen","Yan Li","Minquan Wang","Haoqian Wang","Quan Chen","Han Li","Peng Jiang"],"pdf_url":"https://arxiv.org/pdf/2407.16248v1.pdf","comment":"9 pages, 12 figures"},{"id":"http://arxiv.org/abs/2407.16244v1","updated":"2024-07-23T07:31:42Z","published":"2024-07-23T07:31:42Z","title":"HSVLT: Hierarchical Scale-Aware Vision-Language Transformer for\n  Multi-Label Image Classification","summary":"  The task of multi-label image classification involves recognizing multiple\nobjects within a single image. Considering both valuable semantic information\ncontained in the labels and essential visual features presented in the image,\ntight visual-linguistic interactions play a vital role in improving\nclassification performance. Moreover, given the potential variance in object\nsize and appearance within a single image, attention to features of different\nscales can help to discover possible objects in the image. Recently,\nTransformer-based methods have achieved great success in multi-label image\nclassification by leveraging the advantage of modeling long-range dependencies,\nbut they have several limitations. Firstly, existing methods treat visual\nfeature extraction and cross-modal fusion as separate steps, resulting in\ninsufficient visual-linguistic alignment in the joint semantic space.\nAdditionally, they only extract visual features and perform cross-modal fusion\nat a single scale, neglecting objects with different characteristics. To\naddress these issues, we propose a Hierarchical Scale-Aware Vision-Language\nTransformer (HSVLT) with two appealing designs: (1)~A hierarchical multi-scale\narchitecture that involves a Cross-Scale Aggregation module, which leverages\njoint multi-modal features extracted from multiple scales to recognize objects\nof varying sizes and appearances in images. (2)~Interactive Visual-Linguistic\nAttention, a novel attention mechanism module that tightly integrates\ncross-modal interaction, enabling the joint updating of visual, linguistic and\nmulti-modal features. We have evaluated our method on three benchmark datasets.\nThe experimental results demonstrate that HSVLT surpasses state-of-the-art\nmethods with lower computational cost.\n","authors":["Shuyi Ouyang","Hongyi Wang","Ziwei Niu","Zhenjia Bai","Shiao Xie","Yingying Xu","Ruofeng Tong","Yen-Wei Chen","Lanfen Lin"],"pdf_url":"https://arxiv.org/pdf/2407.16244v1.pdf","comment":"10 pages, 6 figures"},{"id":"http://arxiv.org/abs/2407.16243v1","updated":"2024-07-23T07:29:57Z","published":"2024-07-23T07:29:57Z","title":"Chameleon: Images Are What You Need For Multimodal Learning Robust To\n  Missing Modalities","summary":"  Multimodal learning has demonstrated remarkable performance improvements over\nunimodal architectures. However, multimodal learning methods often exhibit\ndeteriorated performances if one or more modalities are missing. This may be\nattributed to the commonly used multi-branch design containing\nmodality-specific streams making the models reliant on the availability of a\ncomplete set of modalities. In this work, we propose a robust textual-visual\nmultimodal learning method, Chameleon, that completely deviates from the\nconventional multi-branch design. To enable this, we present the unification of\ninput modalities into one format by encoding textual modality into visual\nrepresentations. As a result, our approach does not require modality-specific\nbranches to learn modality-independent multimodal representations making it\nrobust to missing modalities. Extensive experiments are performed on four\npopular challenging datasets including Hateful Memes, UPMC Food-101, MM-IMDb,\nand Ferramenta. Chameleon not only achieves superior performance when all\nmodalities are present at train/test time but also demonstrates notable\nresilience in the case of missing modalities.\n","authors":["Muhammad Irzam Liaqat","Shah Nawaz","Muhammad Zaigham Zaheer","Muhammad Saad Saeed","Hassan Sajjad","Tom De Schepper","Karthik Nandakumar","Muhammad Haris Khan Markus Schedl"],"pdf_url":"https://arxiv.org/pdf/2407.16243v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15451v2","updated":"2024-07-23T07:22:53Z","published":"2024-07-22T08:09:14Z","title":"Domain-Adaptive 2D Human Pose Estimation via Dual Teachers in Extremely\n  Low-Light Conditions","summary":"  Existing 2D human pose estimation research predominantly concentrates on\nwell-lit scenarios, with limited exploration of poor lighting conditions, which\nare a prevalent aspect of daily life. Recent studies on low-light pose\nestimation require the use of paired well-lit and low-light images with ground\ntruths for training, which are impractical due to the inherent challenges\nassociated with annotation on low-light images. To this end, we introduce a\nnovel approach that eliminates the need for low-light ground truths. Our\nprimary novelty lies in leveraging two complementary-teacher networks to\ngenerate more reliable pseudo labels, enabling our model achieves competitive\nperformance on extremely low-light images without the need for training with\nlow-light ground truths. Our framework consists of two stages. In the first\nstage, our model is trained on well-lit data with low-light augmentations. In\nthe second stage, we propose a dual-teacher framework to utilize the unlabeled\nlow-light data, where a center-based main teacher produces the pseudo labels\nfor relatively visible cases, while a keypoints-based complementary teacher\nfocuses on producing the pseudo labels for the missed persons of the main\nteacher. With the pseudo labels from both teachers, we propose a\nperson-specific low-light augmentation to challenge a student model in training\nto outperform the teachers. Experimental results on real low-light dataset\n(ExLPose-OCN) show, our method achieves 6.8% (2.4 AP) improvement over the\nstate-of-the-art (SOTA) method, despite no low-light ground-truth data is used\nin our approach, in contrast to the SOTA method. Our code will be available\nat:https://github.com/ayh015-dev/DA-LLPose.\n","authors":["Yihao Ai","Yifei Qi","Bo Wang","Yu Cheng","Xinchao Wang","Robby T. Tan"],"pdf_url":"https://arxiv.org/pdf/2407.15451v2.pdf","comment":"18 pages, 3 figure. Accepted by ECCV24"},{"id":"http://arxiv.org/abs/2407.16234v1","updated":"2024-07-23T07:17:46Z","published":"2024-07-23T07:17:46Z","title":"A Multi-view Mask Contrastive Learning Graph Convolutional Neural\n  Network for Age Estimation","summary":"  The age estimation task aims to use facial features to predict the age of\npeople and is widely used in public security, marketing, identification, and\nother fields. However, the features are mainly concentrated in facial\nkeypoints, and existing CNN and Transformer-based methods have inflexibility\nand redundancy for modeling complex irregular structures. Therefore, this paper\nproposes a Multi-view Mask Contrastive Learning Graph Convolutional Neural\nNetwork (MMCL-GCN) for age estimation. Specifically, the overall structure of\nthe MMCL-GCN network contains a feature extraction stage and an age estimation\nstage. In the feature extraction stage, we introduce a graph structure to\nconstruct face images as input and then design a Multi-view Mask Contrastive\nLearning (MMCL) mechanism to learn complex structural and semantic information\nabout face images. The learning mechanism employs an asymmetric siamese network\narchitecture, which utilizes an online encoder-decoder structure to reconstruct\nthe missing information from the original graph and utilizes the target encoder\nto learn latent representations for contrastive learning. Furthermore, to\npromote the two learning mechanisms better compatible and complementary, we\nadopt two augmentation strategies and optimize the joint losses. In the age\nestimation stage, we design a Multi-layer Extreme Learning Machine (ML-IELM)\nwith identity mapping to fully use the features extracted by the online\nencoder. Then, a classifier and a regressor were constructed based on ML-IELM,\nwhich were used to identify the age grouping interval and accurately estimate\nthe final age. Extensive experiments show that MMCL-GCN can effectively reduce\nthe error of age estimation on benchmark datasets such as Adience, MORPH-II,\nand LAP-2016.\n","authors":["Yiping Zhang","Yuntao Shou","Tao Meng","Wei Ai","Keqin Li"],"pdf_url":"https://arxiv.org/pdf/2407.16234v1.pdf","comment":"20 pages, 9 figures"},{"id":"http://arxiv.org/abs/2407.16232v1","updated":"2024-07-23T07:17:10Z","published":"2024-07-23T07:17:10Z","title":"Channel-Partitioned Windowed Attention And Frequency Learning for Single\n  Image Super-Resolution","summary":"  Recently, window-based attention methods have shown great potential for\ncomputer vision tasks, particularly in Single Image Super-Resolution (SISR).\nHowever, it may fall short in capturing long-range dependencies and\nrelationships between distant tokens. Additionally, we find that learning on\nspatial domain does not convey the frequency content of the image, which is a\ncrucial aspect in SISR. To tackle these issues, we propose a new\nChannel-Partitioned Attention Transformer (CPAT) to better capture long-range\ndependencies by sequentially expanding windows along the height and width of\nfeature maps. In addition, we propose a novel Spatial-Frequency Interaction\nModule (SFIM), which incorporates information from spatial and frequency\ndomains to provide a more comprehensive information from feature maps. This\nincludes information about the frequency content and enhances the receptive\nfield across the entire image. Experimental findings demonstrate the\neffectiveness of our proposed modules and architecture. In particular, CPAT\nsurpasses current state-of-the-art methods by up to 0.31dB.\n","authors":["Dinh Phu Tran","Dao Duy Hung","Daeyoung Kim"],"pdf_url":"https://arxiv.org/pdf/2407.16232v1.pdf","comment":"Version 1, BMVC 2024"},{"id":"http://arxiv.org/abs/2403.12488v3","updated":"2024-07-23T07:14:54Z","published":"2024-03-19T06:54:33Z","title":"DetToolChain: A New Prompting Paradigm to Unleash Detection Ability of\n  MLLM","summary":"  We present DetToolChain, a novel prompting paradigm, to unleash the zero-shot\nobject detection ability of multimodal large language models (MLLMs), such as\nGPT-4V and Gemini. Our approach consists of a detection prompting toolkit\ninspired by high-precision detection priors and a new Chain-of-Thought to\nimplement these prompts. Specifically, the prompts in the toolkit are designed\nto guide the MLLM to focus on regional information (e.g., zooming in), read\ncoordinates according to measure standards (e.g., overlaying rulers and\ncompasses), and infer from the contextual information (e.g., overlaying scene\ngraphs). Building upon these tools, the new detection chain-of-thought can\nautomatically decompose the task into simple subtasks, diagnose the\npredictions, and plan for progressive box refinements. The effectiveness of our\nframework is demonstrated across a spectrum of detection tasks, especially hard\ncases. Compared to existing state-of-the-art methods, GPT-4V with our\nDetToolChain improves state-of-the-art object detectors by +21.5% AP50 on MS\nCOCO Novel class set for open-vocabulary detection, +24.23% Acc on RefCOCO val\nset for zero-shot referring expression comprehension, +14.5% AP on D-cube\ndescribe object detection FULL setting.\n","authors":["Yixuan Wu","Yizhou Wang","Shixiang Tang","Wenhao Wu","Tong He","Wanli Ouyang","Philip Torr","Jian Wu"],"pdf_url":"https://arxiv.org/pdf/2403.12488v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16224v1","updated":"2024-07-23T07:04:42Z","published":"2024-07-23T07:04:42Z","title":"OutfitAnyone: Ultra-high Quality Virtual Try-On for Any Clothing and Any\n  Person","summary":"  Virtual Try-On (VTON) has become a transformative technology, empowering\nusers to experiment with fashion without ever having to physically try on\nclothing. However, existing methods often struggle with generating\nhigh-fidelity and detail-consistent results. While diffusion models, such as\nStable Diffusion series, have shown their capability in creating high-quality\nand photorealistic images, they encounter formidable challenges in conditional\ngeneration scenarios like VTON. Specifically, these models struggle to maintain\na balance between control and consistency when generating images for virtual\nclothing trials. OutfitAnyone addresses these limitations by leveraging a\ntwo-stream conditional diffusion model, enabling it to adeptly handle garment\ndeformation for more lifelike results. It distinguishes itself with\nscalability-modulating factors such as pose, body shape and broad\napplicability, extending from anime to in-the-wild images. OutfitAnyone's\nperformance in diverse scenarios underscores its utility and readiness for\nreal-world deployment. For more details and animated results, please see\n\\url{https://humanaigc.github.io/outfit-anyone/}.\n","authors":["Ke Sun","Jian Cao","Qi Wang","Linrui Tian","Xindi Zhang","Lian Zhuo","Bang Zhang","Liefeng Bo","Wenbo Zhou","Weiming Zhang","Daiheng Gao"],"pdf_url":"https://arxiv.org/pdf/2407.16224v1.pdf","comment":"10 pages, 13 figures"},{"id":"http://arxiv.org/abs/2403.11821v4","updated":"2024-07-23T07:04:09Z","published":"2024-03-18T14:24:20Z","title":"A Survey on Quality Metrics for Text-to-Image Models","summary":"  Recent AI-based text-to-image models not only excel at generating realistic\nimages, they also give designers more and more fine-grained control over the\nimage content. Consequently, these approaches have gathered increased attention\nwithin the computer graphics research community, which has been historically\ndevoted towards traditional rendering techniques that offer precise control\nover scene parameters such as objects, materials, and lighting, when generating\nrealistic images. While the quality of rendered images is traditionally\nassessed through well-established image quality metrics, such as SSIM or PSNR,\nthe unique challenges presented by text-to-image models, which in contrast to\nrendering interweave the control of scene and rendering parameters, necessitate\nthe development of novel image quality metrics. Therefore, within this survey,\nwe provide a comprehensive overview of existing text-to-image quality metrics\naddressing their nuances and the need for alignment with human preferences.\nBased on our findings, we propose a new taxonomy for categorizing these\nmetrics, which is grounded in the assumption that there are two main quality\ncriteria, namely compositionality and generality, which ideally map to human\npreferences. Ultimately, we derive guidelines for practitioners conducting\ntext-to-image evaluation, discuss open challenges of evaluation mechanisms, and\nsurface limitations of current metrics.\n","authors":["Sebastian Hartwig","Dominik Engel","Leon Sick","Hannah Kniesel","Tristan Payer","Poonam Poonam","Michael Glöckler","Alex Bäuerle","Timo Ropinski"],"pdf_url":"https://arxiv.org/pdf/2403.11821v4.pdf","comment":"preprint"},{"id":"http://arxiv.org/abs/2402.13220v2","updated":"2024-07-23T07:02:30Z","published":"2024-02-20T18:31:27Z","title":"How Easy is It to Fool Your Multimodal LLMs? An Empirical Analysis on\n  Deceptive Prompts","summary":"  The remarkable advancements in Multimodal Large Language Models (MLLMs) have\nnot rendered them immune to challenges, particularly in the context of handling\ndeceptive information in prompts, thus producing hallucinated responses under\nsuch conditions. To quantitatively assess this vulnerability, we present\nMAD-Bench, a carefully curated benchmark that contains 1000 test samples\ndivided into 5 categories, such as non-existent objects, count of objects, and\nspatial relationship. We provide a comprehensive analysis of popular MLLMs,\nranging from GPT-4v, Reka, Gemini-Pro, to open-sourced models, such as\nLLaVA-NeXT and MiniCPM-Llama3. Empirically, we observe significant performance\ngaps between GPT-4o and other models; and previous robust instruction-tuned\nmodels are not effective on this new benchmark. While GPT-4o achieves 82.82%\naccuracy on MAD-Bench, the accuracy of any other model in our experiments\nranges from 9% to 50%. We further propose a remedy that adds an additional\nparagraph to the deceptive prompts to encourage models to think twice before\nanswering the question. Surprisingly, this simple method can even double the\naccuracy; however, the absolute numbers are still too low to be satisfactory.\nWe hope MAD-Bench can serve as a valuable benchmark to stimulate further\nresearch to enhance model resilience against deceptive prompts.\n","authors":["Yusu Qian","Haotian Zhang","Yinfei Yang","Zhe Gan"],"pdf_url":"https://arxiv.org/pdf/2402.13220v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16223v1","updated":"2024-07-23T07:02:01Z","published":"2024-07-23T07:02:01Z","title":"Probabilistic Parameter Estimators and Calibration Metrics for Pose\n  Estimation from Image Features","summary":"  This paper addresses the challenge of probabilistic parameter estimation\ngiven measurement uncertainty in real-time. We provide a general formulation\nand apply this to pose estimation for an autonomous visual landing system. We\npresent three probabilistic parameter estimators: a least-squares sampling\napproach, a linear approximation method, and a probabilistic programming\nestimator. To evaluate these estimators, we introduce novel closed-form\nexpressions for measuring calibration and sharpness specifically for\nmultivariate normal distributions. Our experimental study compares the three\nestimators under various noise conditions. We demonstrate that the linear\napproximation estimator can produce sharp and well-calibrated pose predictions\nsignificantly faster than the other methods but may yield overconfident\npredictions in certain scenarios. Additionally, we demonstrate that these\nestimators can be integrated with a Kalman filter for continuous pose\nestimation during a runway approach where we observe a 50\\% improvement in\nsharpness while maintaining marginal calibration. This work contributes to the\nintegration of data-driven computer vision models into complex safety-critical\naircraft systems and provides a foundation for developing rigorous\ncertification guidelines for such systems.\n","authors":["Romeo Valentin","Sydney M. Katz","Joonghyun Lee","Don Walker","Matthew Sorgenfrei","Mykel J. Kochenderfer"],"pdf_url":"https://arxiv.org/pdf/2407.16223v1.pdf","comment":"Accepted at DASC '24. 9 pages, 4 figures"},{"id":"http://arxiv.org/abs/2404.09640v4","updated":"2024-07-23T06:59:46Z","published":"2024-04-15T10:19:39Z","title":"CREST: Cross-modal Resonance through Evidential Deep Learning for\n  Enhanced Zero-Shot Learning","summary":"  Zero-shot learning (ZSL) enables the recognition of novel classes by\nleveraging semantic knowledge transfer from known to unknown categories. This\nknowledge, typically encapsulated in attribute descriptions, aids in\nidentifying class-specific visual features, thus facilitating visual-semantic\nalignment and improving ZSL performance. However, real-world challenges such as\ndistribution imbalances and attribute co-occurrence among instances often\nhinder the discernment of local variances in images, a problem exacerbated by\nthe scarcity of fine-grained, region-specific attribute annotations. Moreover,\nthe variability in visual presentation within categories can also skew\nattribute-category associations. In response, we propose a bidirectional\ncross-modal ZSL approach CREST. It begins by extracting representations for\nattribute and visual localization and employs Evidential Deep Learning (EDL) to\nmeasure underlying epistemic uncertainty, thereby enhancing the model's\nresilience against hard negatives. CREST incorporates dual learning pathways,\nfocusing on both visual-category and attribute-category alignments, to ensure\nrobust correlation between latent and observable spaces. Moreover, we introduce\nan uncertainty-informed cross-modal fusion technique to refine visual-attribute\ninference. Extensive experiments demonstrate our model's effectiveness and\nunique explainability across multiple datasets. Our code and data are available\nat: https://github.com/JethroJames/CREST\n","authors":["Haojian Huang","Xiaozhen Qiao","Zhuo Chen","Haodong Chen","Bingyu Li","Zhe Sun","Mulin Chen","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2404.09640v4.pdf","comment":"Accepted by ACM MM 2024; 10 pages, 2 Tables, 9 Figures; Repo is\n  available at: https://github.com/JethroJames/CREST"},{"id":"http://arxiv.org/abs/2312.16943v4","updated":"2024-07-23T06:57:26Z","published":"2023-12-28T10:40:11Z","title":"Multi-scale direction-aware SAR object detection network via global\n  information fusion","summary":"  Deep learning has driven significant progress in object detection using\nSynthetic Aperture Radar (SAR) imagery. Existing methods, while achieving\npromising results, often struggle to effectively integrate local and global\ninformation, particularly direction-aware features. This paper proposes\nSAR-Net, a novel framework specifically designed for global fusion of\ndirection-aware information in SAR object detection. SAR-Net leverages two key\ninnovations: the Unity Compensation Mechanism (UCM) and the Direction-aware\nAttention Module (DAM). UCM facilitates the establishment of complementary\nrelationships among features across different scales, enabling efficient global\ninformation fusion and transmission. Additionally, DAM, through bidirectional\nattention polymerization, captures direction-aware information, effectively\neliminating background interference. Extensive experiments demonstrate the\neffectiveness of SAR-Net, achieving state-of-the-art results on aircraft\n(SAR-AIRcraft-1.0) and ship datasets (SSDD, HRSID), confirming its\ngeneralization capability and robustness.\n","authors":["Mingxiang Cao","Weiying Xie","Jie Lei","Jiaqing Zhang","Daixun Li","Yunsong Li"],"pdf_url":"https://arxiv.org/pdf/2312.16943v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14900v2","updated":"2024-07-23T06:56:39Z","published":"2024-07-20T15:17:48Z","title":"AGLLDiff: Guiding Diffusion Models Towards Unsupervised Training-free\n  Real-world Low-light Image Enhancement","summary":"  Existing low-light image enhancement (LIE) methods have achieved noteworthy\nsuccess in solving synthetic distortions, yet they often fall short in\npractical applications. The limitations arise from two inherent challenges in\nreal-world LIE: 1) the collection of distorted/clean image pairs is often\nimpractical and sometimes even unavailable, and 2) accurately modeling complex\ndegradations presents a non-trivial problem. To overcome them, we propose the\nAttribute Guidance Diffusion framework (AGLLDiff), a training-free method for\neffective real-world LIE. Instead of specifically defining the degradation\nprocess, AGLLDiff shifts the paradigm and models the desired attributes, such\nas image exposure, structure and color of normal-light images. These attributes\nare readily available and impose no assumptions about the degradation process,\nwhich guides the diffusion sampling process to a reliable high-quality solution\nspace. Extensive experiments demonstrate that our approach outperforms the\ncurrent leading unsupervised LIE methods across benchmarks in terms of\ndistortion-based and perceptual-based metrics, and it performs well even in\nsophisticated wild degradation.\n","authors":["Yunlong Lin","Tian Ye","Sixiang Chen","Zhenqi Fu","Yingying Wang","Wenhao Chai","Zhaohu Xing","Lei Zhu","Xinghao Ding"],"pdf_url":"https://arxiv.org/pdf/2407.14900v2.pdf","comment":"21 pages, 9 figures"},{"id":"http://arxiv.org/abs/2405.04889v2","updated":"2024-07-23T06:51:06Z","published":"2024-05-08T08:38:28Z","title":"Fast LiDAR Upsampling using Conditional Diffusion Models","summary":"  The search for refining 3D LiDAR data has attracted growing interest\nmotivated by recent techniques such as supervised learning or generative\nmodel-based methods. Existing approaches have shown the possibilities for using\ndiffusion models to generate refined LiDAR data with high fidelity, although\nthe performance and speed of such methods have been limited. These limitations\nmake it difficult to execute in real-time, causing the approaches to struggle\nin real-world tasks such as autonomous navigation and human-robot interaction.\nIn this work, we introduce a novel approach based on conditional diffusion\nmodels for fast and high-quality sparse-to-dense upsampling of 3D scene point\nclouds through an image representation. Our method employs denoising diffusion\nprobabilistic models trained with conditional inpainting masks, which have been\nshown to give high performance on image completion tasks. We introduce a series\nof experiments, including multiple datasets, sampling steps, and conditional\nmasks. This paper illustrates that our method outperforms the baselines in\nsampling speed and quality on upsampling tasks using the KITTI-360 dataset.\nFurthermore, we illustrate the generalization ability of our approach by\nsimultaneously training on real-world and synthetic datasets, introducing\nvariance in quality and environments.\n","authors":["Sander Elias Magnussen Helgesen","Kazuto Nakashima","Jim Tørresen","Ryo Kurazume"],"pdf_url":"https://arxiv.org/pdf/2405.04889v2.pdf","comment":"RO-MAN 2024"},{"id":"http://arxiv.org/abs/2407.16214v1","updated":"2024-07-23T06:42:55Z","published":"2024-07-23T06:42:55Z","title":"Diff-Shadow: Global-guided Diffusion Model for Shadow Removal","summary":"  We propose Diff-Shadow, a global-guided diffusion model for high-quality\nshadow removal. Previous transformer-based approaches can utilize global\ninformation to relate shadow and non-shadow regions but are limited in their\nsynthesis ability and recover images with obvious boundaries. In contrast,\ndiffusion-based methods can generate better content but ignore global\ninformation, resulting in inconsistent illumination. In this work, we combine\nthe advantages of diffusion models and global guidance to realize shadow-free\nrestoration. Specifically, we propose a parallel UNets architecture: 1) the\nlocal branch performs the patch-based noise estimation in the diffusion\nprocess, and 2) the global branch recovers the low-resolution shadow-free\nimages. A Reweight Cross Attention (RCA) module is designed to integrate global\ncontextural information of non-shadow regions into the local branch. We further\ndesign a Global-guided Sampling Strategy (GSS) that mitigates patch boundary\nissues and ensures consistent illumination across shaded and unshaded regions\nin the recovered image. Comprehensive experiments on three publicly standard\ndatasets ISTD, ISTD+, and SRD have demonstrated the effectiveness of\nDiff-Shadow. Compared to state-of-the-art methods, our method achieves a\nsignificant improvement in terms of PSNR, increasing from 32.33dB to 33.69dB on\nthe SRD dataset. Codes will be released.\n","authors":["Jinting Luo","Ru Li","Chengzhi Jiang","Mingyan Han","Xiaoming Zhang","Ting Jiang","Haoqiang Fan","Shuaicheng Liu"],"pdf_url":"https://arxiv.org/pdf/2407.16214v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15613v2","updated":"2024-07-23T06:37:15Z","published":"2024-07-22T13:15:04Z","title":"Visual-Semantic Decomposition and Partial Alignment for Document-based\n  Zero-Shot Learning","summary":"  Recent work shows that documents from encyclopedias serve as helpful\nauxiliary information for zero-shot learning. Existing methods align the entire\nsemantics of a document with corresponding images to transfer knowledge.\nHowever, they disregard that semantic information is not equivalent between\nthem, resulting in a suboptimal alignment. In this work, we propose a novel\nnetwork to extract multi-view semantic concepts from documents and images and\nalign the matching rather than entire concepts. Specifically, we propose a\nsemantic decomposition module to generate multi-view semantic embeddings from\nvisual and textual sides, providing the basic concepts for partial alignment.\nTo alleviate the issue of information redundancy among embeddings, we propose\nthe local-to-semantic variance loss to capture distinct local details and\nmultiple semantic diversity loss to enforce orthogonality among embeddings.\nSubsequently, two losses are introduced to partially align visual-semantic\nembedding pairs according to their semantic relevance at the view and\nword-to-patch levels. Consequently, we consistently outperform state-of-the-art\nmethods under two document sources in three standard benchmarks for\ndocument-based zero-shot learning. Qualitatively, we show that our model learns\nthe interpretable partial association.\n","authors":["Xiangyan Qu","Jing Yu","Keke Gai","Jiamin Zhuang","Yuanmin Tang","Gang Xiong","Gaopeng Gou","Qi Wu"],"pdf_url":"https://arxiv.org/pdf/2407.15613v2.pdf","comment":"Accepted to ACM International Conference on Multimedia (MM) 2024"},{"id":"http://arxiv.org/abs/2407.08364v2","updated":"2024-07-23T06:26:34Z","published":"2024-07-11T10:18:54Z","title":"Scalar Function Topology Divergence: Comparing Topology of 3D Objects","summary":"  We propose a new topological tool for computer vision - Scalar Function\nTopology Divergence (SFTD), which measures the dissimilarity of multi-scale\ntopology between sublevel sets of two functions having a common domain.\nFunctions can be defined on an undirected graph or Euclidean space of any\ndimensionality. Most of the existing methods for comparing topology are based\non Wasserstein distance between persistence barcodes and they don't take into\naccount the localization of topological features. The minimization of SFTD\nensures that the corresponding topological features of scalar functions are\nlocated in the same places. The proposed tool provides useful visualizations\ndepicting areas where functions have topological dissimilarities. We provide\napplications of the proposed method to 3D computer vision. In particular,\nexperiments demonstrate that SFTD as an additional loss improves the\nreconstruction of cellular 3D shapes from 2D fluorescence microscopy images,\nand helps to identify topological errors in 3D segmentation. Additionally, we\nshow that SFTD outperforms Betti matching loss in 2D segmentation problems.\n","authors":["Ilya Trofimov","Daria Voronkova","Eduard Tulchinskii","Evgeny Burnaev","Serguei Barannikov"],"pdf_url":"https://arxiv.org/pdf/2407.08364v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14570v2","updated":"2024-07-23T06:20:43Z","published":"2024-07-19T07:04:57Z","title":"Are handcrafted filters helpful for attributing AI-generated images?","summary":"  Recently, a vast number of image generation models have been proposed, which\nraises concerns regarding the misuse of these artificial intelligence (AI)\ntechniques for generating fake images. To attribute the AI-generated images,\nexisting schemes usually design and train deep neural networks (DNNs) to learn\nthe model fingerprints, which usually requires a large amount of data for\neffective learning. In this paper, we aim to answer the following two questions\nfor AI-generated image attribution, 1) is it possible to design useful\nhandcrafted filters to facilitate the fingerprint learning? and 2) how we could\nreduce the amount of training data after we incorporate the handcrafted\nfilters? We first propose a set of Multi-Directional High-Pass Filters (MHFs)\nwhich are capable to extract the subtle fingerprints from various directions.\nThen, we propose a Directional Enhanced Feature Learning network (DEFL) to take\nboth the MHFs and randomly-initialized filters into consideration. The output\nof the DEFL is fused with the semantic features to produce a compact\nfingerprint. To make the compact fingerprint discriminative among different\nmodels, we propose a Dual-Margin Contrastive (DMC) loss to tune our DEFL.\nFinally, we propose a reference based fingerprint classification scheme for\nimage attribution. Experimental results demonstrate that it is indeed helpful\nto use our MHFs for attributing the AI-generated images. The performance of our\nproposed method is significantly better than the state-of-the-art for both the\nclosed-set and open-set image attribution, where only a small amount of images\nare required for training.\n","authors":["Jialiang Li","Haoyue Wang","Sheng Li","Zhenxing Qian","Xinpeng Zhang","Athanasios V. Vasilakos"],"pdf_url":"https://arxiv.org/pdf/2407.14570v2.pdf","comment":"9 pages, 5 figures"},{"id":"http://arxiv.org/abs/2407.16204v1","updated":"2024-07-23T06:12:19Z","published":"2024-07-23T06:12:19Z","title":"CLII: Visual-Text Inpainting via Cross-Modal Predictive Interaction","summary":"  Image inpainting aims to fill missing pixels in damaged images and has\nachieved significant progress with cut-edging learning techniques.\nNevertheless, state-of-the-art inpainting methods are mainly designed for\nnature images and cannot correctly recover text within scene text images, and\ntraining existing models on the scene text images cannot fix the issues. In\nthis work, we identify the visual-text inpainting task to achieve high-quality\nscene text image restoration and text completion: Given a scene text image with\nunknown missing regions and the corresponding text with unknown missing\ncharacters, we aim to complete the missing information in both images and text\nby leveraging their complementary information. Intuitively, the input text,\neven if damaged, contains language priors of the contents within the images and\ncan guide the image inpainting. Meanwhile, the scene text image includes the\nappearance cues of the characters that could benefit text recovery. To this\nend, we design the cross-modal predictive interaction (CLII) model containing\ntwo branches, i.e., ImgBranch and TxtBranch, for scene text inpainting and text\ncompletion, respectively while leveraging their complementary effectively.\nMoreover, we propose to embed our model into the SOTA scene text spotting\nmethod and significantly enhance its robustness against missing pixels, which\ndemonstrates the practicality of the newly developed task. To validate the\neffectiveness of our method, we construct three real datasets based on existing\ntext-related datasets, containing 1838 images and covering three scenarios with\ncurved, incidental, and styled texts, and conduct extensive experiments to show\nthat our method outperforms baselines significantly.\n","authors":["Liang Zhao","Qing Guo","Xiaoguang Li","Song Wang"],"pdf_url":"https://arxiv.org/pdf/2407.16204v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16198v1","updated":"2024-07-23T06:02:30Z","published":"2024-07-23T06:02:30Z","title":"INF-LLaVA: Dual-perspective Perception for High-Resolution Multimodal\n  Large Language Model","summary":"  With advancements in data availability and computing resources, Multimodal\nLarge Language Models (MLLMs) have showcased capabilities across various\nfields. However, the quadratic complexity of the vision encoder in MLLMs\nconstrains the resolution of input images. Most current approaches mitigate\nthis issue by cropping high-resolution images into smaller sub-images, which\nare then processed independently by the vision encoder. Despite capturing\nsufficient local details, these sub-images lack global context and fail to\ninteract with one another. To address this limitation, we propose a novel MLLM,\nINF-LLaVA, designed for effective high-resolution image perception. INF-LLaVA\nincorporates two innovative components. First, we introduce a Dual-perspective\nCropping Module (DCM), which ensures that each sub-image contains continuous\ndetails from a local perspective and comprehensive information from a global\nperspective. Second, we introduce Dual-perspective Enhancement Module (DEM) to\nenable the mutual enhancement of global and local features, allowing INF-LLaVA\nto effectively process high-resolution images by simultaneously capturing\ndetailed local information and comprehensive global context. Extensive ablation\nstudies validate the effectiveness of these components, and experiments on a\ndiverse set of benchmarks demonstrate that INF-LLaVA outperforms existing\nMLLMs. Code and pretrained model are available at\nhttps://github.com/WeihuangLin/INF-LLaVA.\n","authors":["Yiwei Ma","Zhibin Wang","Xiaoshuai Sun","Weihuang Lin","Qiang Zhou","Jiayi Ji","Rongrong Ji"],"pdf_url":"https://arxiv.org/pdf/2407.16198v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16197v1","updated":"2024-07-23T05:53:05Z","published":"2024-07-23T05:53:05Z","title":"LiCROcc: Teach Radar for Accurate Semantic Occupancy Prediction using\n  LiDAR and Camera","summary":"  Semantic Scene Completion (SSC) is pivotal in autonomous driving perception,\nfrequently confronted with the complexities of weather and illumination\nchanges. The long-term strategy involves fusing multi-modal information to\nbolster the system's robustness. Radar, increasingly utilized for 3D target\ndetection, is gradually replacing LiDAR in autonomous driving applications,\noffering a robust sensing alternative. In this paper, we focus on the potential\nof 3D radar in semantic scene completion, pioneering cross-modal refinement\ntechniques for improved robustness against weather and illumination changes,\nand enhancing SSC performance.Regarding model architecture, we propose a\nthree-stage tight fusion approach on BEV to realize a fusion framework for\npoint clouds and images. Based on this foundation, we designed three\ncross-modal distillation modules-CMRD, BRD, and PDD. Our approach enhances the\nperformance in both radar-only (R-LiCROcc) and radar-camera (RC-LiCROcc)\nsettings by distilling to them the rich semantic and structural information of\nthe fused features of LiDAR and camera. Finally, our LC-Fusion (teacher model),\nR-LiCROcc and RC-LiCROcc achieve the best performance on the nuScenes-Occupancy\ndataset, with mIOU exceeding the baseline by 22.9%, 44.1%, and 15.5%,\nrespectively. The project page is available at\nhttps://hr-zju.github.io/LiCROcc/.\n","authors":["Yukai Ma","Jianbiao Mei","Xuemeng Yang","Licheng Wen","Weihua Xu","Jiangning Zhang","Botian Shi","Yong Liu","Xingxing Zuo"],"pdf_url":"https://arxiv.org/pdf/2407.16197v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16193v1","updated":"2024-07-23T05:35:04Z","published":"2024-07-23T05:35:04Z","title":"CloudFixer: Test-Time Adaptation for 3D Point Clouds via\n  Diffusion-Guided Geometric Transformation","summary":"  3D point clouds captured from real-world sensors frequently encompass noisy\npoints due to various obstacles, such as occlusion, limited resolution, and\nvariations in scale. These challenges hinder the deployment of pre-trained\npoint cloud recognition models trained on clean point clouds, leading to\nsignificant performance degradation. While test-time adaptation (TTA)\nstrategies have shown promising results on this issue in the 2D domain, their\napplication to 3D point clouds remains under-explored. Among TTA methods, an\ninput adaptation approach, which directly converts test instances to the source\ndomain using a pre-trained diffusion model, has been proposed in the 2D domain.\nDespite its robust TTA performance in practical situations, naively adopting\nthis into the 3D domain may be suboptimal due to the neglect of inherent\nproperties of point clouds, and its prohibitive computational cost. Motivated\nby these limitations, we propose CloudFixer, a test-time input adaptation\nmethod tailored for 3D point clouds, employing a pre-trained diffusion model.\nSpecifically, CloudFixer optimizes geometric transformation parameters with\ncarefully designed objectives that leverage the geometric properties of point\nclouds. We also substantially improve computational efficiency by avoiding\nbackpropagation through the diffusion model and a prohibitive generation\nprocess. Furthermore, we propose an online model adaptation strategy by\naligning the original model prediction with that of the adapted input.\nExtensive experiments showcase the superiority of CloudFixer over various TTA\nbaselines, excelling in handling common corruptions and natural distribution\nshifts across diverse real-world scenarios. Our code is available at\nhttps://github.com/shimazing/CloudFixer\n","authors":["Hajin Shim","Changhun Kim","Eunho Yang"],"pdf_url":"https://arxiv.org/pdf/2407.16193v1.pdf","comment":"32 pages; Accepted to ECCV2024"},{"id":"http://arxiv.org/abs/2407.16189v1","updated":"2024-07-23T05:31:05Z","published":"2024-07-23T05:31:05Z","title":"EIANet: A Novel Domain Adaptation Approach to Maximize Class Distinction\n  with Neural Collapse Principles","summary":"  Source-free domain adaptation (SFDA) aims to transfer knowledge from a\nlabelled source domain to an unlabelled target domain. A major challenge in\nSFDA is deriving accurate categorical information for the target domain,\nespecially when sample embeddings from different classes appear similar. This\nissue is particularly pronounced in fine-grained visual categorization tasks,\nwhere inter-class differences are subtle. To overcome this challenge, we\nintroduce a novel ETF-Informed Attention Network (EIANet) to separate class\nprototypes by utilizing attention and neural collapse principles. More\nspecifically, EIANet employs a simplex Equiangular Tight Frame (ETF) classifier\nin conjunction with an attention mechanism, facilitating the model to focus on\ndiscriminative features and ensuring maximum class prototype separation. This\ninnovative approach effectively enlarges the feature difference between\ndifferent classes in the latent space by locating salient regions, thereby\npreventing the misclassification of similar but distinct category samples and\nproviding more accurate categorical information to guide the fine-tuning\nprocess on the target domain. Experimental results across four SFDA datasets\nvalidate EIANet's state-of-the-art performance. Code is available at:\nhttps://github.com/zichengpan/EIANet.\n","authors":["Zicheng Pan","Xiaohan Yu","Yongsheng Gao"],"pdf_url":"https://arxiv.org/pdf/2407.16189v1.pdf","comment":"12 pages, 3 figures. Accepted by BMVC2024"},{"id":"http://arxiv.org/abs/2407.16182v1","updated":"2024-07-23T05:09:07Z","published":"2024-07-23T05:09:07Z","title":"No Re-Train, More Gain: Upgrading Backbones with Diffusion Model for\n  Few-Shot Segmentation","summary":"  Few-Shot Segmentation (FSS) aims to segment novel classes using only a few\nannotated images. Despite considerable process under pixel-wise support\nannotation, current FSS methods still face three issues: the inflexibility of\nbackbone upgrade without re-training, the inability to uniformly handle various\ntypes of annotations (e.g., scribble, bounding box, mask and text), and the\ndifficulty in accommodating different annotation quantity. To address these\nissues simultaneously, we propose DiffUp, a novel FSS method that\nconceptualizes the FSS task as a conditional generative problem using a\ndiffusion process. For the first issue, we introduce a backbone-agnostic\nfeature transformation module that converts different segmentation cues into\nunified coarse priors, facilitating seamless backbone upgrade without\nre-training. For the second issue, due to the varying granularity of\ntransformed priors from diverse annotation types, we conceptualize these\nmulti-granular transformed priors as analogous to noisy intermediates at\ndifferent steps of a diffusion model. This is implemented via a\nself-conditioned modulation block coupled with a dual-level quality modulation\nbranch. For the third issue, we incorporates an uncertainty-aware information\nfusion module that harmonizing the variability across zero-shot, one-shot and\nmany-shot scenarios. Evaluated through rigorous benchmarks, DiffUp\nsignificantly outperforms existing FSS models in terms of flexibility and\naccuracy.\n","authors":["Shuai Chen","Fanman Meng","Chenhao Wu","Haoran Wei","Runtong Zhang","Qingbo Wu","Linfeng Xu","Hongliang Li"],"pdf_url":"https://arxiv.org/pdf/2407.16182v1.pdf","comment":"7 figures"},{"id":"http://arxiv.org/abs/2407.15424v2","updated":"2024-07-23T04:45:58Z","published":"2024-07-22T07:01:50Z","title":"Bidirectional skip-frame prediction for video anomaly detection with\n  intra-domain disparity-driven attention","summary":"  With the widespread deployment of video surveillance devices and the demand\nfor intelligent system development, video anomaly detection (VAD) has become an\nimportant part of constructing intelligent surveillance systems. Expanding the\ndiscriminative boundary between normal and abnormal events to enhance\nperformance is the common goal and challenge of VAD. To address this problem,\nwe propose a Bidirectional Skip-frame Prediction (BiSP) network based on a\ndual-stream autoencoder, from the perspective of learning the intra-domain\ndisparity between different features. The BiSP skips frames in the training\nphase to achieve the forward and backward frame prediction respectively, and in\nthe testing phase, it utilizes bidirectional consecutive frames to co-predict\nthe same intermediate frames, thus expanding the degree of disparity between\nnormal and abnormal events. The BiSP designs the variance channel attention and\ncontext spatial attention from the perspectives of movement patterns and object\nscales, respectively, thus ensuring the maximization of the disparity between\nnormal and abnormal in the feature extraction and delivery with different\ndimensions. Extensive experiments from four benchmark datasets demonstrate the\neffectiveness of the proposed BiSP, which substantially outperforms\nstate-of-the-art competing methods.\n","authors":["Jiahao Lyu","Minghua Zhao","Jing Hu","Runtao Xi","Xuewen Huang","Shuangli Du","Cheng Shi","Tian Ma"],"pdf_url":"https://arxiv.org/pdf/2407.15424v2.pdf","comment":"11 pages,7 figures, 4 tables"},{"id":"http://arxiv.org/abs/2407.16174v1","updated":"2024-07-23T04:41:36Z","published":"2024-07-23T04:41:36Z","title":"Pixel Embedding: Fully Quantized Convolutional Neural Network with\n  Differentiable Lookup Table","summary":"  By quantizing network weights and activations to low bitwidth, we can obtain\nhardware-friendly and energy-efficient networks. However, existing quantization\ntechniques utilizing the straight-through estimator and piecewise constant\nfunctions face the issue of how to represent originally high-bit input data\nwith low-bit values. To fully quantize deep neural networks, we propose pixel\nembedding, which replaces each float-valued input pixel with a vector of\nquantized values by using a lookup table. The lookup table or low-bit\nrepresentation of pixels is differentiable and trainable by backpropagation.\nSuch replacement of inputs with vectors is similar to word embedding in the\nnatural language processing field. Experiments on ImageNet and CIFAR-100 show\nthat pixel embedding reduces the top-5 error gap caused by quantizing the\nfloating points at the first layer to only 1% for the ImageNet dataset, and the\ntop-1 error gap caused by quantizing first and last layers to slightly over 1%\nfor the CIFAR-100 dataset. The usefulness of pixel embedding is further\ndemonstrated by inference time measurements, which demonstrate over 1.7 times\nspeedup compared to floating point precision first layer.\n","authors":["Hiroyuki Tokunaga","Joel Nicholls","Daria Vazhenina","Atsunori Kanemura"],"pdf_url":"https://arxiv.org/pdf/2407.16174v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16173v1","updated":"2024-07-23T04:39:04Z","published":"2024-07-23T04:39:04Z","title":"Integrating Meshes and 3D Gaussians for Indoor Scene Reconstruction with\n  SAM Mask Guidance","summary":"  We present a novel approach for 3D indoor scene reconstruction that combines\n3D Gaussian Splatting (3DGS) with mesh representations. We use meshes for the\nroom layout of the indoor scene, such as walls, ceilings, and floors, while\nemploying 3D Gaussians for other objects. This hybrid approach leverages the\nstrengths of both representations, offering enhanced flexibility and ease of\nediting. However, joint training of meshes and 3D Gaussians is challenging\nbecause it is not clear which primitive should affect which part of the\nrendered image. Objects close to the room layout often struggle during\ntraining, particularly when the room layout is textureless, which can lead to\nincorrect optimizations and unnecessary 3D Gaussians. To overcome these\nchallenges, we employ Segment Anything Model (SAM) to guide the selection of\nprimitives. The SAM mask loss enforces each instance to be represented by\neither Gaussians or meshes, ensuring clear separation and stable training.\nFurthermore, we introduce an additional densification stage without resetting\nthe opacity after the standard densification. This stage mitigates the\ndegradation of image quality caused by a limited number of 3D Gaussians after\nthe standard densification.\n","authors":["Jiyeop Kim","Jongwoo Lim"],"pdf_url":"https://arxiv.org/pdf/2407.16173v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16171v1","updated":"2024-07-23T04:35:56Z","published":"2024-07-23T04:35:56Z","title":"Learning Trimodal Relation for AVQA with Missing Modality","summary":"  Recent Audio-Visual Question Answering (AVQA) methods rely on complete visual\nand audio input to answer questions accurately. However, in real-world\nscenarios, issues such as device malfunctions and data transmission errors\nfrequently result in missing audio or visual modality. In such cases, existing\nAVQA methods suffer significant performance degradation. In this paper, we\npropose a framework that ensures robust AVQA performance even when a modality\nis missing. First, we propose a Relation-aware Missing Modal (RMM) generator\nwith Relation-aware Missing Modal Recalling (RMMR) loss to enhance the ability\nof the generator to recall missing modal information by understanding the\nrelationships and context among the available modalities. Second, we design an\nAudio-Visual Relation-aware (AVR) diffusion model with Audio-Visual Enhancing\n(AVE) loss to further enhance audio-visual features by leveraging the\nrelationships and shared cues between the audio-visual modalities. As a result,\nour method can provide accurate answers by effectively utilizing available\ninformation even when input modalities are missing. We believe our method holds\npotential applications not only in AVQA research but also in various\nmulti-modal scenarios.\n","authors":["Kyu Ri Park","Hong Joo Lee","Jung Uk Kim"],"pdf_url":"https://arxiv.org/pdf/2407.16171v1.pdf","comment":"Accepted at ECCV 2024"},{"id":"http://arxiv.org/abs/2407.16165v1","updated":"2024-07-23T04:18:34Z","published":"2024-07-23T04:18:34Z","title":"Advanced AI Framework for Enhanced Detection and Assessment of Abdominal\n  Trauma: Integrating 3D Segmentation with 2D CNN and RNN Models","summary":"  Trauma is a significant cause of mortality and disability, particularly among\nindividuals under forty. Traditional diagnostic methods for traumatic injuries,\nsuch as X-rays, CT scans, and MRI, are often time-consuming and dependent on\nmedical expertise, which can delay critical interventions. This study explores\nthe application of artificial intelligence (AI) and machine learning (ML) to\nimprove the speed and accuracy of abdominal trauma diagnosis. We developed an\nadvanced AI-based model combining 3D segmentation, 2D Convolutional Neural\nNetworks (CNN), and Recurrent Neural Networks (RNN) to enhance diagnostic\nperformance. Our model processes abdominal CT scans to provide real-time,\nprecise assessments, thereby improving clinical decision-making and patient\noutcomes. Comprehensive experiments demonstrated that our approach\nsignificantly outperforms traditional diagnostic methods, as evidenced by\nrigorous evaluation metrics. This research sets a new benchmark for automated\ntrauma detection, leveraging the strengths of AI and ML to revolutionize trauma\ncare.\n","authors":["Liheng Jiang","Xuechun yang","Chang Yu","Zhizhong Wu","Yuting Wang"],"pdf_url":"https://arxiv.org/pdf/2407.16165v1.pdf","comment":"6 Pages"},{"id":"http://arxiv.org/abs/2407.16164v1","updated":"2024-07-23T04:13:52Z","published":"2024-07-23T04:13:52Z","title":"Representation Magnitude has a Liability to Privacy Vulnerability","summary":"  The privacy-preserving approaches to machine learning (ML) models have made\nsubstantial progress in recent years. However, it is still opaque in which\ncircumstances and conditions the model becomes privacy-vulnerable, leading to a\nchallenge for ML models to maintain both performance and privacy. In this\npaper, we first explore the disparity between member and non-member data in the\nrepresentation of models under common training frameworks. We identify how the\nrepresentation magnitude disparity correlates with privacy vulnerability and\naddress how this correlation impacts privacy vulnerability. Based on the\nobservations, we propose Saturn Ring Classifier Module (SRCM), a plug-in\nmodel-level solution to mitigate membership privacy leakage. Through a confined\nyet effective representation space, our approach ameliorates models' privacy\nvulnerability while maintaining generalizability. The code of this work can be\nfound here: \\url{https://github.com/JEKimLab/AIES2024_SRCM}\n","authors":["Xingli Fang","Jung-Eun Kim"],"pdf_url":"https://arxiv.org/pdf/2407.16164v1.pdf","comment":"Accepted in the AAAI/ACM Conference on Artificial Intelligence,\n  Ethics, and Society, 2024"},{"id":"http://arxiv.org/abs/2401.15318v2","updated":"2024-07-23T04:05:53Z","published":"2024-01-27T06:45:22Z","title":"Gaussian Splashing: Unified Particles for Versatile Motion Synthesis and\n  Rendering","summary":"  We demonstrate the feasibility of integrating physics-based animations of\nsolids and fluids with 3D Gaussian Splatting (3DGS) to create novel effects in\nvirtual scenes reconstructed using 3DGS. Leveraging the coherence of the\nGaussian Splatting and Position-Based Dynamics (PBD) in the underlying\nrepresentation, we manage rendering, view synthesis, and the dynamics of solids\nand fluids in a cohesive manner. Similar to GaussianShader, we enhance each\nGaussian kernel with an added normal, aligning the kernel's orientation with\nthe surface normal to refine the PBD simulation. This approach effectively\neliminates spiky noises that arise from rotational deformation in solids. It\nalso allows us to integrate physically based rendering to augment the dynamic\nsurface reflections on fluids. Consequently, our framework is capable of\nrealistically reproducing surface highlights on dynamic fluids and facilitating\ninteractions between scene objects and fluids from new views. For more\ninformation, please visit our project page at\n\\url{https://gaussiansplashing.github.io/}.\n","authors":["Yutao Feng","Xiang Feng","Yintong Shang","Ying Jiang","Chang Yu","Zeshun Zong","Tianjia Shao","Hongzhi Wu","Kun Zhou","Chenfanfu Jiang","Yin Yang"],"pdf_url":"https://arxiv.org/pdf/2401.15318v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.12050v5","updated":"2024-07-23T04:01:08Z","published":"2023-11-18T09:55:56Z","title":"3D-GOI: 3D GAN Omni-Inversion for Multifaceted and Multi-object Editing","summary":"  The current GAN inversion methods typically can only edit the appearance and\nshape of a single object and background while overlooking spatial information.\nIn this work, we propose a 3D editing framework, 3D-GOI, to enable multifaceted\nediting of affine information (scale, translation, and rotation) on multiple\nobjects. 3D-GOI realizes the complex editing function by inverting the\nabundance of attribute codes (object\nshape/appearance/scale/rotation/translation, background shape/appearance, and\ncamera pose) controlled by GIRAFFE, a renowned 3D GAN. Accurately inverting all\nthe codes is challenging, 3D-GOI solves this challenge following three main\nsteps. First, we segment the objects and the background in a multi-object\nimage. Second, we use a custom Neural Inversion Encoder to obtain coarse codes\nof each object. Finally, we use a round-robin optimization algorithm to get\nprecise codes to reconstruct the image. To the best of our knowledge, 3D-GOI is\nthe first framework to enable multifaceted editing on multiple objects. Both\nqualitative and quantitative experiments demonstrate that 3D-GOI holds immense\npotential for flexible, multifaceted editing in complex multi-object scenes.Our\nproject and code are released at https://3d-goi.github.io .\n","authors":["Haoran Li","Long Ma","Haolin Shi","Yanbin Hao","Yong Liao","Lechao Cheng","Pengyuan Zhou"],"pdf_url":"https://arxiv.org/pdf/2311.12050v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16158v1","updated":"2024-07-23T03:56:02Z","published":"2024-07-23T03:56:02Z","title":"Cross-Domain Separable Translation Network for Multimodal Image Change\n  Detection","summary":"  In the remote sensing community, multimodal change detection (MCD) is\nparticularly critical due to its ability to track changes across different\nimaging conditions and sensor types, making it highly applicable to a wide\nrange of real-world scenarios. This paper focuses on addressing the challenges\nof MCD, especially the difficulty in comparing images from different sensors\nwith varying styles and statistical characteristics of geospatial objects.\nTraditional MCD methods often struggle with these variations, leading to\ninaccurate and unreliable results. To overcome these limitations, a novel\nunsupervised cross-domain separable translation network (CSTN) is proposed,\nwhich uniquely integrates a within-domain self-reconstruction and a\ncross-domain image translation and cycle-reconstruction workflow with change\ndetection constraints. The model is optimized by implementing both the tasks of\nimage translation and MCD simultaneously, thereby guaranteeing the\ncomparability of learned features from multimodal images. Specifically, a\nsimple yet efficient dual-branch convolutional architecture is employed to\nseparate the content and style information of multimodal images. This process\ngenerates a style-independent content-comparable feature space, which is\ncrucial for achieving accurate change detection even in the presence of\nsignificant sensor variations. Extensive experimental results demonstrate the\neffectiveness of the proposed method, showing remarkable improvements over\nstate-of-the-art approaches in terms of accuracy and efficacy for MCD. The\nimplementation of our method will be publicly available at\n\\url{https://github.com/OMEGA-RS/CSTN}\n","authors":["Tao Zhan","Yuanyuan Zhu","Jie Lan","Qianlong Dang"],"pdf_url":"https://arxiv.org/pdf/2407.16158v1.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible"},{"id":"http://arxiv.org/abs/2407.15051v2","updated":"2024-07-23T03:51:43Z","published":"2024-07-21T04:39:06Z","title":"Prior Knowledge Integration via LLM Encoding and Pseudo Event Regulation\n  for Video Moment Retrieval","summary":"  In this paper, we investigate the feasibility of leveraging large language\nmodels (LLMs) for integrating general knowledge and incorporating pseudo-events\nas priors for temporal content distribution in video moment retrieval (VMR)\nmodels. The motivation behind this study arises from the limitations of using\nLLMs as decoders for generating discrete textual descriptions, which hinders\ntheir direct application to continuous outputs like salience scores and\ninter-frame embeddings that capture inter-frame relations. To overcome these\nlimitations, we propose utilizing LLM encoders instead of decoders. Through a\nfeasibility study, we demonstrate that LLM encoders effectively refine\ninter-concept relations in multimodal embeddings, even without being trained on\ntextual embeddings. We also show that the refinement capability of LLM encoders\ncan be transferred to other embeddings, such as BLIP and T5, as long as these\nembeddings exhibit similar inter-concept similarity patterns to CLIP\nembeddings. We present a general framework for integrating LLM encoders into\nexisting VMR architectures, specifically within the fusion module. Through\nexperimental validation, we demonstrate the effectiveness of our proposed\nmethods by achieving state-of-the-art performance in VMR. The source code can\nbe accessed at https://github.com/fletcherjiang/LLMEPET.\n","authors":["Yiyang Jiang","Wengyu Zhang","Xulu Zhang","Xiaoyong Wei","Chang Wen Chen","Qing Li"],"pdf_url":"https://arxiv.org/pdf/2407.15051v2.pdf","comment":"Accepted to ACM Multimedia 2024"},{"id":"http://arxiv.org/abs/2407.16145v1","updated":"2024-07-23T03:09:42Z","published":"2024-07-23T03:09:42Z","title":"Improved Few-Shot Image Classification Through Multiple-Choice Questions","summary":"  Through a simple multiple choice language prompt a VQA model can operate as a\nzero-shot image classifier, producing a classification label. Compared to\ntypical image encoders, VQA models offer an advantage: VQA-produced image\nembeddings can be infused with the most relevant visual information through\ntailored language prompts. Nevertheless, for most tasks, zero-shot VQA\nperformance is lacking, either because of unfamiliar category names, or\ndissimilar pre-training data and test data distributions. We propose a simple\nmethod to boost VQA performance for image classification using only a handful\nof labeled examples and a multiple-choice question. This few-shot method is\ntraining-free and maintains the dynamic and flexible advantages of the VQA\nmodel. Rather than relying on the final language output, our approach uses\nmultiple-choice questions to extract prompt-specific latent representations,\nwhich are enriched with relevant visual information. These representations are\ncombined to create a final overall image embedding, which is decoded via\nreference to latent class prototypes constructed from the few labeled examples.\nWe demonstrate this method outperforms both pure visual encoders and zero-shot\nVQA baselines to achieve impressive performance on common few-shot tasks\nincluding MiniImageNet, Caltech-UCSD Birds, and CIFAR-100. Finally, we show our\napproach does particularly well in settings with numerous diverse visual\nattributes such as the fabric, article-style, texture, and view of different\narticles of clothing, where other few-shot approaches struggle, as we can\ntailor our image representations only on the semantic features of interest.\n","authors":["Dipika Khullar","Emmett Goodman","Negin Sokhandan"],"pdf_url":"https://arxiv.org/pdf/2407.16145v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.02887v3","updated":"2024-07-23T03:03:54Z","published":"2024-07-03T08:03:56Z","title":"Explicitly Guided Information Interaction Network for Cross-modal Point\n  Cloud Completion","summary":"  In this paper, we explore a novel framework, EGIInet (Explicitly Guided\nInformation Interaction Network), a model for View-guided Point cloud\nCompletion (ViPC) task, which aims to restore a complete point cloud from a\npartial one with a single view image. In comparison with previous methods that\nrelied on the global semantics of input images, EGIInet efficiently combines\nthe information from two modalities by leveraging the geometric nature of the\ncompletion task. Specifically, we propose an explicitly guided information\ninteraction strategy supported by modal alignment for point cloud completion.\nFirst, in contrast to previous methods which simply use 2D and 3D backbones to\nencode features respectively, we unified the encoding process to promote modal\nalignment. Second, we propose a novel explicitly guided information interaction\nstrategy that could help the network identify critical information within\nimages, thus achieving better guidance for completion. Extensive experiments\ndemonstrate the effectiveness of our framework, and we achieved a new\nstate-of-the-art (+16% CD over XMFnet) in benchmark datasets despite using\nfewer parameters than the previous methods. The pre-trained model and code and\nare available at https://github.com/WHU-USI3DV/EGIInet.\n","authors":["Hang Xu","Chen Long","Wenxiao Zhang","Yuan Liu","Zhen Cao","Zhen Dong","Bisheng Yang"],"pdf_url":"https://arxiv.org/pdf/2407.02887v3.pdf","comment":"ECCV 2024"},{"id":"http://arxiv.org/abs/2407.16142v1","updated":"2024-07-23T03:00:01Z","published":"2024-07-23T03:00:01Z","title":"Diffusion Models as Optimizers for Efficient Planning in Offline RL","summary":"  Diffusion models have shown strong competitiveness in offline reinforcement\nlearning tasks by formulating decision-making as sequential generation.\nHowever, the practicality of these methods is limited due to the lengthy\ninference processes they require. In this paper, we address this problem by\ndecomposing the sampling process of diffusion models into two decoupled\nsubprocesses: 1) generating a feasible trajectory, which is a time-consuming\nprocess, and 2) optimizing the trajectory. With this decomposition approach, we\nare able to partially separate efficiency and quality factors, enabling us to\nsimultaneously gain efficiency advantages and ensure quality assurance. We\npropose the Trajectory Diffuser, which utilizes a faster autoregressive model\nto handle the generation of feasible trajectories while retaining the\ntrajectory optimization process of diffusion models. This allows us to achieve\nmore efficient planning without sacrificing capability. To evaluate the\neffectiveness and efficiency of the Trajectory Diffuser, we conduct experiments\non the D4RL benchmarks. The results demonstrate that our method achieves $\\it\n3$-$\\it 10 \\times$ faster inference speed compared to previous sequence\nmodeling methods, while also outperforming them in terms of overall\nperformance. https://github.com/RenMing-Huang/TrajectoryDiffuser\n  Keywords: Reinforcement Learning and Efficient Planning and Diffusion Model\n","authors":["Renming Huang","Yunqiang Pei","Guoqing Wang","Yangming Zhang","Yang Yang","Peng Wang","Hengtao Shen"],"pdf_url":"https://arxiv.org/pdf/2407.16142v1.pdf","comment":"The paper was accepted by ECCV2024"},{"id":"http://arxiv.org/abs/2407.16137v1","updated":"2024-07-23T02:50:27Z","published":"2024-07-23T02:50:27Z","title":"3D-UGCN: A Unified Graph Convolutional Network for Robust 3D Human Pose\n  Estimation from Monocular RGB Images","summary":"  Human pose estimation remains a multifaceted challenge in computer vision,\npivotal across diverse domains such as behavior recognition, human-computer\ninteraction, and pedestrian tracking. This paper proposes an improved method\nbased on the spatial-temporal graph convolution net-work (UGCN) to address the\nissue of missing human posture skeleton sequences in single-view videos. We\npresent the improved UGCN, which allows the network to process 3D human pose\ndata and improves the 3D human pose skeleton sequence, thereby resolving the\nocclusion issue.\n","authors":["Jie Zhao","Jianing Li","Weihan Chen","Wentong Wang","Pengfei Yuan","Xu Zhang","Deshu Peng"],"pdf_url":"https://arxiv.org/pdf/2407.16137v1.pdf","comment":"Proceedings of IEEE AICON2024"},{"id":"http://arxiv.org/abs/2407.15728v2","updated":"2024-07-23T23:53:03Z","published":"2024-07-22T15:31:18Z","title":"SAM2CLIP2SAM: Vision Language Model for Segmentation of 3D CT Scans for\n  Covid-19 Detection","summary":"  This paper presents a new approach for effective segmentation of images that\ncan be integrated into any model and methodology; the paradigm that we choose\nis classification of medical images (3-D chest CT scans) for Covid-19\ndetection. Our approach includes a combination of vision-language models that\nsegment the CT scans, which are then fed to a deep neural architecture, named\nRACNet, for Covid-19 detection. In particular, a novel framework, named\nSAM2CLIP2SAM, is introduced for segmentation that leverages the strengths of\nboth Segment Anything Model (SAM) and Contrastive Language-Image Pre-Training\n(CLIP) to accurately segment the right and left lungs in CT scans, subsequently\nfeeding these segmented outputs into RACNet for classification of COVID-19 and\nnon-COVID-19 cases. At first, SAM produces multiple part-based segmentation\nmasks for each slice in the CT scan; then CLIP selects only the masks that are\nassociated with the regions of interest (ROIs), i.e., the right and left lungs;\nfinally SAM is given these ROIs as prompts and generates the final segmentation\nmask for the lungs. Experiments are presented across two Covid-19 annotated\ndatabases which illustrate the improved performance obtained when our method\nhas been used for segmentation of the CT scans.\n","authors":["Dimitrios Kollias","Anastasios Arsenos","James Wingate","Stefanos Kollias"],"pdf_url":"https://arxiv.org/pdf/2407.15728v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10126v4","updated":"2024-07-23T23:52:19Z","published":"2023-03-17T17:07:36Z","title":"IRGen: Generative Modeling for Image Retrieval","summary":"  While generative modeling has become prevalent across numerous research\nfields, its integration into the realm of image retrieval remains largely\nunexplored and underjustified. In this paper, we present a novel methodology,\nreframing image retrieval as a variant of generative modeling and employing a\nsequence-to-sequence model. This approach is harmoniously aligned with the\ncurrent trend towards unification in research, presenting a cohesive framework\nthat allows for end-to-end differentiable searching. This, in turn, facilitates\nsuperior performance via direct optimization techniques. The development of our\nmodel, dubbed IRGen, addresses the critical technical challenge of converting\nan image into a concise sequence of semantic units, which is pivotal for\nenabling efficient and effective search. Extensive experiments demonstrate that\nour model achieves state-of-the-art performance on three widely-used image\nretrieval benchmarks as well as two million-scale datasets, yielding\nsignificant improvement compared to prior competitive retrieval methods. In\naddition, the notable surge in precision scores facilitated by generative\nmodeling presents the potential to bypass the reranking phase, which is\ntraditionally indispensable in practical retrieval workflows.\n","authors":["Yidan Zhang","Ting Zhang","Dong Chen","Yujing Wang","Qi Chen","Xing Xie","Hao Sun","Weiwei Deng","Qi Zhang","Fan Yang","Mao Yang","Qingmin Liao","Jingdong Wang","Baining Guo"],"pdf_url":"https://arxiv.org/pdf/2303.10126v4.pdf","comment":"Accepted by ECCV 2024"},{"id":"http://arxiv.org/abs/2407.15798v2","updated":"2024-07-23T23:49:26Z","published":"2024-07-22T17:00:02Z","title":"Robust Facial Reactions Generation: An Emotion-Aware Framework with\n  Modality Compensation","summary":"  The objective of the Multiple Appropriate Facial Reaction Generation (MAFRG)\ntask is to produce contextually appropriate and diverse listener facial\nbehavioural responses based on the multimodal behavioural data of the\nconversational partner (i.e., the speaker). Current methodologies typically\nassume continuous availability of speech and facial modality data, neglecting\nreal-world scenarios where these data may be intermittently unavailable, which\noften results in model failures. Furthermore, despite utilising advanced deep\nlearning models to extract information from the speaker's multimodal inputs,\nthese models fail to adequately leverage the speaker's emotional context, which\nis vital for eliciting appropriate facial reactions from human listeners. To\naddress these limitations, we propose an Emotion-aware Modality Compensatory\n(EMC) framework. This versatile solution can be seamlessly integrated into\nexisting models, thereby preserving their advantages while significantly\nenhancing performance and robustness in scenarios with missing modalities. Our\nframework ensures resilience when faced with missing modality data through the\nCompensatory Modality Alignment (CMA) module. It also generates more\nappropriate emotion-aware reactions via the Emotion-aware Attention (EA)\nmodule, which incorporates the speaker's emotional information throughout the\nentire encoding and decoding process. Experimental results demonstrate that our\nframework improves the appropriateness metric FRCorr by an average of 57.2\\%\ncompared to the original model structure. In scenarios where speech modality\ndata is missing, the performance of appropriate generation shows an\nimprovement, and when facial data is missing, it only exhibits minimal\ndegradation.\n","authors":["Guanyu Hu","Jie Wei","Siyang Song","Dimitrios Kollias","Xinyu Yang","Zhonglin Sun","Odysseus Kaloidas"],"pdf_url":"https://arxiv.org/pdf/2407.15798v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16875v1","updated":"2024-07-23T22:47:32Z","published":"2024-07-23T22:47:32Z","title":"PathwayBench: Assessing Routability of Pedestrian Pathway Networks\n  Inferred from Multi-City Imagery","summary":"  Applications to support pedestrian mobility in urban areas require a\ncomplete, and routable graph representation of the built environment. Globally\navailable information, including aerial imagery provides a scalable source for\nconstructing these path networks, but the associated learning problem is\nchallenging: Relative to road network pathways, pedestrian network pathways are\nnarrower, more frequently disconnected, often visually and materially variable\nin smaller areas, and their boundaries are broken up by driveway incursions,\nalleyways, marked or unmarked crossings through roadways. Existing algorithms\nto extract pedestrian pathway network graphs are inconsistently evaluated and\ntend to ignore routability, making it difficult to assess utility for mobility\napplications: Even if all path segments are available, discontinuities could\ndramatically and arbitrarily shift the overall path taken by a pedestrian. In\nthis paper, we describe a first standard benchmark for the pedestrian pathway\ngraph extraction problem, comprising the largest available dataset equipped\nwith manually vetted ground truth annotations (covering $3,000 km^2$ land area\nin regions from 8 cities), and a family of evaluation metrics centering\nroutability and downstream utility. By partitioning the data into polygons at\nthe scale of individual intersections, we compute local routability as an\nefficient proxy for global routability. We consider multiple measures of\npolygon-level routability and compare predicted measures with ground truth to\nconstruct evaluation metrics. Using these metrics, we show that this benchmark\ncan surface strengths and weaknesses of existing methods that are hidden by\nsimple edge-counting metrics over single-region datasets used in prior work,\nrepresenting a challenging, high-impact problem in computer vision and machine\nlearning.\n","authors":["Yuxiang Zhang","Bill Howe","Sachin Mehta","Nicholas-J Bolten","Anat Caspi"],"pdf_url":"https://arxiv.org/pdf/2407.16875v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2303.02323"},{"id":"http://arxiv.org/abs/2407.16874v1","updated":"2024-07-23T22:46:21Z","published":"2024-07-23T22:46:21Z","title":"Vision-Based Adaptive Robotics for Autonomous Surface Crack Repair","summary":"  Surface cracks in infrastructure can lead to significant deterioration and\ncostly maintenance if not efficiently repaired. Manual repair methods are\nlabor-intensive, time-consuming, and imprecise and thus difficult to scale to\nlarge areas. Breakthroughs in robotic perception and manipulation have advanced\nautonomous crack repair, but proposed methods lack end-to-end testing and\nadaptability to changing crack size. This paper presents an adaptive,\nautonomous system for surface crack detection and repair using robotics with\nadvanced sensing technologies. The system uses an RGB-D camera for crack\ndetection, a laser scanner for precise measurement, and an extruder and pump\nfor material deposition. A novel validation procedure with 3D-printed crack\nspecimens simulates real-world cracks and ensures testing repeatability. Our\nstudy shows that an adaptive system for crack filling is more efficient and\neffective than a fixed-speed approach, with experimental results confirming\nprecision and consistency. This research paves the way for versatile, reliable\nrobotic infrastructure maintenance.\n","authors":["Joshua Genova","Eric Cabrera","Vedhus Hoskere"],"pdf_url":"https://arxiv.org/pdf/2407.16874v1.pdf","comment":"20 pages, 13 figures, submitted to Automation in Construction"},{"id":"http://arxiv.org/abs/2403.19888v4","updated":"2024-07-23T21:33:06Z","published":"2024-03-29T00:05:13Z","title":"MambaMixer: Efficient Selective State Space Models with Dual Token and\n  Channel Selection","summary":"  Recent advances in deep learning have mainly relied on Transformers due to\ntheir data dependency and ability to learn at scale. The attention module in\nthese architectures, however, exhibits quadratic time and space in input size,\nlimiting their scalability for long-sequence modeling. Despite recent attempts\nto design efficient and effective architecture backbone for multi-dimensional\ndata, such as images and multivariate time series, existing models are either\ndata independent, or fail to allow inter- and intra-dimension communication.\nRecently, State Space Models (SSMs), and more specifically Selective State\nSpace Models, with efficient hardware-aware implementation, have shown\npromising potential for long sequence modeling. Motivated by the success of\nSSMs, we present MambaMixer, a new architecture with data-dependent weights\nthat uses a dual selection mechanism across tokens and channels, called\nSelective Token and Channel Mixer. MambaMixer connects selective mixers using a\nweighted averaging mechanism, allowing layers to have direct access to early\nfeatures. As a proof of concept, we design Vision MambaMixer (ViM2) and Time\nSeries MambaMixer (TSM2) architectures based on the MambaMixer block and\nexplore their performance in various vision and time series forecasting tasks.\nOur results underline the importance of selective mixing across both tokens and\nchannels. In ImageNet classification, object detection, and semantic\nsegmentation tasks, ViM2 achieves competitive performance with well-established\nvision models and outperforms SSM-based vision models. In time series\nforecasting, TSM2 achieves outstanding performance compared to state-of-the-art\nmethods while demonstrating significantly improved computational cost. These\nresults show that while Transformers, cross-channel attention, and MLPs are\nsufficient for good performance in time series forecasting, neither is\nnecessary.\n","authors":["Ali Behrouz","Michele Santacatterina","Ramin Zabih"],"pdf_url":"https://arxiv.org/pdf/2403.19888v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16837v1","updated":"2024-07-23T21:02:38Z","published":"2024-07-23T21:02:38Z","title":"CompBench: A Comparative Reasoning Benchmark for Multimodal LLMs","summary":"  The ability to compare objects, scenes, or situations is crucial for\neffective decision-making and problem-solving in everyday life. For instance,\ncomparing the freshness of apples enables better choices during grocery\nshopping, while comparing sofa designs helps optimize the aesthetics of our\nliving space. Despite its significance, the comparative capability is largely\nunexplored in artificial general intelligence (AGI). In this paper, we\nintroduce CompBench, a benchmark designed to evaluate the comparative reasoning\ncapability of multimodal large language models (MLLMs). CompBench mines and\npairs images through visually oriented questions covering eight dimensions of\nrelative comparison: visual attribute, existence, state, emotion, temporality,\nspatiality, quantity, and quality. We curate a collection of around 40K image\npairs using metadata from diverse vision datasets and CLIP similarity scores.\nThese image pairs span a broad array of visual domains, including animals,\nfashion, sports, and both outdoor and indoor scenes. The questions are\ncarefully crafted to discern relative characteristics between two images and\nare labeled by human annotators for accuracy and relevance. We use CompBench to\nevaluate recent MLLMs, including GPT-4V(ision), Gemini-Pro, and LLaVA-1.6. Our\nresults reveal notable shortcomings in their comparative abilities. We believe\nCompBench not only sheds light on these limitations but also establishes a\nsolid foundation for future enhancements in the comparative capability of\nMLLMs.\n","authors":["Jihyung Kil","Zheda Mai","Justin Lee","Zihe Wang","Kerrie Cheng","Lemeng Wang","Ye Liu","Arpita Chowdhury","Wei-Lun Chao"],"pdf_url":"https://arxiv.org/pdf/2407.16837v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16834v1","updated":"2024-07-23T20:55:25Z","published":"2024-07-23T20:55:25Z","title":"A Multi-Level Hierarchical Framework for the Classification of Weather\n  Conditions and Hazard Prediction","summary":"  This paper presents a multilevel hierarchical framework for the\nclassification of weather conditions and hazard prediction. In recent years,\nthe importance of data has grown significantly, with various types like text,\nnumbers, images, audio, and videos playing a key role. Among these, images make\nup a large portion of the data available. This application shows promise for\nvarious purposes, especially when combined with decision support systems for\ntraffic management, afforestation, and weather forecasting. It's particularly\nuseful in situations where traditional weather predictions are not very\naccurate, such as ensuring the safe operation of self driving cars in dangerous\nweather. While previous studies have looked at this topic with fewer\ncategories, this paper focuses on eleven specific types of weather images. The\ngoal is to create a model that can accurately predict weather conditions after\nbeing trained on a large dataset of images. Accuracy is crucial in real-life\nsituations to prevent accidents, making it the top priority for this paper.\nThis work lays the groundwork for future applications in weather prediction,\nespecially in situations where human expertise is not available or may be\nbiased. The framework, capable of classifying images into eleven weather\ncategories: dew, frost, glaze, rime, snow, hail, rain, lightning, rainbow, and\nsandstorm, provides real-time weather information with an accuracy of 0.9329.\nThe proposed framework addresses the growing need for accurate weather\nclassification and hazard prediction, offering a robust solution for various\napplications in the field.\n","authors":["Harish Neelam"],"pdf_url":"https://arxiv.org/pdf/2407.16834v1.pdf","comment":"6 pages"},{"id":"http://arxiv.org/abs/2407.16829v1","updated":"2024-07-23T20:40:17Z","published":"2024-07-23T20:40:17Z","title":"PlantTrack: Task-Driven Plant Keypoint Tracking with Zero-Shot Sim2Real\n  Transfer","summary":"  Tracking plant features is crucial for various agricultural tasks like\nphenotyping, pruning, or harvesting, but the unstructured, cluttered, and\ndeformable nature of plant environments makes it a challenging task. In this\ncontext, the recent advancements in foundational models show promise in\naddressing this challenge. In our work, we propose PlantTrack where we utilize\nDINOv2 which provides high-dimensional features, and train a keypoint heatmap\npredictor network to identify the locations of semantic features such as fruits\nand leaves which are then used as prompts for point tracking across video\nframes using TAPIR. We show that with as few as 20 synthetic images for\ntraining the keypoint predictor, we achieve zero-shot Sim2Real transfer,\nenabling effective tracking of plant features in real environments.\n","authors":["Samhita Marri","Arun N. Sivakumar","Naveen K. Uppalapati","Girish Chowdhary"],"pdf_url":"https://arxiv.org/pdf/2407.16829v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.16254v3","updated":"2024-07-23T20:38:11Z","published":"2023-11-27T19:02:17Z","title":"Safe-CLIP: Removing NSFW Concepts from Vision-and-Language Models","summary":"  Large-scale vision-and-language models, such as CLIP, are typically trained\non web-scale data, which can introduce inappropriate content and lead to the\ndevelopment of unsafe and biased behavior. This, in turn, hampers their\napplicability in sensitive and trustworthy contexts and could raise significant\nconcerns in their adoption. Our research introduces a novel approach to\nenhancing the safety of vision-and-language models by diminishing their\nsensitivity to NSFW (not safe for work) inputs. In particular, our methodology\nseeks to sever \"toxic\" linguistic and visual concepts, unlearning the linkage\nbetween unsafe linguistic or visual items and unsafe regions of the embedding\nspace. We show how this can be done by fine-tuning a CLIP model on synthetic\ndata obtained from a large language model trained to convert between safe and\nunsafe sentences, and a text-to-image generator. We conduct extensive\nexperiments on the resulting embedding space for cross-modal retrieval,\ntext-to-image, and image-to-text generation, where we show that our model can\nbe remarkably employed with pre-trained generative models. Our source code and\ntrained models are available at: https://github.com/aimagelab/safe-clip.\n","authors":["Samuele Poppi","Tobia Poppi","Federico Cocchi","Marcella Cornia","Lorenzo Baraldi","Rita Cucchiara"],"pdf_url":"https://arxiv.org/pdf/2311.16254v3.pdf","comment":"ECCV 2024"},{"id":"http://arxiv.org/abs/2407.16826v1","updated":"2024-07-23T20:34:23Z","published":"2024-07-23T20:34:23Z","title":"SINDER: Repairing the Singular Defects of DINOv2","summary":"  Vision Transformer models trained on large-scale datasets, although\neffective, often exhibit artifacts in the patch token they extract. While such\ndefects can be alleviated by re-training the entire model with additional\nclassification tokens, the underlying reasons for the presence of these tokens\nremain unclear. In this paper, we conduct a thorough investigation of this\nphenomenon, combining theoretical analysis with empirical observations. Our\nfindings reveal that these artifacts originate from the pre-trained network\nitself, specifically stemming from the leading left singular vector of the\nnetwork's weights. Furthermore, to mitigate these defects, we propose a novel\nfine-tuning smooth regularization that rectifies structural deficiencies using\nonly a small dataset, thereby avoiding the need for complete re-training. We\nvalidate our method on various downstream tasks, including unsupervised\nsegmentation, classification, supervised segmentation, and depth estimation,\ndemonstrating its effectiveness in improving model performance. Codes and\ncheckpoints are available at https://github.com/haoqiwang/sinder.\n","authors":["Haoqi Wang","Tong Zhang","Mathieu Salzmann"],"pdf_url":"https://arxiv.org/pdf/2407.16826v1.pdf","comment":"ECCV 2024"},{"id":"http://arxiv.org/abs/2407.16822v1","updated":"2024-07-23T20:27:16Z","published":"2024-07-23T20:27:16Z","title":"AI-Enhanced 7-Point Checklist for Melanoma Detection Using Clinical\n  Knowledge Graphs and Data-Driven Quantification","summary":"  The 7-point checklist (7PCL) is widely used in dermoscopy to identify\nmalignant melanoma lesions needing urgent medical attention. It assigns point\nvalues to seven attributes: major attributes are worth two points each, and\nminor ones are worth one point each. A total score of three or higher prompts\nfurther evaluation, often including a biopsy. However, a significant limitation\nof current methods is the uniform weighting of attributes, which leads to\nimprecision and neglects their interconnections. Previous deep learning studies\nhave treated the prediction of each attribute with the same importance as\npredicting melanoma, which fails to recognize the clinical significance of the\nattributes for melanoma. To address these limitations, we introduce a novel\ndiagnostic method that integrates two innovative elements: a Clinical\nKnowledge-Based Topological Graph (CKTG) and a Gradient Diagnostic Strategy\nwith Data-Driven Weighting Standards (GD-DDW). The CKTG integrates 7PCL\nattributes with diagnostic information, revealing both internal and external\nassociations. By employing adaptive receptive domains and weighted edges, we\nestablish connections among melanoma's relevant features. Concurrently, GD-DDW\nemulates dermatologists' diagnostic processes, who first observe the visual\ncharacteristics associated with melanoma and then make predictions. Our model\nuses two imaging modalities for the same lesion, ensuring comprehensive feature\nacquisition. Our method shows outstanding performance in predicting malignant\nmelanoma and its features, achieving an average AUC value of 85%. This was\nvalidated on the EDRA dataset, the largest publicly available dataset for the\n7-point checklist algorithm. Specifically, the integrated weighting system can\nprovide clinicians with valuable data-driven benchmarks for their evaluations.\n","authors":["Yuheng Wang","Tianze Yu","Jiayue Cai","Sunil Kalia","Harvey Lui","Z. Jane Wang","Tim K. Lee"],"pdf_url":"https://arxiv.org/pdf/2407.16822v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08967v2","updated":"2024-07-23T20:14:17Z","published":"2024-03-13T21:19:12Z","title":"PathM3: A Multimodal Multi-Task Multiple Instance Learning Framework for\n  Whole Slide Image Classification and Captioning","summary":"  In the field of computational histopathology, both whole slide images (WSIs)\nand diagnostic captions provide valuable insights for making diagnostic\ndecisions. However, aligning WSIs with diagnostic captions presents a\nsignificant challenge. This difficulty arises from two main factors: 1)\nGigapixel WSIs are unsuitable for direct input into deep learning models, and\nthe redundancy and correlation among the patches demand more attention; and 2)\nAuthentic WSI diagnostic captions are extremely limited, making it difficult to\ntrain an effective model. To overcome these obstacles, we present PathM3, a\nmultimodal, multi-task, multiple instance learning (MIL) framework for WSI\nclassification and captioning. PathM3 adapts a query-based transformer to\neffectively align WSIs with diagnostic captions. Given that histopathology\nvisual patterns are redundantly distributed across WSIs, we aggregate each\npatch feature with MIL method that considers the correlations among instances.\nFurthermore, our PathM3 overcomes data scarcity in WSI-level captions by\nleveraging limited WSI diagnostic caption data in the manner of multi-task\njoint learning. Extensive experiments with improved classification accuracy and\ncaption generation demonstrate the effectiveness of our method on both WSI\nclassification and captioning task.\n","authors":["Qifeng Zhou","Wenliang Zhong","Yuzhi Guo","Michael Xiao","Hehuan Ma","Junzhou Huang"],"pdf_url":"https://arxiv.org/pdf/2403.08967v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.03873v2","updated":"2024-07-23T19:43:09Z","published":"2024-06-06T09:04:48Z","title":"Quantum Implicit Neural Representations","summary":"  Implicit neural representations have emerged as a powerful paradigm to\nrepresent signals such as images and sounds. This approach aims to utilize\nneural networks to parameterize the implicit function of the signal. However,\nwhen representing implicit functions, traditional neural networks such as\nReLU-based multilayer perceptrons face challenges in accurately modeling\nhigh-frequency components of signals. Recent research has begun to explore the\nuse of Fourier Neural Networks (FNNs) to overcome this limitation. In this\npaper, we propose Quantum Implicit Representation Network (QIREN), a novel\nquantum generalization of FNNs. Furthermore, through theoretical analysis, we\ndemonstrate that QIREN possesses a quantum advantage over classical FNNs.\nLastly, we conducted experiments in signal representation, image\nsuperresolution, and image generation tasks to show the superior performance of\nQIREN compared to state-of-the-art (SOTA) models. Our work not only\nincorporates quantum advantages into implicit neural representations but also\nuncovers a promising application direction for Quantum Neural Networks.\n","authors":["Jiaming Zhao","Wenbo Qiao","Peng Zhang","Hui Gao"],"pdf_url":"https://arxiv.org/pdf/2406.03873v2.pdf","comment":"This paper was accepted by icml 2024"},{"id":"http://arxiv.org/abs/2407.15799v2","updated":"2024-07-23T19:09:23Z","published":"2024-07-22T17:04:21Z","title":"Adaptive Extensions of Unbiased Risk Estimators for Unsupervised\n  Magnetic Resonance Image Denoising","summary":"  The application of Deep Neural Networks (DNNs) to image denoising has notably\nchallenged traditional denoising methods, particularly within complex noise\nscenarios prevalent in medical imaging. Despite the effectiveness of\ntraditional and some DNN-based methods, their reliance on high-quality,\nnoiseless ground truth images limits their practical utility. In response to\nthis, our work introduces and benchmarks innovative unsupervised learning\nstrategies, notably Stein's Unbiased Risk Estimator (SURE), its extension\n(eSURE), and our novel implementation, the Extended Poisson Unbiased Risk\nEstimator (ePURE), within medical imaging frameworks.\n  This paper presents a comprehensive evaluation of these methods on MRI data\nafflicted with Gaussian and Poisson noise types, a scenario typical in medical\nimaging but challenging for most denoising algorithms. Our main contribution\nlies in the effective adaptation and implementation of the SURE, eSURE, and\nparticularly the ePURE frameworks for medical images, showcasing their\nrobustness and efficacy in environments where traditional noiseless ground\ntruth cannot be obtained.\n","authors":["Reeshad Khan","Dr. John Gauch","Dr. Ukash Nakarmi"],"pdf_url":"https://arxiv.org/pdf/2407.15799v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16803v1","updated":"2024-07-23T19:06:44Z","published":"2024-07-23T19:06:44Z","title":"Fusion and Cross-Modal Transfer for Zero-Shot Human Action Recognition","summary":"  Despite living in a multi-sensory world, most AI models are limited to\ntextual and visual interpretations of human motion and behavior. Inertial\nmeasurement units (IMUs) provide a salient signal to understand human motion;\nhowever, they are challenging to use due to their uninterpretability and\nscarcity of their data. We investigate a method to transfer knowledge between\nvisual and inertial modalities using the structure of an informative joint\nrepresentation space designed for human action recognition (HAR). We apply the\nresulting Fusion and Cross-modal Transfer (FACT) method to a novel setup, where\nthe model does not have access to labeled IMU data during training and is able\nto perform HAR with only IMU data during testing. Extensive experiments on a\nwide range of RGB-IMU datasets demonstrate that FACT significantly outperforms\nexisting methods in zero-shot cross-modal transfer.\n","authors":["Abhi Kamboj","Anh Duy Nguyen","Minh Do"],"pdf_url":"https://arxiv.org/pdf/2407.16803v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16802v1","updated":"2024-07-23T19:06:15Z","published":"2024-07-23T19:06:15Z","title":"Distribution-Aware Robust Learning from Long-Tailed Data with Noisy\n  Labels","summary":"  Deep neural networks have demonstrated remarkable advancements in various\nfields using large, well-annotated datasets. However, real-world data often\nexhibit long-tailed distributions and label noise, significantly degrading\ngeneralization performance. Recent studies addressing these issues have focused\non noisy sample selection methods that estimate the centroid of each class\nbased on high-confidence samples within each target class. The performance of\nthese methods is limited because they use only the training samples within each\nclass for class centroid estimation, making the quality of centroids\nsusceptible to long-tailed distributions and noisy labels. In this study, we\npresent a robust training framework called Distribution-aware Sample Selection\nand Contrastive Learning (DaSC). Specifically, DaSC introduces a\nDistribution-aware Class Centroid Estimation (DaCC) to generate enhanced class\ncentroids. DaCC performs weighted averaging of the features from all samples,\nwith weights determined based on model predictions. Additionally, we propose a\nconfidence-aware contrastive learning strategy to obtain balanced and robust\nrepresentations. The training samples are categorized into high-confidence and\nlow-confidence samples. Our method then applies Semi-supervised Balanced\nContrastive Loss (SBCL) using high-confidence samples, leveraging reliable\nlabel information to mitigate class bias. For the low-confidence samples, our\nmethod computes Mixup-enhanced Instance Discrimination Loss (MIDL) to improve\ntheir representations in a self-supervised manner. Our experimental results on\nCIFAR and real-world noisy-label datasets demonstrate the superior performance\nof the proposed DaSC compared to previous approaches.\n","authors":["Jae Soon Baik","In Young Yoon","Kun Hoon Kim","Jun Won Choi"],"pdf_url":"https://arxiv.org/pdf/2407.16802v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.11677v2","updated":"2024-07-23T18:48:06Z","published":"2024-07-16T12:52:32Z","title":"Video-Language Alignment via Spatio-Temporal Graph Transformer","summary":"  Video-language alignment is a crucial multi-modal task that benefits various\ndownstream applications, e.g., video-text retrieval and video question\nanswering. Existing methods either utilize multi-modal information in\nvideo-text pairs or apply global and local alignment techniques to promote\nalignment precision. However, these methods often fail to fully explore the\nspatio-temporal relationships among vision tokens within video and across\ndifferent video-text pairs. In this paper, we propose a novel Spatio-Temporal\nGraph Transformer module to uniformly learn spatial and temporal contexts for\nvideo-language alignment pre-training (dubbed STGT). Specifically, our STGT\ncombines spatio-temporal graph structure information with attention in\ntransformer block, effectively utilizing the spatio-temporal contexts. In this\nway, we can model the relationships between vision tokens, promoting video-text\nalignment precision for benefiting downstream tasks. In addition, we propose a\nself-similarity alignment loss to explore the inherent self-similarity in the\nvideo and text. With the initial optimization achieved by contrastive learning,\nit can further promote the alignment accuracy between video and text.\nExperimental results on challenging downstream tasks, including video-text\nretrieval and video question answering, verify the superior performance of our\nmethod.\n","authors":["Shi-Xue Zhang","Hongfa Wang","Xiaobin Zhu","Weibo Gu","Tianjin Zhang","Chun Yang","Wei Liu","Xu-Cheng Yin"],"pdf_url":"https://arxiv.org/pdf/2407.11677v2.pdf","comment":"under review"},{"id":"http://arxiv.org/abs/2407.16789v1","updated":"2024-07-23T18:42:37Z","published":"2024-07-23T18:42:37Z","title":"What Matters in Range View 3D Object Detection","summary":"  Lidar-based perception pipelines rely on 3D object detection models to\ninterpret complex scenes. While multiple representations for lidar exist, the\nrange-view is enticing since it losslessly encodes the entire lidar sensor\noutput. In this work, we achieve state-of-the-art amongst range-view 3D object\ndetection models without using multiple techniques proposed in past range-view\nliterature. We explore range-view 3D object detection across two modern\ndatasets with substantially different properties: Argoverse 2 and Waymo Open.\nOur investigation reveals key insights: (1) input feature dimensionality\nsignificantly influences the overall performance, (2) surprisingly, employing a\nclassification loss grounded in 3D spatial proximity works as well or better\ncompared to more elaborate IoU-based losses, and (3) addressing non-uniform\nlidar density via a straightforward range subsampling technique outperforms\nexisting multi-resolution, range-conditioned networks. Our experiments reveal\nthat techniques proposed in recent range-view literature are not needed to\nachieve state-of-the-art performance. Combining the above findings, we\nestablish a new state-of-the-art model for range-view 3D object detection --\nimproving AP by 2.2% on the Waymo Open dataset while maintaining a runtime of\n10 Hz. We establish the first range-view model on the Argoverse 2 dataset and\noutperform strong voxel-based baselines. All models are multi-class and\nopen-source. Code is available at\nhttps://github.com/benjaminrwilson/range-view-3d-detection.\n","authors":["Benjamin Wilson","Nicholas Autio Mitchell","Jhony Kaesemodel Pontes","James Hays"],"pdf_url":"https://arxiv.org/pdf/2407.16789v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16788v1","updated":"2024-07-23T18:41:16Z","published":"2024-07-23T18:41:16Z","title":"Occlusion-Aware 3D Motion Interpretation for Abnormal Behavior Detection","summary":"  Estimating abnormal posture based on 3D pose is vital in human pose analysis,\nyet it presents challenges, especially when reconstructing 3D human poses from\nmonocular datasets with occlusions. Accurate reconstructions enable the\nrestoration of 3D movements, which assist in the extraction of semantic details\nnecessary for analyzing abnormal behaviors. However, most existing methods\ndepend on predefined key points as a basis for estimating the coordinates of\noccluded joints, where variations in data quality have adversely affected the\nperformance of these models. In this paper, we present OAD2D, which\ndiscriminates against motion abnormalities based on reconstructing 3D\ncoordinates of mesh vertices and human joints from monocular videos. The OAD2D\nemploys optical flow to capture motion prior information in video streams,\nenriching the information on occluded human movements and ensuring\ntemporal-spatial alignment of poses. Moreover, we reformulate the abnormal\nposture estimation by coupling it with Motion to Text (M2T) model in which, the\nVQVAE is employed to quantize motion features. This approach maps motion tokens\nto text tokens, allowing for a semantically interpretable analysis of motion,\nand enhancing the generalization of abnormal posture detection boosted by\nLanguage model. Our approach demonstrates the robustness of abnormal behavior\ndetection against severe and self-occlusions, as it reconstructs human motion\ntrajectories in global coordinates to effectively mitigate occlusion issues.\nOur method, validated using the Human3.6M, 3DPW, and NTU RGB+D datasets,\nachieves a high $F_1-$Score of 0.94 on the NTU RGB+D dataset for medical\ncondition detection. And we will release all of our code and data.\n","authors":["Su Li","Wang Liang","Jianye Wang","Ziheng Zhang","Lei Zhang"],"pdf_url":"https://arxiv.org/pdf/2407.16788v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.08333v4","updated":"2024-07-23T18:40:10Z","published":"2023-08-16T12:46:52Z","title":"Improving Depth Gradient Continuity in Transformers: A Comparative Study\n  on Monocular Depth Estimation with CNN","summary":"  Monocular depth estimation is an ongoing challenge in computer vision. Recent\nprogress with Transformer models has demonstrated notable advantages over\nconventional CNNs in this area. However, there's still a gap in understanding\nhow these models prioritize different regions in 2D images and how these\nregions affect depth estimation performance. To explore the differences between\nTransformers and CNNs, we employ a sparse pixel approach to contrastively\nanalyze the distinctions between the two. Our findings suggest that while\nTransformers excel in handling global context and intricate textures, they lag\nbehind CNNs in preserving depth gradient continuity. To further enhance the\nperformance of Transformer models in monocular depth estimation, we propose the\nDepth Gradient Refinement (DGR) module that refines depth estimation through\nhigh-order differentiation, feature fusion, and recalibration. Additionally, we\nleverage optimal transport theory, treating depth maps as spatial probability\ndistributions, and employ the optimal transport distance as a loss function to\noptimize our model. Experimental results demonstrate that models integrated\nwith the plug-and-play Depth Gradient Refinement (DGR) module and the proposed\nloss function enhance performance without increasing complexity and\ncomputational costs on both outdoor KITTI and indoor NYU-Depth-v2 datasets.\nThis research not only offers fresh insights into the distinctions between\nTransformers and CNNs in depth estimation but also paves the way for novel\ndepth estimation methodologies.\n","authors":["Jiawei Yao","Tong Wu","Xiaofeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2308.08333v4.pdf","comment":"Accepted by BMVC 2024. Project page:\n  https://github.com/Jiawei-Yao0812/PixelFormer_DGR"},{"id":"http://arxiv.org/abs/2407.15540v2","updated":"2024-07-23T18:34:55Z","published":"2024-07-22T11:05:58Z","title":"Differentiable Product Quantization for Memory Efficient Camera\n  Relocalization","summary":"  Camera relocalization relies on 3D models of the scene with a large memory\nfootprint that is incompatible with the memory budget of several applications.\nOne solution to reduce the scene memory size is map compression by removing\ncertain 3D points and descriptor quantization. This achieves high compression\nbut leads to performance drop due to information loss. To address the memory\nperformance trade-off, we train a light-weight scene-specific auto-encoder\nnetwork that performs descriptor quantization-dequantization in an end-to-end\ndifferentiable manner updating both product quantization centroids and network\nparameters through back-propagation. In addition to optimizing the network for\ndescriptor reconstruction, we encourage it to preserve the descriptor-matching\nperformance with margin-based metric loss functions. Results show that for a\nlocal descriptor memory of only 1MB, the synergistic combination of the\nproposed network and map compression achieves the best performance on the\nAachen Day-Night compared to existing compression methods.\n","authors":["Zakaria Laskar","Iaroslav Melekhov","Assia Benbihi","Shuzhe Wang","Juho Kannala"],"pdf_url":"https://arxiv.org/pdf/2407.15540v2.pdf","comment":"Accepted to the European Conference on Computer Vision (ECCV) 2024"},{"id":"http://arxiv.org/abs/2407.16777v1","updated":"2024-07-23T18:19:27Z","published":"2024-07-23T18:19:27Z","title":"A Dataset for Crucial Object Recognition in Blind and Low-Vision\n  Individuals' Navigation","summary":"  This paper introduces a dataset for improving real-time object recognition\nsystems to aid blind and low-vision (BLV) individuals in navigation tasks. The\ndataset comprises 21 videos of BLV individuals navigating outdoor spaces, and a\ntaxonomy of 90 objects crucial for BLV navigation, refined through a focus\ngroup study. We also provide object labeling for the 90 objects across 31 video\nsegments created from the 21 videos. A deeper analysis reveals that most\ncontemporary datasets used in training computer vision models contain only a\nsmall subset of the taxonomy in our dataset. Preliminary evaluation of\nstate-of-the-art computer vision models on our dataset highlights shortcomings\nin accurately detecting key objects relevant to BLV navigation, emphasizing the\nneed for specialized datasets. We make our dataset publicly available, offering\nvaluable resources for developing more inclusive navigation systems for BLV\nindividuals.\n","authors":["Md Touhidul Islam","Imran Kabir","Elena Ariel Pearce","Md Alimoor Reza","Syed Masum Billah"],"pdf_url":"https://arxiv.org/pdf/2407.16777v1.pdf","comment":"16 pages, 4 figures"},{"id":"http://arxiv.org/abs/2407.16772v1","updated":"2024-07-23T18:10:43Z","published":"2024-07-23T18:10:43Z","title":"VisMin: Visual Minimal-Change Understanding","summary":"  Fine-grained understanding of objects, attributes, and relationships between\nobjects is crucial for visual-language models (VLMs). Existing benchmarks\nprimarily focus on evaluating VLMs' capability to distinguish between two very\nsimilar \\textit{captions} given an image. In this paper, we introduce a new,\nchallenging benchmark termed \\textbf{Vis}ual \\textbf{Min}imal-Change\nUnderstanding (VisMin), which requires models to predict the correct\nimage-caption match given two images and two captions. The image pair and\ncaption pair contain minimal changes, i.e., only one aspect changes at a time\nfrom among the following: \\textit{object}, \\textit{attribute}, \\textit{count},\nand \\textit{spatial relation}. These changes test the models' understanding of\nobjects, attributes (such as color, material, shape), counts, and spatial\nrelationships between objects. We built an automatic framework using large\nlanguage models and diffusion models, followed by a rigorous 4-step\nverification process by human annotators. Empirical experiments reveal that\ncurrent VLMs exhibit notable deficiencies in understanding spatial\nrelationships and counting abilities. We also generate a large-scale training\ndataset to finetune CLIP and Idefics2, showing significant improvements in\nfine-grained understanding across benchmarks and in CLIP's general image-text\nalignment. We release all resources, including the benchmark, training data,\nand finetuned model checkpoints, at \\url{https://vismin.net/}.\n","authors":["Rabiul Awal","Saba Ahmadi","Le Zhang","Aishwarya Agrawal"],"pdf_url":"https://arxiv.org/pdf/2407.16772v1.pdf","comment":"Project URL at https://vismin.net/"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2407.16594v1","updated":"2024-07-23T15:53:17Z","published":"2024-07-23T15:53:17Z","title":"GenRec: A Flexible Data Generator for Recommendations","summary":"  The scarcity of realistic datasets poses a significant challenge in\nbenchmarking recommender systems and social network analysis methods and\ntechniques. A common and effective solution is to generate synthetic data that\nsimulates realistic interactions. However, although various methods have been\nproposed, the existing literature still lacks generators that are fully\nadaptable and allow easy manipulation of the underlying data distributions and\nstructural properties. To address this issue, the present work introduces\nGenRec, a novel framework for generating synthetic user-item interactions that\nexhibit realistic and well-known properties observed in recommendation\nscenarios. The framework is based on a stochastic generative process based on\nlatent factor modeling. Here, the latent factors can be exploited to yield\nlong-tailed preference distributions, and at the same time they characterize\nsubpopulations of users and topic-based item clusters. Notably, the proposed\nframework is highly flexible and offers a wide range of hyper-parameters for\ncustomizing the generation of user-item interactions. The code used to perform\nthe experiments is publicly available at\nhttps://anonymous.4open.science/r/GenRec-DED3.\n","authors":["Erica Coppolillo","Simone Mungari","Ettore Ritacco","Giuseppe Manco"],"pdf_url":"https://arxiv.org/pdf/2407.16594v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.09252v2","updated":"2024-07-23T12:28:31Z","published":"2024-07-12T13:30:44Z","title":"Context Embeddings for Efficient Answer Generation in RAG","summary":"  Retrieval-Augmented Generation (RAG) allows overcoming the limited knowledge\nof LLMs by extending the input with external information. As a consequence, the\ncontextual inputs to the model become much longer which slows down decoding\ntime directly translating to the time a user has to wait for an answer. We\naddress this challenge by presenting COCOM, an effective context compression\nmethod, reducing long contexts to only a handful of Context Embeddings speeding\nup the generation time by a large margin. Our method allows for different\ncompression rates trading off decoding time for answer quality. Compared to\nearlier methods, COCOM allows for handling multiple contexts more effectively,\nsignificantly reducing decoding time for long inputs. Our method demonstrates a\nspeed-up of up to 5.69 $\\times$ while achieving higher performance compared to\nexisting efficient context compression methods.\n","authors":["David Rau","Shuai Wang","Hervé Déjean","Stéphane Clinchant"],"pdf_url":"https://arxiv.org/pdf/2407.09252v2.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2407.16357v1","updated":"2024-07-23T10:00:45Z","published":"2024-07-23T10:00:45Z","title":"TWIN V2: Scaling Ultra-Long User Behavior Sequence Modeling for Enhanced\n  CTR Prediction at Kuaishou","summary":"  The significance of modeling long-term user interests for CTR prediction\ntasks in large-scale recommendation systems is progressively gaining attention\namong researchers and practitioners. Existing work, such as SIM and TWIN,\ntypically employs a two-stage approach to model long-term user behavior\nsequences for efficiency concerns. The first stage rapidly retrieves a subset\nof sequences related to the target item from a long sequence using a\nsearch-based mechanism namely the General Search Unit (GSU), while the second\nstage calculates the interest scores using the Exact Search Unit (ESU) on the\nretrieved results. Given the extensive length of user behavior sequences\nspanning the entire life cycle, potentially reaching up to 10^6 in scale, there\nis currently no effective solution for fully modeling such expansive user\ninterests. To overcome this issue, we introduced TWIN-V2, an enhancement of\nTWIN, where a divide-and-conquer approach is applied to compress life-cycle\nbehaviors and uncover more accurate and diverse user interests. Specifically, a\nhierarchical clustering method groups items with similar characteristics in\nlife-cycle behaviors into a single cluster during the offline phase. By\nlimiting the size of clusters, we can compress behavior sequences well beyond\nthe magnitude of 10^5 to a length manageable for online inference in GSU\nretrieval. Cluster-aware target attention extracts comprehensive and\nmulti-faceted long-term interests of users, thereby making the final\nrecommendation results more accurate and diverse. Extensive offline experiments\non a multi-billion-scale industrial dataset and online A/B tests have\ndemonstrated the effectiveness of TWIN-V2. Under an efficient deployment\nframework, TWIN-V2 has been successfully deployed to the primary traffic that\nserves hundreds of millions of daily active users at Kuaishou.\n","authors":["Zihua Si","Lin Guan","ZhongXiang Sun","Xiaoxue Zang","Jing Lu","Yiqun Hui","Xingchao Cao","Zeyu Yang","Yichen Zheng","Dewei Leng","Kai Zheng","Chenbin Zhang","Yanan Niu","Yang Song","Kun Gai"],"pdf_url":"https://arxiv.org/pdf/2407.16357v1.pdf","comment":"Accepted by CIKM 2024"},{"id":"http://arxiv.org/abs/2407.15124v2","updated":"2024-07-23T07:11:47Z","published":"2024-07-21T11:27:27Z","title":"Chemical Reaction Extraction from Long Patent Documents","summary":"  The task of searching through patent documents is crucial for chemical patent\nrecommendation and retrieval. This can be enhanced by creating a patent\nknowledge base (ChemPatKB) to aid in prior art searches and to provide a\nplatform for domain experts to explore new innovations in chemical compound\nsynthesis and use-cases. An essential foundational component of this KB is the\nextraction of important reaction snippets from long patents documents which\nfacilitates multiple downstream tasks such as reaction co-reference resolution\nand chemical entity role identification. In this work, we explore the problem\nof extracting reactions spans from chemical patents in order to create a\nreactions resource database. We formulate this task as a paragraph-level\nsequence tagging problem, where the system is required to return a sequence of\nparagraphs that contain a description of a reaction. We propose several\napproaches and modifications of the baseline models and study how different\nmethods generalize across different domains of chemical patents.\n","authors":["Aishwarya Jadhav","Ritam Dutt"],"pdf_url":"https://arxiv.org/pdf/2407.15124v2.pdf","comment":"Work completed in 2022 at Carnegie Mellon University"},{"id":"http://arxiv.org/abs/2406.08214v2","updated":"2024-07-23T07:04:08Z","published":"2024-06-12T13:44:22Z","title":"Graph Bottlenecked Social Recommendation","summary":"  With the emergence of social networks, social recommendation has become an\nessential technique for personalized services. Recently, graph-based social\nrecommendations have shown promising results by capturing the high-order social\ninfluence. Most empirical studies of graph-based social recommendations\ndirectly take the observed social networks into formulation, and produce user\npreferences based on social homogeneity. Despite the effectiveness, we argue\nthat social networks in the real-world are inevitably noisy~(existing redundant\nsocial relations), which may obstruct precise user preference characterization.\nNevertheless, identifying and removing redundant social relations is\nchallenging due to a lack of labels. In this paper, we focus on learning the\ndenoised social structure to facilitate recommendation tasks from an\ninformation bottleneck perspective. Specifically, we propose a novel Graph\nBottlenecked Social Recommendation (GBSR) framework to tackle the social noise\nissue.GBSR is a model-agnostic social denoising framework, that aims to\nmaximize the mutual information between the denoised social graph and\nrecommendation labels, meanwhile minimizing it between the denoised social\ngraph and the original one. This enables GBSR to learn the minimal yet\nsufficient social structure, effectively reducing redundant social relations\nand enhancing social recommendations. Technically, GBSR consists of two\nelaborate components, preference-guided social graph refinement, and HSIC-based\nbottleneck learning. Extensive experimental results demonstrate the superiority\nof the proposed GBSR, including high performances and good generality combined\nwith various backbones. Our code is available at:\nhttps://github.com/yimutianyang/KDD24-GBSR.\n","authors":["Yonghui Yang","Le Wu","Zihan Wang","Zhuangzhuang He","Richang Hong","Meng Wang"],"pdf_url":"https://arxiv.org/pdf/2406.08214v2.pdf","comment":"Accepted by KDD 2024"},{"id":"http://arxiv.org/abs/2407.16192v1","updated":"2024-07-23T05:34:41Z","published":"2024-07-23T05:34:41Z","title":"How to Leverage Personal Textual Knowledge for Personalized\n  Conversational Information Retrieval","summary":"  Personalized conversational information retrieval (CIR) combines\nconversational and personalizable elements to satisfy various users' complex\ninformation needs through multi-turn interaction based on their backgrounds.\nThe key promise is that the personal textual knowledge base (PTKB) can improve\nthe CIR effectiveness because the retrieval results can be more related to the\nuser's background. However, PTKB is noisy: not every piece of knowledge in PTKB\nis relevant to the specific query at hand. In this paper, we explore and test\nseveral ways to select knowledge from PTKB and use it for query reformulation\nby using a large language model (LLM). The experimental results show the PTKB\nmight not always improve the search results when used alone, but LLM can help\ngenerate a more appropriate personalized query when high-quality guidance is\nprovided.\n","authors":["Fengran Mo","Longxiang Zhao","Kaiyu Huang","Yue Dong","Degen Huang","Jian-Yun Nie"],"pdf_url":"https://arxiv.org/pdf/2407.16192v1.pdf","comment":"Accepted to CIKM 2024"},{"id":"http://arxiv.org/abs/2407.16850v1","updated":"2024-07-23T21:26:37Z","published":"2024-07-23T21:26:37Z","title":"Covering a Graph with Dense Subgraph Families, via Triangle-Rich Sets","summary":"  Graphs are a fundamental data structure used to represent relationships in\ndomains as diverse as the social sciences, bioinformatics, cybersecurity, the\nInternet, and more. One of the central observations in network science is that\nreal-world graphs are globally sparse, yet contains numerous \"pockets\" of high\nedge density. A fundamental task in graph mining is to discover these dense\nsubgraphs. Most common formulations of the problem involve finding a single (or\na few) \"optimally\" dense subsets. But in most real applications, one does not\ncare for the optimality. Instead, we want to find a large collection of dense\nsubsets that covers a significant fraction of the input graph.\n  We give a mathematical formulation of this problem, using a new definition of\nregularly triangle-rich (RTR) families. These families capture the notion of\ndense subgraphs that contain many triangles and have degrees comparable to the\nsubgraph size. We design a provable algorithm, RTRExtractor, that can discover\nRTR families that approximately cover any RTR set. The algorithm is efficient\nand is inspired by recent results that use triangle counts for community\ntesting and clustering.\n  We show that RTRExtractor has excellent behavior on a large variety of\nreal-world datasets. It is able to process graphs with hundreds of millions of\nedges within minutes. Across many datasets, RTRExtractor achieves high coverage\nusing high edge density datasets. For example, the output covers a quarter of\nthe vertices with subgraphs of edge density more than (say) $0.5$, for datasets\nwith 10M+ edges. We show an example of how the output of RTRExtractor\ncorrelates with meaningful sets of similar vertices in a citation network,\ndemonstrating the utility of RTRExtractor for unsupervised graph discovery\ntasks.\n","authors":["Sabyasachi Basu","Daniel Paul-Pena","Kun Qian","C. Seshadhri","Edward W Huang","Karthik Subbian"],"pdf_url":"https://arxiv.org/pdf/2407.16850v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2106.00142v2","updated":"2024-07-23T20:46:23Z","published":"2021-05-31T23:20:58Z","title":"FBAdtTracker: An Interactive Data Collection and Analysis Tool for\n  Facebook Advertisements","summary":"  The growing use of social media has led to drastic changes in our\ndecision-making. Especially, Facebook offers marketing API which promotes\nbusiness to target potential groups who are likely to consume their items.\nHowever, this service can be abused by malicious advertisers who attempt to\ndeceive people by disinformation such as propaganda and divisive opinion. To\ncounter this problem, we introduce a new application named FBAdTracker. The\npurpose of this application is to provide an integrated data collection and\nanalysis system for current research on fact-checking related to Facebook\nadvertisements. Our system is capable of monitoring up-to-date Facebook ads and\nanalyzing ads retrieved from Facebook Ads Library.\n","authors":["Ujun Jeong","Kaize Ding","Huan Liu"],"pdf_url":"https://arxiv.org/pdf/2106.00142v2.pdf","comment":"accepted to the International Conference on Social Computing,\n  Behavioral-Cultural Modeling, & Prediction and Behavior Representation in\n  Modeling and Simulation (SBP-BRiMS), demo track"},{"id":"http://arxiv.org/abs/2407.16828v1","updated":"2024-07-23T20:38:23Z","published":"2024-07-23T20:38:23Z","title":"Pareto Front Approximation for Multi-Objective Session-Based Recommender\n  Systems","summary":"  This work introduces MultiTRON, an approach that adapts Pareto front\napproximation techniques to multi-objective session-based recommender systems\nusing a transformer neural network. Our approach optimizes trade-offs between\nkey metrics such as click-through and conversion rates by training on sampled\npreference vectors. A significant advantage is that after training, a single\nmodel can access the entire Pareto front, allowing it to be tailored to meet\nthe specific requirements of different stakeholders by adjusting an additional\ninput vector that weights the objectives. We validate the model's performance\nthrough extensive offline and online evaluation. For broader application and\nresearch, the source code is made available at\nhttps://github.com/otto-de/MultiTRON . The results confirm the model's ability\nto manage multiple recommendation objectives effectively, offering a flexible\ntool for diverse business needs.\n","authors":["Timo Wilm","Philipp Normann","Felix Stepprath"],"pdf_url":"https://arxiv.org/pdf/2407.16828v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.07179v3","updated":"2024-07-23T19:41:05Z","published":"2024-02-11T12:25:41Z","title":"Prompt Perturbation in Retrieval-Augmented Generation based Large\n  Language Models","summary":"  The robustness of large language models (LLMs) becomes increasingly important\nas their use rapidly grows in a wide range of domains. Retrieval-Augmented\nGeneration (RAG) is considered as a means to improve the trustworthiness of\ntext generation from LLMs. However, how the outputs from RAG-based LLMs are\naffected by slightly different inputs is not well studied. In this work, we\nfind that the insertion of even a short prefix to the prompt leads to the\ngeneration of outputs far away from factually correct answers. We\nsystematically evaluate the effect of such prefixes on RAG by introducing a\nnovel optimization technique called Gradient Guided Prompt Perturbation (GGPP).\nGGPP achieves a high success rate in steering outputs of RAG-based LLMs to\ntargeted wrong answers. It can also cope with instructions in the prompts\nrequesting to ignore irrelevant context. We also exploit LLMs' neuron\nactivation difference between prompts with and without GGPP perturbations to\ngive a method that improves the robustness of RAG-based LLMs through a highly\neffective detector trained on neuron activation triggered by GGPP generated\nprompts. Our evaluation on open-sourced LLMs demonstrates the effectiveness of\nour methods.\n","authors":["Zhibo Hu","Chen Wang","Yanfeng Shu"," Helen"," Paik","Liming Zhu"],"pdf_url":"https://arxiv.org/pdf/2402.07179v3.pdf","comment":"12 pages, 9 figures"},{"id":"http://arxiv.org/abs/2407.07004v2","updated":"2024-07-23T18:20:45Z","published":"2024-07-09T16:17:16Z","title":"Empirical analysis of Binding Precedent efficiency in the Brazilian\n  Supreme Court via Similar Case Retrieval","summary":"  Binding precedents (S\\'umulas Vinculantes) constitute a juridical instrument\nunique to the Brazilian legal system and whose objectives include the\nprotection of the Federal Supreme Court against repetitive demands. Studies of\nthe effectiveness of these instruments in decreasing the Court's exposure to\nsimilar cases, however, indicate that they tend to fail in such a direction,\nwith some of the binding precedents seemingly creating new demands. We\nempirically assess the legal impact of five binding precedents, 11, 14, 17, 26\nand 37, at the highest court level through their effects on the legal subjects\nthey address. This analysis is only possible through the comparison of the\nCourt's ruling about the precedents' themes before they are created, which\nmeans that these decisions should be detected through techniques of Similar\nCase Retrieval. The contributions of this article are therefore twofold: on the\nmathematical side, we compare the uses of different methods of Natural Language\nProcessing -- TF-IDF, LSTM, BERT, and regex -- for Similar Case Retrieval,\nwhereas on the legal side, we contrast the inefficiency of these binding\nprecedents with a set of hypotheses that may justify their repeated usage. We\nobserve that the deep learning models performed significantly worse in the\nspecific Similar Case Retrieval task and that the reasons for binding\nprecedents to fail in responding to repetitive demand are heterogeneous and\ncase-dependent, making it impossible to single out a specific cause.\n","authors":["Raphaël Tinarrage","Henrique Ennes","Lucas E. Resck","Lucas T. Gomes","Jean R. Ponciano","Jorge Poco"],"pdf_url":"https://arxiv.org/pdf/2407.07004v2.pdf","comment":"54 pages, 22 figures"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2407.12136v3","updated":"2024-07-23T17:58:52Z","published":"2024-07-16T19:45:34Z","title":"Molecular Topological Profile (MOLTOP) -- Simple and Strong Baseline for\n  Molecular Graph Classification","summary":"  We revisit the effectiveness of topological descriptors for molecular graph\nclassification and design a simple, yet strong baseline. We demonstrate that a\nsimple approach to feature engineering - employing histogram aggregation of\nedge descriptors and one-hot encoding for atomic numbers and bond types - when\ncombined with a Random Forest classifier, can establish a strong baseline for\nGraph Neural Networks (GNNs). The novel algorithm, Molecular Topological\nProfile (MOLTOP), integrates Edge Betweenness Centrality, Adjusted Rand Index\nand SCAN Structural Similarity score. This approach proves to be remarkably\ncompetitive when compared to modern GNNs, while also being simple, fast,\nlow-variance and hyperparameter-free. Our approach is rigorously tested on\nMoleculeNet datasets using fair evaluation protocol provided by Open Graph\nBenchmark. We additionally show out-of-domain generation capabilities on\npeptide classification task from Long Range Graph Benchmark. The evaluations\nacross eleven benchmark datasets reveal MOLTOP's strong discriminative\ncapabilities, surpassing the $1$-WL test and even $3$-WL test for some classes\nof graphs. Our conclusion is that descriptor-based baselines, such as the one\nwe propose, are still crucial for accurately assessing advancements in the GNN\ndomain.\n","authors":["Jakub Adamczyk","Wojciech Czech"],"pdf_url":"https://arxiv.org/pdf/2407.12136v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16695v1","updated":"2024-07-23T17:57:41Z","published":"2024-07-23T17:57:41Z","title":"Stress-Testing Long-Context Language Models with Lifelong ICL and Task\n  Haystack","summary":"  We introduce Lifelong ICL, a problem setting that challenges long-context\nlanguage models (LMs) to learn from a sequence of language tasks through\nin-context learning (ICL). We further introduce Task Haystack, an evaluation\nsuite dedicated to assessing and diagnosing how long-context LMs utilizes\ncontexts in Lifelong ICL. When given a task instruction and test inputs,\nlong-context LMs are expected to leverage the relevant demonstrations in the\nLifelong ICL prompt, avoid distraction and interference from other tasks, and\nachieve test accuracies that are not significantly worse than the Single-task\nICL baseline.\n  Task Haystack draws inspiration from the widely-adopted\n\"needle-in-a-haystack\" (NIAH) evaluation, but presents new and unique\nchallenges. It demands that models (1) utilize the contexts with deeper\nunderstanding, rather than resorting to simple copying and pasting; (2)\nnavigate through long streams of evolving topics and tasks, which closely\napproximates the complexities of real-world usage of long-context LMs.\nAdditionally, Task Haystack inherits the controllability aspect of NIAH,\nproviding model developers with tools and visualizations to identify model\nvulnerabilities effectively.\n  We benchmark 12 long-context LMs using Task Haystack. We find that\nstate-of-the-art closed models such as GPT-4o still struggle in this setting,\nfailing 15% of the cases on average, while all open-weight models we evaluate\nfurther lack behind by a large margin, failing up to 61% of the cases. In our\ncontrolled analysis, we identify factors such as distraction and recency bias\nas contributors to these failure cases. Further, we observe declines in\nperformance when task instructions are paraphrased at test time or when ICL\ndemonstrations are repeated excessively, raising concerns about the robustness,\ninstruction understanding, and true context utilization of current long-context\nLMs.\n","authors":["Xiaoyue Xu","Qinyuan Ye","Xiang Ren"],"pdf_url":"https://arxiv.org/pdf/2407.16695v1.pdf","comment":"Code: https://github.com/INK-USC/Lifelong-ICL; Website:\n  https://inklab.usc.edu/lifelong-icl/"},{"id":"http://arxiv.org/abs/2406.09014v4","updated":"2024-07-23T17:56:10Z","published":"2024-06-13T11:38:58Z","title":"Deep learning empowered sensor fusion boosts infant movement\n  classification","summary":"  There is a recent boom in the development of AI solutions to facilitate and\nenhance diagnostic procedures for established clinical tools. To assess the\nintegrity of the developing nervous system, the Prechtl general movement\nassessment (GMA) is recognized for its clinical value in diagnosing\nneurological impairments in early infancy. GMA has been increasingly augmented\nthrough machine learning approaches intending to scale-up its application,\ncircumvent costs in the training of human assessors and further standardize\nclassification of spontaneous motor patterns. Available deep learning tools,\nall of which are based on single sensor modalities, are however still\nconsiderably inferior to that of well-trained human assessors. These approaches\nare hardly comparable as all models are designed, trained and evaluated on\nproprietary/silo-data sets. With this study we propose a sensor fusion approach\nfor assessing fidgety movements (FMs) comparing three different sensor\nmodalities (pressure, inertial, and visual sensors). Various combinations and\ntwo sensor fusion approaches (late and early fusion) for infant movement\nclassification were tested to evaluate whether a multi-sensor system\noutperforms single modality assessments. The performance of the three-sensor\nfusion (classification accuracy of 94.5\\%) was significantly higher than that\nof any single modality evaluated, suggesting the sensor fusion approach is a\npromising avenue for automated classification of infant motor patterns. The\ndevelopment of a robust sensor fusion system may significantly enhance AI-based\nearly recognition of neurofunctions, ultimately facilitating automated early\ndetection of neurodevelopmental conditions.\n","authors":["Tomas Kulvicius","Dajie Zhang","Luise Poustka","Sven Bölte","Lennart Jahn","Sarah Flügge","Marc Kraft","Markus Zweckstetter","Karin Nielsen-Saines","Florentin Wörgötter","Peter B Marschik"],"pdf_url":"https://arxiv.org/pdf/2406.09014v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16691v1","updated":"2024-07-23T17:55:25Z","published":"2024-07-23T17:55:25Z","title":"Automatic Equalization for Individual Instrument Tracks Using\n  Convolutional Neural Networks","summary":"  We propose a novel approach for the automatic equalization of individual\nmusical instrument tracks. Our method begins by identifying the instrument\npresent within a source recording in order to choose its corresponding ideal\nspectrum as a target. Next, the spectral difference between the recording and\nthe target is calculated, and accordingly, an equalizer matching model is used\nto predict settings for a parametric equalizer. To this end, we build upon a\ndifferentiable parametric equalizer matching neural network, demonstrating\nimprovements relative to previously established state-of-the-art. Unlike past\napproaches, we show how our system naturally allows real-world audio data to be\nleveraged during the training of our matching model, effectively generating\nsuitably produced training targets in an automated manner mirroring conditions\nat inference time. Consequently, we illustrate how fine-tuning our matching\nmodel on such examples considerably improves parametric equalizer matching\nperformance in real-world scenarios, decreasing mean absolute error by 24%\nrelative to methods relying solely on random parameter sampling techniques as a\nself-supervised learning strategy. We perform listening tests, and demonstrate\nthat our proposed automatic equalization solution subjectively enhances the\ntonal characteristics for recordings of common instrument types.\n","authors":["Florian Mockenhaupt","Joscha Simon Rieber","Shahan Nercessian"],"pdf_url":"https://arxiv.org/pdf/2407.16691v1.pdf","comment":"8 pages, 9 figures. Accepted to the 27th International Conference on\n  Digital Audio Effects (DAFx24)"},{"id":"http://arxiv.org/abs/2407.16680v1","updated":"2024-07-23T17:45:16Z","published":"2024-07-23T17:45:16Z","title":"A Simulation Benchmark for Autonomous Racing with Large-Scale Human Data","summary":"  Despite the availability of international prize-money competitions, scaled\nvehicles, and simulation environments, research on autonomous racing and the\ncontrol of sports cars operating close to the limit of handling has been\nlimited by the high costs of vehicle acquisition and management, as well as the\nlimited physics accuracy of open-source simulators. In this paper, we propose a\nracing simulation platform based on the simulator Assetto Corsa to test,\nvalidate, and benchmark autonomous driving algorithms, including reinforcement\nlearning (RL) and classical Model Predictive Control (MPC), in realistic and\nchallenging scenarios. Our contributions include the development of this\nsimulation platform, several state-of-the-art algorithms tailored to the racing\nenvironment, and a comprehensive dataset collected from human drivers.\nAdditionally, we evaluate algorithms in the offline RL setting. All the\nnecessary code (including environment and benchmarks), working examples,\ndatasets, and videos are publicly released and can be found at:\n\\url{https://assetto-corsa-gym.github.io}.\n","authors":["Adrian Remonda","Nicklas Hansen","Ayoub Raji","Nicola Musiu","Marko Bertogna","Eduardo Veas","Xiaolong Wang"],"pdf_url":"https://arxiv.org/pdf/2407.16680v1.pdf","comment":"https://assetto-corsa-gym.github.io/"},{"id":"http://arxiv.org/abs/2407.16677v1","updated":"2024-07-23T17:44:54Z","published":"2024-07-23T17:44:54Z","title":"From Imitation to Refinement -- Residual RL for Precise Visual Assembly","summary":"  Behavior cloning (BC) currently stands as a dominant paradigm for learning\nreal-world visual manipulation. However, in tasks that require locally\ncorrective behaviors like multi-part assembly, learning robust policies purely\nfrom human demonstrations remains challenging. Reinforcement learning (RL) can\nmitigate these limitations by allowing policies to acquire locally corrective\nbehaviors through task reward supervision and exploration. This paper explores\nthe use of RL fine-tuning to improve upon BC-trained policies in precise\nmanipulation tasks. We analyze and overcome technical challenges associated\nwith using RL to directly train policy networks that incorporate modern\narchitectural components like diffusion models and action chunking. We propose\ntraining residual policies on top of frozen BC-trained diffusion models using\nstandard policy gradient methods and sparse rewards, an approach we call ResiP\n(Residual for Precise manipulation). Our experimental results demonstrate that\nthis residual learning framework can significantly improve success rates beyond\nthe base BC-trained models in high-precision assembly tasks by learning\ncorrective actions. We also show that by combining ResiP with teacher-student\ndistillation and visual domain randomization, our method can enable learning\nreal-world policies for robotic assembly directly from RGB images. Find videos\nand code at \\url{https://residual-assembly.github.io}.\n","authors":["Lars Ankile","Anthony Simeonov","Idan Shenfeld","Marcel Torne","Pulkit Agrawal"],"pdf_url":"https://arxiv.org/pdf/2407.16677v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16674v1","updated":"2024-07-23T17:43:35Z","published":"2024-07-23T17:43:35Z","title":"KAN or MLP: A Fairer Comparison","summary":"  This paper does not introduce a novel method. Instead, it offers a fairer and\nmore comprehensive comparison of KAN and MLP models across various tasks,\nincluding machine learning, computer vision, audio processing, natural language\nprocessing, and symbolic formula representation. Specifically, we control the\nnumber of parameters and FLOPs to compare the performance of KAN and MLP. Our\nmain observation is that, except for symbolic formula representation tasks, MLP\ngenerally outperforms KAN. We also conduct ablation studies on KAN and find\nthat its advantage in symbolic formula representation mainly stems from its\nB-spline activation function. When B-spline is applied to MLP, performance in\nsymbolic formula representation significantly improves, surpassing or matching\nthat of KAN. However, in other tasks where MLP already excels over KAN,\nB-spline does not substantially enhance MLP's performance. Furthermore, we find\nthat KAN's forgetting issue is more severe than that of MLP in a standard\nclass-incremental continual learning setting, which differs from the findings\nreported in the KAN paper. We hope these results provide insights for future\nresearch on KAN and other MLP alternatives. Project link:\nhttps://github.com/yu-rp/KANbeFair\n","authors":["Runpeng Yu","Weihao Yu","Xinchao Wang"],"pdf_url":"https://arxiv.org/pdf/2407.16674v1.pdf","comment":"Technical Report"},{"id":"http://arxiv.org/abs/2407.16663v1","updated":"2024-07-23T17:26:38Z","published":"2024-07-23T17:26:38Z","title":"Computable learning of natural hypothesis classes","summary":"  This paper is about the recent notion of computably probably approximately\ncorrect learning, which lies between the statistical learning theory where\nthere is no computational requirement on the learner and efficient PAC where\nthe learner must be polynomially bounded. Examples have recently been given of\nhypothesis classes which are PAC learnable but not computably PAC learnable,\nbut these hypothesis classes are unnatural or non-canonical in the sense that\nthey depend on a numbering of proofs, formulas, or programs. We use the\non-a-cone machinery from computability theory to prove that, under mild\nassumptions such as that the hypothesis class can be computably listable, any\nnatural hypothesis class which is learnable must be computably learnable. Thus\nthe counterexamples given previously are necessarily unnatural.\n","authors":["Matthew Harrison-Trainor","Syed Akbari"],"pdf_url":"https://arxiv.org/pdf/2407.16663v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07712v2","updated":"2024-07-23T17:01:12Z","published":"2024-07-10T14:44:25Z","title":"Deep-Graph-Sprints: Accelerated Representation Learning in\n  Continuous-Time Dynamic Graphs","summary":"  Continuous-time dynamic graphs (CTDGs) are essential for modeling\ninterconnected, evolving systems. Traditional methods for extracting knowledge\nfrom these graphs often depend on feature engineering or deep learning. Feature\nengineering is limited by the manual and time-intensive nature of crafting\nfeatures, while deep learning approaches suffer from high inference latency,\nmaking them impractical for real-time applications. This paper introduces\nDeep-Graph-Sprints (DGS), a novel deep learning architecture designed for\nefficient representation learning on CTDGs with low-latency inference\nrequirements. We benchmark DGS against state-of-the-art feature engineering and\ngraph neural network methods using five diverse datasets. The results indicate\nthat DGS achieves competitive performance while improving inference speed up to\n12x compared to other deep learning approaches on our tested benchmarks. Our\nmethod effectively bridges the gap between deep representation learning and\nlow-latency application requirements for CTDGs.\n","authors":["Ahmad Naser Eddin","Jacopo Bono","David Aparício","Hugo Ferreira","Pedro Ribeiro","Pedro Bizarro"],"pdf_url":"https://arxiv.org/pdf/2407.07712v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16643v1","updated":"2024-07-23T16:58:14Z","published":"2024-07-23T16:58:14Z","title":"Synthesizer Sound Matching Using Audio Spectrogram Transformers","summary":"  Systems for synthesizer sound matching, which automatically set the\nparameters of a synthesizer to emulate an input sound, have the potential to\nmake the process of synthesizer programming faster and easier for novice and\nexperienced musicians alike, whilst also affording new means of interaction\nwith synthesizers. Considering the enormous variety of synthesizers in the\nmarketplace, and the complexity of many of them, general-purpose sound matching\nsystems that function with minimal knowledge or prior assumptions about the\nunderlying synthesis architecture are particularly desirable. With this in\nmind, we introduce a synthesizer sound matching model based on the Audio\nSpectrogram Transformer. We demonstrate the viability of this model by training\non a large synthetic dataset of randomly generated samples from the popular\nMassive synthesizer. We show that this model can reconstruct parameters of\nsamples generated from a set of 16 parameters, highlighting its improved\nfidelity relative to multi-layer perceptron and convolutional neural network\nbaselines. We also provide audio examples demonstrating the out-of-domain model\nperformance in emulating vocal imitations, and sounds from other synthesizers\nand musical instruments.\n","authors":["Fred Bruford","Frederik Blang","Shahan Nercessian"],"pdf_url":"https://arxiv.org/pdf/2407.16643v1.pdf","comment":"4 pages, 1 figure. Accepted to the 27th International Conference on\n  Digital Audio Effects (DAFx24)"},{"id":"http://arxiv.org/abs/2407.16642v1","updated":"2024-07-23T16:57:10Z","published":"2024-07-23T16:57:10Z","title":"Aggregation of expert advice, revisited","summary":"  We revisit the classic problem of aggregating binary advice from\nconditionally independent experts, also known as the Naive Bayes setting. Our\nquantity of interest is the error probability of the optimal decision rule. In\nthe symmetric case (sensitivity = specificity), reasonably tight bounds on the\noptimal error probability are known. In the general asymmetric case, we are not\naware of any nontrivial estimates on this quantity. Our contribution consists\nof sharp upper and lower bounds on the optimal error probability in the general\ncase, which recover and sharpen the best known results in the symmetric special\ncase. Since this amounts to estimating the total variation distance between two\nproduct distributions, our results also have bearing on this important and\nchallenging problem.\n","authors":["Aryeh Kontorovich"],"pdf_url":"https://arxiv.org/pdf/2407.16642v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16641v1","updated":"2024-07-23T16:56:59Z","published":"2024-07-23T16:56:59Z","title":"A Geometry-Aware Algorithm to Learn Hierarchical Embeddings in\n  Hyperbolic Space","summary":"  Hyperbolic embeddings are a class of representation learning methods that\noffer competitive performances when data can be abstracted as a tree-like\ngraph. However, in practice, learning hyperbolic embeddings of hierarchical\ndata is difficult due to the different geometry between hyperbolic space and\nthe Euclidean space. To address such difficulties, we first categorize three\nkinds of illness that harm the performance of the embeddings. Then, we develop\na geometry-aware algorithm using a dilation operation and a transitive closure\nregularization to tackle these illnesses. We empirically validate these\ntechniques and present a theoretical analysis of the mechanism behind the\ndilation operation. Experiments on synthetic and real-world datasets reveal\nsuperior performances of our algorithm.\n","authors":["Zhangyu Wang","Lantian Xu","Zhifeng Kong","Weilong Wang","Xuyu Peng","Enyang Zheng"],"pdf_url":"https://arxiv.org/pdf/2407.16641v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16637v1","updated":"2024-07-23T16:54:28Z","published":"2024-07-23T16:54:28Z","title":"Course-Correction: Safety Alignment Using Synthetic Preferences","summary":"  The risk of harmful content generated by large language models (LLMs) becomes\na critical concern. This paper presents a systematic study on assessing and\nimproving LLMs' capability to perform the task of \\textbf{course-correction},\n\\ie, the model can steer away from generating harmful content autonomously. To\nstart with, we introduce the \\textsc{C$^2$-Eval} benchmark for quantitative\nassessment and analyze 10 popular LLMs, revealing varying proficiency of\ncurrent safety-tuned LLMs in course-correction. To improve, we propose\nfine-tuning LLMs with preference learning, emphasizing the preference for\ntimely course-correction. Using an automated pipeline, we create\n\\textsc{C$^2$-Syn}, a synthetic dataset with 750K pairwise preferences, to\nteach models the concept of timely course-correction through data-driven\npreference learning. Experiments on 2 LLMs, \\textsc{Llama2-Chat 7B} and\n\\textsc{Qwen2 7B}, show that our method effectively enhances course-correction\nskills without affecting general performance. Additionally, it effectively\nimproves LLMs' safety, particularly in resisting jailbreak attacks.\n","authors":["Rongwu Xu","Yishuo Cai","Zhenhong Zhou","Renjie Gu","Haiqin Weng","Yan Liu","Tianwei Zhang","Wei Xu","Han Qiu"],"pdf_url":"https://arxiv.org/pdf/2407.16637v1.pdf","comment":"Dataset and script will be available at\n  https://github.com/pillowsofwind/Course-Correction"},{"id":"http://arxiv.org/abs/2307.09379v2","updated":"2024-07-23T16:37:22Z","published":"2023-07-18T16:01:01Z","title":"Generalization within in silico screening","summary":"  In silico screening uses predictive models to select a batch of compounds\nwith favorable properties from a library for experimental validation. Unlike\nconventional learning paradigms, success in this context is measured by the\nperformance of the predictive model on the selected subset of compounds rather\nthan the entire set of predictions. By extending learning theory, we show that\nthe selectivity of the selection policy can significantly impact\ngeneralization, with a higher risk of errors occurring when exclusively\nselecting predicted positives and when targeting rare properties. Our analysis\nsuggests a way to mitigate these challenges. We show that generalization can be\nmarkedly enhanced when considering a model's ability to predict the fraction of\ndesired outcomes in a batch. This is promising, as the primary aim of screening\nis not necessarily to pinpoint the label of each compound individually, but\nrather to assemble a batch enriched for desirable compounds. Our theoretical\ninsights are empirically validated across diverse tasks, architectures, and\nscreening scenarios, underscoring their applicability.\n","authors":["Andreas Loukas","Pan Kessel","Vladimir Gligorijevic","Richard Bonneau"],"pdf_url":"https://arxiv.org/pdf/2307.09379v2.pdf","comment":"9 pages, 3 figures"},{"id":"http://arxiv.org/abs/2403.12143v3","updated":"2024-07-23T16:30:10Z","published":"2024-03-18T18:01:01Z","title":"Graph Neural Networks for Learning Equivariant Representations of Neural\n  Networks","summary":"  Neural networks that process the parameters of other neural networks find\napplications in domains as diverse as classifying implicit neural\nrepresentations, generating neural network weights, and predicting\ngeneralization errors. However, existing approaches either overlook the\ninherent permutation symmetry in the neural network or rely on intricate\nweight-sharing patterns to achieve equivariance, while ignoring the impact of\nthe network architecture itself. In this work, we propose to represent neural\nnetworks as computational graphs of parameters, which allows us to harness\npowerful graph neural networks and transformers that preserve permutation\nsymmetry. Consequently, our approach enables a single model to encode neural\ncomputational graphs with diverse architectures. We showcase the effectiveness\nof our method on a wide range of tasks, including classification and editing of\nimplicit neural representations, predicting generalization performance, and\nlearning to optimize, while consistently outperforming state-of-the-art\nmethods. The source code is open-sourced at\nhttps://github.com/mkofinas/neural-graphs.\n","authors":["Miltiadis Kofinas","Boris Knyazev","Yan Zhang","Yunlu Chen","Gertjan J. Burghouts","Efstratios Gavves","Cees G. M. Snoek","David W. Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.12143v3.pdf","comment":"In ICLR 2024. Source code: https://github.com/mkofinas/neural-graphs"},{"id":"http://arxiv.org/abs/2407.16615v1","updated":"2024-07-23T16:23:04Z","published":"2024-07-23T16:23:04Z","title":"Lawma: The Power of Specialization for Legal Tasks","summary":"  Annotation and classification of legal text are central components of\nempirical legal research. Traditionally, these tasks are often delegated to\ntrained research assistants. Motivated by the advances in language modeling,\nempirical legal scholars are increasingly turning to prompting commercial\nmodels, hoping that it will alleviate the significant cost of human annotation.\nDespite growing use, our understanding of how to best utilize large language\nmodels for legal tasks remains limited. We conduct a comprehensive study of 260\nlegal text classification tasks, nearly all new to the machine learning\ncommunity. Starting from GPT-4 as a baseline, we show that it has non-trivial\nbut highly varied zero-shot accuracy, often exhibiting performance that may be\ninsufficient for legal work. We then demonstrate that a lightly fine-tuned\nLlama 3 model vastly outperforms GPT-4 on almost all tasks, typically by\ndouble-digit percentage points. We find that larger models respond better to\nfine-tuning than smaller models. A few tens to hundreds of examples suffice to\nachieve high classification accuracy. Notably, we can fine-tune a single model\non all 260 tasks simultaneously at a small loss in accuracy relative to having\na separate model for each task. Our work points to a viable alternative to the\npredominant practice of prompting commercial models. For concrete legal tasks\nwith some available labeled data, researchers are better off using a fine-tuned\nopen-source model.\n","authors":["Ricardo Dominguez-Olmedo","Vedant Nanda","Rediet Abebe","Stefan Bechtold","Christoph Engel","Jens Frankenreiter","Krishna Gummadi","Moritz Hardt","Michael Livermore"],"pdf_url":"https://arxiv.org/pdf/2407.16615v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09215v3","updated":"2024-07-23T16:20:54Z","published":"2023-11-15T18:56:51Z","title":"ConvNet vs Transformer, Supervised vs CLIP: Beyond ImageNet Accuracy","summary":"  Modern computer vision offers a great variety of models to practitioners, and\nselecting a model from multiple options for specific applications can be\nchallenging. Conventionally, competing model architectures and training\nprotocols are compared by their classification accuracy on ImageNet. However,\nthis single metric does not fully capture performance nuances critical for\nspecialized tasks. In this work, we conduct an in-depth comparative analysis of\nmodel behaviors beyond ImageNet accuracy, for both ConvNet and Vision\nTransformer architectures, each across supervised and CLIP training paradigms.\nAlthough our selected models have similar ImageNet accuracies and compute\nrequirements, we find that they differ in many other aspects: types of\nmistakes, output calibration, transferability, and feature invariance, among\nothers. This diversity in model characteristics, not captured by traditional\nmetrics, highlights the need for more nuanced analysis when choosing among\ndifferent models. Our code is available at\nhttps://github.com/kirill-vish/Beyond-INet.\n","authors":["Kirill Vishniakov","Zhiqiang Shen","Zhuang Liu"],"pdf_url":"https://arxiv.org/pdf/2311.09215v3.pdf","comment":"Project page: https://kirill-vish.github.io/beyond-imagenet-accuracy/"},{"id":"http://arxiv.org/abs/2407.16611v1","updated":"2024-07-23T16:18:00Z","published":"2024-07-23T16:18:00Z","title":"Local vs Global continual learning","summary":"  Continual learning is the problem of integrating new information in a model\nwhile retaining the knowledge acquired in the past. Despite the tangible\nimprovements achieved in recent years, the problem of continual learning is\nstill an open one. A better understanding of the mechanisms behind the\nsuccesses and failures of existing continual learning algorithms can unlock the\ndevelopment of new successful strategies. In this work, we view continual\nlearning from the perspective of the multi-task loss approximation, and we\ncompare two alternative strategies, namely local and global approximations. We\nclassify existing continual learning algorithms based on the approximation\nused, and we assess the practical effects of this distinction in common\ncontinual learning settings.Additionally, we study optimal continual learning\nobjectives in the case of local polynomial approximations and we provide\nexamples of existing algorithms implementing the optimal objectives\n","authors":["Giulia Lanzillotta","Sidak Pal Singh","Benjamin F. Grewe","Thomas Hofmann"],"pdf_url":"https://arxiv.org/pdf/2407.16611v1.pdf","comment":"(10 pages, Will appear in the proceedings of CoLLAs 2024)"},{"id":"http://arxiv.org/abs/2306.09459v4","updated":"2024-07-23T16:17:36Z","published":"2023-06-15T19:29:08Z","title":"Recurrent Action Transformer with Memory","summary":"  Recently, the use of transformers in offline reinforcement learning has\nbecome a rapidly developing area. This is due to their ability to treat the\nagent's trajectory in the environment as a sequence, thereby reducing the\npolicy learning problem to sequence modeling. In environments where the agent's\ndecisions depend on past events (POMDPs), capturing both the event itself and\nthe decision point in the context of the model is essential. However, the\nquadratic complexity of the attention mechanism limits the potential for\ncontext expansion. One solution to this problem is to enhance transformers with\nmemory mechanisms. This paper proposes a Recurrent Action Transformer with\nMemory (RATE), a novel model architecture incorporating a recurrent memory\nmechanism designed to regulate information retention. To evaluate our model, we\nconducted extensive experiments on memory-intensive environments\n(ViZDoom-Two-Colors, T-Maze, Memory Maze, Minigrid.Memory), classic Atari games\nand MuJoCo control environments. The results show that using memory can\nsignificantly improve performance in memory-intensive environments while\nmaintaining or improving results in classic environments. We hope our findings\nwill stimulate research on memory mechanisms for transformers applicable to\noffline reinforcement learning.\n","authors":["Egor Cherepanov","Alexey Staroverov","Dmitry Yudin","Alexey K. Kovalev","Aleksandr I. Panov"],"pdf_url":"https://arxiv.org/pdf/2306.09459v4.pdf","comment":"18 pages, 9 figures"},{"id":"http://arxiv.org/abs/2407.15247v2","updated":"2024-07-23T16:16:27Z","published":"2024-07-21T19:10:40Z","title":"TimeInf: Time Series Data Contribution via Influence Functions","summary":"  Evaluating the contribution of individual data points to a model's prediction\nis critical for interpreting model predictions and improving model performance.\nExisting data contribution methods have been applied to various data types,\nincluding tabular data, images, and texts; however, their primary focus has\nbeen on i.i.d. settings. Despite the pressing need for principled approaches\ntailored to time series datasets, the problem of estimating data contribution\nin such settings remains unexplored, possibly due to challenges associated with\nhandling inherent temporal dependencies. This paper introduces TimeInf, a data\ncontribution estimation method for time-series datasets. TimeInf uses influence\nfunctions to attribute model predictions to individual time points while\npreserving temporal structures. Our extensive empirical results demonstrate\nthat TimeInf outperforms state-of-the-art methods in identifying harmful\nanomalies and helpful time points for forecasting. Additionally, TimeInf offers\nintuitive and interpretable attributions of data values, allowing us to easily\ndistinguish diverse anomaly patterns through visualizations.\n","authors":["Yizi Zhang","Jingyan Shen","Xiaoxue Xiong","Yongchan Kwon"],"pdf_url":"https://arxiv.org/pdf/2407.15247v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14668v2","updated":"2024-07-23T16:14:27Z","published":"2024-07-19T21:05:28Z","title":"Towards a \"universal translator\" for neural dynamics at single-cell,\n  single-spike resolution","summary":"  Neuroscience research has made immense progress over the last decade, but our\nunderstanding of the brain remains fragmented and piecemeal: the dream of\nprobing an arbitrary brain region and automatically reading out the information\nencoded in its neural activity remains out of reach. In this work, we build\ntowards a first foundation model for neural spiking data that can solve a\ndiverse set of tasks across multiple brain areas. We introduce a novel\nself-supervised modeling approach for population activity in which the model\nalternates between masking out and reconstructing neural activity across\ndifferent time steps, neurons, and brain regions. To evaluate our approach, we\ndesign unsupervised and supervised prediction tasks using the International\nBrain Laboratory repeated site dataset, which is comprised of Neuropixels\nrecordings targeting the same brain locations across 48 animals and\nexperimental sessions. The prediction tasks include single-neuron and\nregion-level activity prediction, forward prediction, and behavior decoding. We\ndemonstrate that our multi-task-masking (MtM) approach significantly improves\nthe performance of current state-of-the-art population models and enables\nmulti-task learning. We also show that by training on multiple animals, we can\nimprove the generalization ability of the model to unseen animals, paving the\nway for a foundation model of the brain at single-cell, single-spike\nresolution.\n","authors":["Yizi Zhang","Yanchen Wang","Donato Jimenez-Beneto","Zixuan Wang","Mehdi Azabou","Blake Richards","Olivier Winter","International Brain Laboratory","Eva Dyer","Liam Paninski","Cole Hurwitz"],"pdf_url":"https://arxiv.org/pdf/2407.14668v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16607v1","updated":"2024-07-23T16:13:22Z","published":"2024-07-23T16:13:22Z","title":"Data Mixture Inference: What do BPE Tokenizers Reveal about their\n  Training Data?","summary":"  The pretraining data of today's strongest language models is opaque. In\nparticular, little is known about the proportions of various domains or\nlanguages represented. In this work, we tackle a task which we call data\nmixture inference, which aims to uncover the distributional make-up of training\ndata. We introduce a novel attack based on a previously overlooked source of\ninformation -- byte-pair encoding (BPE) tokenizers, used by the vast majority\nof modern language models. Our key insight is that the ordered list of merge\nrules learned by a BPE tokenizer naturally reveals information about the token\nfrequencies in its training data: the first merge is the most common byte pair,\nthe second is the most common pair after merging the first token, and so on.\nGiven a tokenizer's merge list along with data samples for each category of\ninterest, we formulate a linear program that solves for the proportion of each\ncategory in the tokenizer's training set. Importantly, to the extent to which\ntokenizer training data is representative of the pretraining data, we\nindirectly learn about the pretraining data. In controlled experiments, we show\nthat our attack recovers mixture ratios with high precision for tokenizers\ntrained on known mixtures of natural languages, programming languages, and data\nsources. We then apply our approach to off-the-shelf tokenizers released with\nrecent LMs. We confirm much publicly disclosed information about these models,\nand also make several new inferences: GPT-4o's tokenizer is much more\nmultilingual than its predecessors, training on 39% non-English data; Llama3\nextends GPT-3.5's tokenizer primarily for multilingual (48%) use; GPT-3.5's and\nClaude's tokenizers are trained on predominantly code (~60%). We hope our work\nsheds light on current design practices for pretraining data, and inspires\ncontinued research into data mixture inference for LMs.\n","authors":["Jonathan Hayase","Alisa Liu","Yejin Choi","Sewoong Oh","Noah A. Smith"],"pdf_url":"https://arxiv.org/pdf/2407.16607v1.pdf","comment":"19 pages, 5 figures"},{"id":"http://arxiv.org/abs/2407.16606v1","updated":"2024-07-23T16:11:08Z","published":"2024-07-23T16:11:08Z","title":"Learning to Play Foosball: System and Baselines","summary":"  This work stages Foosball as a versatile platform for advancing scientific\nresearch, particularly in the realm of robot learning. We present an automated\nFoosball table along with its corresponding simulated counterpart, showcasing a\ndiverse range of challenges through example tasks within the Foosball\nenvironment. Initial findings are shared using a simple baseline approach.\nFoosball constitutes a versatile learning environment with the potential to\nyield cutting-edge research in various fields of artificial intelligence and\nmachine learning, notably robust learning, while also extending its\napplicability to industrial robotics and automation setups. To transform our\nphysical Foosball table into a research-friendly system, we augmented it with a\n2 degrees of freedom kinematic chain to control the goalkeeper rod as an\ninitial setup with the intention to be extended to the full game as soon as\npossible. Our experiments reveal that a realistic simulation is essential for\nmastering complex robotic tasks, yet translating these accomplishments to the\nreal system remains challenging, often accompanied by a performance decline.\nThis emphasizes the critical importance of research in this direction. In this\nconcern, we spotlight the automated Foosball table as an invaluable tool,\npossessing numerous desirable attributes, to serve as a demanding learning\nenvironment for advancing robotics and automation research.\n","authors":["Janosch Moos","Cedric Derstroff","Niklas Schröder","Debora Clever"],"pdf_url":"https://arxiv.org/pdf/2407.16606v1.pdf","comment":"7 pages, 5 figures, 2024 IEEE International Conference on Robotics\n  and Automation (ICRA 2024)"},{"id":"http://arxiv.org/abs/2403.10923v2","updated":"2024-07-23T16:10:52Z","published":"2024-03-16T13:35:15Z","title":"Interpretable Machine Learning for TabPFN","summary":"  The recently developed Prior-Data Fitted Networks (PFNs) have shown very\npromising results for applications in low-data regimes. The TabPFN model, a\nspecial case of PFNs for tabular data, is able to achieve state-of-the-art\nperformance on a variety of classification tasks while producing posterior\npredictive distributions in mere seconds by in-context learning without the\nneed for learning parameters or hyperparameter tuning. This makes TabPFN a very\nattractive option for a wide range of domain applications. However, a major\ndrawback of the method is its lack of interpretability. Therefore, we propose\nseveral adaptations of popular interpretability methods that we specifically\ndesign for TabPFN. By taking advantage of the unique properties of the model,\nour adaptations allow for more efficient computations than existing\nimplementations. In particular, we show how in-context learning facilitates the\nestimation of Shapley values by avoiding approximate retraining and enables the\nuse of Leave-One-Covariate-Out (LOCO) even when working with large-scale\nTransformers. In addition, we demonstrate how data valuation methods can be\nused to address scalability challenges of TabPFN. Our proposed methods are\nimplemented in a package tabpfn_iml and made available at\nhttps://github.com/david-rundel/tabpfn_iml.\n","authors":["David Rundel","Julius Kobialka","Constantin von Crailsheim","Matthias Feurer","Thomas Nagler","David Rügamer"],"pdf_url":"https://arxiv.org/pdf/2403.10923v2.pdf","comment":"This preprint has not undergone peer review or any post-submission\n  improvements or corrections. The Version of Record of this contribution is\n  published in Explainable Artificial Intelligence, and is available online at\n  https://doi.org/10.1007/978-3-031-63797-1_23"},{"id":"http://arxiv.org/abs/2407.16602v1","updated":"2024-07-23T16:04:55Z","published":"2024-07-23T16:04:55Z","title":"Functional Acceleration for Policy Mirror Descent","summary":"  We apply functional acceleration to the Policy Mirror Descent (PMD) general\nfamily of algorithms, which cover a wide range of novel and fundamental methods\nin Reinforcement Learning (RL). Leveraging duality, we propose a momentum-based\nPMD update. By taking the functional route, our approach is independent of the\npolicy parametrization and applicable to large-scale optimization, covering\nprevious applications of momentum at the level of policy parameters as a\nspecial case. We theoretically analyze several properties of this approach and\ncomplement with a numerical ablation study, which serves to illustrate the\npolicy optimization dynamics on the value polytope, relative to different\nalgorithmic design choices in this space. We further characterize numerically\nseveral features of the problem setting relevant for functional acceleration,\nand lastly, we investigate the impact of approximation on their learning\nmechanics.\n","authors":["Veronica Chelu","Doina Precup"],"pdf_url":"https://arxiv.org/pdf/2407.16602v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.15870v4","updated":"2024-07-23T15:30:32Z","published":"2023-07-29T02:35:37Z","title":"SemiSFL: Split Federated Learning on Unlabeled and Non-IID Data","summary":"  Federated Learning (FL) has emerged to allow multiple clients to\ncollaboratively train machine learning models on their private data at the\nnetwork edge. However, training and deploying large-scale models on\nresource-constrained devices is challenging. Fortunately, Split Federated\nLearning (SFL) offers a feasible solution by alleviating the computation and/or\ncommunication burden on clients. However, existing SFL works often assume\nsufficient labeled data on clients, which is usually impractical. Besides, data\nnon-IIDness poses another challenge to ensure efficient model training. To our\nbest knowledge, the above two issues have not been simultaneously addressed in\nSFL. Herein, we propose a novel Semi-supervised SFL system, termed SemiSFL,\nwhich incorporates clustering regularization to perform SFL with unlabeled and\nnon-IID client data. Moreover, our theoretical and experimental investigations\ninto model convergence reveal that the inconsistent training processes on\nlabeled and unlabeled data have an influence on the effectiveness of clustering\nregularization. To mitigate the training inconsistency, we develop an algorithm\nfor dynamically adjusting the global updating frequency, so as to improve\ntraining performance. Extensive experiments on benchmark models and datasets\nshow that our system provides a 3.8x speed-up in training time, reduces the\ncommunication cost by about 70.3% while reaching the target accuracy, and\nachieves up to 5.8% improvement in accuracy under non-IID scenarios compared to\nthe state-of-the-art baselines.\n","authors":["Yang Xu","Yunming Liao","Hongli Xu","Zhipeng Sun","Liusheng Huang","Chunming Qiao"],"pdf_url":"https://arxiv.org/pdf/2307.15870v4.pdf","comment":"16 pages"},{"id":"http://arxiv.org/abs/2401.17460v2","updated":"2024-07-23T15:14:08Z","published":"2024-01-30T21:46:09Z","title":"Rendering Wireless Environments Useful for Gradient Estimators: A\n  Zero-Order Stochastic Federated Learning Method","summary":"  Cross-device federated learning (FL) is a growing machine learning setting\nwhereby multiple edge devices collaborate to train a model without disclosing\ntheir raw data. With the great number of mobile devices participating in more\nFL applications via the wireless environment, the practical implementation of\nthese applications will be hindered due to the limited uplink capacity of\ndevices, causing critical bottlenecks. In this work, we propose a novel doubly\ncommunication-efficient zero-order (ZO) method with a one-point gradient\nestimator that replaces communicating long vectors with scalar values and that\nharnesses the nature of the wireless communication channel, overcoming the need\nto know the channel state coefficient. It is the first method that includes the\nwireless channel in the learning algorithm itself instead of wasting resources\nto analyze it and remove its impact. We then offer a thorough analysis of the\nproposed zero-order federated learning (ZOFL) framework and prove that our\nmethod converges \\textit{almost surely}, which is a novel result in nonconvex\nZO optimization. We further prove a convergence rate of\n$O(\\frac{1}{\\sqrt[3]{K}})$ in the nonconvex setting. We finally demonstrate the\npotential of our algorithm with experimental results.\n","authors":["Elissa Mhanna","Mohamad Assaad"],"pdf_url":"https://arxiv.org/pdf/2401.17460v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16556v1","updated":"2024-07-23T15:09:40Z","published":"2024-07-23T15:09:40Z","title":"DC is all you need: describing ReLU from a signal processing standpoint","summary":"  Non-linear activation functions are crucial in Convolutional Neural Networks.\nHowever, until now they have not been well described in the frequency domain.\nIn this work, we study the spectral behavior of ReLU, a popular activation\nfunction. We use the ReLU's Taylor expansion to derive its frequency domain\nbehavior. We demonstrate that ReLU introduces higher frequency oscillations in\nthe signal and a constant DC component. Furthermore, we investigate the\nimportance of this DC component, where we demonstrate that it helps the model\nextract meaningful features related to the input frequency content. We\naccompany our theoretical derivations with experiments and real-world examples.\nFirst, we numerically validate our frequency response model. Then we observe\nReLU's spectral behavior on two example models and a real-world one. Finally,\nwe experimentally investigate the role of the DC component introduced by ReLU\nin the CNN's representations. Our results indicate that the DC helps to\nconverge to a weight configuration that is close to the initial random weights.\n","authors":["Christodoulos Kechris","Jonathan Dan","Jose Miranda","David Atienza"],"pdf_url":"https://arxiv.org/pdf/2407.16556v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.14496v5","updated":"2024-07-23T15:01:06Z","published":"2023-09-25T19:45:45Z","title":"Era Splitting: Invariant Learning for Decision Trees","summary":"  Real-life machine learning problems exhibit distributional shifts in the data\nfrom one time to another or from one place to another. This behavior is beyond\nthe scope of the traditional empirical risk minimization paradigm, which\nassumes i.i.d. distribution of data over time and across locations. The\nemerging field of out-of-distribution (OOD) generalization addresses this\nreality with new theory and algorithms which incorporate \"environmental\", or\n\"era-wise\" information into the algorithms. So far, most research has been\nfocused on linear models and/or neural networks . In this research we develop\ntwo new splitting criteria for decision trees, which allow us to apply ideas\nfrom OOD generalization research to decision tree models, namely, gradient\nboosting decision trees (GBDTs). The new splitting criteria use era-wise\ninformation associated with the data to grow tree-based models that are optimal\nacross all disjoint eras in the data, instead of optimal over the entire data\nset pooled together, which is the default setting. In this paper, two new\nsplitting criteria are defined and analyzed theoretically. Effectiveness is\ntested on four experiments, ranging from simple, synthetic to complex,\nreal-world applications. In particular we cast the OOD domain-adaptation\nproblem in the context of financial markets, where the new models out-perform\nstate-of-the-art GBDT models on the Numerai data set. The new criteria are\nincorporated into the Scikit-Learn code base and made freely available online.\n","authors":["Timothy DeLise"],"pdf_url":"https://arxiv.org/pdf/2309.14496v5.pdf","comment":"29 pages, 9 figures, 3 tables, 2 algorithms"},{"id":"http://arxiv.org/abs/2402.15602v2","updated":"2024-07-23T15:00:52Z","published":"2024-02-23T20:51:31Z","title":"Minimax Optimality of Score-based Diffusion Models: Beyond the Density\n  Lower Bound Assumptions","summary":"  We study the asymptotic error of score-based diffusion model sampling in\nlarge-sample scenarios from a non-parametric statistics perspective. We show\nthat a kernel-based score estimator achieves an optimal mean square error of\n$\\widetilde{O}\\left(n^{-1} t^{-\\frac{d+2}{2}}(t^{\\frac{d}{2}} \\vee 1)\\right)$\nfor the score function of $p_0*\\mathcal{N}(0,t\\boldsymbol{I}_d)$, where $n$ and\n$d$ represent the sample size and the dimension, $t$ is bounded above and below\nby polynomials of $n$, and $p_0$ is an arbitrary sub-Gaussian distribution. As\na consequence, this yields an $\\widetilde{O}\\left(n^{-1/2}\nt^{-\\frac{d}{4}}\\right)$ upper bound for the total variation error of the\ndistribution of the sample generated by the diffusion model under a mere\nsub-Gaussian assumption. If in addition, $p_0$ belongs to the nonparametric\nfamily of the $\\beta$-Sobolev space with $\\beta\\le 2$, by adopting an early\nstopping strategy, we obtain that the diffusion model is nearly (up to log\nfactors) minimax optimal. This removes the crucial lower bound assumption on\n$p_0$ in previous proofs of the minimax optimality of the diffusion model for\nnonparametric families.\n","authors":["Kaihong Zhang","Caitlyn H. Yin","Feng Liang","Jingbo Liu"],"pdf_url":"https://arxiv.org/pdf/2402.15602v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.13782v3","updated":"2024-07-23T14:49:43Z","published":"2024-01-24T20:05:49Z","title":"Position: AI/ML Influencers Have a Place in the Academic Process","summary":"  As the number of accepted papers at AI and ML conferences reaches into the\nthousands, it has become unclear how researchers access and read research\npublications. In this paper, we investigate the role of social media\ninfluencers in enhancing the visibility of machine learning research,\nparticularly the citation counts of papers they share. We have compiled a\ncomprehensive dataset of over 8,000 papers, spanning tweets from December 2018\nto October 2023, alongside controls precisely matched by 9 key covariates. Our\nstatistical and causal inference analysis reveals a significant increase in\ncitations for papers endorsed by these influencers, with median citation counts\n2-3 times higher than those of the control group. Additionally, the study\ndelves into the geographic, gender, and institutional diversity of highlighted\nauthors. Given these findings, we advocate for a responsible approach to\ncuration, encouraging influencers to uphold the journalistic standard that\nincludes showcasing diverse research topics, authors, and institutions.\n","authors":["Iain Xie Weissburg","Mehir Arora","Xinyi Wang","Liangming Pan","William Yang Wang"],"pdf_url":"https://arxiv.org/pdf/2401.13782v3.pdf","comment":"15 Pages, 22 Figures, ICML 2024"},{"id":"http://arxiv.org/abs/2403.12116v2","updated":"2024-07-23T14:49:22Z","published":"2024-03-18T16:14:28Z","title":"Unsupervised End-to-End Training with a Self-Defined Target","summary":"  Designing algorithms for versatile AI hardware that can learn on the edge\nusing both labeled and unlabeled data is challenging. Deep end-to-end training\nmethods incorporating phases of self-supervised and supervised learning are\naccurate and adaptable to input data but self-supervised learning requires even\nmore computational and memory resources than supervised learning, too high for\ncurrent embedded hardware. Conversely, unsupervised layer-by-layer training,\nsuch as Hebbian learning, is more compatible with existing hardware but does\nnot integrate well with supervised learning. To address this, we propose a\nmethod enabling networks or hardware designed for end-to-end supervised\nlearning to also perform high-performance unsupervised learning by adding two\nsimple elements to the output layer: Winner-Take-All (WTA) selectivity and\nhomeostasis regularization. These mechanisms introduce a \"self-defined target\"\nfor unlabeled data, allowing purely unsupervised training for both\nfully-connected and convolutional layers using backpropagation or equilibrium\npropagation on datasets like MNIST (up to 99.2%), Fashion-MNIST (up to 90.3%),\nand SVHN (up to 81.5%). We extend this method to semi-supervised learning,\nadjusting targets based on data type, achieving 96.6% accuracy with only 600\nlabeled MNIST samples in a multi-layer perceptron. Our results show that this\napproach can effectively enable networks and hardware initially dedicated to\nsupervised learning to also perform unsupervised learning, adapting to varying\navailability of labeled data.\n","authors":["Dongshu Liu","Jérémie Laydevant","Adrien Pontlevy","Damien Querlioz","Julie Grollier"],"pdf_url":"https://arxiv.org/pdf/2403.12116v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16539v1","updated":"2024-07-23T14:49:17Z","published":"2024-07-23T14:49:17Z","title":"Enhancing Encrypted Internet Traffic Classification Through Advanced\n  Data Augmentation Techniques","summary":"  The increasing popularity of online services has made Internet Traffic\nClassification a critical field of study. However, the rapid development of\ninternet protocols and encryption limits usable data availability. This paper\naddresses the challenges of classifying encrypted internet traffic, focusing on\nthe scarcity of open-source datasets and limitations of existing ones. We\npropose two Data Augmentation (DA) techniques to synthetically generate data\nbased on real samples: Average augmentation and MTU augmentation. Both\naugmentations are aimed to improve the performance of the classifier, each from\na different perspective: The Average augmentation aims to increase dataset size\nby generating new synthetic samples, while the MTU augmentation enhances\nclassifier robustness to varying Maximum Transmission Units (MTUs). Our\nexperiments, conducted on two well-known academic datasets and a commercial\ndataset, demonstrate the effectiveness of these approaches in improving model\nperformance and mitigating constraints associated with limited and homogeneous\ndatasets. Our findings underscore the potential of data augmentation in\naddressing the challenges of modern internet traffic classification.\nSpecifically, we show that our augmentation techniques significantly enhance\nencrypted traffic classification models. This improvement can positively impact\nuser Quality of Experience (QoE) by more accurately classifying traffic as\nvideo streaming (e.g., YouTube) or chat (e.g., Google Chat). Additionally, it\ncan enhance Quality of Service (QoS) for file downloading activities (e.g.,\nGoogle Docs).\n","authors":["Yehonatan Zion","Porat Aharon","Ran Dubin","Amit Dvir","Chen Hajaj"],"pdf_url":"https://arxiv.org/pdf/2407.16539v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16526v1","updated":"2024-07-23T14:39:40Z","published":"2024-07-23T14:39:40Z","title":"Imperfect Vision Encoders: Efficient and Robust Tuning for\n  Vision-Language Models","summary":"  Vision language models (VLMs) demonstrate impressive capabilities in visual\nquestion answering and image captioning, acting as a crucial link between\nvisual and language models. However, existing open-source VLMs heavily rely on\npretrained and frozen vision encoders (such as CLIP). Despite CLIP's robustness\nacross diverse domains, it still exhibits non-negligible image understanding\nerrors. These errors propagate to the VLM responses, resulting in sub-optimal\nperformance. In our work, we propose an efficient and robust method for\nupdating vision encoders within VLMs. Our approach selectively and locally\nupdates encoders, leading to substantial performance improvements on data where\nprevious mistakes occurred, while maintaining overall robustness. Furthermore,\nwe demonstrate the effectiveness of our method during continual few-shot\nupdates. Theoretical grounding, generality, and computational efficiency\ncharacterize our approach.\n","authors":["Aristeidis Panos","Rahaf Aljundi","Daniel Olmeda Reino","Richard E Turner"],"pdf_url":"https://arxiv.org/pdf/2407.16526v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08320v4","updated":"2024-07-23T14:39:23Z","published":"2023-10-12T13:33:04Z","title":"Defending Our Privacy With Backdoors","summary":"  The proliferation of large AI models trained on uncurated, often sensitive\nweb-scraped data has raised significant privacy concerns. One of the concerns\nis that adversaries can extract information about the training data using\nprivacy attacks. Unfortunately, the task of removing specific information from\nthe models without sacrificing performance is not straightforward and has\nproven to be challenging. We propose a rather easy yet effective defense based\non backdoor attacks to remove private information, such as names and faces of\nindividuals, from vision-language models by fine-tuning them for only a few\nminutes instead of re-training them from scratch. Specifically, by\nstrategically inserting backdoors into text encoders, we align the embeddings\nof sensitive phrases with those of neutral terms-\"a person\" instead of the\nperson's actual name. For image encoders, we map individuals' embeddings to be\nremoved from the model to a universal, anonymous embedding. The results of our\nextensive experimental evaluation demonstrate the effectiveness of our\nbackdoor-based defense on CLIP by assessing its performance using a specialized\nprivacy attack for zero-shot classifiers. Our approach provides a new\n\"dual-use\" perspective on backdoor attacks and presents a promising avenue to\nenhance the privacy of individuals within models trained on uncurated\nweb-scraped data.\n","authors":["Dominik Hintersdorf","Lukas Struppek","Daniel Neider","Kristian Kersting"],"pdf_url":"https://arxiv.org/pdf/2310.08320v4.pdf","comment":"Accepted at ECAI 2024"},{"id":"http://arxiv.org/abs/2303.13123v2","updated":"2024-07-23T14:38:34Z","published":"2023-03-23T09:23:57Z","title":"Laplacian Segmentation Networks Improve Epistemic Uncertainty\n  Quantification","summary":"  Image segmentation relies heavily on neural networks which are known to be\noverconfident, especially when making predictions on out-of-distribution (OOD)\nimages. This is a common scenario in the medical domain due to variations in\nequipment, acquisition sites, or image corruptions. This work addresses the\nchallenge of OOD detection by proposing Laplacian Segmentation Networks (LSN):\nmethods which jointly model epistemic (model) and aleatoric (data) uncertainty\nfor OOD detection. In doing so, we propose the first Laplace approximation of\nthe weight posterior that scales to large neural networks with skip connections\nthat have high-dimensional outputs. We demonstrate on three datasets that the\nLSN-modeled parameter distributions, in combination with suitable uncertainty\nmeasures, gives superior OOD detection.\n","authors":["Kilian Zepf","Selma Wanna","Marco Miani","Juston Moore","Jes Frellsen","Søren Hauberg","Frederik Warburg","Aasa Feragen"],"pdf_url":"https://arxiv.org/pdf/2303.13123v2.pdf","comment":"Published in the Conference Proceedings of the 27th International\n  Conference on Medical Image Computing and Computer Assisted Intervention\n  (MICCAI)"},{"id":"http://arxiv.org/abs/2407.16515v1","updated":"2024-07-23T14:30:53Z","published":"2024-07-23T14:30:53Z","title":"Spurious Correlations in Concept Drift: Can Explanatory Interaction\n  Help?","summary":"  Long-running machine learning models face the issue of concept drift (CD),\nwhereby the data distribution changes over time, compromising prediction\nperformance. Updating the model requires detecting drift by monitoring the data\nand/or the model for unexpected changes. We show that, however, spurious\ncorrelations (SCs) can spoil the statistics tracked by detection algorithms.\nMotivated by this, we introduce ebc-exstream, a novel detector that leverages\nmodel explanations to identify potential SCs and human feedback to correct for\nthem. It leverages an entropy-based heuristic to reduce the amount of necessary\nfeedback, cutting annotation costs. Our preliminary experiments on artificially\nconfounded data highlight the promise of ebc-exstream for reducing the impact\nof SCs on detection.\n","authors":["Cristiana Lalletti","Stefano Teso"],"pdf_url":"https://arxiv.org/pdf/2407.16515v1.pdf","comment":"Extended abstract"},{"id":"http://arxiv.org/abs/2407.04149v2","updated":"2024-07-23T14:25:08Z","published":"2024-07-04T20:53:19Z","title":"SineKAN: Kolmogorov-Arnold Networks Using Sinusoidal Activation\n  Functions","summary":"  Recent work has established an alternative to traditional multi-layer\nperceptron neural networks in the form of Kolmogorov-Arnold Networks (KAN). The\ngeneral KAN framework uses learnable activation functions on the edges of the\ncomputational graph followed by summation on nodes. The learnable edge\nactivation functions in the original implementation are basis spline functions\n(B-Spline). Here, we present a model in which learnable grids of B-Spline\nactivation functions are replaced by grids of re-weighted sine functions. We\nshow that this leads to better or comparable numerical performance to B-Spline\nKAN models on the MNIST benchmark, while also providing a substantial speed\nincrease on the order of 4-8 times.\n","authors":["Eric A. F. Reinhardt","P. R. Dinesh","Sergei Gleyzer"],"pdf_url":"https://arxiv.org/pdf/2407.04149v2.pdf","comment":"9 pages, 9 figures"},{"id":"http://arxiv.org/abs/2404.12368v3","updated":"2024-07-23T14:13:48Z","published":"2024-04-18T17:50:23Z","title":"Gradient-Regularized Out-of-Distribution Detection","summary":"  One of the challenges for neural networks in real-life applications is the\noverconfident errors these models make when the data is not from the original\ntraining distribution.\n  Addressing this issue is known as Out-of-Distribution (OOD) detection.\n  Many state-of-the-art OOD methods employ an auxiliary dataset as a surrogate\nfor OOD data during training to achieve improved performance.\n  However, these methods fail to fully exploit the local information embedded\nin the auxiliary dataset.\n  In this work, we propose the idea of leveraging the information embedded in\nthe gradient of the loss function during training to enable the network to not\nonly learn a desired OOD score for each sample but also to exhibit similar\nbehavior in a local neighborhood around each sample.\n  We also develop a novel energy-based sampling method to allow the network to\nbe exposed to more informative OOD samples during the training phase. This is\nespecially important when the auxiliary dataset is large. We demonstrate the\neffectiveness of our method through extensive experiments on several OOD\nbenchmarks, improving the existing state-of-the-art FPR95 by 4% on our ImageNet\nexperiment.\n  We further provide a theoretical analysis through the lens of certified\nrobustness and Lipschitz analysis to showcase the theoretical foundation of our\nwork. Our code is available at https://github.com/o4lc/Greg-OOD.\n","authors":["Sina Sharifi","Taha Entesari","Bardia Safaei","Vishal M. Patel","Mahyar Fazlyab"],"pdf_url":"https://arxiv.org/pdf/2404.12368v3.pdf","comment":"Accepted to ECCV 2024"},{"id":"http://arxiv.org/abs/2407.16496v1","updated":"2024-07-23T14:11:12Z","published":"2024-07-23T14:11:12Z","title":"Articulation Work and Tinkering for Fairness in Machine Learning","summary":"  The field of fair AI aims to counter biased algorithms through computational\nmodelling. However, it faces increasing criticism for perpetuating the use of\noverly technical and reductionist methods. As a result, novel approaches appear\nin the field to address more socially-oriented and interdisciplinary (SOI)\nperspectives on fair AI. In this paper, we take this dynamic as the starting\npoint to study the tension between computer science (CS) and SOI research. By\ndrawing on STS and CSCW theory, we position fair AI research as a matter of\n'organizational alignment': what makes research 'doable' is the successful\nalignment of three levels of work organization (the social world, the\nlaboratory and the experiment). Based on qualitative interviews with CS\nresearchers, we analyze the tasks, resources, and actors required for doable\nresearch in the case of fair AI. We find that CS researchers engage with SOI to\nsome extent, but organizational conditions, articulation work, and ambiguities\nof the social world constrain the doability of SOI research. Based on our\nfindings, we identify and discuss problems for aligning CS and SOI as fair AI\ncontinues to evolve.\n","authors":["Miriam Fahimi","Mayra Russo","Kristen M. Scott","Maria-Esther Vidal","Bettina Berendt","Katharina Kinder-Kurlanda"],"pdf_url":"https://arxiv.org/pdf/2407.16496v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16485v1","updated":"2024-07-23T14:00:18Z","published":"2024-07-23T14:00:18Z","title":"Learning General Continuous Constraint from Demonstrations via\n  Positive-Unlabeled Learning","summary":"  Planning for a wide range of real-world tasks necessitates to know and write\nall constraints. However, instances exist where these constraints are either\nunknown or challenging to specify accurately. A possible solution is to infer\nthe unknown constraints from expert demonstration. The majority of prior works\nlimit themselves to learning simple linear constraints, or require strong\nknowledge of the true constraint parameterization or environmental model. To\nmitigate these problems, this paper presents a positive-unlabeled (PU) learning\napproach to infer a continuous, arbitrary and possibly nonlinear, constraint\nfrom demonstration. From a PU learning view, We treat all data in\ndemonstrations as positive (feasible) data, and learn a (sub)-optimal policy to\ngenerate high-reward-winning but potentially infeasible trajectories, which\nserve as unlabeled data containing both feasible and infeasible states. Under\nan assumption on data distribution, a feasible-infeasible classifier (i.e.,\nconstraint model) is learned from the two datasets through a postprocessing PU\nlearning technique. The entire method employs an iterative framework\nalternating between updating the policy, which generates and selects\nhigher-reward policies, and updating the constraint model. Additionally, a\nmemory buffer is introduced to record and reuse samples from previous\niterations to prevent forgetting. The effectiveness of the proposed method is\nvalidated in two Mujoco environments, successfully inferring continuous\nnonlinear constraints and outperforming a baseline method in terms of\nconstraint accuracy and policy safety.\n","authors":["Baiyu Peng","Aude Billard"],"pdf_url":"https://arxiv.org/pdf/2407.16485v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.02984v2","updated":"2024-07-23T13:56:20Z","published":"2024-05-05T16:07:23Z","title":"E-TSL: A Continuous Educational Turkish Sign Language Dataset with\n  Baseline Methods","summary":"  This study introduces the continuous Educational Turkish Sign Language\n(E-TSL) dataset, collected from online Turkish language lessons for 5th, 6th,\nand 8th grades. The dataset comprises 1,410 videos totaling nearly 24 hours and\nincludes performances from 11 signers. Turkish, an agglutinative language,\nposes unique challenges for sign language translation, particularly with a\nvocabulary where 64% are singleton words and 85% are rare words, appearing less\nthan five times. We developed two baseline models to address these challenges:\nthe Pose to Text Transformer (P2T-T) and the Graph Neural Network based\nTransformer (GNN-T) models. The GNN-T model achieved 19.13% BLEU-1 score and\n3.28% BLEU-4 score, presenting a significant challenge compared to existing\nbenchmarks. The P2T-T model, while demonstrating slightly lower performance in\nBLEU scores, achieved a higher ROUGE-L score of 22.09%. Additionally, we\nbenchmarked our model using the well-known PHOENIX-Weather 2014T dataset to\nvalidate our approach.\n","authors":["Şükrü Öztürk","Hacer Yalim Keles"],"pdf_url":"https://arxiv.org/pdf/2405.02984v2.pdf","comment":"7 pages, 3 figures, 4 tables"},{"id":"http://arxiv.org/abs/2402.14973v3","updated":"2024-07-23T13:54:16Z","published":"2024-02-22T21:22:04Z","title":"GenCeption: Evaluate Multimodal LLMs with Unlabeled Unimodal Data","summary":"  Multimodal Large Language Models (MLLMs) are typically assessed using\nexpensive annotated multimodal benchmarks, which often lag behind the rapidly\nevolving demands of MLLM evaluation. This paper outlines and validates\nGenCeption, a novel, annotation-free evaluation method that requires only\nunimodal data to measure inter-modality semantic coherence and inversely\nassesses MLLMs' tendency to hallucinate. This approach eliminates the need for\ncostly data annotation, minimizes the risk of training data contamination,\nresults in slower benchmark saturation, and avoids the illusion of emerging\nabilities. Inspired by the DrawCeption game, GenCeption begins with a\nnon-textual sample and proceeds through iterative description and generation\nsteps. The semantic drift across iterations is quantified using the GC@T\nmetric. Based on the GenCeption method, we establish the MMECeption benchmark\nfor evaluating Vision LLMs (VLLMs), and compare performance of several popular\nVLLMs and human annotators. Our empirical results validate GenCeption's\neffectiveness, demonstrating strong correlations with established VLLM\nbenchmarks. VLLMs still significantly lack behind human performance and\nstruggle especially with text-intensive tasks.\n","authors":["Lele Cao","Valentin Buchner","Zineb Senane","Fangkai Yang"],"pdf_url":"https://arxiv.org/pdf/2402.14973v3.pdf","comment":"Significantly extended from v2. Source code:\n  https://github.com/llcresearch/GenCeption. Leaderboard:\n  https://huggingface.co/spaces/valbuc/GenCeption"},{"id":"http://arxiv.org/abs/2407.16482v1","updated":"2024-07-23T13:53:22Z","published":"2024-07-23T13:53:22Z","title":"BONES: a Benchmark fOr Neural Estimation of Shapley values","summary":"  Shapley Values are concepts established for eXplainable AI. They are used to\nexplain black-box predictive models by quantifying the features' contributions\nto the model's outcomes. Since computing the exact Shapley Values is known to\nbe computationally intractable on real-world datasets, neural estimators have\nemerged as alternative, more scalable approaches to get approximated Shapley\nValues estimates. However, experiments with neural estimators are currently\nhard to replicate as algorithm implementations, explainer evaluators, and\nresults visualizations are neither standardized nor promptly usable. To bridge\nthis gap, we present BONES, a new benchmark focused on neural estimation of\nShapley Value. It provides researchers with a suite of state-of-the-art neural\nand traditional estimators, a set of commonly used benchmark datasets, ad hoc\nmodules for training black-box models, as well as specific functions to easily\ncompute the most popular evaluation metrics and visualize results. The purpose\nis to simplify XAI model usage, evaluation, and comparison. In this paper, we\nshowcase BONES results and visualizations for XAI model benchmarking on both\ntabular and image data. The open-source library is available at the following\nlink: https://github.com/DavideNapolitano/BONES.\n","authors":["Davide Napolitano","Luca Cagliero"],"pdf_url":"https://arxiv.org/pdf/2407.16482v1.pdf","comment":"6 pages"},{"id":"http://arxiv.org/abs/2402.08823v2","updated":"2024-07-23T13:52:28Z","published":"2024-02-13T22:07:29Z","title":"RanDumb: A Simple Approach that Questions the Efficacy of Continual\n  Representation Learning","summary":"  Continual learning has primarily focused on the issue of catastrophic\nforgetting and the associated stability-plasticity tradeoffs. However, little\nattention has been paid to the efficacy of continually learned representations,\nas representations are learned alongside classifiers throughout the learning\nprocess. Our primary contribution is empirically demonstrating that existing\nonline continually trained deep networks produce inferior representations\ncompared to a simple pre-defined random transforms. Our approach embeds raw\npixels using a fixed random transform, approximating an RBF-Kernel initialized\nbefore any data is seen. We then train a simple linear classifier on top\nwithout storing any exemplars, processing one sample at a time in an online\ncontinual learning setting. This method, called RanDumb, significantly\noutperforms state-of-the-art continually learned representations across all\nstandard online continual learning benchmarks. Our study reveals the\nsignificant limitations of representation learning, particularly in\nlow-exemplar and online continual learning scenarios. Extending our\ninvestigation to popular exemplar-free scenarios with pretrained models, we\nfind that training only a linear classifier on top of pretrained\nrepresentations surpasses most continual fine-tuning and prompt-tuning\nstrategies. Overall, our investigation challenges the prevailing assumptions\nabout effective representation learning in online continual learning. Our code\nis available at://github.com/drimpossible/RanDumb.\n","authors":["Ameya Prabhu","Shiven Sinha","Ponnurangam Kumaraguru","Philip H. S. Torr","Ozan Sener","Puneet K. Dokania"],"pdf_url":"https://arxiv.org/pdf/2402.08823v2.pdf","comment":"Tech Report"},{"id":"http://arxiv.org/abs/2303.01335v3","updated":"2024-07-23T13:36:51Z","published":"2023-03-02T15:13:37Z","title":"First-order ANIL provably learns representations despite\n  overparametrization","summary":"  Due to its empirical success in few-shot classification and reinforcement\nlearning, meta-learning has recently received significant interest.\nMeta-learning methods leverage data from previous tasks to learn a new task in\na sample-efficient manner. In particular, model-agnostic methods look for\ninitialization points from which gradient descent quickly adapts to any new\ntask. Although it has been empirically suggested that such methods perform well\nby learning shared representations during pretraining, there is limited\ntheoretical evidence of such behavior. More importantly, it has not been shown\nthat these methods still learn a shared structure, despite architectural\nmisspecifications. In this direction, this work shows, in the limit of an\ninfinite number of tasks, that first-order ANIL with a linear two-layer network\narchitecture successfully learns linear shared representations. This result\neven holds with overparametrization; having a width larger than the dimension\nof the shared representations results in an asymptotically low-rank solution.\nThe learned solution then yields a good adaptation performance on any new task\nafter a single gradient step. Overall, this illustrates how well model-agnostic\nmethods such as first-order ANIL can learn shared representations.\n","authors":["Oğuz Kaan Yüksel","Etienne Boursier","Nicolas Flammarion"],"pdf_url":"https://arxiv.org/pdf/2303.01335v3.pdf","comment":"42 pages, 17 figures"},{"id":"http://arxiv.org/abs/2407.16468v1","updated":"2024-07-23T13:34:35Z","published":"2024-07-23T13:34:35Z","title":"Enhancing GNNs Performance on Combinatorial Optimization by Recurrent\n  Feature Update","summary":"  Combinatorial optimization (CO) problems are crucial in various scientific\nand industrial applications. Recently, researchers have proposed using\nunsupervised Graph Neural Networks (GNNs) to address NP-hard combinatorial\noptimization problems, which can be reformulated as Quadratic Unconstrained\nBinary Optimization (QUBO) problems. GNNs have demonstrated high performance\nwith nearly linear scalability and significantly outperformed classic\nheuristic-based algorithms in terms of computational efficiency on large-scale\nproblems. However, when utilizing standard node features, GNNs tend to get\ntrapped to suboptimal local minima of the energy landscape, resulting in low\nquality solutions. We introduce a novel algorithm, denoted hereafter as\nQRF-GNN, leveraging the power of GNNs to efficiently solve CO problems with\nQUBO formulation. It relies on unsupervised learning by minimizing the loss\nfunction derived from QUBO relaxation. The proposed key components of the\narchitecture include the recurrent use of intermediate GNN predictions,\nparallel convolutional layers and combination of static node features as input.\nAltogether, it helps to adapt the intermediate solution candidate to minimize\nQUBO-based loss function, taking into account not only static graph features,\nbut also intermediate predictions treated as dynamic, i.e. iteratively changing\nrecurrent features. The performance of the proposed algorithm has been\nevaluated on the canonical benchmark datasets for maximum cut, graph coloring\nand maximum independent set problems. Results of experiments show that QRF-GNN\ndrastically surpasses existing learning-based approaches and is comparable to\nthe state-of-the-art conventional heuristics, improving their scalability on\nlarge instances.\n","authors":["Daria Pugacheva","Andrei Ermakov","Igor Lyskov","Ilya Makarov","Yuriy Zotov"],"pdf_url":"https://arxiv.org/pdf/2407.16468v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16466v1","updated":"2024-07-23T13:28:07Z","published":"2024-07-23T13:28:07Z","title":"Sobolev neural network with residual weighting as a surrogate in linear\n  and non-linear mechanics","summary":"  Areas of computational mechanics such as uncertainty quantification and\noptimization usually involve repeated evaluation of numerical models that\nrepresent the behavior of engineering systems. In the case of complex nonlinear\nsystems however, these models tend to be expensive to evaluate, making\nsurrogate models quite valuable. Artificial neural networks approximate systems\nvery well by taking advantage of the inherent information of its given training\ndata. In this context, this paper investigates the improvement of the training\nprocess by including sensitivity information, which are partial derivatives\nw.r.t. inputs, as outlined by Sobolev training. In computational mechanics,\nsensitivities can be applied to neural networks by expanding the training loss\nfunction with additional loss terms, thereby improving training convergence\nresulting in lower generalisation error. This improvement is shown in two\nexamples of linear and non-linear material behavior. More specifically, the\nSobolev designed loss function is expanded with residual weights adjusting the\neffect of each loss on the training step. Residual weighting is the given\nscaling to the different training data, which in this case are response and\nsensitivities. These residual weights are optimized by an adaptive scheme,\nwhereby varying objective functions are explored, with some showing\nimprovements in accuracy and precision of the general training convergence.\n","authors":["A. O. M. Kilicsoy","J. Liedmann","M. A. Valdebenito","F. -J. Barthold","M. G. R. Faes"],"pdf_url":"https://arxiv.org/pdf/2407.16466v1.pdf","comment":"Submitted to IEEE Access, 40 pages, 18 figures"},{"id":"http://arxiv.org/abs/2407.16464v1","updated":"2024-07-23T13:27:44Z","published":"2024-07-23T13:27:44Z","title":"Lymphoid Infiltration Assessment of the Tumor Margins in H&E Slides","summary":"  Lymphoid infiltration at tumor margins is a key prognostic marker in solid\ntumors, playing a crucial role in guiding immunotherapy decisions. Current\nassessment methods, heavily reliant on immunohistochemistry (IHC), face\nchallenges in tumor margin delineation and are affected by tissue preservation\nconditions. In contrast, we propose a Hematoxylin and Eosin (H&E)\nstaining-based approach, underpinned by an advanced lymphocyte segmentation\nmodel trained on a public dataset for the precise detection of CD3+ and CD20+\nlymphocytes. In our colorectal cancer study, we demonstrate that our H&E-based\nmethod offers a compelling alternative to traditional IHC, achieving comparable\nresults in many cases. Our method's validity is further explored through a\nTuring test, involving blinded assessments by a pathologist of anonymized\ncurves from H&E and IHC slides. This approach invites the medical community to\nconsider Turing tests as a standard for evaluating medical applications\ninvolving expert human evaluation, thereby opening new avenues for enhancing\ncancer management and immunotherapy planning.\n","authors":["Zhuxian Guo","Amine Marzouki","Jean-François Emile","Henning Müller","Camille Kurtz","Nicolas Loménie"],"pdf_url":"https://arxiv.org/pdf/2407.16464v1.pdf","comment":"Published in Medical Optical Imaging and Virtual Microscopy Image\n  Analysis (MOVI) at MICCAI 2024"},{"id":"http://arxiv.org/abs/2407.16463v1","updated":"2024-07-23T13:26:05Z","published":"2024-07-23T13:26:05Z","title":"Advances in Land Surface Model-based Forecasting: A comparative study of\n  LSTM, Gradient Boosting, and Feedforward Neural Network Models as prognostic\n  state emulators","summary":"  Most useful weather prediction for the public is near the surface. The\nprocesses that are most relevant for near-surface weather prediction are also\nthose that are most interactive and exhibit positive feedback or have key role\nin energy partitioning. Land surface models (LSMs) consider these processes\ntogether with surface heterogeneity and forecast water, carbon and energy\nfluxes, and coupled with an atmospheric model provide boundary and initial\nconditions. This numerical parametrization of atmospheric boundaries being\ncomputationally expensive, statistical surrogate models are increasingly used\nto accelerated progress in experimental research. We evaluated the efficiency\nof three surrogate models in speeding up experimental research by simulating\nland surface processes, which are integral to forecasting water, carbon, and\nenergy fluxes in coupled atmospheric models. Specifically, we compared the\nperformance of a Long-Short Term Memory (LSTM) encoder-decoder network, extreme\ngradient boosting, and a feed-forward neural network within a physics-informed\nmulti-objective framework. This framework emulates key states of the ECMWF's\nIntegrated Forecasting System (IFS) land surface scheme, ECLand, across\ncontinental and global scales. Our findings indicate that while all models on\naverage demonstrate high accuracy over the forecast period, the LSTM network\nexcels in continental long-range predictions when carefully tuned, the XGB\nscores consistently high across tasks and the MLP provides an excellent\nimplementation-time-accuracy trade-off. The runtime reduction achieved by the\nemulators in comparison to the full numerical models are significant, offering\na faster, yet reliable alternative for conducting numerical experiments on land\nsurfaces.\n","authors":["Marieke Wesselkamp","Matthew Chantry","Ewan Pinnington","Margarita Choulga","Souhail Boussetta","Maria Kalweit","Joschka Boedecker","Carsten F. Dormann","Florian Pappenberger","Gianpaolo Balsamo"],"pdf_url":"https://arxiv.org/pdf/2407.16463v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07792v3","updated":"2024-07-23T13:25:57Z","published":"2023-12-12T23:17:29Z","title":"Differentially private projection-depth-based medians","summary":"  We develop $(\\epsilon,\\delta)$-differentially private projection-depth-based\nmedians using the propose-test-release (PTR) and exponential mechanisms. Under\ngeneral conditions on the input parameters and the population measure, (e.g. we\ndo not assume any moment bounds), we quantify the probability the test in PTR\nfails, as well as the cost of privacy via finite sample deviation bounds. Next,\nwe show that when some observations are contaminated, the private\nprojection-depth-based median does not break down, provided its input location\nand scale estimators do not break down. We demonstrate our main results on the\ncanonical projection-depth-based median, as well as on projection-depth-based\nmedians derived from trimmed estimators. In the Gaussian setting, we show that\nthe resulting deviation bound matches the known lower bound for private\nGaussian mean estimation. In the Cauchy setting, we show that the ``outlier\nerror amplification'' effect resulting from the heavy tails outweighs the cost\nof privacy. This result is then verified via numerical simulations.\nAdditionally, we present results on general PTR mechanisms and a uniform\nconcentration result on the projected spacings of order statistics, which may\nbe of general interest.\n","authors":["Kelly Ramsay","Dylan Spicker"],"pdf_url":"https://arxiv.org/pdf/2312.07792v3.pdf","comment":"45 pages, 1 figure"},{"id":"http://arxiv.org/abs/2404.02577v2","updated":"2024-07-23T13:15:01Z","published":"2024-04-03T08:53:42Z","title":"Solving a Real-World Optimization Problem Using Proximal Policy\n  Optimization with Curriculum Learning and Reward Engineering","summary":"  We present a proximal policy optimization (PPO) agent trained through\ncurriculum learning (CL) principles and meticulous reward engineering to\noptimize a real-world high-throughput waste sorting facility. Our work\naddresses the challenge of effectively balancing the competing objectives of\noperational safety, volume optimization, and minimizing resource usage. A\nvanilla agent trained from scratch on these multiple criteria fails to solve\nthe problem due to its inherent complexities. This problem is particularly\ndifficult due to the environment's extremely delayed rewards with long time\nhorizons and class (or action) imbalance, with important actions being\ninfrequent in the optimal policy. This forces the agent to anticipate long-term\naction consequences and prioritize rare but rewarding behaviours, creating a\nnon-trivial reinforcement learning task. Our five-stage CL approach tackles\nthese challenges by gradually increasing the complexity of the environmental\ndynamics during policy transfer while simultaneously refining the reward\nmechanism. This iterative and adaptable process enables the agent to learn a\ndesired optimal policy. Results demonstrate that our approach significantly\nimproves inference-time safety, achieving near-zero safety violations in\naddition to enhancing waste sorting plant efficiency.\n","authors":["Abhijeet Pendyala","Asma Atamna","Tobias Glasmachers"],"pdf_url":"https://arxiv.org/pdf/2404.02577v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.04395v4","updated":"2024-07-23T12:55:13Z","published":"2023-10-06T17:41:41Z","title":"Leveraging Self-Consistency for Data-Efficient Amortized Bayesian\n  Inference","summary":"  We propose a method to improve the efficiency and accuracy of amortized\nBayesian inference by leveraging universal symmetries in the joint\nprobabilistic model of parameters and data. In a nutshell, we invert Bayes'\ntheorem and estimate the marginal likelihood based on approximate\nrepresentations of the joint model. Upon perfect approximation, the marginal\nlikelihood is constant across all parameter values by definition. However,\nerrors in approximate inference lead to undesirable variance in the marginal\nlikelihood estimates across different parameter values. We penalize violations\nof this symmetry with a \\textit{self-consistency loss} which significantly\nimproves the quality of approximate inference in low data regimes and can be\nused to augment the training of popular neural density estimators. We apply our\nmethod to a number of synthetic problems and realistic scientific models,\ndiscovering notable advantages in the context of both neural posterior and\nlikelihood approximation.\n","authors":["Marvin Schmitt","Desi R. Ivanova","Daniel Habermann","Ullrich Köthe","Paul-Christian Bürkner","Stefan T. Radev"],"pdf_url":"https://arxiv.org/pdf/2310.04395v4.pdf","comment":"Proceedings of the 41st International Conference on Machine Learning\n  (ICML), Vienna, Austria. PMLR 235, 2024"},{"id":"http://arxiv.org/abs/2407.16445v1","updated":"2024-07-23T12:54:06Z","published":"2024-07-23T12:54:06Z","title":"Can time series forecasting be automated? A benchmark and analysis","summary":"  In the field of machine learning and artificial intelligence, time series\nforecasting plays a pivotal role across various domains such as finance,\nhealthcare, and weather. However, the task of selecting the most suitable\nforecasting method for a given dataset is a complex task due to the diversity\nof data patterns and characteristics. This research aims to address this\nchallenge by proposing a comprehensive benchmark for evaluating and ranking\ntime series forecasting methods across a wide range of datasets. This study\ninvestigates the comparative performance of many methods from two prominent\ntime series forecasting frameworks, AutoGluon-Timeseries, and sktime to shed\nlight on their applicability in different real-world scenarios. This research\ncontributes to the field of time series forecasting by providing a robust\nbenchmarking methodology and facilitating informed decision-making when\nchoosing forecasting methods for achieving optimal prediction.\n","authors":["Anvitha Thirthapura Sreedhara"],"pdf_url":"https://arxiv.org/pdf/2407.16445v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17536v3","updated":"2024-07-23T12:23:10Z","published":"2024-06-25T13:20:39Z","title":"MedMNIST-C: Comprehensive benchmark and improved classifier robustness\n  by simulating realistic image corruptions","summary":"  The integration of neural-network-based systems into clinical practice is\nlimited by challenges related to domain generalization and robustness. The\ncomputer vision community established benchmarks such as ImageNet-C as a\nfundamental prerequisite to measure progress towards those challenges. Similar\ndatasets are largely absent in the medical imaging community which lacks a\ncomprehensive benchmark that spans across imaging modalities and applications.\nTo address this gap, we create and open-source MedMNIST-C, a benchmark dataset\nbased on the MedMNIST+ collection covering 12 datasets and 9 imaging\nmodalities. We simulate task and modality-specific image corruptions of varying\nseverity to comprehensively evaluate the robustness of established algorithms\nagainst real-world artifacts and distribution shifts. We further provide\nquantitative evidence that our simple-to-use artificial corruptions allow for\nhighly performant, lightweight data augmentation to enhance model robustness.\nUnlike traditional, generic augmentation strategies, our approach leverages\ndomain knowledge, exhibiting significantly higher robustness when compared to\nwidely adopted methods. By introducing MedMNIST-C and open-sourcing the\ncorresponding library allowing for targeted data augmentations, we contribute\nto the development of increasingly robust methods tailored to the challenges of\nmedical imaging. The code is available at\nhttps://github.com/francescodisalvo05/medmnistc-api .\n","authors":["Francesco Di Salvo","Sebastian Doerrich","Christian Ledig"],"pdf_url":"https://arxiv.org/pdf/2406.17536v3.pdf","comment":"Accepted at MICCAI Workshop on Advancing Data Solutions in Medical\n  Imaging AI (ADSMI @ MICCAI 2024)"},{"id":"http://arxiv.org/abs/2312.05705v4","updated":"2024-07-23T12:13:44Z","published":"2023-12-09T23:13:32Z","title":"Structured Inverse-Free Natural Gradient: Memory-Efficient &\n  Numerically-Stable KFAC","summary":"  Second-order methods such as KFAC can be useful for neural net training.\nHowever, they are often memory-inefficient since their preconditioning\nKronecker factors are dense, and numerically unstable in low precision as they\nrequire matrix inversion or decomposition. These limitations render such\nmethods unpopular for modern mixed-precision training. We address them by (i)\nformulating an inverse-free KFAC update and (ii) imposing structures in the\nKronecker factors, resulting in structured inverse-free natural gradient\ndescent (SINGD). On modern neural networks, we show that SINGD is\nmemory-efficient and numerically robust, in contrast to KFAC, and often\noutperforms AdamW even in half precision. Our work closes a gap between first-\nand second-order methods in modern low-precision training.\n","authors":["Wu Lin","Felix Dangel","Runa Eschenhagen","Kirill Neklyudov","Agustinus Kristiadi","Richard E. Turner","Alireza Makhzani"],"pdf_url":"https://arxiv.org/pdf/2312.05705v4.pdf","comment":"A long version of the ICML 2024 paper, updated the text about a\n  related work"},{"id":"http://arxiv.org/abs/2405.10624v2","updated":"2024-07-23T12:04:52Z","published":"2024-05-17T08:39:05Z","title":"Sample-Efficient Constrained Reinforcement Learning with General\n  Parameterization","summary":"  We consider a constrained Markov Decision Problem (CMDP) where the goal of an\nagent is to maximize the expected discounted sum of rewards over an infinite\nhorizon while ensuring that the expected discounted sum of costs exceeds a\ncertain threshold. Building on the idea of momentum-based acceleration, we\ndevelop the Primal-Dual Accelerated Natural Policy Gradient (PD-ANPG) algorithm\nthat guarantees an $\\epsilon$ global optimality gap and $\\epsilon$ constraint\nviolation with $\\tilde{\\mathcal{O}}(\\epsilon^{-2})$ sample complexity for\ngeneral parameterized policies. This improves the state-of-the-art sample\ncomplexity in general parameterized CMDPs by a factor of\n$\\mathcal{O}(\\epsilon^{-2})$ and achieves the theoretical lower bound.\n","authors":["Washim Uddin Mondal","Vaneet Aggarwal"],"pdf_url":"https://arxiv.org/pdf/2405.10624v2.pdf","comment":"Improved the sample complexity result in comparison to the earlier\n  version. The new result coincides with the theoretical lower bound"},{"id":"http://arxiv.org/abs/2407.16417v1","updated":"2024-07-23T12:00:44Z","published":"2024-07-23T12:00:44Z","title":"On the Utility of Speech and Audio Foundation Models for Marmoset Call\n  Analysis","summary":"  Marmoset monkeys encode vital information in their calls and serve as a\nsurrogate model for neuro-biologists to understand the evolutionary origins of\nhuman vocal communication. Traditionally analyzed with signal processing-based\nfeatures, recent approaches have utilized self-supervised models pre-trained on\nhuman speech for feature extraction, capitalizing on their ability to learn a\nsignal's intrinsic structure independently of its acoustic domain. However, the\nutility of such foundation models remains unclear for marmoset call analysis in\nterms of multi-class classification, bandwidth, and pre-training domain. This\nstudy assesses feature representations derived from speech and general audio\ndomains, across pre-training bandwidths of 4, 8, and 16 kHz for marmoset\ncall-type and caller classification tasks. Results show that models with higher\nbandwidth improve performance, and pre-training on speech or general audio\nyields comparable results, improving over a spectral baseline.\n","authors":["Eklavya Sarkar","Mathew Magimai. -Doss"],"pdf_url":"https://arxiv.org/pdf/2407.16417v1.pdf","comment":"Accepted at Interspeech 2024 Satellite event (VIHAR 2024)"},{"id":"http://arxiv.org/abs/2404.12488v2","updated":"2024-07-23T11:58:53Z","published":"2024-04-18T20:03:56Z","title":"Global Counterfactual Directions","summary":"  Despite increasing progress in development of methods for generating visual\ncounterfactual explanations, especially with the recent rise of Denoising\nDiffusion Probabilistic Models, previous works consider them as an entirely\nlocal technique. In this work, we take the first step at globalizing them.\nSpecifically, we discover that the latent space of Diffusion Autoencoders\nencodes the inference process of a given classifier in the form of global\ndirections. We propose a novel proxy-based approach that discovers two types of\nthese directions with the use of only single image in an entirely black-box\nmanner. Precisely, g-directions allow for flipping the decision of a given\nclassifier on an entire dataset of images, while h-directions further increase\nthe diversity of explanations. We refer to them in general as Global\nCounterfactual Directions (GCDs). Moreover, we show that GCDs can be naturally\ncombined with Latent Integrated Gradients resulting in a new black-box\nattribution method, while simultaneously enhancing the understanding of\ncounterfactual explanations. We validate our approach on existing benchmarks\nand show that it generalizes to real-world use-cases.\n","authors":["Bartlomiej Sobieski","Przemysław Biecek"],"pdf_url":"https://arxiv.org/pdf/2404.12488v2.pdf","comment":"ECCV 2024"},{"id":"http://arxiv.org/abs/2407.14328v2","updated":"2024-07-23T11:56:22Z","published":"2024-07-19T14:06:01Z","title":"Modality-Order Matters! A Novel Hierarchical Feature Fusion Method for\n  CoSAm: A Code-Switched Autism Corpus","summary":"  Autism Spectrum Disorder (ASD) is a complex neuro-developmental challenge,\npresenting a spectrum of difficulties in social interaction, communication, and\nthe expression of repetitive behaviors in different situations. This increasing\nprevalence underscores the importance of ASD as a major public health concern\nand the need for comprehensive research initiatives to advance our\nunderstanding of the disorder and its early detection methods. This study\nintroduces a novel hierarchical feature fusion method aimed at enhancing the\nearly detection of ASD in children through the analysis of code-switched speech\n(English and Hindi). Employing advanced audio processing techniques, the\nresearch integrates acoustic, paralinguistic, and linguistic information using\nTransformer Encoders. This innovative fusion strategy is designed to improve\nclassification robustness and accuracy, crucial for early and precise ASD\nidentification. The methodology involves collecting a code-switched speech\ncorpus, CoSAm, from children diagnosed with ASD and a matched control group.\nThe dataset comprises 61 voice recordings from 30 children diagnosed with ASD\nand 31 from neurotypical children, aged between 3 and 13 years, resulting in a\ntotal of 159.75 minutes of voice recordings. The feature analysis focuses on\nMFCCs and extensive statistical attributes to capture speech pattern\nvariability and complexity. The best model performance is achieved using a\nhierarchical fusion technique with an accuracy of 98.75% using a combination of\nacoustic and linguistic features first, followed by paralinguistic features in\na hierarchical manner.\n","authors":["Mohd Mujtaba Akhtar"," Girish","Muskaan Singh","Orchid Chetia Phukan"],"pdf_url":"https://arxiv.org/pdf/2407.14328v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16407v1","updated":"2024-07-23T11:53:03Z","published":"2024-07-23T11:53:03Z","title":"Data-Driven Optimal Feedback Laws via Kernel Mean Embeddings","summary":"  This paper proposes a fully data-driven approach for optimal control of\nnonlinear control-affine systems represented by a stochastic diffusion. The\nfocus is on the scenario where both the nonlinear dynamics and stage cost\nfunctions are unknown, while only control penalty function and constraints are\nprovided. Leveraging the theory of reproducing kernel Hilbert spaces, we\nintroduce novel kernel mean embeddings (KMEs) to identify the Markov transition\noperators associated with controlled diffusion processes. The KME learning\napproach seamlessly integrates with modern convex operator-theoretic\nHamilton-Jacobi-Bellman recursions. Thus, unlike traditional dynamic\nprogramming methods, our approach exploits the ``kernel trick'' to break the\ncurse of dimensionality. We demonstrate the effectiveness of our method through\nnumerical examples, highlighting its ability to solve a large class of\nnonlinear optimal control problems.\n","authors":["Petar Bevanda","Nicolas Hoischen","Stefan Sosnowski","Sandra Hirche","Boris Houska"],"pdf_url":"https://arxiv.org/pdf/2407.16407v1.pdf","comment":"author-submitted electronic preprint version: 16 pages, 3 figures, 4\n  tables"},{"id":"http://arxiv.org/abs/2407.16406v1","updated":"2024-07-23T11:50:59Z","published":"2024-07-23T11:50:59Z","title":"Hi-EF: Benchmarking Emotion Forecasting in Human-interaction","summary":"  Affective Forecasting, a research direction in psychology that predicts\nindividuals future emotions, is often constrained by numerous external factors\nlike social influence and temporal distance. To address this, we transform\nAffective Forecasting into a Deep Learning problem by designing an Emotion\nForecasting paradigm based on two-party interactions. We propose a novel\nEmotion Forecasting (EF) task grounded in the theory that an individuals\nemotions are easily influenced by the emotions or other information conveyed\nduring interactions with another person. To tackle this task, we have developed\na specialized dataset, Human-interaction-based Emotion Forecasting (Hi-EF),\nwhich contains 3069 two-party Multilayered-Contextual Interaction Samples\n(MCIS) with abundant affective-relevant labels and three modalities. Hi-EF not\nonly demonstrates the feasibility of the EF task but also highlights its\npotential. Additionally, we propose a methodology that establishes a\nfoundational and referential baseline model for the EF task and extensive\nexperiments are provided. The dataset and code is available at\nhttps://github.com/Anonymize-Author/Hi-EF.\n","authors":["Haoran Wang","Xinji Mai","Zeng Tao","Yan Wang","Jiawen Yu","Ziheng Zhou","Xuan Tong","Shaoqi Yan","Qing Zhao","Shuyong Gao","Wenqiang Zhang"],"pdf_url":"https://arxiv.org/pdf/2407.16406v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.13247v2","updated":"2024-07-23T11:42:14Z","published":"2023-12-20T18:22:49Z","title":"Enhancing Neural Training via a Correlated Dynamics Model","summary":"  As neural networks grow in scale, their training becomes both computationally\ndemanding and rich in dynamics. Amidst the flourishing interest in these\ntraining dynamics, we present a novel observation: Parameters during training\nexhibit intrinsic correlations over time. Capitalizing on this, we introduce\nCorrelation Mode Decomposition (CMD). This algorithm clusters the parameter\nspace into groups, termed modes, that display synchronized behavior across\nepochs. This enables CMD to efficiently represent the training dynamics of\ncomplex networks, like ResNets and Transformers, using only a few modes.\nMoreover, test set generalization is enhanced. We introduce an efficient CMD\nvariant, designed to run concurrently with training. Our experiments indicate\nthat CMD surpasses the state-of-the-art method for compactly modeled dynamics\non image classification. Our modeling can improve training efficiency and lower\ncommunication overhead, as shown by our preliminary experiments in the context\nof federated learning.\n","authors":["Jonathan Brokman","Roy Betser","Rotem Turjeman","Tom Berkov","Ido Cohen","Guy Gilboa"],"pdf_url":"https://arxiv.org/pdf/2312.13247v2.pdf","comment":"ICLR 2024 accepted URL: https://openreview.net/forum?id=c9xsaASm9L"},{"id":"http://arxiv.org/abs/2302.06637v3","updated":"2024-07-23T11:38:37Z","published":"2023-02-13T19:00:37Z","title":"PerAda: Parameter-Efficient Federated Learning Personalization with\n  Generalization Guarantees","summary":"  Personalized Federated Learning (pFL) has emerged as a promising solution to\ntackle data heterogeneity across clients in FL. However, existing pFL methods\neither (1) introduce high communication and computation costs or (2) overfit to\nlocal data, which can be limited in scope, and are vulnerable to evolved test\nsamples with natural shifts. In this paper, we propose PerAda, a\nparameter-efficient pFL framework that reduces communication and computational\ncosts and exhibits superior generalization performance, especially under\ntest-time distribution shifts. PerAda reduces the costs by leveraging the power\nof pretrained models and only updates and communicates a small number of\nadditional parameters from adapters. PerAda has good generalization since it\nregularizes each client's personalized adapter with a global adapter, while the\nglobal adapter uses knowledge distillation to aggregate generalized information\nfrom all clients. Theoretically, we provide generalization bounds to explain\nwhy PerAda improves generalization, and we prove its convergence to stationary\npoints under non-convex settings. Empirically, PerAda demonstrates competitive\npersonalized performance (+4.85% on CheXpert) and enables better\nout-of-distribution generalization (+5.23% on CIFAR-10-C) on different datasets\nacross natural and medical domains compared with baselines, while only updating\n12.6% of parameters per model based on the adapter. Our code is available at\nhttps://github.com/NVlabs/PerAda.\n","authors":["Chulin Xie","De-An Huang","Wenda Chu","Daguang Xu","Chaowei Xiao","Bo Li","Anima Anandkumar"],"pdf_url":"https://arxiv.org/pdf/2302.06637v3.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2405.15429v3","updated":"2024-07-23T11:37:53Z","published":"2024-05-24T10:55:38Z","title":"E(n) Equivariant Topological Neural Networks","summary":"  Graph neural networks excel at modeling pairwise interactions, but they\ncannot flexibly accommodate higher-order interactions and features. Topological\ndeep learning (TDL) has emerged recently as a promising tool for addressing\nthis issue. TDL enables the principled modeling of arbitrary multi-way,\nhierarchical higher-order interactions by operating on combinatorial\ntopological spaces, such as simplicial or cell complexes, instead of graphs.\nHowever, little is known about how to leverage geometric features such as\npositions and velocities for TDL. This paper introduces E(n)-Equivariant\nTopological Neural Networks (ETNNs), which are E(n)-equivariant message-passing\nnetworks operating on combinatorial complexes, formal objects unifying graphs,\nhypergraphs, simplicial, path, and cell complexes. ETNNs incorporate geometric\nnode features while respecting rotation and translation equivariance. Moreover,\nETNNs are natively ready for settings with heterogeneous interactions. We\nprovide a theoretical analysis to show the improved expressiveness of ETNNs\nover architectures for geometric graphs. We also show how several E(n)\nequivariant variants of TDL models can be directly derived from our framework.\nThe broad applicability of ETNNs is demonstrated through two tasks of vastly\ndifferent nature: i) molecular property prediction on the QM9 benchmark and ii)\nland-use regression for hyper-local estimation of air pollution with\nmulti-resolution irregular geospatial data. The experiment results indicate\nthat ETNNs are an effective tool for learning from diverse types of richly\nstructured data, highlighting the benefits of principled geometric inductive\nbias.\n","authors":["Claudio Battiloro","Ege Karaismailoğlu","Mauricio Tec","George Dasoulas","Michelle Audirac","Francesca Dominici"],"pdf_url":"https://arxiv.org/pdf/2405.15429v3.pdf","comment":"36 pages, 11 figures, 9 tables"},{"id":"http://arxiv.org/abs/2407.16397v1","updated":"2024-07-23T11:35:42Z","published":"2024-07-23T11:35:42Z","title":"On ADMM in Heterogeneous Federated Learning: Personalization,\n  Robustness, and Fairness","summary":"  Statistical heterogeneity is a root cause of tension among accuracy,\nfairness, and robustness of federated learning (FL), and is key in paving a\npath forward. Personalized FL (PFL) is an approach that aims to reduce the\nimpact of statistical heterogeneity by developing personalized models for\nindividual users, while also inherently providing benefits in terms of fairness\nand robustness. However, existing PFL frameworks focus on improving the\nperformance of personalized models while neglecting the global model. Moreover,\nthese frameworks achieve sublinear convergence rates and rely on strong\nassumptions. In this paper, we propose FLAME, an optimization framework by\nutilizing the alternating direction method of multipliers (ADMM) to train\npersonalized and global models. We propose a model selection strategy to\nimprove performance in situations where clients have different types of\nheterogeneous data. Our theoretical analysis establishes the global convergence\nand two kinds of convergence rates for FLAME under mild assumptions. We\ntheoretically demonstrate that FLAME is more robust and fair than the\nstate-of-the-art methods on a class of linear problems. Our experimental\nfindings show that FLAME outperforms state-of-the-art methods in convergence\nand accuracy, and it achieves higher test accuracy under various attacks and\nperforms more uniformly across clients.\n","authors":["Shengkun Zhu","Jinshan Zeng","Sheng Wang","Yuan Sun","Xiaodong Li","Yuan Yao","Zhiyong Peng"],"pdf_url":"https://arxiv.org/pdf/2407.16397v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2311.06756"},{"id":"http://arxiv.org/abs/2402.07204v3","updated":"2024-07-23T11:25:26Z","published":"2024-02-11T13:30:53Z","title":"ITINERA: Integrating Spatial Optimization with Large Language Models for\n  Open-domain Urban Itinerary Planning","summary":"  Citywalk, a recently popular form of urban travel, requires genuine\npersonalization and understanding of fine-grained requests compared to\ntraditional itinerary planning. In this paper, we introduce the novel task of\nOpen-domain Urban Itinerary Planning (OUIP), which generates personalized urban\nitineraries from user requests in natural language. We then present ITINERA, an\nOUIP system that integrates spatial optimization with large language models to\nprovide customized urban itineraries based on user needs. This involves\ndecomposing user requests, selecting candidate points of interest (POIs),\nordering the POIs based on cluster-aware spatial optimization, and generating\nthe itinerary. Experiments on real-world datasets and the performance of the\ndeployed system demonstrate our system's capacity to deliver personalized and\nspatially coherent itineraries compared to current solutions.\n","authors":["Yihong Tang","Zhaokai Wang","Ao Qu","Yihao Yan","Zhaofeng Wu","Dingyi Zhuang","Jushi Kai","Kebing Hou","Xiaotong Guo","Jinhua Zhao","Zhan Zhao","Wei Ma"],"pdf_url":"https://arxiv.org/pdf/2402.07204v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16388v1","updated":"2024-07-23T11:22:33Z","published":"2024-07-23T11:22:33Z","title":"Anwendung von Causal-Discovery-Algorithmen zur Root-Cause-Analyse in der\n  Fahrzeugmontage","summary":"  Root Cause Analysis (RCA) is a quality management method that aims to\nsystematically investigate and identify the cause-and-effect relationships of\nproblems and their underlying causes. Traditional methods are based on the\nanalysis of problems by subject matter experts. In modern production processes,\nlarge amounts of data are collected. For this reason, increasingly\ncomputer-aided and data-driven methods are used for RCA. One of these methods\nare Causal Discovery Algorithms (CDA). This publication demonstrates the\napplication of CDA on data from the assembly of a leading automotive\nmanufacturer. The algorithms used learn the causal structure between the\ncharacteristics of the manufactured vehicles, the ergonomics and the temporal\nscope of the involved assembly processes, and quality-relevant product features\nbased on representative data. This publication compares various CDAs in terms\nof their suitability in the context of quality management. For this purpose,\nthe causal structures learned by the algorithms as well as their runtime are\ncompared. This publication provides a contribution to quality management and\ndemonstrates how CDAs can be used for RCA in assembly processes.\n","authors":["Lucas Possner","Lukas Bahr","Leonard Roehl","Christoph Wehner","Sophie Groeger"],"pdf_url":"https://arxiv.org/pdf/2407.16388v1.pdf","comment":"in German language"},{"id":"http://arxiv.org/abs/2407.15727v2","updated":"2024-07-23T11:19:18Z","published":"2024-07-22T15:30:21Z","title":"Inferring turbulent velocity and temperature fields and their statistics\n  from Lagrangian velocity measurements using physics-informed\n  Kolmogorov-Arnold Networks","summary":"  We propose the Artificial Intelligence Velocimetry-Thermometry (AIVT) method\nto infer hidden temperature fields from experimental turbulent velocity data.\nThis physics-informed machine learning method enables us to infer continuous\ntemperature fields using only sparse velocity data, hence eliminating the need\nfor direct temperature measurements. Specifically, AIVT is based on\nphysics-informed Kolmogorov-Arnold Networks (not neural networks) and is\ntrained by optimizing a combined loss function that minimizes the residuals of\nthe velocity data, boundary conditions, and the governing equations. We apply\nAIVT to a unique set of experimental volumetric and simultaneous temperature\nand velocity data of Rayleigh-B\\'enard convection (RBC) that we acquired by\ncombining Particle Image Thermometry and Lagrangian Particle Tracking. This\nallows us to compare AIVT predictions and measurements directly. We demonstrate\nthat we can reconstruct and infer continuous and instantaneous velocity and\ntemperature fields from sparse experimental data at a fidelity comparable to\ndirect numerical simulations (DNS) of turbulence. This, in turn, enables us to\ncompute important quantities for quantifying turbulence, such as fluctuations,\nviscous and thermal dissipation, and QR distribution. This paradigm shift in\nprocessing experimental data using AIVT to infer turbulent fields at DNS-level\nfidelity is a promising avenue in breaking the current deadlock of quantitative\nunderstanding of turbulence at high Reynolds numbers, where DNS is\ncomputationally infeasible.\n","authors":["Juan Diego Toscano","Theo Käufer","Zhibo Wang","Martin Maxey","Christian Cierpka","George Em Karniadakis"],"pdf_url":"https://arxiv.org/pdf/2407.15727v2.pdf","comment":"turbulence, data assimilation, physics-informed machine learning,\n  experimental methods, Kolmogorov-Arnold networks. 50 pages, 8 figures"},{"id":"http://arxiv.org/abs/2407.03542v2","updated":"2024-07-23T11:16:22Z","published":"2024-07-03T23:27:53Z","title":"Probing Perfection: The Relentless Art of Meddling for Pulmonary Airway\n  Segmentation from HRCT via a Human-AI Collaboration Based Active Learning\n  Method","summary":"  In pulmonary tracheal segmentation, the scarcity of annotated data is a\nprevalent issue in medical segmentation. Additionally, Deep Learning (DL)\nmethods face challenges: the opacity of 'black box' models and the need for\nperformance enhancement. Our Human-Computer Interaction (HCI) based models\n(RS_UNet, LC_UNet, UUNet, and WD_UNet) address these challenges by combining\ndiverse query strategies with various DL models. We train four HCI models and\nrepeat these steps: (1) Query Strategy: The HCI models select samples that\nprovide the most additional representative information when labeled in each\niteration and identify unlabeled samples with the greatest predictive disparity\nusing Wasserstein Distance, Least Confidence, Entropy Sampling, and Random\nSampling. (2) Central line correction: Selected samples are used for expert\ncorrection of system-generated tracheal central lines in each training round.\n(3) Update training dataset: Experts update the training dataset after each DL\nmodel's training epoch, enhancing the trustworthiness and performance of the\nmodels. (4) Model training: The HCI model is trained using the updated dataset\nand an enhanced UNet version. Experimental results confirm the effectiveness of\nthese HCI-based approaches, showing that WD-UNet, LC-UNet, UUNet, and RS-UNet\nachieve comparable or superior performance to state-of-the-art DL models.\nNotably, WD-UNet achieves this with only 15%-35% of the training data, reducing\nphysician annotation time by 65%-85%.\n","authors":["Shiyi Wang","Yang Nan","Sheng Zhang","Federico Felder","Xiaodan Xing","Yingying Fang","Javier Del Ser","Simon L F Walsh","Guang Yang"],"pdf_url":"https://arxiv.org/pdf/2407.03542v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16377v1","updated":"2024-07-23T11:04:33Z","published":"2024-07-23T11:04:33Z","title":"Reinforcement Learning-based Adaptive Mitigation of Uncorrected DRAM\n  Errors in the Field","summary":"  Scaling to larger systems, with current levels of reliability, requires\ncost-effective methods to mitigate hardware failures. One of the main causes of\nhardware failure is an uncorrected error in memory, which terminates the\ncurrent job and wastes all computation since the last checkpoint. This paper\npresents the first adaptive method for triggering uncorrected error mitigation.\nIt uses a prediction approach that considers the likelihood of an uncorrected\nerror and its current potential cost. The method is based on reinforcement\nlearning, and the only user-defined parameters are the mitigation cost and\nwhether the job can be restarted from a mitigation point. We evaluate our\nmethod using classical machine learning metrics together with a cost-benefit\nanalysis, which compares the cost of mitigation actions with the benefits from\nmitigating some of the errors. On two years of production logs from the\nMareNostrum supercomputer, our method reduces lost compute time by 54% compared\nwith no mitigation and is just 6% below the optimal Oracle method. All source\ncode is open source.\n","authors":["Isaac Boixaderas","Sergi Moré","Javier Bartolome","David Vicente","Petar Radojković","Paul M. Carpenter","Eduard Ayguadé"],"pdf_url":"https://arxiv.org/pdf/2407.16377v1.pdf","comment":"Published in HPDC'24"},{"id":"http://arxiv.org/abs/2308.00008v2","updated":"2024-07-23T11:02:52Z","published":"2023-07-29T14:51:56Z","title":"Interpolation-Split: a data-centric deep learning approach with big\n  interpolated data to boost airway segmentation performance","summary":"  The morphology and distribution of airway tree abnormalities enables\ndiagnosis and disease characterisation across a variety of chronic respiratory\nconditions. In this regard, airway segmentation plays a critical role in the\nproduction of the outline of the entire airway tree to enable estimation of\ndisease extent and severity. In this study, we propose a data-centric deep\nlearning technique to segment the airway tree. The proposed technique utilises\ninterpolation and image split to improve data usefulness and quality. Then, an\nensemble learning strategy is implemented to aggregate the segmented airway\ntrees at different scales. In terms of segmentation performance (dice\nsimilarity coefficient), our method outperforms the baseline model by 2.5% on\naverage when a combined loss is used. Further, our proposed technique has a low\nGPU usage and high flexibility enabling it to be deployed on any 2D deep\nlearning model.\n","authors":["Wing Keung Cheung","Ashkan Pakzad","Nesrin Mogulkoc","Sarah Needleman","Bojidar Rangelov","Eyjolfur Gudmundsson","An Zhao","Mariam Abbas","Davina McLaverty","Dimitrios Asimakopoulos","Robert Chapman","Recep Savas","Sam M Janes","Yipeng Hu","Daniel C. Alexander","John R Hurst","Joseph Jacob"],"pdf_url":"https://arxiv.org/pdf/2308.00008v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16376v1","updated":"2024-07-23T10:57:13Z","published":"2024-07-23T10:57:13Z","title":"Bayesian Autoregressive Online Change-Point Detection with Time-Varying\n  Parameters","summary":"  Change points in real-world systems mark significant regime shifts in system\ndynamics, possibly triggered by exogenous or endogenous factors. These points\ndefine regimes for the time evolution of the system and are crucial for\nunderstanding transitions in financial, economic, social, environmental, and\ntechnological contexts. Building upon the Bayesian approach introduced in\n\\cite{c:07}, we devise a new method for online change point detection in the\nmean of a univariate time series, which is well suited for real-time\napplications and is able to handle the general temporal patterns displayed by\ndata in many empirical contexts. We first describe time series as an\nautoregressive process of an arbitrary order. Second, the variance and\ncorrelation of the data are allowed to vary within each regime driven by a\nscoring rule that updates the value of the parameters for a better fit of the\nobservations. Finally, a change point is detected in a probabilistic framework\nvia the posterior distribution of the current regime length. By modeling\ntemporal dependencies and time-varying parameters, the proposed approach\nenhances both the estimate accuracy and the forecasting power. Empirical\nvalidations using various datasets demonstrate the method's effectiveness in\ncapturing memory and dynamic patterns, offering deeper insights into the\nnon-stationary dynamics of real-world systems.\n","authors":["Ioanna-Yvonni Tsaknaki","Fabrizio Lillo","Piero Mazzarisi"],"pdf_url":"https://arxiv.org/pdf/2407.16376v1.pdf","comment":"38 pages, 9 figures, 3 tables"},{"id":"http://arxiv.org/abs/2402.19078v3","updated":"2024-07-23T10:46:28Z","published":"2024-02-29T12:03:05Z","title":"Smooth Tchebycheff Scalarization for Multi-Objective Optimization","summary":"  Multi-objective optimization problems can be found in many real-world\napplications, where the objectives often conflict each other and cannot be\noptimized by a single solution. In the past few decades, numerous methods have\nbeen proposed to find Pareto solutions that represent optimal trade-offs among\nthe objectives for a given problem. However, these existing methods could have\nhigh computational complexity or may not have good theoretical properties for\nsolving a general differentiable multi-objective optimization problem. In this\nwork, by leveraging the smooth optimization technique, we propose a lightweight\nand efficient smooth Tchebycheff scalarization approach for gradient-based\nmulti-objective optimization. It has good theoretical properties for finding\nall Pareto solutions with valid trade-off preferences, while enjoying\nsignificantly lower computational complexity compared to other methods.\nExperimental results on various real-world application problems fully\ndemonstrate the effectiveness of our proposed method.\n","authors":["Xi Lin","Xiaoyuan Zhang","Zhiyuan Yang","Fei Liu","Zhenkun Wang","Qingfu Zhang"],"pdf_url":"https://arxiv.org/pdf/2402.19078v3.pdf","comment":"Accepted by the 41st International Conference on Machine Learning\n  (ICML 2024)"},{"id":"http://arxiv.org/abs/2406.14149v2","updated":"2024-07-23T10:34:19Z","published":"2024-06-20T09:43:36Z","title":"CheMFi: A Multifidelity Dataset of Quantum Chemical Properties of\n  Diverse Molecules","summary":"  Progress in both Machine Learning (ML) and Quantum Chemistry (QC) methods\nhave resulted in high accuracy ML models for QC properties. Datasets such as\nMD17 and WS22 have been used to benchmark these models at some level of QC\nmethod, or fidelity, which refers to the accuracy of the chosen QC method.\nMultifidelity ML (MFML) methods, where models are trained on data from more\nthan one fidelity, have shown to be effective over single fidelity methods.\nMuch research is progressing in this direction for diverse applications ranging\nfrom energy band gaps to excitation energies. One hurdle for effective research\nhere is the lack of a diverse multifidelity dataset for benchmarking. We\nprovide the quantum Chemistry MultiFidelity (CheMFi) dataset consisting of five\nfidelities calculated with the TD-DFT formalism. The fidelities differ in their\nbasis set choice: STO-3G, 3-21G, 6-31G, def2-SVP, and def2-TZVP. CheMFi offers\nto the community a variety of QC properties such as vertical excitation\nproperties and molecular dipole moments, further including QC computation times\nallowing for a time benefit benchmark of multifidelity models for ML-QC.\n","authors":["Vivin Vinod","Peter Zaspel"],"pdf_url":"https://arxiv.org/pdf/2406.14149v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16367v1","updated":"2024-07-23T10:21:18Z","published":"2024-07-23T10:21:18Z","title":"Navigating Uncertainty in Medical Image Segmentation","summary":"  We address the selection and evaluation of uncertain segmentation methods in\nmedical imaging and present two case studies: prostate segmentation,\nillustrating that for minimal annotator variation simple deterministic models\ncan suffice, and lung lesion segmentation, highlighting the limitations of the\nGeneralized Energy Distance (GED) in model selection. Our findings lead to\nguidelines for accurately choosing and developing uncertain segmentation\nmodels, that integrate aleatoric and epistemic components. These guidelines are\ndesigned to aid researchers and practitioners in better developing, selecting,\nand evaluating uncertain segmentation methods, thereby facilitating enhanced\nadoption and effective application of segmentation uncertainty in practice.\n","authors":["Kilian Zepf","Jes Frellsen","Aasa Feragen"],"pdf_url":"https://arxiv.org/pdf/2407.16367v1.pdf","comment":"Published in the conference proceedings of the 21st IEEE\n  International Symposium on Biomedical Imaging (ISBI 2024)"},{"id":"http://arxiv.org/abs/2407.16355v1","updated":"2024-07-23T09:59:43Z","published":"2024-07-23T09:59:43Z","title":"Online Learning with Sublinear Best-Action Queries","summary":"  In online learning, a decision maker repeatedly selects one of a set of\nactions, with the goal of minimizing the overall loss incurred. Following the\nrecent line of research on algorithms endowed with additional predictive\nfeatures, we revisit this problem by allowing the decision maker to acquire\nadditional information on the actions to be selected. In particular, we study\nthe power of \\emph{best-action queries}, which reveal beforehand the identity\nof the best action at a given time step. In practice, predictive features may\nbe expensive, so we allow the decision maker to issue at most $k$ such queries.\nWe establish tight bounds on the performance any algorithm can achieve when\ngiven access to $k$ best-action queries for different types of feedback models.\nIn particular, we prove that in the full feedback model, $k$ queries are enough\nto achieve an optimal regret of $\\Theta\\left(\\min\\left\\{\\sqrt T, \\frac\nTk\\right\\}\\right)$. This finding highlights the significant multiplicative\nadvantage in the regret rate achievable with even a modest (sublinear) number\n$k \\in \\Omega(\\sqrt{T})$ of queries. Additionally, we study the challenging\nsetting in which the only available feedback is obtained during the time steps\ncorresponding to the $k$ best-action queries. There, we provide a tight regret\nrate of $\\Theta\\left(\\min\\left\\{\\frac{T}{\\sqrt\nk},\\frac{T^2}{k^2}\\right\\}\\right)$, which improves over the standard\n$\\Theta\\left(\\frac{T}{\\sqrt k}\\right)$ regret rate for label efficient\nprediction for $k \\in \\Omega(T^{2/3})$.\n","authors":["Matteo Russo","Andrea Celli","Riccardo Colini Baldeschi","Federico Fusco","Daniel Haimovich","Dima Karamshuk","Stefano Leonardi","Niek Tax"],"pdf_url":"https://arxiv.org/pdf/2407.16355v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16354v1","updated":"2024-07-23T09:58:20Z","published":"2024-07-23T09:58:20Z","title":"Strike a Balance in Continual Panoptic Segmentation","summary":"  This study explores the emerging area of continual panoptic segmentation,\nhighlighting three key balances. First, we introduce past-class backtrace\ndistillation to balance the stability of existing knowledge with the\nadaptability to new information. This technique retraces the features\nassociated with past classes based on the final label assignment results,\nperforming knowledge distillation targeting these specific features from the\nprevious model while allowing other features to flexibly adapt to new\ninformation. Additionally, we introduce a class-proportional memory strategy,\nwhich aligns the class distribution in the replay sample set with that of the\nhistorical training data. This strategy maintains a balanced class\nrepresentation during replay, enhancing the utility of the limited-capacity\nreplay sample set in recalling prior classes. Moreover, recognizing that replay\nsamples are annotated only for the classes of their original step, we devise\nbalanced anti-misguidance losses, which combat the impact of incomplete\nannotations without incurring classification bias. Building upon these\ninnovations, we present a new method named Balanced Continual Panoptic\nSegmentation (BalConpas). Our evaluation on the challenging ADE20K dataset\ndemonstrates its superior performance compared to existing state-of-the-art\nmethods. The official code is available at\nhttps://github.com/jinpeng0528/BalConpas.\n","authors":["Jinpeng Chen","Runmin Cong","Yuxuan Luo","Horace Ho Shing Ip","Sam Kwong"],"pdf_url":"https://arxiv.org/pdf/2407.16354v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16346v1","updated":"2024-07-23T09:49:22Z","published":"2024-07-23T09:49:22Z","title":"Data-driven Multistage Distributionally Robust Linear Optimization with\n  Nested Distance","summary":"  We study multistage distributionally robust linear optimization, where the\nuncertainty set is defined as a ball of distribution centered at a scenario\ntree using the nested distance. The resulting minimax problem is notoriously\ndifficult to solve due to its inherent non-convexity. In this paper, we\ndemonstrate that, under mild conditions, the robust risk evaluation of a given\npolicy can be expressed in an equivalent recursive form. Furthermore, assuming\nstagewise independence, we derive equivalent dynamic programming reformulations\nto find an optimal robust policy that is time-consistent and well-defined on\nunseen sample paths. Our reformulations reconcile two modeling frameworks: the\nmultistage-static formulation (with nested distance) and the multistage-dynamic\nformulation (with one-period Wasserstein distance). Moreover, we identify\ntractable cases when the value functions can be computed efficiently using\nconvex optimization techniques.\n","authors":["Rui Gao","Rohit Arora","Yizhe Huang"],"pdf_url":"https://arxiv.org/pdf/2407.16346v1.pdf","comment":"First appeared online at https://optimization-online.org/?p=20641 on\n  Oct 15, 2022"},{"id":"http://arxiv.org/abs/2308.08634v3","updated":"2024-07-23T09:42:29Z","published":"2023-08-16T19:14:52Z","title":"FedPop: Federated Population-based Hyperparameter Tuning","summary":"  Federated Learning (FL) is a distributed machine learning (ML) paradigm, in\nwhich multiple clients collaboratively train ML models without centralizing\ntheir local data. Similar to conventional ML pipelines, the client local\noptimization and server aggregation procedure in FL are sensitive to the\nhyperparameter (HP) selection. Despite extensive research on tuning HPs for\ncentralized ML, these methods yield suboptimal results when employed in FL.\nThis is mainly because their \"training-after-tuning\" framework is unsuitable\nfor FL with limited client computation power. While some approaches have been\nproposed for HP-Tuning in FL, they are limited to the HPs for client local\nupdates. In this work, we propose a novel HP-tuning algorithm, called Federated\nPopulation-based Hyperparameter Tuning (FedPop), to address this vital yet\nchallenging problem. FedPop employs population-based evolutionary algorithms to\noptimize the HPs, which accommodates various HP types at both the client and\nserver sides. Compared with prior tuning methods, FedPop employs an online\n\"tuning-while-training\" framework, offering computational efficiency and\nenabling the exploration of a broader HP search space. Our empirical validation\non the common FL benchmarks and complex real-world FL datasets, including\nfull-sized Non-IID ImageNet-1K, demonstrates the effectiveness of the proposed\nmethod, which substantially outperforms the concurrent state-of-the-art\nHP-tuning methods in FL.\n","authors":["Haokun Chen","Denis Krompass","Jindong Gu","Volker Tresp"],"pdf_url":"https://arxiv.org/pdf/2308.08634v3.pdf","comment":"Code: https://github.com/HaokunChen245/FedPop"},{"id":"http://arxiv.org/abs/2407.16337v1","updated":"2024-07-23T09:35:59Z","published":"2024-07-23T09:35:59Z","title":"STATE: A Robust ATE Estimator of Heavy-Tailed Metrics for Variance\n  Reduction in Online Controlled Experiments","summary":"  Online controlled experiments play a crucial role in enabling data-driven\ndecisions across a wide range of companies. Variance reduction is an effective\ntechnique to improve the sensitivity of experiments, achieving higher\nstatistical power while using fewer samples and shorter experimental periods.\nHowever, typical variance reduction methods (e.g., regression-adjusted\nestimators) are built upon the intuitional assumption of Gaussian distributions\nand cannot properly characterize the real business metrics with heavy-tailed\ndistributions. Furthermore, outliers diminish the correlation between\npre-experiment covariates and outcome metrics, greatly limiting the\neffectiveness of variance reduction.\n  In this paper, we develop a novel framework that integrates the Student's\nt-distribution with machine learning tools to fit heavy-tailed metrics and\nconstruct a robust average treatment effect estimator in online controlled\nexperiments, which we call STATE. By adopting a variational EM method to\noptimize the loglikehood function, we can infer a robust solution that greatly\neliminates the negative impact of outliers and achieves significant variance\nreduction. Moreover, we extend the STATE method from count metrics to ratio\nmetrics by utilizing linear transformation that preserves unbiased estimation,\nwhose variance reduction is more complex but less investigated in existing\nworks. Finally, both simulations on synthetic data and long-term empirical\nresults on Meituan experiment platform demonstrate the effectiveness of our\nmethod. Compared with the state-of-the-art estimators (CUPAC/MLRATE), STATE\nachieves over 50% variance reduction, indicating it can reach the same\nstatistical power with only half of the observations, or half the experimental\nduration.\n","authors":["Hao Zhou","Kun Sun","Shaoming Li","Yangfeng Fan","Guibin Jiang","Jiaqi Zheng","Tao Li"],"pdf_url":"https://arxiv.org/pdf/2407.16337v1.pdf","comment":"Accepted by KDD 2024"},{"id":"http://arxiv.org/abs/2311.04780v2","updated":"2024-07-23T09:33:23Z","published":"2023-11-08T15:59:41Z","title":"FetMRQC: a robust quality control system for multi-centric fetal brain\n  MRI","summary":"  Fetal brain MRI is becoming an increasingly relevant complement to\nneurosonography for perinatal diagnosis, allowing fundamental insights into\nfetal brain development throughout gestation. However, uncontrolled fetal\nmotion and heterogeneity in acquisition protocols lead to data of variable\nquality, potentially biasing the outcome of subsequent studies. We present\nFetMRQC, an open-source machine-learning framework for automated image quality\nassessment and quality control that is robust to domain shifts induced by the\nheterogeneity of clinical data. FetMRQC extracts an ensemble of quality metrics\nfrom unprocessed anatomical MRI and combines them to predict experts' ratings\nusing random forests. We validate our framework on a pioneeringly large and\ndiverse dataset of more than 1600 manually rated fetal brain T2-weighted images\nfrom four clinical centers and 13 different scanners. Our study shows that\nFetMRQC's predictions generalize well to unseen data while being interpretable.\nFetMRQC is a step towards more robust fetal brain neuroimaging, which has the\npotential to shed new insights on the developing human brain.\n","authors":["Thomas Sanchez","Oscar Esteban","Yvan Gomez","Alexandre Pron","Mériam Koob","Vincent Dunet","Nadine Girard","Andras Jakab","Elisenda Eixarch","Guillaume Auzias","Meritxell Bach Cuadra"],"pdf_url":"https://arxiv.org/pdf/2311.04780v2.pdf","comment":"24 pages, 10 Figures. Accepted for publication at Medical Image\n  Analysis"},{"id":"http://arxiv.org/abs/2407.15455v2","updated":"2024-07-23T09:25:16Z","published":"2024-07-22T08:13:13Z","title":"Score matching for bridges without time-reversals","summary":"  We propose a new algorithm for learning a bridged diffusion process using\nscore-matching methods. Our method relies on reversing the dynamics of the\nforward process and using this to learn a score function, which, via Doob's\n$h$-transform, gives us a bridged diffusion process; that is, a process\nconditioned on an endpoint. In contrast to prior methods, ours learns the score\nterm $\\nabla_x \\log p(t, x; T, y)$, for given $t, Y$ directly, completely\navoiding the need for first learning a time reversal. We compare the\nperformance of our algorithm with existing methods and see that it outperforms\nusing the (learned) time-reversals to learn the score term. The code can be\nfound at https://github.com/libbylbaker/forward_bridge.\n","authors":["Elizabeth L. Baker","Moritz Schauer","Stefan Sommer"],"pdf_url":"https://arxiv.org/pdf/2407.15455v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16326v1","updated":"2024-07-23T09:21:38Z","published":"2024-07-23T09:21:38Z","title":"On The Expressive Power of Knowledge Graph Embedding Methods","summary":"  Knowledge Graph Embedding (KGE) is a popular approach, which aims to\nrepresent entities and relations of a knowledge graph in latent spaces. Their\nrepresentations are known as embeddings. To measure the plausibility of\ntriplets, score functions are defined over embedding spaces. Despite wide\ndissemination of KGE in various tasks, KGE methods have limitations in\nreasoning abilities. In this paper we propose a mathematical framework to\ncompare reasoning abilities of KGE methods. We show that STransE has a higher\ncapability than TransComplEx, and then present new STransCoRe method, which\nimproves the STransE by combining it with the TransCoRe insights, which can\nreduce the STransE space complexity.\n","authors":["Jiexing Gao","Dmitry Rodin","Vasily Motolygin","Denis Zaytsev"],"pdf_url":"https://arxiv.org/pdf/2407.16326v1.pdf","comment":"11 pages, 1 figure"},{"id":"http://arxiv.org/abs/2407.16313v1","updated":"2024-07-23T09:05:23Z","published":"2024-07-23T09:05:23Z","title":"Deep Learning for Pancreas Segmentation: a Systematic Review","summary":"  Pancreas segmentation has been traditionally challenging due to its small\nsize in computed tomography abdominal volumes, high variability of shape and\npositions among patients, and blurred boundaries due to low contrast between\nthe pancreas and surrounding organs. Many deep learning models for pancreas\nsegmentation have been proposed in the past few years. We present a thorough\nsystematic review based on the Preferred Reporting Items for Systematic Reviews\nand Meta-analyses (PRISMA) statement. The literature search was conducted on\nPubMed, Web of Science, Scopus, and IEEE Xplore on original studies published\nin peer-reviewed journals from 2013 to 2023. Overall, 130 studies were\nretrieved. We initially provided an overview of the technical background of the\nmost common network architectures and publicly available datasets. Then, the\nanalysis of the studies combining visual presentation in tabular form and text\ndescription was reported. The tables grouped the studies specifying the\napplication, dataset size, design (model architecture, learning strategy, and\nloss function), results, and main contributions. We first analyzed the studies\nfocusing on parenchyma segmentation using coarse-to-fine approaches,\nmulti-organ segmentation, semi-supervised learning, and unsupervised learning,\nfollowed by those studies on generalization to other datasets and those\nconcerning the design of new loss functions. Then, we analyzed the studies on\nsegmentation of tumors, cysts, and inflammation reporting multi-stage methods,\nsemi-supervised learning, generalization to other datasets, and design of new\nloss functions. Finally, we provided a critical discussion on the subject based\non the published evidence underlining current issues that need to be addressed\nbefore clinical translation.\n","authors":["Andrea Moglia","Matteo Cavicchioli","Luca Mainardi","Pietro Cerveri"],"pdf_url":"https://arxiv.org/pdf/2407.16313v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16298v1","updated":"2024-07-23T08:54:55Z","published":"2024-07-23T08:54:55Z","title":"EffiSegNet: Gastrointestinal Polyp Segmentation through a Pre-Trained\n  EfficientNet-based Network with a Simplified Decoder","summary":"  This work introduces EffiSegNet, a novel segmentation framework leveraging\ntransfer learning with a pre-trained Convolutional Neural Network (CNN)\nclassifier as its backbone. Deviating from traditional architectures with a\nsymmetric U-shape, EffiSegNet simplifies the decoder and utilizes full-scale\nfeature fusion to minimize computational cost and the number of parameters. We\nevaluated our model on the gastrointestinal polyp segmentation task using the\npublicly available Kvasir-SEG dataset, achieving state-of-the-art results.\nSpecifically, the EffiSegNet-B4 network variant achieved an F1 score of 0.9552,\nmean Dice (mDice) 0.9483, mean Intersection over Union (mIoU) 0.9056, Precision\n0.9679, and Recall 0.9429 with a pre-trained backbone - to the best of our\nknowledge, the highest reported scores in the literature for this dataset.\nAdditional training from scratch also demonstrated exceptional performance\ncompared to previous work, achieving an F1 score of 0.9286, mDice 0.9207, mIoU\n0.8668, Precision 0.9311 and Recall 0.9262. These results underscore the\nimportance of a well-designed encoder in image segmentation networks and the\neffectiveness of transfer learning approaches.\n","authors":["Ioannis A. Vezakis","Konstantinos Georgas","Dimitrios Fotiadis","George K. Matsopoulos"],"pdf_url":"https://arxiv.org/pdf/2407.16298v1.pdf","comment":"To be published in IEEE Engineering in Medicine and Biology (EMBC)\n  2024 conference proceedings"},{"id":"http://arxiv.org/abs/2308.12110v3","updated":"2024-07-23T08:52:31Z","published":"2023-08-23T12:58:40Z","title":"Constrained Stein Variational Trajectory Optimization","summary":"  We present Constrained Stein Variational Trajectory Optimization (CSVTO), an\nalgorithm for performing trajectory optimization with constraints on a set of\ntrajectories in parallel. We frame constrained trajectory optimization as a\nnovel form of constrained functional minimization over trajectory\ndistributions, which avoids treating the constraints as a penalty in the\nobjective and allows us to generate diverse sets of constraint-satisfying\ntrajectories. Our method uses Stein Variational Gradient Descent (SVGD) to find\na set of particles that approximates a distribution over low-cost trajectories\nwhile obeying constraints. CSVTO is applicable to problems with differentiable\nequality and inequality constraints and includes a novel particle re-sampling\nstep to escape local minima. By explicitly generating diverse sets of\ntrajectories, CSVTO is better able to avoid poor local minima and is more\nrobust to initialization. We demonstrate that CSVTO outperforms baselines in\nchallenging highly-constrained tasks, such as a 7DoF wrench manipulation task,\nwhere CSVTO outperforms all baselines both in success and constraint\nsatisfaction.\n","authors":["Thomas Power","Dmitry Berenson"],"pdf_url":"https://arxiv.org/pdf/2308.12110v3.pdf","comment":"18 pages, 10 figures, 3 tables"},{"id":"http://arxiv.org/abs/2407.16293v1","updated":"2024-07-23T08:51:29Z","published":"2024-07-23T08:51:29Z","title":"A new Linear Time Bi-level $\\ell_{1,\\infty}$ projection ; Application to\n  the sparsification of auto-encoders neural networks","summary":"  The $\\ell_{1,\\infty}$ norm is an efficient-structured projection, but the\ncomplexity of the best algorithm is, unfortunately, $\\mathcal{O}\\big(n m \\log(n\nm)\\big)$ for a matrix $n\\times m$.\\\\ In this paper, we propose a new bi-level\nprojection method, for which we show that the time complexity for the\n$\\ell_{1,\\infty}$ norm is only $\\mathcal{O}\\big(n m \\big)$ for a matrix\n$n\\times m$. Moreover, we provide a new $\\ell_{1,\\infty}$ identity with\nmathematical proof and experimental validation. Experiments show that our\nbi-level $\\ell_{1,\\infty}$ projection is $2.5$ times faster than the actual\nfastest algorithm and provides the best sparsity while keeping the same\naccuracy in classification applications.\n","authors":["Michel Barlaud","Guillaume Perez","Jean-Paul Marmorat"],"pdf_url":"https://arxiv.org/pdf/2407.16293v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2405.02086"},{"id":"http://arxiv.org/abs/2402.12499v2","updated":"2024-07-23T08:50:09Z","published":"2024-02-19T20:06:15Z","title":"Automated Security Response through Online Learning with Adaptive\n  Conjectures","summary":"  We study automated security response for an IT infrastructure and formulate\nthe interaction between an attacker and a defender as a partially observed,\nnon-stationary game. We relax the standard assumption that the game model is\ncorrectly specified and consider that each player has a probabilistic\nconjecture about the model, which may be misspecified in the sense that the\ntrue model has probability 0. This formulation allows us to capture uncertainty\nabout the infrastructure and the intents of the players. To learn effective\ngame strategies online, we design a novel method where a player iteratively\nadapts its conjecture using Bayesian learning and updates its strategy through\nrollout. We prove that the conjectures converge to best fits, and we provide a\nbound on the performance improvement that rollout enables with a conjectured\nmodel. To characterize the steady state of the game, we propose a variant of\nthe Berk-Nash equilibrium. We present our method through an advanced persistent\nthreat use case. Testbed evaluations show that our method produces effective\nsecurity strategies that adapt to a changing environment. We also find that our\nmethod enables faster convergence than current reinforcement learning\ntechniques.\n","authors":["Kim Hammar","Tao Li","Rolf Stadler","Quanyan Zhu"],"pdf_url":"https://arxiv.org/pdf/2402.12499v2.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible"},{"id":"http://arxiv.org/abs/2407.16289v1","updated":"2024-07-23T08:43:42Z","published":"2024-07-23T08:43:42Z","title":"Federated Learning for Face Recognition via Intra-subject\n  Self-supervised Learning","summary":"  Federated Learning (FL) for face recognition aggregates locally optimized\nmodels from individual clients to construct a generalized face recognition\nmodel. However, previous studies present two major challenges: insufficient\nincorporation of self-supervised learning and the necessity for clients to\naccommodate multiple subjects. To tackle these limitations, we propose FedFS\n(Federated Learning for personalized Face recognition via intra-subject\nSelf-supervised learning framework), a novel federated learning architecture\ntailored to train personalized face recognition models without imposing\nsubjects. Our proposed FedFS comprises two crucial components that leverage\naggregated features of the local and global models to cooperate with\nrepresentations of an off-the-shelf model. These components are (1) adaptive\nsoft label construction, utilizing dot product operations to reformat labels\nwithin intra-instances, and (2) intra-subject self-supervised learning,\nemploying cosine similarity operations to strengthen robust intra-subject\nrepresentations. Additionally, we introduce a regularization loss to prevent\noverfitting and ensure the stability of the optimized model. To assess the\neffectiveness of FedFS, we conduct comprehensive experiments on the DigiFace-1M\nand VGGFace datasets, demonstrating superior performance compared to previous\nmethods.\n","authors":["Hansol Kim","Hoyeol Choi","Youngjun Kwak"],"pdf_url":"https://arxiv.org/pdf/2407.16289v1.pdf","comment":"Accepted at the The 35th British Machine Vision Conference 2024 (BMVC\n  2024), Glasgow, UK. Youngjun Kwak is corresponding author"},{"id":"http://arxiv.org/abs/2407.16286v1","updated":"2024-07-23T08:40:27Z","published":"2024-07-23T08:40:27Z","title":"A deeper look at depth pruning of LLMs","summary":"  Large Language Models (LLMs) are not only resource-intensive to train but\neven more costly to deploy in production. Therefore, recent work has attempted\nto prune blocks of LLMs based on cheap proxies for estimating block importance,\neffectively removing 10% of blocks in well-trained LLaMa-2 and Mistral 7b\nmodels without any significant degradation of downstream metrics. In this\npaper, we explore different block importance metrics by considering adaptive\nmetrics such as Shapley value in addition to static ones explored in prior\nwork. We show that adaptive metrics exhibit a trade-off in performance between\ntasks i.e., improvement on one task may degrade performance on the other due to\ndifferences in the computed block influences. Furthermore, we extend this\nanalysis from a complete block to individual self-attention and feed-forward\nlayers, highlighting the propensity of the self-attention layers to be more\namendable to pruning, even allowing removal of upto 33% of the self-attention\nlayers without incurring any performance degradation on MMLU for Mistral 7b\n(significant reduction in costly maintenance of KV-cache). Finally, we look at\nsimple performance recovery techniques to emulate the pruned layers by training\nlightweight additive bias or low-rank linear adapters. Performance recovery\nusing emulated updates avoids performance degradation for the initial blocks\n(up to 5% absolute improvement on MMLU), which is either competitive or\nsuperior to the learning-based technique.\n","authors":["Shoaib Ahmed Siddiqui","Xin Dong","Greg Heinrich","Thomas Breuel","Jan Kautz","David Krueger","Pavlo Molchanov"],"pdf_url":"https://arxiv.org/pdf/2407.16286v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16280v1","updated":"2024-07-23T08:31:24Z","published":"2024-07-23T08:31:24Z","title":"Efficient Detection of Commutative Factors in Factor Graphs","summary":"  Lifted probabilistic inference exploits symmetries in probabilistic graphical\nmodels to allow for tractable probabilistic inference with respect to domain\nsizes. To exploit symmetries in, e.g., factor graphs, it is crucial to identify\ncommutative factors, i.e., factors having symmetries within themselves due to\ntheir arguments being exchangeable. The current state of the art to check\nwhether a factor is commutative with respect to a subset of its arguments\niterates over all possible subsets of the factor's arguments, i.e., $O(2^n)$\niterations for a factor with $n$ arguments in the worst case. In this paper, we\nefficiently solve the problem of detecting commutative factors in a factor\ngraph. In particular, we introduce the detection of commutative factors (DECOR)\nalgorithm, which allows us to drastically reduce the computational effort for\nchecking whether a factor is commutative in practice. We prove that DECOR\nefficiently identifies restrictions to drastically reduce the number of\nrequired iterations and validate the efficiency of DECOR in our empirical\nevaluation.\n","authors":["Malte Luttermann","Johann Machemer","Marcel Gehrke"],"pdf_url":"https://arxiv.org/pdf/2407.16280v1.pdf","comment":"Accepted to the Proceedings of the 12th Conference on Probabilistic\n  Graphical Models (PGM 2024)"},{"id":"http://arxiv.org/abs/2407.16255v1","updated":"2024-07-23T07:49:35Z","published":"2024-07-23T07:49:35Z","title":"Self-Reasoning Assistant Learning for non-Abelian Gauge Fields Design","summary":"  Non-Abelian braiding has attracted substantial attention because of its\npivotal role in describing the exchange behaviour of anyons, in which the input\nand outcome of non-Abelian braiding are connected by a unitary matrix.\nImplementing braiding in a classical system can assist the experimental\ninvestigation of non-Abelian physics. However, the design of non-Abelian gauge\nfields faces numerous challenges stemmed from the intricate interplay of group\nstructures, Lie algebra properties, representation theory, topology, and\nsymmetry breaking. The extreme diversity makes it a powerful tool for the study\nof condensed matter physics. Whereas the widely used artificial intelligence\nwith data-driven approaches has greatly promoted the development of physics,\nmost works are limited on the data-to-data design. Here we propose a\nself-reasoning assistant learning framework capable of directly generating\nnon-Abelian gauge fields. This framework utilizes the forward diffusion process\nto capture and reproduce the complex patterns and details inherent in the\ntarget distribution through continuous transformation. Then the reverse\ndiffusion process is used to make the generated data closer to the distribution\nof the original situation. Thus, it owns strong self-reasoning capabilities,\nallowing to automatically discover the feature representation and capture more\nsubtle relationships from the dataset. Moreover, the self-reasoning eliminates\nthe need for manual feature engineering and simplifies the process of model\nbuilding. Our framework offers a disruptive paradigm shift to parse complex\nphysical processes, automatically uncovering patterns from massive datasets.\n","authors":["Jinyang Sun","Xi Chen","Xiumei Wang","Dandan Zhu","Xingping Zhou"],"pdf_url":"https://arxiv.org/pdf/2407.16255v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.18347v2","updated":"2024-07-23T07:31:18Z","published":"2024-05-28T16:43:57Z","title":"Dataset Growth","summary":"  Deep learning benefits from the growing abundance of available data.\nMeanwhile, efficiently dealing with the growing data scale has become a\nchallenge. Data publicly available are from different sources with various\nqualities, and it is impractical to do manual cleaning against noise and\nredundancy given today's data scale. There are existing techniques for\ncleaning/selecting the collected data. However, these methods are mainly\nproposed for offline settings that target one of the cleanness and redundancy\nproblems. In practice, data are growing exponentially with both problems. This\nleads to repeated data curation with sub-optimal efficiency. To tackle this\nchallenge, we propose InfoGrowth, an efficient online algorithm for data\ncleaning and selection, resulting in a growing dataset that keeps up to date\nwith awareness of cleanliness and diversity. InfoGrowth can improve data\nquality/efficiency on both single-modal and multi-modal tasks, with an\nefficient and scalable design. Its framework makes it practical for real-world\ndata engines.\n","authors":["Ziheng Qin","Zhaopan Xu","Yukun Zhou","Zangwei Zheng","Zebang Cheng","Hao Tang","Lei Shang","Baigui Sun","Xiaojiang Peng","Radu Timofte","Hongxun Yao","Kai Wang","Yang You"],"pdf_url":"https://arxiv.org/pdf/2405.18347v2.pdf","comment":"arXiv admin note: text overlap with arXiv:2305.20087 by other authors"},{"id":"http://arxiv.org/abs/2407.16239v1","updated":"2024-07-23T07:26:38Z","published":"2024-07-23T07:26:38Z","title":"Identifiable latent bandits: Combining observational data and\n  exploration for personalized healthcare","summary":"  Bandit algorithms hold great promise for improving personalized\ndecision-making but are notoriously sample-hungry. In most health applications,\nit is infeasible to fit a new bandit for each patient, and observable variables\nare often insufficient to determine optimal treatments, ruling out applying\ncontextual bandits learned from multiple patients. Latent bandits offer both\nrapid exploration and personalization beyond what context variables can reveal\nbut require that a latent variable model can be learned consistently. In this\nwork, we propose bandit algorithms based on nonlinear independent component\nanalysis that can be provably identified from observational data to a degree\nsufficient to infer the optimal action in a new bandit instance consistently.\nWe verify this strategy in simulated data, showing substantial improvement over\nlearning independent multi-armed bandits for every instance.\n","authors":["Ahmet Zahid Balcıoğlu","Emil Carlsson","Fredrik D. Johansson"],"pdf_url":"https://arxiv.org/pdf/2407.16239v1.pdf","comment":"9 pages, 2 figures"},{"id":"http://arxiv.org/abs/2407.16237v1","updated":"2024-07-23T07:22:25Z","published":"2024-07-23T07:22:25Z","title":"OriGen:Enhancing RTL Code Generation with Code-to-Code Augmentation and\n  Self-Reflection","summary":"  Recent studies have illuminated that Large Language Models (LLMs) exhibit\nsubstantial potential in the realm of RTL (Register Transfer Level) code\ngeneration, with notable advancements evidenced by commercial models such as\nGPT-4 and Claude3-Opus. Despite their proficiency, these commercial LLMs often\nraise concerns regarding privacy and security. Conversely, open-source LLMs,\nwhich offer solutions to these concerns, have inferior performance in RTL code\ngeneration tasks to commercial models due to the lack of highquality\nopen-source RTL datasets. To address this issue, we introduce OriGen, a fully\nopen-source framework featuring self-reflection capabilities and a dataset\naugmentation methodology for generating high-quality, large-scale RTL code. We\npropose a novel code-to-code augmentation methodology that leverages knowledge\ndistillation to enhance the quality of the open-source RTL code datasets.\nAdditionally, OriGen is capable of correcting syntactic errors by leveraging a\nself-reflection process based on feedback from the compiler. The\nself-reflection ability of the model is facilitated by a carefully constructed\ndataset, which comprises a comprehensive collection of samples. Experimental\nresults demonstrate that OriGen remarkably outperforms other open-source\nalternatives in RTL code generation, surpassing the previous best-performing\nLLM by 9.8% on the VerilogEval-Human benchmark. Furthermore, OriGen exhibits\nsuperior capabilities in self-reflection and error rectification, surpassing\nGPT-4 by 18.1% on the benchmark designed to evaluate the capability of\nself-reflection.\n","authors":["Fan Cui","Chenyang Yin","Kexing Zhou","Youwei Xiao","Guangyu Sun","Qiang Xu","Qipeng Guo","Demin Song","Dahua Lin","Xingcheng Zhang"," Yun"," Liang"],"pdf_url":"https://arxiv.org/pdf/2407.16237v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16233v1","updated":"2024-07-23T07:17:45Z","published":"2024-07-23T07:17:45Z","title":"Algebraic Adversarial Attacks on Integrated Gradients","summary":"  Adversarial attacks on explainability models have drastic consequences when\nexplanations are used to understand the reasoning of neural networks in safety\ncritical systems. Path methods are one such class of attribution methods\nsusceptible to adversarial attacks. Adversarial learning is typically phrased\nas a constrained optimisation problem. In this work, we propose algebraic\nadversarial examples and study the conditions under which one can generate\nadversarial examples for integrated gradients. Algebraic adversarial examples\nprovide a mathematically tractable approach to adversarial examples.\n","authors":["Lachlan Simpson","Federico Costanza","Kyle Millar","Adriel Cheng","Cheng-Chew Lim","Hong Gunn Chew"],"pdf_url":"https://arxiv.org/pdf/2407.16233v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15124v2","updated":"2024-07-23T07:11:47Z","published":"2024-07-21T11:27:27Z","title":"Chemical Reaction Extraction from Long Patent Documents","summary":"  The task of searching through patent documents is crucial for chemical patent\nrecommendation and retrieval. This can be enhanced by creating a patent\nknowledge base (ChemPatKB) to aid in prior art searches and to provide a\nplatform for domain experts to explore new innovations in chemical compound\nsynthesis and use-cases. An essential foundational component of this KB is the\nextraction of important reaction snippets from long patents documents which\nfacilitates multiple downstream tasks such as reaction co-reference resolution\nand chemical entity role identification. In this work, we explore the problem\nof extracting reactions spans from chemical patents in order to create a\nreactions resource database. We formulate this task as a paragraph-level\nsequence tagging problem, where the system is required to return a sequence of\nparagraphs that contain a description of a reaction. We propose several\napproaches and modifications of the baseline models and study how different\nmethods generalize across different domains of chemical patents.\n","authors":["Aishwarya Jadhav","Ritam Dutt"],"pdf_url":"https://arxiv.org/pdf/2407.15124v2.pdf","comment":"Work completed in 2022 at Carnegie Mellon University"},{"id":"http://arxiv.org/abs/2407.16220v1","updated":"2024-07-23T06:52:52Z","published":"2024-07-23T06:52:52Z","title":"ODGR: Online Dynamic Goal Recognition","summary":"  Traditionally, Reinforcement Learning (RL) problems are aimed at optimization\nof the behavior of an agent. This paper proposes a novel take on RL, which is\nused to learn the policy of another agent, to allow real-time recognition of\nthat agent's goals. Goal Recognition (GR) has traditionally been framed as a\nplanning problem where one must recognize an agent's objectives based on its\nobserved actions. Recent approaches have shown how reinforcement learning can\nbe used as part of the GR pipeline, but are limited to recognizing predefined\ngoals and lack scalability in domains with a large goal space. This paper\nformulates a novel problem, \"Online Dynamic Goal Recognition\" (ODGR), as a\nfirst step to address these limitations. Contributions include introducing the\nconcept of dynamic goals into the standard GR problem definition, revisiting\ncommon approaches by reformulating them using ODGR, and demonstrating the\nfeasibility of solving ODGR in a navigation domain using transfer learning.\nThese novel formulations open the door for future extensions of existing\ntransfer learning-based GR methods, which will be robust to changing and\nexpansive real-time environments.\n","authors":["Matan Shamir","Osher Elhadad","Matthew E. Taylor","Reuth Mirsky"],"pdf_url":"https://arxiv.org/pdf/2407.16220v1.pdf","comment":"8 pages, 1 figure, RLC workshop, WAHT workshop"},{"id":"http://arxiv.org/abs/2312.06942v5","updated":"2024-07-23T06:47:13Z","published":"2023-12-12T02:34:06Z","title":"AI Control: Improving Safety Despite Intentional Subversion","summary":"  As large language models (LLMs) become more powerful and are deployed more\nautonomously, it will be increasingly important to prevent them from causing\nharmful outcomes. Researchers have investigated a variety of safety techniques\nfor this purpose, e.g. using models to review the outputs of other models, or\nred-teaming techniques to surface subtle failure modes. However, researchers\nhave not evaluated whether such techniques still ensure safety if the model is\nitself intentionally trying to subvert them. In this paper, we develop and\nevaluate pipelines of safety techniques (\"protocols\") that are robust to\nintentional subversion.\n  We investigate a scenario in which we want to solve a sequence of programming\nproblems, using access to a powerful but untrusted model (in our case, GPT-4),\naccess to a less powerful trusted model (in our case, GPT-3.5), and limited\naccess to high-quality trusted labor. We investigate protocols that aim to\nnever submit solutions containing backdoors, which we operationalize here as\nlogical errors that are not caught by test cases.\n  We investigate a range of protocols and test each against strategies that the\nuntrusted model could use to subvert them. One protocol is what we call trusted\nediting. This protocol first asks GPT-4 to write code, and then asks GPT-3.5 to\nrate the suspiciousness of that code. If the code is below some suspiciousness\nthreshold, it is submitted. Otherwise, GPT-3.5 edits the solution to remove\nparts that seem suspicious and then submits the edited code. Another protocol\nis untrusted monitoring. This protocol asks GPT-4 to write code, and then asks\nanother instance of GPT-4 whether the code is backdoored, using various\ntechniques to prevent the GPT-4 instances from colluding. These protocols\nimprove substantially on simple baselines.\n","authors":["Ryan Greenblatt","Buck Shlegeris","Kshitij Sachan","Fabien Roger"],"pdf_url":"https://arxiv.org/pdf/2312.06942v5.pdf","comment":"Edit: Fix minor typos, clarify abstract, add glossary, expand related\n  work. ICML version: https://openreview.net/pdf?id=KviM5k8pcP"},{"id":"http://arxiv.org/abs/2304.01391v3","updated":"2024-07-23T06:43:57Z","published":"2023-04-03T21:42:42Z","title":"Counterfactual Learning on Graphs: A Survey","summary":"  Graph-structured data are pervasive in the real-world such as social\nnetworks, molecular graphs and transaction networks. Graph neural networks\n(GNNs) have achieved great success in representation learning on graphs,\nfacilitating various downstream tasks. However, GNNs have several drawbacks\nsuch as lacking interpretability, can easily inherit the bias of data and\ncannot model casual relations. Recently, counterfactual learning on graphs has\nshown promising results in alleviating these drawbacks. Various approaches have\nbeen proposed for counterfactual fairness, explainability, link prediction and\nother applications on graphs. To facilitate the development of this promising\ndirection, in this survey, we categorize and comprehensively review papers on\ngraph counterfactual learning. We divide existing methods into four categories\nbased on problems studied. For each category, we provide background and\nmotivating examples, a general framework summarizing existing works and a\ndetailed review of these works. We point out promising future research\ndirections at the intersection of graph-structured data, counterfactual\nlearning, and real-world applications. To offer a comprehensive view of\nresources for future studies, we compile a collection of open-source\nimplementations, public datasets, and commonly-used evaluation metrics. This\nsurvey aims to serve as a ``one-stop-shop'' for building a unified\nunderstanding of graph counterfactual learning categories and current\nresources. We also maintain a repository for papers and resources and will keep\nupdating the repository\nhttps://github.com/TimeLovercc/Awesome-Graph-Causal-Learning.\n","authors":["Zhimeng Guo","Teng Xiao","Zongyu Wu","Charu Aggarwal","Hui Liu","Suhang Wang"],"pdf_url":"https://arxiv.org/pdf/2304.01391v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16210v1","updated":"2024-07-23T06:31:13Z","published":"2024-07-23T06:31:13Z","title":"Strategy and Skill Learning for Physics-based Table Tennis Animation","summary":"  Recent advancements in physics-based character animation leverage deep\nlearning to generate agile and natural motion, enabling characters to execute\nmovements such as backflips, boxing, and tennis. However, reproducing the\nselection and use of diverse motor skills in dynamic environments to solve\ncomplex tasks, as humans do, still remains a challenge. We present a strategy\nand skill learning approach for physics-based table tennis animation. Our\nmethod addresses the issue of mode collapse, where the characters do not fully\nutilize the motor skills they need to perform to execute complex tasks. More\nspecifically, we demonstrate a hierarchical control system for diversified\nskill learning and a strategy learning framework for effective decision-making.\nWe showcase the efficacy of our method through comparative analysis with\nstate-of-the-art methods, demonstrating its capabilities in executing various\nskills for table tennis. Our strategy learning framework is validated through\nboth agent-agent interaction and human-agent interaction in Virtual Reality,\nhandling both competitive and cooperative tasks.\n","authors":["Jiashun Wang","Jessica Hodgins","Jungdam Won"],"pdf_url":"https://arxiv.org/pdf/2407.16210v1.pdf","comment":"SIGGRAPH 2024"},{"id":"http://arxiv.org/abs/2407.08364v2","updated":"2024-07-23T06:26:34Z","published":"2024-07-11T10:18:54Z","title":"Scalar Function Topology Divergence: Comparing Topology of 3D Objects","summary":"  We propose a new topological tool for computer vision - Scalar Function\nTopology Divergence (SFTD), which measures the dissimilarity of multi-scale\ntopology between sublevel sets of two functions having a common domain.\nFunctions can be defined on an undirected graph or Euclidean space of any\ndimensionality. Most of the existing methods for comparing topology are based\non Wasserstein distance between persistence barcodes and they don't take into\naccount the localization of topological features. The minimization of SFTD\nensures that the corresponding topological features of scalar functions are\nlocated in the same places. The proposed tool provides useful visualizations\ndepicting areas where functions have topological dissimilarities. We provide\napplications of the proposed method to 3D computer vision. In particular,\nexperiments demonstrate that SFTD as an additional loss improves the\nreconstruction of cellular 3D shapes from 2D fluorescence microscopy images,\nand helps to identify topological errors in 3D segmentation. Additionally, we\nshow that SFTD outperforms Betti matching loss in 2D segmentation problems.\n","authors":["Ilya Trofimov","Daria Voronkova","Eduard Tulchinskii","Evgeny Burnaev","Serguei Barannikov"],"pdf_url":"https://arxiv.org/pdf/2407.08364v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07718v5","updated":"2024-07-23T06:19:28Z","published":"2024-03-12T14:58:45Z","title":"WorkArena: How Capable Are Web Agents at Solving Common Knowledge Work\n  Tasks?","summary":"  We study the use of large language model-based agents for interacting with\nsoftware via web browsers. Unlike prior work, we focus on measuring the agents'\nability to perform tasks that span the typical daily work of knowledge workers\nutilizing enterprise software systems. To this end, we propose WorkArena, a\nremote-hosted benchmark of 33 tasks based on the widely-used ServiceNow\nplatform. We also introduce BrowserGym, an environment for the design and\nevaluation of such agents, offering a rich set of actions as well as multimodal\nobservations. Our empirical evaluation reveals that while current agents show\npromise on WorkArena, there remains a considerable gap towards achieving full\ntask automation. Notably, our analysis uncovers a significant performance\ndisparity between open and closed-source LLMs, highlighting a critical area for\nfuture exploration and development in the field.\n","authors":["Alexandre Drouin","Maxime Gasse","Massimo Caccia","Issam H. Laradji","Manuel Del Verme","Tom Marty","Léo Boisvert","Megh Thakkar","Quentin Cappart","David Vazquez","Nicolas Chapados","Alexandre Lacoste"],"pdf_url":"https://arxiv.org/pdf/2403.07718v5.pdf","comment":"21 pages, 11 figures, preprint"},{"id":"http://arxiv.org/abs/2407.03641v2","updated":"2024-07-23T06:19:21Z","published":"2024-07-04T05:23:22Z","title":"Learning Scalable Model Soup on a Single GPU: An Efficient Subspace\n  Training Strategy","summary":"  Pre-training followed by fine-tuning is widely adopted among practitioners.\nThe performance can be improved by \"model soups\"~\\cite{wortsman2022model} via\nexploring various hyperparameter configurations.The Learned-Soup, a variant of\nmodel soups, significantly improves the performance but suffers from\nsubstantial memory and time costs due to the requirements of (i) having to load\nall fine-tuned models simultaneously, and (ii) a large computational graph\nencompassing all fine-tuned models. In this paper, we propose Memory Efficient\nHyperplane Learned Soup (MEHL-Soup) to tackle this issue by formulating the\nlearned soup as a hyperplane optimization problem and introducing block\ncoordinate gradient descent to learn the mixing coefficients. At each\niteration, MEHL-Soup only needs to load a few fine-tuned models and build a\ncomputational graph with one combined model. We further extend MEHL-Soup to\nMEHL-Soup+ in a layer-wise manner. Experimental results on various ViT models\nand data sets show that MEHL-Soup(+) outperforms Learned-Soup(+) in terms of\ntest accuracy, and also reduces memory usage by more than $13\\times$. Moreover,\nMEHL-Soup(+) can be run on a single GPU and achieves $9\\times$ speed up in soup\nconstruction compared with the Learned-Soup. The code is released at\nhttps://github.com/nblt/MEHL-Soup.\n","authors":["Tao Li","Weisen Jiang","Fanghui Liu","Xiaolin Huang","James T. Kwok"],"pdf_url":"https://arxiv.org/pdf/2407.03641v2.pdf","comment":"ECCV 2024"},{"id":"http://arxiv.org/abs/2407.16205v1","updated":"2024-07-23T06:14:41Z","published":"2024-07-23T06:14:41Z","title":"Figure it Out: Analyzing-based Jailbreak Attack on Large Language Models","summary":"  The rapid development of Large Language Models (LLMs) has brought remarkable\ngenerative capabilities across diverse tasks. However, despite the impressive\nachievements, these models still have numerous security vulnerabilities,\nparticularly when faced with jailbreak attacks. Therefore, by investigating\njailbreak attacks, we can uncover hidden weaknesses in LLMs and guide us in\ndeveloping more robust defense mechanisms to fortify their security. In this\npaper, we further explore the boundary of jailbreak attacks on LLMs and propose\nAnalyzing-based Jailbreak (ABJ). This effective jailbreak attack method takes\nadvantage of LLMs' growing analyzing and reasoning capability and reveals their\nunderlying vulnerabilities when facing analysis-based tasks. We conduct a\ndetailed evaluation of ABJ across various open-source and closed-source LLMs,\nwhich achieves 94.8% Attack Success Rate (ASR) and 1.06 Attack Efficiency (AE)\non GPT-4-turbo-0409, demonstrating state-of-the-art attack effectiveness and\nefficiency. Our research highlights the importance of prioritizing and\nenhancing the safety of LLMs to mitigate the risks of misuse.\n","authors":["Shi Lin","Rongchang Li","Xun Wang","Changting Lin","Wenpeng Xing","Meng Han"],"pdf_url":"https://arxiv.org/pdf/2407.16205v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16186v1","updated":"2024-07-23T05:22:29Z","published":"2024-07-23T05:22:29Z","title":"Automatic Environment Shaping is the Next Frontier in RL","summary":"  Many roboticists dream of presenting a robot with a task in the evening and\nreturning the next morning to find the robot capable of solving the task. What\nis preventing us from achieving this? Sim-to-real reinforcement learning (RL)\nhas achieved impressive performance on challenging robotics tasks, but requires\nsubstantial human effort to set up the task in a way that is amenable to RL.\nIt's our position that algorithmic improvements in policy optimization and\nother ideas should be guided towards resolving the primary bottleneck of\nshaping the training environment, i.e., designing observations, actions,\nrewards and simulation dynamics. Most practitioners don't tune the RL\nalgorithm, but other environment parameters to obtain a desirable controller.\nWe posit that scaling RL to diverse robotic tasks will only be achieved if the\ncommunity focuses on automating environment shaping procedures.\n","authors":["Younghyo Park","Gabriel B. Margolis","Pulkit Agrawal"],"pdf_url":"https://arxiv.org/pdf/2407.16186v1.pdf","comment":"ICML 2024 Position Track; Website at\n  https://auto-env-shaping.github.io/"},{"id":"http://arxiv.org/abs/2407.16177v1","updated":"2024-07-23T04:47:58Z","published":"2024-07-23T04:47:58Z","title":"Logifold: A Geometrical Foundation of Ensemble Machine Learning","summary":"  We present a local-to-global and measure-theoretical approach to\nunderstanding datasets. The core idea is to formulate a logifold structure and\nto interpret network models with restricted domains as local charts of\ndatasets. In particular, this provides a mathematical foundation for ensemble\nmachine learning. Our experiments demonstrate that logifolds can be implemented\nto identify fuzzy domains and improve accuracy compared to taking average of\nmodel outputs. Additionally, we provide a theoretical example of a logifold,\nhighlighting the importance of restricting to domains of classifiers in an\nensemble.\n","authors":["Inkee Jung","Siu-Cheong Lau"],"pdf_url":"https://arxiv.org/pdf/2407.16177v1.pdf","comment":"6 pages"},{"id":"http://arxiv.org/abs/2306.06338v2","updated":"2024-07-23T04:43:31Z","published":"2023-06-10T03:29:48Z","title":"Machine Learning Based Missing Values Imputation in Categorical Datasets","summary":"  In order to predict and fill in the gaps in categorical datasets, this\nresearch looked into the use of machine learning algorithms. The emphasis was\non ensemble models constructed using the Error Correction Output Codes\nframework, including models based on SVM and KNN as well as a hybrid classifier\nthat combines models based on SVM, KNN,and MLP. Three diverse datasets, the\nCPU, Hypothyroid, and Breast Cancer datasets were employed to validate these\nalgorithms. Results indicated that these machine learning techniques provided\nsubstantial performance in predicting and completing missing data, with the\neffectiveness varying based on the specific dataset and missing data pattern.\nCompared to solo models, ensemble models that made use of the ECOC framework\nsignificantly improved prediction accuracy and robustness. Deep learning for\nmissing data imputation has obstacles despite these encouraging results,\nincluding the requirement for large amounts of labeled data and the possibility\nof overfitting. Subsequent research endeavors ought to evaluate the feasibility\nand efficacy of deep learning algorithms in the context of the imputation of\nmissing data.\n","authors":["Muhammad Ishaq","Sana Zahir","Laila Iftikhar","Mohammad Farhad Bulbul","Seungmin Rho","Mi Young Lee"],"pdf_url":"https://arxiv.org/pdf/2306.06338v2.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2407.16174v1","updated":"2024-07-23T04:41:36Z","published":"2024-07-23T04:41:36Z","title":"Pixel Embedding: Fully Quantized Convolutional Neural Network with\n  Differentiable Lookup Table","summary":"  By quantizing network weights and activations to low bitwidth, we can obtain\nhardware-friendly and energy-efficient networks. However, existing quantization\ntechniques utilizing the straight-through estimator and piecewise constant\nfunctions face the issue of how to represent originally high-bit input data\nwith low-bit values. To fully quantize deep neural networks, we propose pixel\nembedding, which replaces each float-valued input pixel with a vector of\nquantized values by using a lookup table. The lookup table or low-bit\nrepresentation of pixels is differentiable and trainable by backpropagation.\nSuch replacement of inputs with vectors is similar to word embedding in the\nnatural language processing field. Experiments on ImageNet and CIFAR-100 show\nthat pixel embedding reduces the top-5 error gap caused by quantizing the\nfloating points at the first layer to only 1% for the ImageNet dataset, and the\ntop-1 error gap caused by quantizing first and last layers to slightly over 1%\nfor the CIFAR-100 dataset. The usefulness of pixel embedding is further\ndemonstrated by inference time measurements, which demonstrate over 1.7 times\nspeedup compared to floating point precision first layer.\n","authors":["Hiroyuki Tokunaga","Joel Nicholls","Daria Vazhenina","Atsunori Kanemura"],"pdf_url":"https://arxiv.org/pdf/2407.16174v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16165v1","updated":"2024-07-23T04:18:34Z","published":"2024-07-23T04:18:34Z","title":"Advanced AI Framework for Enhanced Detection and Assessment of Abdominal\n  Trauma: Integrating 3D Segmentation with 2D CNN and RNN Models","summary":"  Trauma is a significant cause of mortality and disability, particularly among\nindividuals under forty. Traditional diagnostic methods for traumatic injuries,\nsuch as X-rays, CT scans, and MRI, are often time-consuming and dependent on\nmedical expertise, which can delay critical interventions. This study explores\nthe application of artificial intelligence (AI) and machine learning (ML) to\nimprove the speed and accuracy of abdominal trauma diagnosis. We developed an\nadvanced AI-based model combining 3D segmentation, 2D Convolutional Neural\nNetworks (CNN), and Recurrent Neural Networks (RNN) to enhance diagnostic\nperformance. Our model processes abdominal CT scans to provide real-time,\nprecise assessments, thereby improving clinical decision-making and patient\noutcomes. Comprehensive experiments demonstrated that our approach\nsignificantly outperforms traditional diagnostic methods, as evidenced by\nrigorous evaluation metrics. This research sets a new benchmark for automated\ntrauma detection, leveraging the strengths of AI and ML to revolutionize trauma\ncare.\n","authors":["Liheng Jiang","Xuechun yang","Chang Yu","Zhizhong Wu","Yuting Wang"],"pdf_url":"https://arxiv.org/pdf/2407.16165v1.pdf","comment":"6 Pages"},{"id":"http://arxiv.org/abs/2407.16164v1","updated":"2024-07-23T04:13:52Z","published":"2024-07-23T04:13:52Z","title":"Representation Magnitude has a Liability to Privacy Vulnerability","summary":"  The privacy-preserving approaches to machine learning (ML) models have made\nsubstantial progress in recent years. However, it is still opaque in which\ncircumstances and conditions the model becomes privacy-vulnerable, leading to a\nchallenge for ML models to maintain both performance and privacy. In this\npaper, we first explore the disparity between member and non-member data in the\nrepresentation of models under common training frameworks. We identify how the\nrepresentation magnitude disparity correlates with privacy vulnerability and\naddress how this correlation impacts privacy vulnerability. Based on the\nobservations, we propose Saturn Ring Classifier Module (SRCM), a plug-in\nmodel-level solution to mitigate membership privacy leakage. Through a confined\nyet effective representation space, our approach ameliorates models' privacy\nvulnerability while maintaining generalizability. The code of this work can be\nfound here: \\url{https://github.com/JEKimLab/AIES2024_SRCM}\n","authors":["Xingli Fang","Jung-Eun Kim"],"pdf_url":"https://arxiv.org/pdf/2407.16164v1.pdf","comment":"Accepted in the AAAI/ACM Conference on Artificial Intelligence,\n  Ethics, and Society, 2024"},{"id":"http://arxiv.org/abs/2401.15318v2","updated":"2024-07-23T04:05:53Z","published":"2024-01-27T06:45:22Z","title":"Gaussian Splashing: Unified Particles for Versatile Motion Synthesis and\n  Rendering","summary":"  We demonstrate the feasibility of integrating physics-based animations of\nsolids and fluids with 3D Gaussian Splatting (3DGS) to create novel effects in\nvirtual scenes reconstructed using 3DGS. Leveraging the coherence of the\nGaussian Splatting and Position-Based Dynamics (PBD) in the underlying\nrepresentation, we manage rendering, view synthesis, and the dynamics of solids\nand fluids in a cohesive manner. Similar to GaussianShader, we enhance each\nGaussian kernel with an added normal, aligning the kernel's orientation with\nthe surface normal to refine the PBD simulation. This approach effectively\neliminates spiky noises that arise from rotational deformation in solids. It\nalso allows us to integrate physically based rendering to augment the dynamic\nsurface reflections on fluids. Consequently, our framework is capable of\nrealistically reproducing surface highlights on dynamic fluids and facilitating\ninteractions between scene objects and fluids from new views. For more\ninformation, please visit our project page at\n\\url{https://gaussiansplashing.github.io/}.\n","authors":["Yutao Feng","Xiang Feng","Yintong Shang","Ying Jiang","Chang Yu","Zeshun Zong","Tianjia Shao","Hongzhi Wu","Kun Zhou","Chenfanfu Jiang","Yin Yang"],"pdf_url":"https://arxiv.org/pdf/2401.15318v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16161v1","updated":"2024-07-23T04:05:29Z","published":"2024-07-23T04:05:29Z","title":"TransFeat-TPP: An Interpretable Deep Covariate Temporal Point Processes","summary":"  The classical temporal point process (TPP) constructs an intensity function\nby taking the occurrence times into account. Nevertheless, occurrence time may\nnot be the only relevant factor, other contextual data, termed covariates, may\nalso impact the event evolution. Incorporating such covariates into the model\nis beneficial, while distinguishing their relevance to the event dynamics is of\ngreat practical significance. In this work, we propose a Transformer-based\ncovariate temporal point process (TransFeat-TPP) model to improve the\ninterpretability of deep covariate-TPPs while maintaining powerful\nexpressiveness. TransFeat-TPP can effectively model complex relationships\nbetween events and covariates, and provide enhanced interpretability by\ndiscerning the importance of various covariates. Experimental results on\nsynthetic and real datasets demonstrate improved prediction accuracy and\nconsistently interpretable feature importance when compared to existing deep\ncovariate-TPPs.\n","authors":["Zizhuo Meng","Boyu Li","Xuhui Fan","Zhidong Li","Yang Wang","Fang Chen","Feng Zhou"],"pdf_url":"https://arxiv.org/pdf/2407.16161v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15351v2","updated":"2024-07-23T04:01:19Z","published":"2024-07-22T03:36:38Z","title":"LLMExplainer: Large Language Model based Bayesian Inference for Graph\n  Explanation Generation","summary":"  Recent studies seek to provide Graph Neural Network (GNN) interpretability\nvia multiple unsupervised learning models. Due to the scarcity of datasets,\ncurrent methods easily suffer from learning bias. To solve this problem, we\nembed a Large Language Model (LLM) as knowledge into the GNN explanation\nnetwork to avoid the learning bias problem. We inject LLM as a Bayesian\nInference (BI) module to mitigate learning bias. The efficacy of the BI module\nhas been proven both theoretically and experimentally. We conduct experiments\non both synthetic and real-world datasets. The innovation of our work lies in\ntwo parts: 1. We provide a novel view of the possibility of an LLM functioning\nas a Bayesian inference to improve the performance of existing algorithms; 2.\nWe are the first to discuss the learning bias issues in the GNN explanation\nproblem.\n","authors":["Jiaxing Zhang","Jiayi Liu","Dongsheng Luo","Jennifer Neville","Hua Wei"],"pdf_url":"https://arxiv.org/pdf/2407.15351v2.pdf","comment":"Preprint Paper with 13 pages"},{"id":"http://arxiv.org/abs/2407.16153v1","updated":"2024-07-23T03:40:24Z","published":"2024-07-23T03:40:24Z","title":"On the Benefits of Rank in Attention Layers","summary":"  Attention-based mechanisms are widely used in machine learning, most\nprominently in transformers. However, hyperparameters such as the rank of the\nattention matrices and the number of heads are scaled nearly the same way in\nall realizations of this architecture, without theoretical justification. In\nthis work we show that there are dramatic trade-offs between the rank and\nnumber of heads of the attention mechanism. Specifically, we present a simple\nand natural target function that can be represented using a single full-rank\nattention head for any context length, but that cannot be approximated by\nlow-rank attention unless the number of heads is exponential in the embedding\ndimension, even for short context lengths. Moreover, we prove that, for short\ncontext lengths, adding depth allows the target to be approximated by low-rank\nattention. For long contexts, we conjecture that full-rank attention is\nnecessary. Finally, we present experiments with off-the-shelf transformers that\nvalidate our theoretical findings.\n","authors":["Noah Amsel","Gilad Yehudai","Joan Bruna"],"pdf_url":"https://arxiv.org/pdf/2407.16153v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.15036v3","updated":"2024-07-23T03:33:16Z","published":"2023-11-25T14:18:29Z","title":"On-Device Soft Sensors: Real-Time Fluid Flow Estimation from Level\n  Sensor Data","summary":"  Soft sensors are crucial in bridging autonomous systems' physical and digital\nrealms, enhancing sensor fusion and perception. Instead of deploying soft\nsensors on the Cloud, this study shift towards employing on-device soft\nsensors, promising heightened efficiency and bolstering data security. Our\napproach substantially improves energy efficiency by deploying Artificial\nIntelligence (AI) directly on devices within a wireless sensor network.\nFurthermore, the synergistic integration of the Microcontroller Unit and\nField-Programmable Gate Array (FPGA) leverages the rapid AI inference\ncapabilities of the latter. Empirical evidence from our real-world use case\ndemonstrates that FPGA-based soft sensors achieve inference times ranging\nremarkably from 1.04 to 12.04 microseconds. These compelling results highlight\nthe considerable potential of our innovative approach for executing real-time\ninference tasks efficiently, thereby presenting a feasible alternative that\neffectively addresses the latency challenges intrinsic to Cloud-based\ndeployments.\n","authors":["Tianheng Ling","Chao Qian","Gregor Schiele"],"pdf_url":"https://arxiv.org/pdf/2311.15036v3.pdf","comment":"8 pages, 6 figures, 1 Table, Accepted by the 1st AUTONOMOUS\n  UBIQUITOUS SYSTEMS (AUTOQUITOUS) WORKSHOP of EAI MobiQuitous 2023 - 20th EAI\n  International Conference on Mobile and Ubiquitous Systems: Computing,\n  Networking and Services"},{"id":"http://arxiv.org/abs/2302.12177v3","updated":"2024-07-23T03:32:32Z","published":"2023-02-23T17:18:26Z","title":"EquiPocket: an E(3)-Equivariant Geometric Graph Neural Network for\n  Ligand Binding Site Prediction","summary":"  Predicting the binding sites of target proteins plays a fundamental role in\ndrug discovery. Most existing deep-learning methods consider a protein as a 3D\nimage by spatially clustering its atoms into voxels and then feed the voxelized\nprotein into a 3D CNN for prediction. However, the CNN-based methods encounter\nseveral critical issues: 1) defective in representing irregular protein\nstructures; 2) sensitive to rotations; 3) insufficient to characterize the\nprotein surface; 4) unaware of protein size shift. To address the above issues,\nthis work proposes EquiPocket, an E(3)-equivariant Graph Neural Network (GNN)\nfor binding site prediction, which comprises three modules: the first one to\nextract local geometric information for each surface atom, the second one to\nmodel both the chemical and spatial structure of protein and the last one to\ncapture the geometry of the surface via equivariant message passing over the\nsurface atoms. We further propose a dense attention output layer to alleviate\nthe effect incurred by variable protein size. Extensive experiments on several\nrepresentative benchmarks demonstrate the superiority of our framework to the\nstate-of-the-art methods.\n","authors":["Yang Zhang","Zhewei Wei","Ye Yuan","Chongxuan Li","Wenbing Huang"],"pdf_url":"https://arxiv.org/pdf/2302.12177v3.pdf","comment":"Accepted to ICML 2024 (Oral)"},{"id":"http://arxiv.org/abs/2407.16150v1","updated":"2024-07-23T03:26:07Z","published":"2024-07-23T03:26:07Z","title":"Predicting Stock Prices with FinBERT-LSTM: Integrating News Sentiment\n  Analysis","summary":"  The stock market's ascent typically mirrors the flourishing state of the\neconomy, whereas its decline is often an indicator of an economic downturn.\nTherefore, for a long time, significant correlation elements for predicting\ntrends in financial stock markets have been widely discussed, and people are\nbecoming increasingly interested in the task of financial text mining. The\ninherent instability of stock prices makes them acutely responsive to\nfluctuations within the financial markets. In this article, we use deep\nlearning networks, based on the history of stock prices and articles of\nfinancial, business, technical news that introduce market information to\npredict stock prices. We illustrate the enhancement of predictive precision by\nintegrating weighted news categories into the forecasting model. We developed a\npre-trained NLP model known as FinBERT, designed to discern the sentiments\nwithin financial texts. Subsequently, we advanced this model by incorporating\nthe sophisticated Long Short Term Memory (LSTM) architecture, thus constructing\nthe innovative FinBERT-LSTM model. This model utilizes news categories related\nto the stock market structure hierarchy, namely market, industry, and stock\nrelated news categories, combined with the stock market's stock price situation\nin the previous week for prediction. We selected NASDAQ-100 index stock data\nand trained the model on Benzinga news articles, and utilized Mean Absolute\nError (MAE), Mean Absolute Percentage Error (MAPE), and Accuracy as the key\nmetrics for the assessment and comparative analysis of the model's performance.\nThe results indicate that FinBERT-LSTM performs the best, followed by LSTM, and\nDNN model ranks third in terms of effectiveness.\n","authors":["Wenjun Gu","Yihao Zhong","Shizun Li","Changsong Wei","Liting Dong","Zhuoyue Wang","Chao Yan"],"pdf_url":"https://arxiv.org/pdf/2407.16150v1.pdf","comment":"10 pages, 6 figures, 2 tables, 2024 8th International Conference on\n  Cloud and Big Data Computing"},{"id":"http://arxiv.org/abs/2407.16145v1","updated":"2024-07-23T03:09:42Z","published":"2024-07-23T03:09:42Z","title":"Improved Few-Shot Image Classification Through Multiple-Choice Questions","summary":"  Through a simple multiple choice language prompt a VQA model can operate as a\nzero-shot image classifier, producing a classification label. Compared to\ntypical image encoders, VQA models offer an advantage: VQA-produced image\nembeddings can be infused with the most relevant visual information through\ntailored language prompts. Nevertheless, for most tasks, zero-shot VQA\nperformance is lacking, either because of unfamiliar category names, or\ndissimilar pre-training data and test data distributions. We propose a simple\nmethod to boost VQA performance for image classification using only a handful\nof labeled examples and a multiple-choice question. This few-shot method is\ntraining-free and maintains the dynamic and flexible advantages of the VQA\nmodel. Rather than relying on the final language output, our approach uses\nmultiple-choice questions to extract prompt-specific latent representations,\nwhich are enriched with relevant visual information. These representations are\ncombined to create a final overall image embedding, which is decoded via\nreference to latent class prototypes constructed from the few labeled examples.\nWe demonstrate this method outperforms both pure visual encoders and zero-shot\nVQA baselines to achieve impressive performance on common few-shot tasks\nincluding MiniImageNet, Caltech-UCSD Birds, and CIFAR-100. Finally, we show our\napproach does particularly well in settings with numerous diverse visual\nattributes such as the fabric, article-style, texture, and view of different\narticles of clothing, where other few-shot approaches struggle, as we can\ntailor our image representations only on the semantic features of interest.\n","authors":["Dipika Khullar","Emmett Goodman","Negin Sokhandan"],"pdf_url":"https://arxiv.org/pdf/2407.16145v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16142v1","updated":"2024-07-23T03:00:01Z","published":"2024-07-23T03:00:01Z","title":"Diffusion Models as Optimizers for Efficient Planning in Offline RL","summary":"  Diffusion models have shown strong competitiveness in offline reinforcement\nlearning tasks by formulating decision-making as sequential generation.\nHowever, the practicality of these methods is limited due to the lengthy\ninference processes they require. In this paper, we address this problem by\ndecomposing the sampling process of diffusion models into two decoupled\nsubprocesses: 1) generating a feasible trajectory, which is a time-consuming\nprocess, and 2) optimizing the trajectory. With this decomposition approach, we\nare able to partially separate efficiency and quality factors, enabling us to\nsimultaneously gain efficiency advantages and ensure quality assurance. We\npropose the Trajectory Diffuser, which utilizes a faster autoregressive model\nto handle the generation of feasible trajectories while retaining the\ntrajectory optimization process of diffusion models. This allows us to achieve\nmore efficient planning without sacrificing capability. To evaluate the\neffectiveness and efficiency of the Trajectory Diffuser, we conduct experiments\non the D4RL benchmarks. The results demonstrate that our method achieves $\\it\n3$-$\\it 10 \\times$ faster inference speed compared to previous sequence\nmodeling methods, while also outperforming them in terms of overall\nperformance. https://github.com/RenMing-Huang/TrajectoryDiffuser\n  Keywords: Reinforcement Learning and Efficient Planning and Diffusion Model\n","authors":["Renming Huang","Yunqiang Pei","Guoqing Wang","Yangming Zhang","Yang Yang","Peng Wang","Hengtao Shen"],"pdf_url":"https://arxiv.org/pdf/2407.16142v1.pdf","comment":"The paper was accepted by ECCV2024"},{"id":"http://arxiv.org/abs/2407.16139v1","updated":"2024-07-23T02:52:52Z","published":"2024-07-23T02:52:52Z","title":"Tackling Feature-Classifier Mismatch in Federated Learning via\n  Prompt-Driven Feature Transformation","summary":"  In traditional Federated Learning approaches like FedAvg, the global model\nunderperforms when faced with data heterogeneity. Personalized Federated\nLearning (PFL) enables clients to train personalized models to fit their local\ndata distribution better. However, we surprisingly find that the feature\nextractor in FedAvg is superior to those in most PFL methods. More\ninterestingly, by applying a linear transformation on local features extracted\nby the feature extractor to align with the classifier, FedAvg can surpass the\nmajority of PFL methods. This suggests that the primary cause of FedAvg's\ninadequate performance stems from the mismatch between the locally extracted\nfeatures and the classifier. While current PFL methods mitigate this issue to\nsome extent, their designs compromise the quality of the feature extractor,\nthus limiting the full potential of PFL. In this paper, we propose a new PFL\nframework called FedPFT to address the mismatch problem while enhancing the\nquality of the feature extractor. FedPFT integrates a feature transformation\nmodule, driven by personalized prompts, between the global feature extractor\nand classifier. In each round, clients first train prompts to transform local\nfeatures to match the global classifier, followed by training model parameters.\nThis approach can also align the training objectives of clients, reducing the\nimpact of data heterogeneity on model collaboration. Moreover, FedPFT's feature\ntransformation module is highly scalable, allowing for the use of different\nprompts to tailor local features to various tasks. Leveraging this, we\nintroduce a collaborative contrastive learning task to further refine feature\nextractor quality. Our experiments demonstrate that FedPFT outperforms\nstate-of-the-art methods by up to 7.08%.\n","authors":["Xinghao Wu","Jianwei Niu","Xuefeng Liu","Mingjia Shi","Guogang Zhu","Shaojie Tang"],"pdf_url":"https://arxiv.org/pdf/2407.16139v1.pdf","comment":"23 pages, 9 figures"},{"id":"http://arxiv.org/abs/2407.16134v1","updated":"2024-07-23T02:42:43Z","published":"2024-07-23T02:42:43Z","title":"Diffusion Transformer Captures Spatial-Temporal Dependencies: A Theory\n  for Gaussian Process Data","summary":"  Diffusion Transformer, the backbone of Sora for video generation,\nsuccessfully scales the capacity of diffusion models, pioneering new avenues\nfor high-fidelity sequential data generation. Unlike static data such as\nimages, sequential data consists of consecutive data frames indexed by time,\nexhibiting rich spatial and temporal dependencies. These dependencies represent\nthe underlying dynamic model and are critical to validate the generated data.\nIn this paper, we make the first theoretical step towards bridging diffusion\ntransformers for capturing spatial-temporal dependencies. Specifically, we\nestablish score approximation and distribution estimation guarantees of\ndiffusion transformers for learning Gaussian process data with covariance\nfunctions of various decay patterns. We highlight how the spatial-temporal\ndependencies are captured and affect learning efficiency. Our study proposes a\nnovel transformer approximation theory, where the transformer acts to unroll an\nalgorithm. We support our theoretical results by numerical experiments,\nproviding strong evidence that spatial-temporal dependencies are captured\nwithin attention layers, aligning with our approximation theory.\n","authors":["Hengyu Fu","Zehao Dou","Jiawei Guo","Mengdi Wang","Minshuo Chen"],"pdf_url":"https://arxiv.org/pdf/2407.16134v1.pdf","comment":"52 pages, 8 figures"},{"id":"http://arxiv.org/abs/2407.16131v1","updated":"2024-07-23T02:31:06Z","published":"2024-07-23T02:31:06Z","title":"Crystals with Transformers on Graphs, for Prediction of Unconventional\n  Crystal Material Properties and the Benchmark","summary":"  The ionic bonding across the lattice and ordered microscopic structures endow\ncrystals with unique symmetry and determine their macroscopic properties.\nUnconventional crystals, in particular, exhibit non-traditional lattice\nstructures or possess exotic physical properties, making them intriguing\nsubjects for investigation. Therefore, to accurately predict the physical and\nchemical properties of crystals, it is crucial to consider long-range orders.\nWhile GNN excels at capturing the local environment of atoms in crystals, they\noften face challenges in effectively capturing longer-ranged interactions due\nto their limited depth. In this paper, we propose CrysToGraph\n($\\textbf{Crys}$tals with $\\textbf{T}$ransformers $\\textbf{o}$n\n$\\textbf{Graph}$s), a novel transformer-based geometric graph network designed\nspecifically for unconventional crystalline systems, and UnconvBench, a\ncomprehensive benchmark to evaluate models' predictive performance on\nunconventional crystal materials such as defected crystals, low-dimension\ncrystals and MOF. CrysToGraph effectively captures short-range interactions\nwith transformer-based graph convolution blocks as well as long-range\ninteractions with graph-wise transformer blocks. CrysToGraph proofs its\neffectiveness in modelling unconventional crystal materials in multiple tasks,\nand moreover, it outperforms most existing methods, achieving new\nstate-of-the-art results on the benchmarks of both unconventional crystals and\ntraditional crystals.\n","authors":["Hongyi Wang","Ji Sun","Jinzhe Liang","Li Zhai","Zitian Tang","Zijian Li","Wei Zhai","Xusheng Wang","Weihao Gao","Sheng Gong","Bolong Huang","Hua Zhang"],"pdf_url":"https://arxiv.org/pdf/2407.16131v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14477v2","updated":"2024-07-23T02:10:12Z","published":"2024-07-19T17:27:52Z","title":"Data-Centric Human Preference Optimization with Rationales","summary":"  Reinforcement learning from human feedback plays a crucial role in aligning\nlanguage models towards human preferences, traditionally represented through\ncomparisons between pairs or sets of responses within a given context. While\nmany studies have enhanced algorithmic techniques to optimize learning from\nsuch data, this work shifts focus to improving preference learning through a\ndata-centric approach. Specifically, we propose enriching existing preference\ndatasets with machine-generated rationales that explain the reasons behind\nchoices. We develop a simple and principled framework to augment current\npreference learning methods with rationale information. Our comprehensive\nanalysis highlights how rationales enhance learning efficiency. Extensive\nexperiments reveal that rationale-enriched preference learning offers multiple\nadvantages: it improves data efficiency, accelerates convergence to\nhigher-performing models, and reduces verbosity bias and hallucination.\nFurthermore, this framework is versatile enough to integrate with various\npreference optimization algorithms. Overall, our findings highlight the\npotential of re-imagining data design for preference learning, demonstrating\nthat even freely available machine-generated rationales can significantly boost\nperformance across multiple dimensions. The code repository is available at\nhttps: //github.com/reds-lab/preference-learning-with-rationales\n","authors":["Hoang Anh Just","Ming Jin","Anit Sahu","Huy Phan","Ruoxi Jia"],"pdf_url":"https://arxiv.org/pdf/2407.14477v2.pdf","comment":"Preference Learning with Rationales"},{"id":"http://arxiv.org/abs/2407.16123v1","updated":"2024-07-23T02:08:22Z","published":"2024-07-23T02:08:22Z","title":"Towards Effective Fusion and Forecasting of Multimodal Spatio-temporal\n  Data for Smart Mobility","summary":"  With the rapid development of location based services, multimodal\nspatio-temporal (ST) data including trajectories, transportation modes, traffic\nflow and social check-ins are being collected for deep learning based methods.\nThese deep learning based methods learn ST correlations to support the\ndownstream tasks in the fields such as smart mobility, smart city and other\nintelligent transportation systems. Despite their effectiveness, ST data fusion\nand forecasting methods face practical challenges in real-world scenarios.\nFirst, forecasting performance for ST data-insufficient area is inferior,\nmaking it necessary to transfer meta knowledge from heterogeneous area to\nenhance the sparse representations. Second, it is nontrivial to accurately\nforecast in multi-transportation-mode scenarios due to the fine-grained ST\nfeatures of similar transportation modes, making it necessary to distinguish\nand measure the ST correlations to alleviate the influence caused by entangled\nST features. At last, partial data modalities (e.g., transportation mode) are\nlost due to privacy or technical issues in certain scenarios, making it\nnecessary to effectively fuse the multimodal sparse ST features and enrich the\nST representations. To tackle these challenges, our research work aim to\ndevelop effective fusion and forecasting methods for multimodal ST data in\nsmart mobility scenario. In this paper, we will introduce our recent works that\ninvestigates the challenges in terms of various real-world applications and\nestablish the open challenges in this field for future work.\n","authors":["Chenxing Wang"],"pdf_url":"https://arxiv.org/pdf/2407.16123v1.pdf","comment":"4 pages"},{"id":"http://arxiv.org/abs/2407.16119v1","updated":"2024-07-23T01:59:58Z","published":"2024-07-23T01:59:58Z","title":"Uncertainty-Aware Deep Neural Representations for Visual Analysis of\n  Vector Field Data","summary":"  The widespread use of Deep Neural Networks (DNNs) has recently resulted in\ntheir application to challenging scientific visualization tasks. While advanced\nDNNs demonstrate impressive generalization abilities, understanding factors\nlike prediction quality, confidence, robustness, and uncertainty is crucial.\nThese insights aid application scientists in making informed decisions.\nHowever, DNNs lack inherent mechanisms to measure prediction uncertainty,\nprompting the creation of distinct frameworks for constructing robust\nuncertainty-aware models tailored to various visualization tasks. In this work,\nwe develop uncertainty-aware implicit neural representations to model\nsteady-state vector fields effectively. We comprehensively evaluate the\nefficacy of two principled deep uncertainty estimation techniques: (1) Deep\nEnsemble and (2) Monte Carlo Dropout, aimed at enabling uncertainty-informed\nvisual analysis of features within steady vector field data. Our detailed\nexploration using several vector data sets indicate that uncertainty-aware\nmodels generate informative visualization results of vector field features.\nFurthermore, incorporating prediction uncertainty improves the resilience and\ninterpretability of our DNN model, rendering it applicable for the analysis of\nnon-trivial vector field data sets.\n","authors":["Atul Kumar","Siddharth Garg","Soumya Dutta"],"pdf_url":"https://arxiv.org/pdf/2407.16119v1.pdf","comment":"Accepted for publication at IEEE Visualization 2024"},{"id":"http://arxiv.org/abs/2405.07987v3","updated":"2024-07-23T01:42:21Z","published":"2024-05-13T17:58:30Z","title":"The Platonic Representation Hypothesis","summary":"  We argue that representations in AI models, particularly deep networks, are\nconverging. First, we survey many examples of convergence in the literature:\nover time and across multiple domains, the ways by which different neural\nnetworks represent data are becoming more aligned. Next, we demonstrate\nconvergence across data modalities: as vision models and language models get\nlarger, they measure distance between datapoints in a more and more alike way.\nWe hypothesize that this convergence is driving toward a shared statistical\nmodel of reality, akin to Plato's concept of an ideal reality. We term such a\nrepresentation the platonic representation and discuss several possible\nselective pressures toward it. Finally, we discuss the implications of these\ntrends, their limitations, and counterexamples to our analysis.\n","authors":["Minyoung Huh","Brian Cheung","Tongzhou Wang","Phillip Isola"],"pdf_url":"https://arxiv.org/pdf/2405.07987v3.pdf","comment":"Equal contributions Project: https://phillipi.github.io/prh/ Code:\n  https://github.com/minyoungg/platonic-rep"},{"id":"http://arxiv.org/abs/2407.16115v1","updated":"2024-07-23T01:33:21Z","published":"2024-07-23T01:33:21Z","title":"Transformer-based Graph Neural Networks for Battery Range Prediction in\n  AIoT Battery-Swap Services","summary":"  The concept of the sharing economy has gained broad recognition, and within\nthis context, Sharing E-Bike Battery (SEB) have emerged as a focal point of\nsocietal interest. Despite the popularity, a notable discrepancy remains\nbetween user expectations regarding the remaining battery range of SEBs and the\nreality, leading to a pronounced inclination among users to find an available\nSEB during emergency situations. In response to this challenge, the integration\nof Artificial Intelligence of Things (AIoT) and battery-swap services has\nsurfaced as a viable solution. In this paper, we propose a novel structural\nTransformer-based model, referred to as the SEB-Transformer, designed\nspecifically for predicting the battery range of SEBs. The scenario is\nconceptualized as a dynamic heterogeneous graph that encapsulates the\ninteractions between users and bicycles, providing a comprehensive framework\nfor analysis. Furthermore, we incorporate the graph structure into the\nSEB-Transformer to facilitate the estimation of the remaining e-bike battery\nrange, in conjunction with mean structural similarity, enhancing the prediction\naccuracy. By employing the predictions made by our model, we are able to\ndynamically adjust the optimal cycling routes for users in real-time, while\nalso considering the strategic locations of charging stations, thereby\noptimizing the user experience. Empirically our results on real-world datasets\ndemonstrate the superiority of our model against nine competitive baselines.\nThese innovations, powered by AIoT, not only bridge the gap between user\nexpectations and the physical limitations of battery range but also\nsignificantly improve the operational efficiency and sustainability of SEB\nservices. Through these advancements, the shared electric bicycle ecosystem is\nevolving, making strides towards a more reliable, user-friendly, and\nsustainable mode of transportation.\n","authors":["Zhao Li","Yang Liu","Chuan Zhou","Xuanwu Liu","Xuming Pan","Buqing Cao","Xindong Wu"],"pdf_url":"https://arxiv.org/pdf/2407.16115v1.pdf","comment":"9pages, 6figures, accepted by IEEE ICWS 2024 The International\n  Conference on Web Services"},{"id":"http://arxiv.org/abs/2401.03653v4","updated":"2024-07-23T00:50:15Z","published":"2024-01-08T03:50:03Z","title":"An Exploratory Study on Automatic Identification of Assumptions in the\n  Development of Deep Learning Frameworks","summary":"  Stakeholders constantly make assumptions in the development of deep learning\n(DL) frameworks. These assumptions are related to various types of software\nartifacts (e.g., requirements, design decisions, and technical debt) and can\nturn out to be invalid, leading to system failures. Existing approaches and\ntools for assumption management usually depend on manual identification of\nassumptions. However, assumptions are scattered in various sources (e.g., code\ncomments, commits, and issues) of DL framework development, and manually\nidentifying assumptions has high costs (e.g., time and resources). The\nobjective of the study is to evaluate different classification models for the\npurpose of identification with respect to assumptions from the point of view of\ndevelopers and users in the context of DL framework projects (i.e., issues,\npull requests, and commits) on GitHub. We constructed a new and largest dataset\n(i.e., AssuEval) of assumptions collected from the TensorFlow and Keras\nrepositories on GitHub; explored the performance of seven non-transformers\nbased models (e.g., Support Vector Machine, Classification and Regression\nTrees), the ALBERT model, and three large language models (i.e., ChatGPT,\nClaude, and Gemini) for identifying assumptions on the AssuEval dataset. The\nstudy results show that ALBERT achieves the best performance (f1-score: 0.9584)\nfor identifying assumptions on the AssuEval dataset, which is much better than\nthe other models (the 2nd best f1-score is 0.8858, achieved by the Claude 3.5\nSonnet model). Though ChatGPT, Claude, and Gemini are popular large language\nmodels, we do not recommend using them to identify assumptions in DL framework\ndevelopment because of their low performance. This study provides researchers\nwith the largest dataset of assumptions for further research and helps\npractitioners better understand assumptions and how to manage them in their\nprojects.\n","authors":["Chen Yang","Peng Liang","Zinan Ma"],"pdf_url":"https://arxiv.org/pdf/2401.03653v4.pdf","comment":"32 pages, 15 images, 13 tables, Manuscript revision submitted to a\n  journal (2024)"},{"id":"http://arxiv.org/abs/2406.04496v2","updated":"2024-07-23T00:46:37Z","published":"2024-06-06T20:41:36Z","title":"Time Sensitive Knowledge Editing through Efficient Finetuning","summary":"  Large Language Models (LLMs) have demonstrated impressive capability in\ndifferent tasks and are bringing transformative changes to many domains.\nHowever, keeping the knowledge in LLMs up-to-date remains a challenge once\npretraining is complete. It is thus essential to design effective methods to\nboth update obsolete knowledge and induce new knowledge into LLMs. Existing\nlocate-and-edit knowledge editing (KE) method suffers from two limitations.\nFirst, the post-edit LLMs by such methods generally have poor capability in\nanswering complex queries that require multi-hop reasoning. Second, the long\nrun-time of such locate-and-edit methods to perform knowledge edits make it\ninfeasible for large scale KE in practice. In this paper, we explore\nParameter-Efficient Fine-Tuning (PEFT) techniques as an alternative for KE. We\ncurate a more comprehensive temporal KE dataset with both knowledge update and\nknowledge injection examples for KE performance benchmarking. We further probe\nthe effect of fine-tuning on a range of layers in an LLM for the multi-hop QA\ntask. We find that PEFT performs better than locate-and-edit techniques for\ntime-sensitive knowledge edits.\n","authors":["Xiou Ge","Ali Mousavi","Edouard Grave","Armand Joulin","Kun Qian","Benjamin Han","Mostafa Arefiyan","Yunyao Li"],"pdf_url":"https://arxiv.org/pdf/2406.04496v2.pdf","comment":"ACL 2024 main"},{"id":"http://arxiv.org/abs/2407.16103v1","updated":"2024-07-23T00:16:27Z","published":"2024-07-23T00:16:27Z","title":"Reinforcement Learning Pair Trading: A Dynamic Scaling approach","summary":"  Cryptocurrency is a cryptography-based digital asset with extremely volatile\nprices. Around $70 billion worth of crypto-currency is traded daily on\nexchanges. Trading crypto-currency is difficult due to the inherent volatility\nof the crypto-market. In this work, we want to test the hypothesis: \"Can\ntechniques from artificial intelligence help with algorithmically trading\ncryptocurrencies?\". In order to address this question, we combine Reinforcement\nLearning (RL) with pair trading. Pair trading is a statistical arbitrage\ntrading technique which exploits the price difference between statistically\ncorrelated assets. We train reinforcement learners to determine when and how to\ntrade pairs of cryptocurrencies. We develop new reward shaping and\nobservation/action spaces for reinforcement learning. We performed experiments\nwith the developed reinforcement learner on pairs of BTC-GBP and BTC-EUR data\nseparated by 1-minute intervals (n = 263,520). The traditional non-RL pair\ntrading technique achieved an annualised profit of 8.33%, while the proposed\nRL-based pair trading technique achieved annualised profits from 9.94% -\n31.53%, depending upon the RL learner. Our results show that RL can\nsignificantly outperform manual and traditional pair trading techniques when\napplied to volatile markets such as cryptocurrencies.\n","authors":["Hongshen Yang","Avinash Malik"],"pdf_url":"https://arxiv.org/pdf/2407.16103v1.pdf","comment":"31 pages"},{"id":"http://arxiv.org/abs/2407.16908v1","updated":"2024-07-23T23:58:19Z","published":"2024-07-23T23:58:19Z","title":"Generation Constraint Scaling Can Mitigate Hallucination","summary":"  Addressing the issue of hallucinations in large language models (LLMs) is a\ncritical challenge. As the cognitive mechanisms of hallucination have been\nrelated to memory, here we explore hallucination for LLM that is enabled with\nexplicit memory mechanisms. We empirically demonstrate that by simply scaling\nthe readout vector that constrains generation in a memory-augmented LLM\ndecoder, hallucination mitigation can be achieved in a training-free manner.\nOur method is geometry-inspired and outperforms a state-of-the-art LLM editing\nmethod on the task of generation of Wikipedia-like biography entries both in\nterms of generation quality and runtime complexity.\n","authors":["Georgios Kollias","Payel Das","Subhajit Chaudhury"],"pdf_url":"https://arxiv.org/pdf/2407.16908v1.pdf","comment":"7 pages; accepted at ICML 2024 Workshop on Large Language Models and\n  Cognition"},{"id":"http://arxiv.org/abs/2407.16877v1","updated":"2024-07-23T22:57:23Z","published":"2024-07-23T22:57:23Z","title":"Neural Network-Based Bandit: A Medium Access Control for the IIoT Alarm\n  Scenario","summary":"  Efficient Random Access (RA) is critical for enabling reliable communication\nin Industrial Internet of Things (IIoT) networks. Herein, we propose a deep\nreinforcement learning based distributed RA scheme, entitled Neural\nNetwork-Based Bandit (NNBB), for the IIoT alarm scenario. In such a scenario,\nthe devices may detect a common critical event, and the goal is to ensure the\nalarm information is delivered successfully from at least one device. The\nproposed NNBB scheme is implemented at each device, where it trains itself\nonline and establishes implicit inter-device coordination to achieve the common\ngoal. Devices can transmit simultaneously on multiple orthogonal channels and\neach possible transmission pattern constitutes a possible action for the NNBB,\nwhich uses a deep neural network to determine the action. Our simulation\nresults show that as the number of devices in the network increases, so does\nthe performance gain of the NNBB compared to the Multi-Armed Bandit (MAB) RA\nbenchmark. For instance, NNBB experiences a 7% success rate drop when there are\nfour channels and the number of devices increases from 10 to 60, while MAB\nfaces a 25% drop.\n","authors":["Prasoon Raghuwanshi","Onel Luis Alcaraz López","Neelesh B. Mehta","Hirley Alves","Matti Latva-aho"],"pdf_url":"https://arxiv.org/pdf/2407.16877v1.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2407.16670v1","updated":"2024-07-23T17:39:49Z","published":"2024-07-23T17:39:49Z","title":"FakingRecipe: Detecting Fake News on Short Video Platforms from the\n  Perspective of Creative Process","summary":"  As short-form video-sharing platforms become a significant channel for news\nconsumption, fake news in short videos has emerged as a serious threat in the\nonline information ecosystem, making developing detection methods for this new\nscenario an urgent need. Compared with that in text and image formats, fake\nnews on short video platforms contains rich but heterogeneous information in\nvarious modalities, posing a challenge to effective feature utilization. Unlike\nexisting works mostly focusing on analyzing what is presented, we introduce a\nnovel perspective that considers how it might be created. Through the lens of\nthe creative process behind news video production, our empirical analysis\nuncovers the unique characteristics of fake news videos in material selection\nand editing. Based on the obtained insights, we design FakingRecipe, a creative\nprocess-aware model for detecting fake news short videos. It captures the fake\nnews preferences in material selection from sentimental and semantic aspects\nand considers the traits of material editing from spatial and temporal aspects.\nTo improve evaluation comprehensiveness, we first construct FakeTT, an English\ndataset for this task, and conduct experiments on both FakeTT and the existing\nChinese FakeSV dataset. The results show FakingRecipe's superiority in\ndetecting fake news on short video platforms.\n","authors":["Yuyan Bu","Qiang Sheng","Juan Cao","Peng Qi","Danding Wang","Jintao Li"],"pdf_url":"https://arxiv.org/pdf/2407.16670v1.pdf","comment":"Will appear at ACM Multimedia 2024 (MM 2024), 13 pages, 15 figures"},{"id":"http://arxiv.org/abs/2407.16554v1","updated":"2024-07-23T15:07:52Z","published":"2024-07-23T15:07:52Z","title":"Coarse-to-Fine Proposal Refinement Framework for Audio Temporal Forgery\n  Detection and Localization","summary":"  Recently, a novel form of audio partial forgery has posed challenges to its\nforensics, requiring advanced countermeasures to detect subtle forgery\nmanipulations within long-duration audio. However, existing countermeasures\nstill serve a classification purpose and fail to perform meaningful analysis of\nthe start and end timestamps of partial forgery segments. To address this\nchallenge, we introduce a novel coarse-to-fine proposal refinement framework\n(CFPRF) that incorporates a frame-level detection network (FDN) and a proposal\nrefinement network (PRN) for audio temporal forgery detection and localization.\nSpecifically, the FDN aims to mine informative inconsistency cues between real\nand fake frames to obtain discriminative features that are beneficial for\nroughly indicating forgery regions. The PRN is responsible for predicting\nconfidence scores and regression offsets to refine the coarse-grained proposals\nderived from the FDN. To learn robust discriminative features, we devise a\ndifference-aware feature learning (DAFL) module guided by contrastive\nrepresentation learning to enlarge the sensitive differences between different\nframes induced by minor manipulations. We further design a boundary-aware\nfeature enhancement (BAFE) module to capture the contextual information of\nmultiple transition boundaries and guide the interaction between boundary\ninformation and temporal features via a cross-attention mechanism. Extensive\nexperiments show that our CFPRF achieves state-of-the-art performance on\nvarious datasets, including LAV-DF, ASVS2019PS, and HAD.\n","authors":["Junyan Wu","Wei Lu","Xiangyang Luo","Rui Yang","Qian Wang","Xiaochun Cao"],"pdf_url":"https://arxiv.org/pdf/2407.16554v1.pdf","comment":"9pages, 3figures. This paper has been accepted for ACM MM 2024"},{"id":"http://arxiv.org/abs/2407.16552v1","updated":"2024-07-23T15:05:55Z","published":"2024-07-23T15:05:55Z","title":"MicroEmo: Time-Sensitive Multimodal Emotion Recognition with\n  Micro-Expression Dynamics in Video Dialogues","summary":"  Multimodal Large Language Models (MLLMs) have demonstrated remarkable\nmultimodal emotion recognition capabilities, integrating multimodal cues from\nvisual, acoustic, and linguistic contexts in the video to recognize human\nemotional states. However, existing methods ignore capturing local facial\nfeatures of temporal dynamics of micro-expressions and do not leverage the\ncontextual dependencies of the utterance-aware temporal segments in the video,\nthereby limiting their expected effectiveness to a certain extent. In this\nwork, we propose MicroEmo, a time-sensitive MLLM aimed at directing attention\nto the local facial micro-expression dynamics and the contextual dependencies\nof utterance-aware video clips. Our model incorporates two key architectural\ncontributions: (1) a global-local attention visual encoder that integrates\nglobal frame-level timestamp-bound image features with local facial features of\ntemporal dynamics of micro-expressions; (2) an utterance-aware video Q-Former\nthat captures multi-scale and contextual dependencies by generating visual\ntoken sequences for each utterance segment and for the entire video then\ncombining them. Preliminary qualitative experiments demonstrate that in a new\nExplainable Multimodal Emotion Recognition (EMER) task that exploits\nmulti-modal and multi-faceted clues to predict emotions in an open-vocabulary\n(OV) manner, MicroEmo demonstrates its effectiveness compared with the latest\nmethods.\n","authors":["Liyun Zhang"],"pdf_url":"https://arxiv.org/pdf/2407.16552v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16541v1","updated":"2024-07-23T14:53:47Z","published":"2024-07-23T14:53:47Z","title":"QPT V2: Masked Image Modeling Advances Visual Scoring","summary":"  Quality assessment and aesthetics assessment aim to evaluate the perceived\nquality and aesthetics of visual content. Current learning-based methods suffer\ngreatly from the scarcity of labeled data and usually perform sub-optimally in\nterms of generalization. Although masked image modeling (MIM) has achieved\nnoteworthy advancements across various high-level tasks (e.g., classification,\ndetection etc.). In this work, we take on a novel perspective to investigate\nits capabilities in terms of quality- and aesthetics-awareness. To this end, we\npropose Quality- and aesthetics-aware pretraining (QPT V2), the first\npretraining framework based on MIM that offers a unified solution to quality\nand aesthetics assessment. To perceive the high-level semantics and\nfine-grained details, pretraining data is curated. To comprehensively encompass\nquality- and aesthetics-related factors, degradation is introduced. To capture\nmulti-scale quality and aesthetic information, model structure is modified.\nExtensive experimental results on 11 downstream benchmarks clearly show the\nsuperior performance of QPT V2 in comparison with current state-of-the-art\napproaches and other pretraining paradigms. Code and models will be released at\n\\url{https://github.com/KeiChiTse/QPT-V2}.\n","authors":["Qizhi Xie","Kun Yuan","Yunpeng Qu","Mingda Wu","Ming Sun","Chao Zhou","Jihong Zhu"],"pdf_url":"https://arxiv.org/pdf/2407.16541v1.pdf","comment":"8 pages, 6 figures"},{"id":"http://arxiv.org/abs/2404.00636v3","updated":"2024-07-23T10:47:22Z","published":"2024-03-31T10:13:55Z","title":"Learning to Generate Conditional Tri-plane for 3D-aware Expression\n  Controllable Portrait Animation","summary":"  In this paper, we present Export3D, a one-shot 3D-aware portrait animation\nmethod that is able to control the facial expression and camera view of a given\nportrait image. To achieve this, we introduce a tri-plane generator with an\neffective expression conditioning method, which directly generates a tri-plane\nof 3D prior by transferring the expression parameter of 3DMM into the source\nimage. The tri-plane is then decoded into the image of different view through a\ndifferentiable volume rendering. Existing portrait animation methods heavily\nrely on image warping to transfer the expression in the motion space,\nchallenging on disentanglement of appearance and expression. In contrast, we\npropose a contrastive pre-training framework for appearance-free expression\nparameter, eliminating undesirable appearance swap when transferring a\ncross-identity expression. Extensive experiments show that our pre-training\nframework can learn the appearance-free expression representation hidden in\n3DMM, and our model can generate 3D-aware expression controllable portrait\nimages without appearance swap in the cross-identity manner.\n","authors":["Taekyung Ki","Dongchan Min","Gyeongsu Chae"],"pdf_url":"https://arxiv.org/pdf/2404.00636v3.pdf","comment":"ECCV 2024. Project page: https://export3d.github.io"},{"id":"http://arxiv.org/abs/2407.16307v1","updated":"2024-07-23T09:00:52Z","published":"2024-07-23T09:00:52Z","title":"Multimodal Unlearnable Examples: Protecting Data against Multimodal\n  Contrastive Learning","summary":"  Multimodal contrastive learning (MCL) has shown remarkable advances in\nzero-shot classification by learning from millions of image-caption pairs\ncrawled from the Internet. However, this reliance poses privacy risks, as\nhackers may unauthorizedly exploit image-text data for model training,\npotentially including personal and privacy-sensitive information. Recent works\npropose generating unlearnable examples by adding imperceptible perturbations\nto training images to build shortcuts for protection. However, they are\ndesigned for unimodal classification, which remains largely unexplored in MCL.\nWe first explore this context by evaluating the performance of existing methods\non image-caption pairs, and they do not generalize effectively to multimodal\ndata and exhibit limited impact to build shortcuts due to the lack of labels\nand the dispersion of pairs in MCL. In this paper, we propose Multi-step Error\nMinimization (MEM), a novel optimization process for generating multimodal\nunlearnable examples. It extends the Error-Minimization (EM) framework to\noptimize both image noise and an additional text trigger, thereby enlarging the\noptimized space and effectively misleading the model to learn the shortcut\nbetween the noise features and the text trigger. Specifically, we adopt\nprojected gradient descent to solve the noise minimization problem and use\nHotFlip to approximate the gradient and replace words to find the optimal text\ntrigger. Extensive experiments demonstrate the effectiveness of MEM, with\npost-protection retrieval results nearly half of random guessing, and its high\ntransferability across different models. Our code is available on the\nhttps://github.com/thinwayliu/Multimodal-Unlearnable-Examples\n","authors":["Xinwei Liu","Xiaojun Jia","Yuan Xun","Siyuan Liang","Xiaochun Cao"],"pdf_url":"https://arxiv.org/pdf/2407.16307v1.pdf","comment":"ACM MM2024"},{"id":"http://arxiv.org/abs/2407.15423v2","updated":"2024-07-23T08:46:03Z","published":"2024-07-22T07:00:21Z","title":"Integrating IP Broadcasting with Audio Tags: Workflow and Challenges","summary":"  The broadcasting industry is increasingly adopting IP techniques,\nrevolutionising both live and pre-recorded content production, from news\ngathering to live music events. IP broadcasting allows for the transport of\naudio and video signals in an easily configurable way, aligning with modern\nnetworking techniques. This shift towards an IP workflow allows for much\ngreater flexibility, not only in routing signals but with the integration of\ntools using standard web development techniques. One possible tool could\ninclude the use of live audio tagging, which has a number of uses in the\nproduction of content. These include from automated closed captioning to\nidentifying unwanted sound events within a scene. In this paper, we describe\nthe process of containerising an audio tagging model into a microservice, a\nsmall segregated code module that can be integrated into a multitude of\ndifferent network setups. The goal is to develop a modular, accessible, and\nflexible tool capable of seamless deployment into broadcasting workflows of all\nsizes, from small productions to large corporations. Challenges surrounding\nlatency of the selected audio tagging model and its effect on the usefulness of\nthe end product are discussed.\n","authors":["Rhys Burchett-Vass","Arshdeep Singh","Gabriel Bibbó","Mark D. Plumbley"],"pdf_url":"https://arxiv.org/pdf/2407.15423v2.pdf","comment":"Submitted to DCASE 2024 Workshop"},{"id":"http://arxiv.org/abs/2407.16248v1","updated":"2024-07-23T07:36:54Z","published":"2024-07-23T07:36:54Z","title":"Spatiotemporal Graph Guided Multi-modal Network for Livestreaming\n  Product Retrieval","summary":"  With the rapid expansion of e-commerce, more consumers have become accustomed\nto making purchases via livestreaming. Accurately identifying the products\nbeing sold by salespeople, i.e., livestreaming product retrieval (LPR), poses a\nfundamental and daunting challenge. The LPR task encompasses three primary\ndilemmas in real-world scenarios: 1) the recognition of intended products from\ndistractor products present in the background; 2) the video-image heterogeneity\nthat the appearance of products showcased in live streams often deviates\nsubstantially from standardized product images in stores; 3) there are numerous\nconfusing products with subtle visual nuances in the shop. To tackle these\nchallenges, we propose the Spatiotemporal Graphing Multi-modal Network (SGMN).\nFirst, we employ a text-guided attention mechanism that leverages the spoken\ncontent of salespeople to guide the model to focus toward intended products,\nemphasizing their salience over cluttered background products. Second, a\nlong-range spatiotemporal graph network is further designed to achieve both\ninstance-level interaction and frame-level matching, solving the misalignment\ncaused by video-image heterogeneity. Third, we propose a multi-modal hard\nexample mining, assisting the model in distinguishing highly similar products\nwith fine-grained features across the video-image-text domain. Through\nextensive quantitative and qualitative experiments, we demonstrate the superior\nperformance of our proposed SGMN model, surpassing the state-of-the-art methods\nby a substantial margin. The code is available at\n\\url{https://github.com/Huxiaowan/SGMN}.\n","authors":["Xiaowan Hu","Yiyi Chen","Yan Li","Minquan Wang","Haoqian Wang","Quan Chen","Han Li","Peng Jiang"],"pdf_url":"https://arxiv.org/pdf/2407.16248v1.pdf","comment":"9 pages, 12 figures"},{"id":"http://arxiv.org/abs/2407.16244v1","updated":"2024-07-23T07:31:42Z","published":"2024-07-23T07:31:42Z","title":"HSVLT: Hierarchical Scale-Aware Vision-Language Transformer for\n  Multi-Label Image Classification","summary":"  The task of multi-label image classification involves recognizing multiple\nobjects within a single image. Considering both valuable semantic information\ncontained in the labels and essential visual features presented in the image,\ntight visual-linguistic interactions play a vital role in improving\nclassification performance. Moreover, given the potential variance in object\nsize and appearance within a single image, attention to features of different\nscales can help to discover possible objects in the image. Recently,\nTransformer-based methods have achieved great success in multi-label image\nclassification by leveraging the advantage of modeling long-range dependencies,\nbut they have several limitations. Firstly, existing methods treat visual\nfeature extraction and cross-modal fusion as separate steps, resulting in\ninsufficient visual-linguistic alignment in the joint semantic space.\nAdditionally, they only extract visual features and perform cross-modal fusion\nat a single scale, neglecting objects with different characteristics. To\naddress these issues, we propose a Hierarchical Scale-Aware Vision-Language\nTransformer (HSVLT) with two appealing designs: (1)~A hierarchical multi-scale\narchitecture that involves a Cross-Scale Aggregation module, which leverages\njoint multi-modal features extracted from multiple scales to recognize objects\nof varying sizes and appearances in images. (2)~Interactive Visual-Linguistic\nAttention, a novel attention mechanism module that tightly integrates\ncross-modal interaction, enabling the joint updating of visual, linguistic and\nmulti-modal features. We have evaluated our method on three benchmark datasets.\nThe experimental results demonstrate that HSVLT surpasses state-of-the-art\nmethods with lower computational cost.\n","authors":["Shuyi Ouyang","Hongyi Wang","Ziwei Niu","Zhenjia Bai","Shiao Xie","Yingying Xu","Ruofeng Tong","Yen-Wei Chen","Lanfen Lin"],"pdf_url":"https://arxiv.org/pdf/2407.16244v1.pdf","comment":"10 pages, 6 figures"},{"id":"http://arxiv.org/abs/2407.16171v1","updated":"2024-07-23T04:35:56Z","published":"2024-07-23T04:35:56Z","title":"Learning Trimodal Relation for AVQA with Missing Modality","summary":"  Recent Audio-Visual Question Answering (AVQA) methods rely on complete visual\nand audio input to answer questions accurately. However, in real-world\nscenarios, issues such as device malfunctions and data transmission errors\nfrequently result in missing audio or visual modality. In such cases, existing\nAVQA methods suffer significant performance degradation. In this paper, we\npropose a framework that ensures robust AVQA performance even when a modality\nis missing. First, we propose a Relation-aware Missing Modal (RMM) generator\nwith Relation-aware Missing Modal Recalling (RMMR) loss to enhance the ability\nof the generator to recall missing modal information by understanding the\nrelationships and context among the available modalities. Second, we design an\nAudio-Visual Relation-aware (AVR) diffusion model with Audio-Visual Enhancing\n(AVE) loss to further enhance audio-visual features by leveraging the\nrelationships and shared cues between the audio-visual modalities. As a result,\nour method can provide accurate answers by effectively utilizing available\ninformation even when input modalities are missing. We believe our method holds\npotential applications not only in AVQA research but also in various\nmulti-modal scenarios.\n","authors":["Kyu Ri Park","Hong Joo Lee","Jung Uk Kim"],"pdf_url":"https://arxiv.org/pdf/2407.16171v1.pdf","comment":"Accepted at ECCV 2024"},{"id":"http://arxiv.org/abs/2303.13397v4","updated":"2024-07-23T01:44:15Z","published":"2023-03-23T16:15:18Z","title":"DiffMesh: A Motion-aware Diffusion Framework for Human Mesh Recovery\n  from Videos","summary":"  Human mesh recovery (HMR) provides rich human body information for various\nreal-world applications. While image-based HMR methods have achieved impressive\nresults, they often struggle to recover humans in dynamic scenarios, leading to\ntemporal inconsistencies and non-smooth 3D motion predictions due to the\nabsence of human motion. In contrast, video-based approaches leverage temporal\ninformation to mitigate this issue. In this paper, we present DiffMesh, an\ninnovative motion-aware Diffusion-like framework for video-based HMR. DiffMesh\nestablishes a bridge between diffusion models and human motion, efficiently\ngenerating accurate and smooth output mesh sequences by incorporating human\nmotion within the forward process and reverse process in the diffusion model.\nExtensive experiments are conducted on the widely used datasets (Human3.6M\n\\cite{h36m_pami} and 3DPW \\cite{pw3d2018}), which demonstrate the effectiveness\nand efficiency of our DiffMesh. Visual comparisons in real-world scenarios\nfurther highlight DiffMesh's suitability for practical applications.\n","authors":["Ce Zheng","Xianpeng Liu","Qucheng Peng","Tianfu Wu","Pu Wang","Chen Chen"],"pdf_url":"https://arxiv.org/pdf/2303.13397v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.16254v3","updated":"2024-07-23T20:38:11Z","published":"2023-11-27T19:02:17Z","title":"Safe-CLIP: Removing NSFW Concepts from Vision-and-Language Models","summary":"  Large-scale vision-and-language models, such as CLIP, are typically trained\non web-scale data, which can introduce inappropriate content and lead to the\ndevelopment of unsafe and biased behavior. This, in turn, hampers their\napplicability in sensitive and trustworthy contexts and could raise significant\nconcerns in their adoption. Our research introduces a novel approach to\nenhancing the safety of vision-and-language models by diminishing their\nsensitivity to NSFW (not safe for work) inputs. In particular, our methodology\nseeks to sever \"toxic\" linguistic and visual concepts, unlearning the linkage\nbetween unsafe linguistic or visual items and unsafe regions of the embedding\nspace. We show how this can be done by fine-tuning a CLIP model on synthetic\ndata obtained from a large language model trained to convert between safe and\nunsafe sentences, and a text-to-image generator. We conduct extensive\nexperiments on the resulting embedding space for cross-modal retrieval,\ntext-to-image, and image-to-text generation, where we show that our model can\nbe remarkably employed with pre-trained generative models. Our source code and\ntrained models are available at: https://github.com/aimagelab/safe-clip.\n","authors":["Samuele Poppi","Tobia Poppi","Federico Cocchi","Marcella Cornia","Lorenzo Baraldi","Rita Cucchiara"],"pdf_url":"https://arxiv.org/pdf/2311.16254v3.pdf","comment":"ECCV 2024"},{"id":"http://arxiv.org/abs/2407.16171v1","updated":"2024-07-23T04:35:56Z","published":"2024-07-23T04:35:56Z","title":"Learning Trimodal Relation for Audio-Visual Question Answering with\n  Missing Modality","summary":"  Recent Audio-Visual Question Answering (AVQA) methods rely on complete visual\nand audio input to answer questions accurately. However, in real-world\nscenarios, issues such as device malfunctions and data transmission errors\nfrequently result in missing audio or visual modality. In such cases, existing\nAVQA methods suffer significant performance degradation. In this paper, we\npropose a framework that ensures robust AVQA performance even when a modality\nis missing. First, we propose a Relation-aware Missing Modal (RMM) generator\nwith Relation-aware Missing Modal Recalling (RMMR) loss to enhance the ability\nof the generator to recall missing modal information by understanding the\nrelationships and context among the available modalities. Second, we design an\nAudio-Visual Relation-aware (AVR) diffusion model with Audio-Visual Enhancing\n(AVE) loss to further enhance audio-visual features by leveraging the\nrelationships and shared cues between the audio-visual modalities. As a result,\nour method can provide accurate answers by effectively utilizing available\ninformation even when input modalities are missing. We believe our method holds\npotential applications not only in AVQA research but also in various\nmulti-modal scenarios.\n","authors":["Kyu Ri Park","Hong Joo Lee","Jung Uk Kim"],"pdf_url":"https://arxiv.org/pdf/2407.16171v1.pdf","comment":"Accepted at ECCV 2024"}]},"2024-07-24T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2407.17469v1","updated":"2024-07-24T17:59:07Z","published":"2024-07-24T17:59:07Z","title":"I Could've Asked That: Reformulating Unanswerable Questions","summary":"  When seeking information from unfamiliar documents, users frequently pose\nquestions that cannot be answered by the documents. While existing large\nlanguage models (LLMs) identify these unanswerable questions, they do not\nassist users in reformulating their questions, thereby reducing their overall\nutility. We curate CouldAsk, an evaluation benchmark composed of existing and\nnew datasets for document-grounded question answering, specifically designed to\nstudy reformulating unanswerable questions. We evaluate state-of-the-art\nopen-source and proprietary LLMs on CouldAsk. The results demonstrate the\nlimited capabilities of these models in reformulating questions. Specifically,\nGPT-4 and Llama2-7B successfully reformulate questions only 26% and 12% of the\ntime, respectively. Error analysis shows that 62% of the unsuccessful\nreformulations stem from the models merely rephrasing the questions or even\ngenerating identical questions. We publicly release the benchmark and the code\nto reproduce the experiments.\n","authors":["Wenting Zhao","Ge Gao","Claire Cardie","Alexander M. Rush"],"pdf_url":"https://arxiv.org/pdf/2407.17469v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17468v1","updated":"2024-07-24T17:59:05Z","published":"2024-07-24T17:59:05Z","title":"WildHallucinations: Evaluating Long-form Factuality in LLMs with\n  Real-World Entity Queries","summary":"  While hallucinations of large language models (LLMs) prevail as a major\nchallenge, existing evaluation benchmarks on factuality do not cover the\ndiverse domains of knowledge that the real-world users of LLMs seek information\nabout. To bridge this gap, we introduce WildHallucinations, a benchmark that\nevaluates factuality. It does so by prompting LLMs to generate information\nabout entities mined from user-chatbot conversations in the wild. These\ngenerations are then automatically fact-checked against a systematically\ncurated knowledge source collected from web search. Notably, half of these\nreal-world entities do not have associated Wikipedia pages. We evaluate 118,785\ngenerations from 15 LLMs on 7,919 entities. We find that LLMs consistently\nhallucinate more on entities without Wikipedia pages and exhibit varying\nhallucination rates across different domains. Finally, given the same base\nmodels, adding a retrieval component only slightly reduces hallucinations but\ndoes not eliminate hallucinations.\n","authors":["Wenting Zhao","Tanya Goyal","Yu Ying Chiu","Liwei Jiang","Benjamin Newman","Abhilasha Ravichander","Khyathi Chandu","Ronan Le Bras","Claire Cardie","Yuntian Deng","Yejin Choi"],"pdf_url":"https://arxiv.org/pdf/2407.17468v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17467v1","updated":"2024-07-24T17:59:02Z","published":"2024-07-24T17:59:02Z","title":"CMR Scaling Law: Predicting Critical Mixture Ratios for Continual\n  Pre-training of Language Models","summary":"  Large Language Models (LLMs) excel in diverse tasks but often underperform in\nspecialized fields due to limited domain-specific or proprietary corpus.\nContinual pre-training (CPT) enhances LLM capabilities by imbuing new\ndomain-specific or proprietary knowledge while replaying general corpus to\nprevent catastrophic forgetting. The data mixture ratio of general corpus and\ndomain-specific corpus, however, has been chosen heuristically, leading to\nsub-optimal training efficiency in practice. In this context, we attempt to\nre-visit the scaling behavior of LLMs under the hood of CPT, and discover a\npower-law relationship between loss, mixture ratio, and training tokens scale.\nWe formalize the trade-off between general and domain-specific capabilities,\nleading to a well-defined Critical Mixture Ratio (CMR) of general and domain\ndata. By striking the balance, CMR maintains the model's general ability and\nachieves the desired domain transfer, ensuring the highest utilization of\navailable resources. Therefore, if we value the balance between efficiency and\neffectiveness, CMR can be consider as the optimal mixture ratio.Through\nextensive experiments, we ascertain the predictability of CMR, and propose CMR\nscaling law and have substantiated its generalization. These findings offer\npractical guidelines for optimizing LLM training in specialized domains,\nensuring both general and domain-specific performance while efficiently\nmanaging training resources.\n","authors":["Jiawei Gu","Zacc Yang","Chuanghao Ding","Rui Zhao","Fei Tan"],"pdf_url":"https://arxiv.org/pdf/2407.17467v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14236v3","updated":"2024-07-24T17:56:32Z","published":"2024-03-21T08:54:24Z","title":"A Unified Framework for Model Editing","summary":"  ROME and MEMIT are largely believed to be two different model editing\nalgorithms, with the major difference between them being the ability to perform\nbatched edits. In this paper, we unify these two algorithms under a single\nconceptual umbrella, optimizing for the same goal, which we call the\npreservation-memorization objective. ROME uses an equality constraint to\noptimize this objective to perform one edit at a time, whereas MEMIT employs a\nmore flexible least-square constraint that allows for batched edits. We\ngeneralize ROME and enable batched editing with equality constraint in the form\nof EMMET - an Equality-constrained Mass Model Editing algorithm for\nTransformers, a new batched memory-editing algorithm. EMMET can perform\nbatched-edits up to a batch-size of 10,000, with very similar performance to\nMEMIT across multiple dimensions. With the introduction of EMMET, we truly\nunify ROME and MEMIT and show that both algorithms are equivalent in terms of\ntheir optimization objective, their abilities (singular and batched editing),\ntheir model editing performance and their limitations.\n","authors":["Akshat Gupta","Dev Sajnani","Gopala Anumanchipalli"],"pdf_url":"https://arxiv.org/pdf/2403.14236v3.pdf","comment":"Under review. To appear as poster at KnowledgeableLM Workshop\n  co-located with ACL 2024"},{"id":"http://arxiv.org/abs/2407.17447v1","updated":"2024-07-24T17:23:18Z","published":"2024-07-24T17:23:18Z","title":"Fluent Student-Teacher Redteaming","summary":"  Many publicly available language models have been safety tuned to reduce the\nlikelihood of toxic or liability-inducing text. Users or security analysts\nattempt to jailbreak or redteam these models with adversarial prompts which\ncause compliance with requests. One attack method is to apply discrete\noptimization techniques to the prompt. However, the resulting attack strings\nare often gibberish text, easily filtered by defenders due to high measured\nperplexity, and may fail for unseen tasks and/or well-tuned models. In this\nwork, we improve existing algorithms (primarily GCG and BEAST) to develop\npowerful and fluent attacks on safety-tuned models like Llama-2 and Phi-3. Our\ntechnique centers around a new distillation-based approach that encourages the\nvictim model to emulate a toxified finetune, either in terms of output\nprobabilities or internal activations. To encourage human-fluent attacks, we\nadd a multi-model perplexity penalty and a repetition penalty to the objective.\nWe also enhance optimizer strength by allowing token insertions, token swaps,\nand token deletions and by using longer attack sequences. The resulting process\nis able to reliably jailbreak the most difficult target models with prompts\nthat appear similar to human-written prompts. On Advbench we achieve attack\nsuccess rates $>93$% for Llama-2-7B, Llama-3-8B, and Vicuna-7B, while\nmaintaining model-measured perplexity $<33$; we achieve $95$% attack success\nfor Phi-3, though with higher perplexity. We also find a universally-optimized\nsingle fluent prompt that induces $>88$% compliance on previously unseen tasks\nacross Llama-2-7B, Phi-3-mini and Vicuna-7B and transfers to other black-box\nmodels.\n","authors":["T. Ben Thompson","Michael Sklar"],"pdf_url":"https://arxiv.org/pdf/2407.17447v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.01267v2","updated":"2024-07-24T17:13:55Z","published":"2024-03-02T17:10:44Z","title":"Dissecting Language Models: Machine Unlearning via Selective Pruning","summary":"  Understanding and shaping the behaviour of Large Language Models (LLMs) is\nincreasingly important as applications become more powerful and more frequently\nadopted. This paper introduces a machine unlearning method specifically\ndesigned for LLMs. We introduce a selective pruning method for LLMs that\nremoves neurons based on their relative importance on a targeted capability\ncompared to overall network performance. This approach is a compute- and\ndata-efficient method for identifying and removing neurons that enable specific\nbehaviours. Our findings reveal that both feed-forward and attention neurons in\nLLMs are specialized; that is, for specific tasks, certain neurons are more\ncrucial than others. Code from all experiments is available at\nhttps://github.com/nickypro/selective-pruning\n","authors":["Nicholas Pochinkov","Nandi Schoots"],"pdf_url":"https://arxiv.org/pdf/2403.01267v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14933v2","updated":"2024-07-24T16:52:51Z","published":"2024-07-20T16:50:18Z","title":"Consent in Crisis: The Rapid Decline of the AI Data Commons","summary":"  General-purpose artificial intelligence (AI) systems are built on massive\nswathes of public web data, assembled into corpora such as C4, RefinedWeb, and\nDolma. To our knowledge, we conduct the first, large-scale, longitudinal audit\nof the consent protocols for the web domains underlying AI training corpora.\nOur audit of 14,000 web domains provides an expansive view of crawlable web\ndata and how codified data use preferences are changing over time. We observe a\nproliferation of AI-specific clauses to limit use, acute differences in\nrestrictions on AI developers, as well as general inconsistencies between\nwebsites' expressed intentions in their Terms of Service and their robots.txt.\nWe diagnose these as symptoms of ineffective web protocols, not designed to\ncope with the widespread re-purposing of the internet for AI. Our longitudinal\nanalyses show that in a single year (2023-2024) there has been a rapid\ncrescendo of data restrictions from web sources, rendering ~5%+ of all tokens\nin C4, or 28%+ of the most actively maintained, critical sources in C4, fully\nrestricted from use. For Terms of Service crawling restrictions, a full 45% of\nC4 is now restricted. If respected or enforced, these restrictions are rapidly\nbiasing the diversity, freshness, and scaling laws for general-purpose AI\nsystems. We hope to illustrate the emerging crises in data consent, for both\ndevelopers and creators. The foreclosure of much of the open web will impact\nnot only commercial AI, but also non-commercial AI and academic research.\n","authors":["Shayne Longpre","Robert Mahari","Ariel Lee","Campbell Lund","Hamidah Oderinwale","William Brannon","Nayan Saxena","Naana Obeng-Marnu","Tobin South","Cole Hunter","Kevin Klyman","Christopher Klamm","Hailey Schoelkopf","Nikhil Singh","Manuel Cherep","Ahmad Anis","An Dinh","Caroline Chitongo","Da Yin","Damien Sileo","Deividas Mataciunas","Diganta Misra","Emad Alghamdi","Enrico Shippole","Jianguo Zhang","Joanna Materzynska","Kun Qian","Kush Tiwary","Lester Miranda","Manan Dey","Minnie Liang","Mohammed Hamdy","Niklas Muennighoff","Seonghyeon Ye","Seungone Kim","Shrestha Mohanty","Vipul Gupta","Vivek Sharma","Vu Minh Chien","Xuhui Zhou","Yizhi Li","Caiming Xiong","Luis Villa","Stella Biderman","Hanlin Li","Daphne Ippolito","Sara Hooker","Jad Kabbara","Sandy Pentland"],"pdf_url":"https://arxiv.org/pdf/2407.14933v2.pdf","comment":"41 pages (13 main), 5 figures, 9 tables"},{"id":"http://arxiv.org/abs/2407.17406v1","updated":"2024-07-24T16:38:38Z","published":"2024-07-24T16:38:38Z","title":"Dependency Transformer Grammars: Integrating Dependency Structures into\n  Transformer Language Models","summary":"  Syntactic Transformer language models aim to achieve better generalization\nthrough simultaneously modeling syntax trees and sentences. While prior work\nhas been focusing on adding constituency-based structures to Transformers, we\nintroduce Dependency Transformer Grammars (DTGs), a new class of Transformer\nlanguage model with explicit dependency-based inductive bias. DTGs simulate\ndependency transition systems with constrained attention patterns by modifying\nattention masks, incorporate the stack information through relative positional\nencoding, and augment dependency arc representation with a combination of token\nembeddings and operation embeddings. When trained on a dataset of sentences\nannotated with dependency trees, DTGs achieve better generalization while\nmaintaining comparable perplexity with Transformer language model baselines.\nDTGs also outperform recent constituency-based models, showing that dependency\ncan better guide Transformer language models. Our code is released at\nhttps://github.com/zhaoyd1/Dep_Transformer_Grammars.\n","authors":["Yida Zhao","Chao Lou","Kewei Tu"],"pdf_url":"https://arxiv.org/pdf/2407.17406v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17390v1","updated":"2024-07-24T16:14:15Z","published":"2024-07-24T16:14:15Z","title":"CovScore: Evaluation of Multi-Document Abstractive Title Set Generation","summary":"  This paper introduces CovScore, an automatic reference-less methodology for\nevaluating thematic title sets, extracted from a corpus of documents. While\nsuch extraction methods are widely used, evaluating their effectiveness remains\nan open question. Moreover, some existing practices heavily rely on slow and\nlaborious human annotation procedures. Inspired by recently introduced\nLLM-based judge methods, we propose a novel methodology that decomposes quality\ninto five main metrics along different aspects of evaluation. This framing\nsimplifies and expedites the manual evaluation process and enables automatic\nand independent LLM-based evaluation. As a test case, we apply our approach to\na corpus of Holocaust survivor testimonies, motivated both by its relevance to\ntitle set extraction and by the moral significance of this pursuit. We validate\nthe methodology by experimenting with naturalistic and synthetic title set\ngeneration systems and compare their performance with the methodology.\n","authors":["Itamar Trainin","Omri Abend"],"pdf_url":"https://arxiv.org/pdf/2407.17390v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17387v1","updated":"2024-07-24T16:11:39Z","published":"2024-07-24T16:11:39Z","title":"PERSONA: A Reproducible Testbed for Pluralistic Alignment","summary":"  The rapid advancement of language models (LMs) necessitates robust alignment\nwith diverse user values. However, current preference optimization approaches\noften fail to capture the plurality of user opinions, instead reinforcing\nmajority viewpoints and marginalizing minority perspectives. We introduce\nPERSONA, a reproducible test bed designed to evaluate and improve pluralistic\nalignment of LMs. We procedurally generate diverse user profiles from US census\ndata, resulting in 1,586 synthetic personas with varied demographic and\nidiosyncratic attributes. We then generate a large-scale evaluation dataset\ncontaining 3,868 prompts and 317,200 feedback pairs obtained from our synthetic\npersonas. Leveraging this dataset, we systematically evaluate LM capabilities\nin role-playing diverse users, verified through human judges, and the\nestablishment of both a benchmark, PERSONA Bench, for pluralistic alignment\napproaches as well as an extensive dataset to create new and future benchmarks.\nThe full dataset and benchmarks are available here:\nhttps://www.synthlabs.ai/research/persona.\n","authors":["Louis Castricato","Nathan Lile","Rafael Rafailov","Jan-Philipp Fränken","Chelsea Finn"],"pdf_url":"https://arxiv.org/pdf/2407.17387v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17383v1","updated":"2024-07-24T16:07:11Z","published":"2024-07-24T16:07:11Z","title":"A Comprehensive Approach to Misspelling Correction with BERT and\n  Levenshtein Distance","summary":"  Writing, as an omnipresent form of human communication, permeates nearly\nevery aspect of contemporary life. Consequently, inaccuracies or errors in\nwritten communication can lead to profound consequences, ranging from financial\nlosses to potentially life-threatening situations. Spelling mistakes, among the\nmost prevalent writing errors, are frequently encountered due to various\nfactors. This research aims to identify and rectify diverse spelling errors in\ntext using neural networks, specifically leveraging the Bidirectional Encoder\nRepresentations from Transformers (BERT) masked language model. To achieve this\ngoal, we compiled a comprehensive dataset encompassing both non-real-word and\nreal-word errors after categorizing different types of spelling mistakes.\nSubsequently, multiple pre-trained BERT models were employed. To ensure optimal\nperformance in correcting misspelling errors, we propose a combined approach\nutilizing the BERT masked language model and Levenshtein distance. The results\nfrom our evaluation data demonstrate that the system presented herein exhibits\nremarkable capabilities in identifying and rectifying spelling mistakes, often\nsurpassing existing systems tailored for the Persian language.\n","authors":["Amirreza Naziri","Hossein Zeinali"],"pdf_url":"https://arxiv.org/pdf/2407.17383v1.pdf","comment":"12 pages, 9 figures, 5 tables"},{"id":"http://arxiv.org/abs/2407.17379v1","updated":"2024-07-24T15:59:01Z","published":"2024-07-24T15:59:01Z","title":"MMRA: A Benchmark for Multi-granularity Multi-image Relational\n  Association","summary":"  Given the remarkable success that large visual language models (LVLMs) have\nachieved in image perception tasks, the endeavor to make LVMLs perceive the\nworld like humans is drawing increasing attention. Current multi-modal\nbenchmarks mainly focus on the objective fact or certain topic related\npotential knowledge within a image, but overlook the associative relations\nbetween multiple images. Therefore, we define a multi-image relation\nassociation task, and meticulously curate \\textbf{MMRA} benchmark, a\n\\textbf{M}ulti-granularity \\textbf{M}ulti-image \\textbf{R}elational\n\\textbf{A}ssociation benchmark, consisted of \\textbf{1026} samples. In order to\nsystematically and comprehensively evaluate mainstream LVLMs, we establish an\nassociational relation system among images that contain \\textbf{11 subtasks}\n(e.g, UsageSimilarity, SubEvent, etc.) at two granularity levels (i.e.,\n\"\\textbf{image}\" and \"\\textbf{entity}\") according to the relations in\nConceptNet. Our experiments demonstrate that, on our MMRA benchmark, current\nmainstream LVLMs all have their own advantages and disadvantages across\ndifferent subtasks. It is worth noting that, at the entity level, the\nperformance of all models is worse than that of them at the image level,\nindicating that the fine-grained multi-image perception task is still\nchallenging for LVLMs. The tasks related to spatial perception are relatively\ndifficult for LVLMs to handle. Furthermore, we find that LVMLs exhibit a good\nability to perceive image details, and the key to enhancing their multi-image\nassociation capability is to strengthen the reasoning ability of their language\nmodel component. All our codes and data are released at\nhtt\\url{https://github.com/Wusiwei0410/MMRA}.\n","authors":["Siwei Wu","Kang Zhu","Yu Bai","Yiming Liang","Yizhi Li","Haoning Wu","Jiaheng Liu","Ruibo Liu","Xingwei Qu","Xuxin Cheng","Ge Zhang","Wenhao Huang","Chenghua Lin"],"pdf_url":"https://arxiv.org/pdf/2407.17379v1.pdf","comment":"VLMS, Multi-Image Association"},{"id":"http://arxiv.org/abs/2404.03302v3","updated":"2024-07-24T15:51:08Z","published":"2024-04-04T08:52:30Z","title":"How Easily do Irrelevant Inputs Skew the Responses of Large Language\n  Models?","summary":"  By leveraging the retrieval of information from external knowledge databases,\nLarge Language Models (LLMs) exhibit enhanced capabilities for accomplishing\nmany knowledge-intensive tasks. However, due to the inherent flaws of current\nretrieval systems, there might exist irrelevant information within those\nretrieving top-ranked passages. In this work, we present a comprehensive\ninvestigation into the robustness of LLMs to different types of irrelevant\ninformation under various conditions. We initially introduce a framework to\nconstruct high-quality irrelevant information that ranges from semantically\nunrelated, partially related, and related to questions. Furthermore, our\nanalysis demonstrates that the constructed irrelevant information not only\nscores highly on similarity metrics, being highly retrieved by existing\nsystems, but also bears semantic connections to the context. Our investigation\nreveals that current LLMs still face challenges in discriminating highly\nsemantically related information and can be easily distracted by these\nirrelevant yet misleading content. Besides, we also find that current solutions\nfor handling irrelevant information have limitations in improving the\nrobustness of LLMs to such distractions. All the resources are available on\nGitHub at https://github.com/Di-viner/LLM-Robustness-to-Irrelevant-Information.\n","authors":["Siye Wu","Jian Xie","Jiangjie Chen","Tinghui Zhu","Kai Zhang","Yanghua Xiao"],"pdf_url":"https://arxiv.org/pdf/2404.03302v3.pdf","comment":"COLM 2024"},{"id":"http://arxiv.org/abs/2402.14154v2","updated":"2024-07-24T15:19:20Z","published":"2024-02-21T22:27:40Z","title":"MM-Soc: Benchmarking Multimodal Large Language Models in Social Media\n  Platforms","summary":"  Social media platforms are hubs for multimodal information exchange,\nencompassing text, images, and videos, making it challenging for machines to\ncomprehend the information or emotions associated with interactions in online\nspaces. Multimodal Large Language Models (MLLMs) have emerged as a promising\nsolution to these challenges, yet they struggle to accurately interpret human\nemotions and complex content such as misinformation. This paper introduces\nMM-Soc, a comprehensive benchmark designed to evaluate MLLMs' understanding of\nmultimodal social media content. MM-Soc compiles prominent multimodal datasets\nand incorporates a novel large-scale YouTube tagging dataset, targeting a range\nof tasks from misinformation detection, hate speech detection, and social\ncontext generation. Through our exhaustive evaluation on ten size-variants of\nfour open-source MLLMs, we have identified significant performance disparities,\nhighlighting the need for advancements in models' social understanding\ncapabilities. Our analysis reveals that, in a zero-shot setting, various types\nof MLLMs generally exhibit difficulties in handling social media tasks.\nHowever, MLLMs demonstrate performance improvements post fine-tuning,\nsuggesting potential pathways for improvement. Our code and data are available\nat https://github.com/claws-lab/MMSoc.git.\n","authors":["Yiqiao Jin","Minje Choi","Gaurav Verma","Jindong Wang","Srijan Kumar"],"pdf_url":"https://arxiv.org/pdf/2402.14154v2.pdf","comment":"In Proceedings of ACL 2024"},{"id":"http://arxiv.org/abs/2407.17349v1","updated":"2024-07-24T15:18:17Z","published":"2024-07-24T15:18:17Z","title":"Boosting Large Language Models with Socratic Method for Conversational\n  Mathematics Teaching","summary":"  With the introduction of large language models (LLMs), automatic math\nreasoning has seen tremendous success. However, current methods primarily focus\non providing solutions or using techniques like Chain-of-Thought to enhance\nproblem-solving accuracy. In this paper, we focus on improving the capability\nof mathematics teaching via a Socratic teaching-based LLM\n(\\texttt{SocraticLLM}), which guides learners toward profound thinking with\nclarity and self-discovery via conversation. We collect and release a\nhigh-quality mathematical teaching dataset, named \\texttt{SocraticMATH}, which\nprovides Socratic-style conversations of problems with extra knowledge. Also,\nwe propose a knowledge-enhanced LLM as a strong baseline to generate reliable\nresponses with review, guidance/heuristic, rectification, and summarization.\nExperimental results show the great advantages of \\texttt{SocraticLLM} by\ncomparing it with several strong generative models. The codes and datasets are\navailable on \\url{https://github.com/ECNU-ICALK/SocraticMath}.\n","authors":["Yuyang Ding","Hanglei Hu","Jie Zhou","Qin Chen","Bo Jiang","Liang He"],"pdf_url":"https://arxiv.org/pdf/2407.17349v1.pdf","comment":"Accepted By CIKM 2024"},{"id":"http://arxiv.org/abs/2407.17344v1","updated":"2024-07-24T15:13:12Z","published":"2024-07-24T15:13:12Z","title":"Label Alignment and Reassignment with Generalist Large Language Model\n  for Enhanced Cross-Domain Named Entity Recognition","summary":"  Named entity recognition on the in-domain supervised and few-shot settings\nhave been extensively discussed in the NLP community and made significant\nprogress. However, cross-domain NER, a more common task in practical scenarios,\nstill poses a challenge for most NER methods. Previous research efforts in that\narea primarily focus on knowledge transfer such as correlate label information\nfrom source to target domains but few works pay attention to the problem of\nlabel conflict. In this study, we introduce a label alignment and reassignment\napproach, namely LAR, to address this issue for enhanced cross-domain named\nentity recognition, which includes two core procedures: label alignment between\nsource and target domains and label reassignment for type inference. The\nprocess of label reassignment can significantly be enhanced by integrating with\nan advanced large-scale language model such as ChatGPT. We conduct an extensive\nrange of experiments on NER datasets involving both supervised and zero-shot\nscenarios. Empirical experimental results demonstrate the validation of our\nmethod with remarkable performance under the supervised and zero-shot\nout-of-domain settings compared to SOTA methods.\n","authors":["Ke Bao","Chonghuan Yang"],"pdf_url":"https://arxiv.org/pdf/2407.17344v1.pdf","comment":"9 pages, 4 figures"},{"id":"http://arxiv.org/abs/2407.16521v2","updated":"2024-07-24T15:12:09Z","published":"2024-07-23T14:34:38Z","title":"AMONGAGENTS: Evaluating Large Language Models in the Interactive\n  Text-Based Social Deduction Game","summary":"  Strategic social deduction games serve as valuable testbeds for evaluating\nthe understanding and inference skills of language models, offering crucial\ninsights into social science, artificial intelligence, and strategic gaming.\nThis paper focuses on creating proxies of human behavior in simulated\nenvironments, with Among Us utilized as a tool for studying simulated human\nbehavior. The study introduces a text-based game environment, named\nAmongAgents, that mirrors the dynamics of Among Us. Players act as crew members\naboard a spaceship, tasked with identifying impostors who are sabotaging the\nship and eliminating the crew. Within this environment, the behavior of\nsimulated language agents is analyzed. The experiments involve diverse game\nsequences featuring different configurations of Crewmates and Impostor\npersonality archetypes. Our work demonstrates that state-of-the-art large\nlanguage models (LLMs) can effectively grasp the game rules and make decisions\nbased on the current context. This work aims to promote further exploration of\nLLMs in goal-oriented games with incomplete information and complex action\nspaces, as these settings offer valuable opportunities to assess language model\nperformance in socially driven scenarios.\n","authors":["Yizhou Chi","Lingjun Mao","Zineng Tang"],"pdf_url":"https://arxiv.org/pdf/2407.16521v2.pdf","comment":"Wordplay @ ACL 2024"},{"id":"http://arxiv.org/abs/2305.12517v5","updated":"2024-07-24T15:10:41Z","published":"2023-05-21T17:14:31Z","title":"Description-Based Text Similarity","summary":"  Identifying texts with a given semantics is central for many information\nseeking scenarios. Similarity search over vector embeddings appear to be\ncentral to this ability, yet the similarity reflected in current text\nembeddings is corpus-driven, and is inconsistent and sub-optimal for many use\ncases. What, then, is a good notion of similarity for effective retrieval of\ntext?\n  We identify the need to search for texts based on abstract descriptions of\ntheir content, and the corresponding notion of \\emph{description based\nsimilarity}. We demonstrate the inadequacy of current text embeddings and\npropose an alternative model that significantly improves when used in standard\nnearest neighbor search. The model is trained using positive and negative pairs\nsourced through prompting a LLM, demonstrating how data from LLMs can be used\nfor creating new capabilities not immediately possible using the original\nmodel.\n","authors":["Shauli Ravfogel","Valentina Pyatkin","Amir DN Cohen","Avshalom Manevich","Yoav Goldberg"],"pdf_url":"https://arxiv.org/pdf/2305.12517v5.pdf","comment":"Accepted in COLM 2024"},{"id":"http://arxiv.org/abs/2407.14829v2","updated":"2024-07-24T15:09:29Z","published":"2024-07-20T10:13:54Z","title":"Overview of AI-Debater 2023: The Challenges of Argument Generation Tasks","summary":"  In this paper we present the results of the AI-Debater 2023 Challenge held by\nthe Chinese Conference on Affect Computing (CCAC 2023), and introduce the\nrelated datasets. We organize two tracks to handle the argumentative generation\ntasks in different scenarios, namely, Counter-Argument Generation (Track 1) and\nClaim-based Argument Generation (Track 2). Each track is equipped with its\ndistinct dataset and baseline model respectively. In total, 32 competing teams\nregister for the challenge, from which we received 11 successful submissions.\nIn this paper, we will present the results of the challenge and a summary of\nthe systems, highlighting commonalities and innovations among participating\nsystems. Datasets and baseline models of the AI-Debater 2023 Challenge have\nbeen already released and can be accessed through the official website of the\nchallenge.\n","authors":["Jiayu Lin","Guanrong Chen","Bojun Jin","Chenyang Li","Shutong Jia","Wancong Lin","Yang Sun","Yuhang He","Caihua Yang","Jianzhu Bao","Jipeng Wu","Wen Su","Jinglu Chen","Xinyi Li","Tianyu Chen","Mingjie Han","Shuaiwen Du","Zijian Wang","Jiyin Li","Fuzhong Suo","Hao Wang","Nuanchen Lin","Xuanjing Huang","Changjian Jiang","RuiFeng Xu","Long Zhang","Jiuxin Cao","Ting Jin","Zhongyu Wei"],"pdf_url":"https://arxiv.org/pdf/2407.14829v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10969v3","updated":"2024-07-24T14:57:48Z","published":"2024-07-15T17:59:29Z","title":"Q-Sparse: All Large Language Models can be Fully Sparsely-Activated","summary":"  We introduce, Q-Sparse, a simple yet effective approach to training\nsparsely-activated large language models (LLMs). Q-Sparse enables full sparsity\nof activations in LLMs which can bring significant efficiency gains in\ninference. This is achieved by applying top-K sparsification to the activations\nand the straight-through-estimator to the training. We also introduce Block\nQ-Sparse for batch training and inference. The key results from this work are,\n(1) Q-Sparse can achieve results comparable to those of baseline LLMs while\nbeing much more efficient at inference time; (2) We present an\ninference-optimal scaling law for sparsely-activated LLMs; (3) Q-Sparse is\neffective in different settings, including training-from-scratch,\ncontinue-training of off-the-shelf LLMs, and finetuning; (4) Q-Sparse works for\nboth full-precision and 1-bit LLMs (e.g., BitNet b1.58). Particularly, the\nsynergy of BitNet b1.58 and Q-Sparse (can be equipped with MoE) provides the\ncornerstone and a clear path to revolutionize the efficiency, including cost\nand energy consumption, of future LLMs.\n","authors":["Hongyu Wang","Shuming Ma","Ruiping Wang","Furu Wei"],"pdf_url":"https://arxiv.org/pdf/2407.10969v3.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2407.17291v1","updated":"2024-07-24T14:02:20Z","published":"2024-07-24T14:02:20Z","title":"How Good (Or Bad) Are LLMs at Detecting Misleading Visualizations?","summary":"  In this study, we address the growing issue of misleading charts, a prevalent\nproblem that undermines the integrity of information dissemination. Misleading\ncharts can distort the viewer's perception of data, leading to\nmisinterpretations and decisions based on false information. The development of\neffective automatic detection methods for misleading charts is an urgent field\nof research. The recent advancement of multimodal Large Language Models (LLMs)\nhas introduced a promising direction for addressing this challenge. We explored\nthe capabilities of these models in analyzing complex charts and assessing the\nimpact of different prompting strategies on the models' analyses. We utilized a\ndataset of misleading charts collected from the internet by prior research and\ncrafted nine distinct prompts, ranging from simple to complex, to test the\nability of four different multimodal LLMs in detecting over 21 different chart\nissues. Through three experiments--from initial exploration to detailed\nanalysis--we progressively gained insights into how to effectively prompt LLMs\nto identify misleading charts and developed strategies to address the\nscalability challenges encountered as we expanded our detection range from the\ninitial five issues to 21 issues in the final experiment. Our findings reveal\nthat multimodal LLMs possess a strong capability for chart comprehension and\ncritical thinking in data interpretation. There is significant potential in\nemploying multimodal LLMs to counter misleading information by supporting\ncritical thinking and enhancing visualization literacy. This study demonstrates\nthe applicability of LLMs in addressing the pressing concern of misleading\ncharts.\n","authors":["Leo Yu-Ho Lo","Huamin Qu"],"pdf_url":"https://arxiv.org/pdf/2407.17291v1.pdf","comment":"To be presented at IEEE VIS 2024"},{"id":"http://arxiv.org/abs/2311.14324v2","updated":"2024-07-24T13:34:14Z","published":"2023-11-24T07:53:48Z","title":"Large Language Models as Topological Structure Enhancers for\n  Text-Attributed Graphs","summary":"  The latest advancements in large language models (LLMs) have revolutionized\nthe field of natural language processing (NLP). Inspired by the success of LLMs\nin NLP tasks, some recent work has begun investigating the potential of\napplying LLMs in graph learning tasks. However, most of the existing work\nfocuses on utilizing LLMs as powerful node feature augmenters, leaving\nemploying LLMs to enhance graph topological structures an understudied problem.\nIn this work, we explore how to leverage the information retrieval and text\ngeneration capabilities of LLMs to refine/enhance the topological structure of\ntext-attributed graphs (TAGs) under the node classification setting. First, we\npropose using LLMs to help remove unreliable edges and add reliable ones in the\nTAG. Specifically, we first let the LLM output the semantic similarity between\nnode attributes through delicate prompt designs, and then perform edge deletion\nand edge addition based on the similarity. Second, we propose using\npseudo-labels generated by the LLM to improve graph topology, that is, we\nintroduce the pseudo-label propagation as a regularization to guide the graph\nneural network (GNN) in learning proper edge weights. Finally, we incorporate\nthe two aforementioned LLM-based methods for graph topological refinement into\nthe process of GNN training, and perform extensive experiments on four\nreal-world datasets. The experimental results demonstrate the effectiveness of\nLLM-based graph topology refinement (achieving a 0.15%--2.47% performance gain\non public benchmarks).\n","authors":["Shengyin Sun","Yuxiang Ren","Chen Ma","Xuecang Zhang"],"pdf_url":"https://arxiv.org/pdf/2311.14324v2.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2401.17505v4","updated":"2024-07-24T12:57:56Z","published":"2024-01-30T23:46:35Z","title":"Arrows of Time for Large Language Models","summary":"  We study the probabilistic modeling performed by Autoregressive Large\nLanguage Models (LLMs) through the angle of time directionality, addressing a\nquestion first raised in (Shannon, 1951). For large enough models, we\nempirically find a time asymmetry in their ability to learn natural language: a\ndifference in the average log-perplexity when trying to predict the next token\nversus when trying to predict the previous one. This difference is at the same\ntime subtle and very consistent across various modalities (language, model\nsize, training time, ...). Theoretically, this is surprising: from an\ninformation-theoretic point of view, there should be no such difference. We\nprovide a theoretical framework to explain how such an asymmetry can appear\nfrom sparsity and computational complexity considerations, and outline a number\nof perspectives opened by our results.\n","authors":["Vassilis Papadopoulos","Jérémie Wenger","Clément Hongler"],"pdf_url":"https://arxiv.org/pdf/2401.17505v4.pdf","comment":"Corrected typos in Table 2. Added links. 12 figures, 20 pages"},{"id":"http://arxiv.org/abs/2407.09835v2","updated":"2024-07-24T12:43:33Z","published":"2024-07-13T10:08:55Z","title":"Investigating Low-Rank Training in Transformer Language Models:\n  Efficiency and Scaling Analysis","summary":"  State-of-the-art LLMs often rely on scale with high computational costs,\nwhich has sparked a research agenda to reduce parameter counts and costs\nwithout significantly impacting performance. Our study focuses on\nTransformer-based LLMs, specifically applying low-rank parametrization to the\ncomputationally intensive feedforward networks (FFNs), which are less studied\nthan attention blocks. In contrast to previous works, (i) we explore low-rank\nparametrization at scale, up to 1.3B parameters; (ii) within Transformer\nlanguage models rather than convolutional architectures; and (iii) starting\nfrom training from scratch. Experiments on the large RefinedWeb dataset show\nthat low-rank parametrization is both efficient (e.g., 2.6$\\times$ FFN speed-up\nwith 32\\% parameters) and effective during training. Interestingly, these\nstructured FFNs exhibit steeper scaling curves than the original models.\nMotivated by this finding, we develop the wide and structured networks\nsurpassing the current medium-sized and large-sized Transformer in perplexity\nand throughput performance. Our code is available at\nhttps://github.com/CLAIRE-Labo/StructuredFFN/tree/main.\n","authors":["Xiuying Wei","Skander Moalla","Razvan Pascanu","Caglar Gulcehre"],"pdf_url":"https://arxiv.org/pdf/2407.09835v2.pdf","comment":"Accepted by ICML 2024 Next Generation of Sequence Modeling\n  Architectures Workshop. Short version of arXiv:2406.16450"},{"id":"http://arxiv.org/abs/2407.17230v1","updated":"2024-07-24T12:34:23Z","published":"2024-07-24T12:34:23Z","title":"Improving ICD coding using Chapter based Named Entities and Attentional\n  Models","summary":"  Recent advancements in natural language processing (NLP) have led to\nautomation in various domains. However, clinical NLP often relies on benchmark\ndatasets that may not reflect real-world scenarios accurately. Automatic ICD\ncoding, a vital NLP task, typically uses outdated and imbalanced datasets like\nMIMIC-III, with existing methods yielding micro-averaged F1 scores between 0.4\nand 0.7 due to many false positives. Our research introduces an enhanced\napproach to ICD coding that improves F1 scores by using chapter-based named\nentities and attentional models. This method categorizes discharge summaries\ninto ICD-9 Chapters and develops attentional models with chapter-specific data,\neliminating the need to consider external data for code identification. For\ncategorization, we use Chapter-IV to de-bias and influence key entities and\nweights without neural networks, creating accurate thresholds and providing\ninterpretability for human validation. Post-validation, we develop attentional\nmodels for three frequent and three non-frequent codes from Chapter-IV using\nBidirectional-Gated Recurrent Units (GRUs) with Attention and Transformer with\nMulti-head Attention architectures. The average Micro-F1 scores of 0.79 and\n0.81 from these models demonstrate significant performance improvements in ICD\ncoding.\n","authors":["Abhijith R. Beeravolu","Mirjam Jonkman","Sami Azam","Friso De Boer"],"pdf_url":"https://arxiv.org/pdf/2407.17230v1.pdf","comment":"10 Pages"},{"id":"http://arxiv.org/abs/2407.17227v1","updated":"2024-07-24T12:28:03Z","published":"2024-07-24T12:28:03Z","title":"LEAN-GitHub: Compiling GitHub LEAN repositories for a versatile LEAN\n  prover","summary":"  Recently, large language models have presented promising results in aiding\nformal mathematical reasoning. However, their performance is restricted due to\nthe scarcity of formal theorem-proving data, which requires additional effort\nto be extracted from raw formal language corpora. Meanwhile, a significant\namount of human-written formal language corpora remains underutilized. To\naddress this issue, we propose LEAN-GitHub, a dataset consisting of large-scale\nformal data extracted from almost all Lean 4 repositories on GitHub. After\nfine-tuning InternLM-math-plus on this dataset, our model achieved accuracies\nof 48.8% with a single pass and 54.5% with 64 passes on the Lean 4 miniF2F\ntest, surpassing state-of-the-art method at 52%. And it also achieves\nstate-of-the-art on two other Lean 4 benchmarks (ProofNet and Putnam) targeting\ndifferent fields/levels of math. These results demonstrate that our proposed\ndataset is beneficial for formal reasoning on a wide range of math topics. We\nopen-source our model at https://GitHub. com/InternLM/InternLM-Math and our\ndata at https://huggingface.co/ datasets/InternLM/Lean-GitHub\n","authors":["Zijian Wu","Jiayu Wang","Dahua Lin","Kai Chen"],"pdf_url":"https://arxiv.org/pdf/2407.17227v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08582v2","updated":"2024-07-24T12:25:17Z","published":"2023-10-12T17:59:50Z","title":"Tree-Planner: Efficient Close-loop Task Planning with Large Language\n  Models","summary":"  This paper studies close-loop task planning, which refers to the process of\ngenerating a sequence of skills (a plan) to accomplish a specific goal while\nadapting the plan based on real-time observations. Recently, prompting Large\nLanguage Models (LLMs) to generate actions iteratively has become a prevalent\nparadigm due to its superior performance and user-friendliness. However, this\nparadigm is plagued by two inefficiencies: high token consumption and redundant\nerror correction, both of which hinder its scalability for large-scale testing\nand applications. To address these issues, we propose Tree-Planner, which\nreframes task planning with LLMs into three distinct phases: plan sampling,\naction tree construction, and grounded deciding. Tree-Planner starts by using\nan LLM to sample a set of potential plans before execution, followed by the\naggregation of them to form an action tree. Finally, the LLM performs a\ntop-down decision-making process on the tree, taking into account real-time\nenvironmental information. Experiments show that Tree-Planner achieves\nstate-of-the-art performance while maintaining high efficiency. By decomposing\nLLM queries into a single plan-sampling call and multiple grounded-deciding\ncalls, a considerable part of the prompt are less likely to be repeatedly\nconsumed. As a result, token consumption is reduced by 92.2% compared to the\npreviously best-performing model. Additionally, by enabling backtracking on the\naction tree as needed, the correction process becomes more flexible, leading to\na 40.5% decrease in error corrections.\n","authors":["Mengkang Hu","Yao Mu","Xinmiao Yu","Mingyu Ding","Shiguang Wu","Wenqi Shao","Qiguang Chen","Bin Wang","Yu Qiao","Ping Luo"],"pdf_url":"https://arxiv.org/pdf/2310.08582v2.pdf","comment":"Published in ICLR 2024"},{"id":"http://arxiv.org/abs/2407.01976v2","updated":"2024-07-24T11:45:48Z","published":"2024-07-02T06:29:05Z","title":"A Bounding Box is Worth One Token: Interleaving Layout and Text in a\n  Large Language Model for Document Understanding","summary":"  Recently, many studies have demonstrated that exclusively incorporating\nOCR-derived text and spatial layouts with large language models (LLMs) can be\nhighly effective for document understanding tasks. However, existing methods\nthat integrate spatial layouts with text have limitations, such as producing\noverly long text sequences or failing to fully leverage the autoregressive\ntraits of LLMs. In this work, we introduce Interleaving Layout and Text in a\nLarge Language Model (LayTextLLM)} for document understanding. In particular,\nLayTextLLM projects each bounding box to a single embedding and interleaves it\nwith text, efficiently avoiding long sequence issues while leveraging\nautoregressive traits of LLMs. LayTextLLM not only streamlines the interaction\nof layout and textual data but also shows enhanced performance in Key\nInformation Extraction (KIE) and Visual Question Answering (VQA). Comprehensive\nbenchmark evaluations reveal significant improvements, with a 27.2% increase on\nKIE tasks and 12.0% on VQA tasks compared to previous state-of-the-art document\nunderstanding MLLMs, as well as a 15.1% improvement over other SOTA OCR-based\nLLMs on KIE tasks.\n","authors":["Jinghui Lu","Haiyang Yu","Yanjie Wang","Yongjie Ye","Jingqun Tang","Ziwei Yang","Binghong Wu","Qi Liu","Hao Feng","Han Wang","Hao Liu","Can Huang"],"pdf_url":"https://arxiv.org/pdf/2407.01976v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17174v1","updated":"2024-07-24T11:24:25Z","published":"2024-07-24T11:24:25Z","title":"NarrationDep: Narratives on Social Media For Automatic Depression\n  Detection","summary":"  Social media posts provide valuable insight into the narrative of users and\ntheir intentions, including providing an opportunity to automatically model\nwhether a social media user is depressed or not. The challenge lies in\nfaithfully modelling user narratives from their online social media posts,\nwhich could potentially be useful in several different applications. We have\ndeveloped a novel and effective model called \\texttt{NarrationDep}, which\nfocuses on detecting narratives associated with depression. By analyzing a\nuser's tweets, \\texttt{NarrationDep} accurately identifies crucial narratives.\n\\texttt{NarrationDep} is a deep learning framework that jointly models\nindividual user tweet representations and clusters of users' tweets. As a\nresult, \\texttt{NarrationDep} is characterized by a novel two-layer deep\nlearning model: the first layer models using social media text posts, and the\nsecond layer learns semantic representations of tweets associated with a\ncluster. To faithfully model these cluster representations, the second layer\nincorporates a novel component that hierarchically learns from users' posts.\nThe results demonstrate that our framework outperforms other comparative models\nincluding recently developed models on a variety of datasets.\n","authors":["Hamad Zogan","Imran Razzak","Shoaib Jameel","Guandong Xu"],"pdf_url":"https://arxiv.org/pdf/2407.17174v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17172v1","updated":"2024-07-24T11:22:57Z","published":"2024-07-24T11:22:57Z","title":"Speech Editing -- a Summary","summary":"  With the rise of video production and social media, speech editing has become\ncrucial for creators to address issues like mispronunciations, missing words,\nor stuttering in audio recordings. This paper explores text-based speech\nediting methods that modify audio via text transcripts without manual waveform\nediting. These approaches ensure edited audio is indistinguishable from the\noriginal by altering the mel-spectrogram. Recent advancements, such as\ncontext-aware prosody correction and advanced attention mechanisms, have\nimproved speech editing quality. This paper reviews state-of-the-art methods,\ncompares key metrics, and examines widely used datasets. The aim is to\nhighlight ongoing issues and inspire further research and innovation in speech\nediting.\n","authors":["Tobias Kässmann","Yining Liu","Danni Liu"],"pdf_url":"https://arxiv.org/pdf/2407.17172v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17167v1","updated":"2024-07-24T11:14:06Z","published":"2024-07-24T11:14:06Z","title":"Zero-Shot vs. Few-Shot Multi-Speaker TTS Using Pre-trained Czech\n  SpeechT5 Model","summary":"  In this paper, we experimented with the SpeechT5 model pre-trained on\nlarge-scale datasets. We pre-trained the foundation model from scratch and\nfine-tuned it on a large-scale robust multi-speaker text-to-speech (TTS) task.\nWe tested the model capabilities in a zero- and few-shot scenario. Based on two\nlistening tests, we evaluated the synthetic audio quality and the similarity of\nhow synthetic voices resemble real voices. Our results showed that the SpeechT5\nmodel can generate a synthetic voice for any speaker using only one minute of\nthe target speaker's data. We successfully demonstrated the high quality and\nsimilarity of our synthetic voices on publicly known Czech politicians and\ncelebrities.\n","authors":["Jan Lehečka","Zdeněk Hanzlíček","Jindřich Matoušek","Daniel Tihelka"],"pdf_url":"https://arxiv.org/pdf/2407.17167v1.pdf","comment":"Accepted to TSD2024"},{"id":"http://arxiv.org/abs/2407.17160v1","updated":"2024-07-24T11:03:47Z","published":"2024-07-24T11:03:47Z","title":"A Comparative Analysis of Bilingual and Trilingual Wav2Vec Models for\n  Automatic Speech Recognition in Multilingual Oral History Archives","summary":"  In this paper, we are comparing monolingual Wav2Vec 2.0 models with various\nmultilingual models to see whether we could improve speech recognition\nperformance on a unique oral history archive containing a lot of mixed-language\nsentences. Our main goal is to push forward research on this unique dataset,\nwhich is an extremely valuable part of our cultural heritage. Our results\nsuggest that monolingual speech recognition models are, in most cases, superior\nto multilingual models, even when processing the oral history archive full of\nmixed-language sentences from non-native speakers. We also performed the same\nexperiments on the public CommonVoice dataset to verify our results. We are\ncontributing to the research community by releasing our pre-trained models to\nthe public.\n","authors":["Jan Lehečka","Josef V. Psutka","Luboš Šmídl","Pavel Ircing","Josef Psutka"],"pdf_url":"https://arxiv.org/pdf/2407.17160v1.pdf","comment":"Accepted to INTERSPEECH2024"},{"id":"http://arxiv.org/abs/2407.17150v1","updated":"2024-07-24T10:49:19Z","published":"2024-07-24T10:49:19Z","title":"SimCT: A Simple Consistency Test Protocol in LLMs Development Lifecycle","summary":"  In this work, we report our efforts to advance the standard operation\nprocedure of developing Large Language Models (LLMs) or LLMs-based systems or\nservices in industry. We introduce the concept of Large Language Model\nDevelopment Lifecycle (LDLC) and then highlight the importance of consistency\ntest in ensuring the delivery quality. The principled solution of consistency\ntest, however, is usually overlooked by industrial practitioners and not urgent\nin academia, and current practical solutions are insufficiently rigours and\nlabor-intensive. We thus propose a simple yet effective consistency test\nprotocol, named SimCT. SimCT is mainly to proactively check the consistency\nacross different development stages of \"bare metal\" LLMs or associated services\nwithout accessing the model artifacts, in an attempt to expedite the delivery\nby reducing the back-and-forth alignment communications among multiple teams\ninvolved in different development stages.\n  Specifically, SimCT encompasses response-wise and model-wise tests. We\nimplement the protocol with LightGBM and Student's t-test for two components\nrespectively, and perform extensive experiments to substantiate the\neffectiveness of SimCT and the involved components.\n","authors":["Fufangchen Zhao","Guoqiang Jin","Rui Zhao","Jiangheng Huang","Fei Tan"],"pdf_url":"https://arxiv.org/pdf/2407.17150v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17126v1","updated":"2024-07-24T09:57:51Z","published":"2024-07-24T09:57:51Z","title":"SDoH-GPT: Using Large Language Models to Extract Social Determinants of\n  Health (SDoH)","summary":"  Extracting social determinants of health (SDoH) from unstructured medical\nnotes depends heavily on labor-intensive annotations, which are typically\ntask-specific, hampering reusability and limiting sharing. In this study we\nintroduced SDoH-GPT, a simple and effective few-shot Large Language Model (LLM)\nmethod leveraging contrastive examples and concise instructions to extract SDoH\nwithout relying on extensive medical annotations or costly human intervention.\nIt achieved tenfold and twentyfold reductions in time and cost respectively,\nand superior consistency with human annotators measured by Cohen's kappa of up\nto 0.92. The innovative combination of SDoH-GPT and XGBoost leverages the\nstrengths of both, ensuring high accuracy and computational efficiency while\nconsistently maintaining 0.90+ AUROC scores. Testing across three distinct\ndatasets has confirmed its robustness and accuracy. This study highlights the\npotential of leveraging LLMs to revolutionize medical note classification,\ndemonstrating their capability to achieve highly accurate classifications with\nsignificantly reduced time and cost.\n","authors":["Bernardo Consoli","Xizhi Wu","Song Wang","Xinyu Zhao","Yanshan Wang","Justin Rousseau","Tom Hartvigsen","Li Shen","Huanmei Wu","Yifan Peng","Qi Long","Tianlong Chen","Ying Ding"],"pdf_url":"https://arxiv.org/pdf/2407.17126v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17125v1","updated":"2024-07-24T09:48:48Z","published":"2024-07-24T09:48:48Z","title":"Behavioral Testing: Can Large Language Models Implicitly Resolve\n  Ambiguous Entities?","summary":"  One of the major aspects contributing to the striking performance of large\nlanguage models (LLMs) is the vast amount of factual knowledge accumulated\nduring pre-training. Yet, many LLMs suffer from self-inconsistency, which\nraises doubts about their trustworthiness and reliability. In this paper, we\nfocus on entity type ambiguity and analyze current state-of-the-art LLMs for\ntheir proficiency and consistency in applying their factual knowledge when\nprompted for entities under ambiguity. To do so, we propose an evaluation\nprotocol that disentangles knowing from applying knowledge, and test\nstate-of-the-art LLMs on 49 entities. Our experiments reveal that LLMs perform\npoorly with ambiguous prompts, achieving only 80% accuracy. Our results further\ndemonstrate systematic discrepancies in LLM behavior and their failure to\nconsistently apply information, indicating that the models can exhibit\nknowledge without being able to utilize it, significant biases for preferred\nreadings, as well as self inconsistencies. Our study highlights the importance\nof handling entity ambiguity in future for more trustworthy LLMs\n","authors":["Anastasiia Sedova","Robert Litschko","Diego Frassinelli","Benjamin Roth","Barbara Plank"],"pdf_url":"https://arxiv.org/pdf/2407.17125v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.15569v2","updated":"2024-07-24T08:56:11Z","published":"2024-01-28T05:12:09Z","title":"Efficient Tuning and Inference for Large Language Models on Textual\n  Graphs","summary":"  Rich textual and topological information of textual graphs need to be modeled\nin real-world applications such as webpages, e-commerce, and academic articles.\nPractitioners have been long following the path of adopting a shallow text\nencoder and a subsequent graph neural network (GNN) to solve this problem. In\nlight of recent advancements in large language models (LLMs), it is apparent\nthat integrating LLMs for enhanced textual encoding can substantially improve\nthe performance of textual graphs. Nevertheless, the efficiency of these\nmethods poses a significant challenge. In this paper, we propose ENGINE, a\nparameter- and memory-efficient fine-tuning method for textual graphs with an\nLLM encoder. The key insight is to combine the LLMs and GNNs through a tunable\nside structure, which significantly reduces the training complexity without\nimpairing the joint model's capacity. Extensive experiments on textual graphs\ndemonstrate our method's effectiveness by achieving the best model performance,\nmeanwhile having the lowest training cost compared to previous methods.\nMoreover, we introduce two variants with caching and dynamic early exit to\nfurther enhance training and inference speed. Specifically, caching accelerates\nENGINE's training by 12x, and dynamic early exit achieves up to 5x faster\ninference with a negligible performance drop (at maximum 1.17% relevant drop\nacross 7 datasets). Our codes are available at:\nhttps://github.com/ZhuYun97/ENGINE\n","authors":["Yun Zhu","Yaoke Wang","Haizhou Shi","Siliang Tang"],"pdf_url":"https://arxiv.org/pdf/2401.15569v2.pdf","comment":"Accepted by IJCAI2024"},{"id":"http://arxiv.org/abs/2309.03227v2","updated":"2024-07-24T08:31:21Z","published":"2023-09-04T02:30:19Z","title":"Learning a Patent-Informed Biomedical Knowledge Graph Reveals\n  Technological Potential of Drug Repositioning Candidates","summary":"  Drug repositioning-a promising strategy for discovering new therapeutic uses\nfor existing drugs-has been increasingly explored in the computational science\nliterature using biomedical databases. However, the technological potential of\ndrug repositioning candidates has often been overlooked. This study presents a\nnovel protocol to comprehensively analyse various sources such as\npharmaceutical patents and biomedical databases, and identify drug\nrepositioning candidates with both technological potential and scientific\nevidence. To this end, first, we constructed a scientific biomedical knowledge\ngraph (s-BKG) comprising relationships between drugs, diseases, and genes\nderived from biomedical databases. Our protocol involves identifying drugs that\nexhibit limited association with the target disease but are closely located in\nthe s-BKG, as potential drug candidates. We constructed a patent-informed\nbiomedical knowledge graph (p-BKG) by adding pharmaceutical patent information.\nFinally, we developed a graph embedding protocol to ascertain the structure of\nthe p-BKG, thereby calculating the relevance scores of those candidates with\ntarget disease-related patents to evaluate their technological potential. Our\ncase study on Alzheimer's disease demonstrates its efficacy and feasibility,\nwhile the quantitative outcomes and systematic methods are expected to bridge\nthe gap between computational discoveries and successful market applications in\ndrug repositioning research.\n","authors":["Yongseung Jegal","Jaewoong Choi","Jiho Lee","Ki-Su Park","Seyoung Lee","Janghyeok Yoon"],"pdf_url":"https://arxiv.org/pdf/2309.03227v2.pdf","comment":"We are sorry to withdraw this paper. We found some critical errors in\n  the introduction and results sections. Specifically, we found that the first\n  author have wrongly inserted citations on background works and he made\n  mistakes in the graph embedding methods and relevant results are wrongly\n  calculated. In this regard, we tried to revise this paper and withdraw the\n  current version. Thank you"},{"id":"http://arxiv.org/abs/2308.14484v2","updated":"2024-07-24T08:24:21Z","published":"2023-08-28T10:51:11Z","title":"Multimodal Detection of Bots on X (Twitter) using Transformers","summary":"  Although not all bots are malicious, the vast majority of them are\nresponsible for spreading misinformation and manipulating the public opinion\nabout several issues, i.e., elections and many more. Therefore, the early\ndetection of bots is crucial. Although there have been proposed methods for\ndetecting bots in social media, there are still substantial limitations. For\ninstance, existing research initiatives still extract a large number of\nfeatures and train traditional machine learning algorithms or use GloVe\nembeddings and train LSTMs. However, feature extraction is a tedious procedure\ndemanding domain expertise. Also, language models based on transformers have\nbeen proved to be better than LSTMs. Other approaches create large graphs and\ntrain graph neural networks requiring in this way many hours for training and\naccess to computational resources. To tackle these limitations, this is the\nfirst study employing only the user description field and images of three\nchannels denoting the type and content of tweets posted by the users. Firstly,\nwe create digital DNA sequences, transform them to 3d images, and apply\npretrained models of the vision domain, including EfficientNet, AlexNet, VGG16,\netc. Next, we propose a multimodal approach, where we use TwHIN-BERT for\ngetting the textual representation of the user description field and employ\nVGG16 for acquiring the visual representation for the image modality. We\npropose three different fusion methods, namely concatenation, gated multimodal\nunit, and crossmodal attention, for fusing the different modalities and compare\ntheir performances. Finally, we present a qualitative analysis of the behavior\nof our best performing model. Extensive experiments conducted on the Cresci'17\nand TwiBot-20 datasets demonstrate valuable advantages of our introduced\napproaches over state-of-the-art ones.\n","authors":["Loukas Ilias","Ioannis Michail Kazelidis","Dimitris Askounis"],"pdf_url":"https://arxiv.org/pdf/2308.14484v2.pdf","comment":"IEEE Transactions on Information Forensics and Security (Accepted)"},{"id":"http://arxiv.org/abs/2407.17081v1","updated":"2024-07-24T08:17:37Z","published":"2024-07-24T08:17:37Z","title":"A Survey Forest Diagram : Gain a Divergent Insight View on a Specific\n  Research Topic","summary":"  With the exponential growth in the number of papers and the trend of AI\nresearch, the use of Generative AI for information retrieval and\nquestion-answering has become popular for conducting research surveys. However,\nnovice researchers unfamiliar with a particular field may not significantly\nimprove their efficiency in interacting with Generative AI because they have\nnot developed divergent thinking in that field. This study aims to develop an\nin-depth Survey Forest Diagram that guides novice researchers in divergent\nthinking about the research topic by indicating the citation clues among\nmultiple papers, to help expand the survey perspective for novice researchers.\n","authors":["Jinghong Li","Wen Gu","Koichi Ota","Shinobu Hasegawa"],"pdf_url":"https://arxiv.org/pdf/2407.17081v1.pdf","comment":"This paper will submit to IEEE SMC 2024"},{"id":"http://arxiv.org/abs/2407.11100v3","updated":"2024-07-24T08:10:29Z","published":"2024-07-15T07:20:02Z","title":"Building Intelligence Identification System via Large Language Model\n  Watermarking: A Survey and Beyond","summary":"  Large Language Models (LLMs) are increasingly integrated into diverse\nindustries, posing substantial security risks due to unauthorized replication\nand misuse. To mitigate these concerns, robust identification mechanisms are\nwidely acknowledged as an effective strategy. Identification systems for LLMs\nnow rely heavily on watermarking technology to manage and protect intellectual\nproperty and ensure data security. However, previous studies have primarily\nconcentrated on the basic principles of algorithms and lacked a comprehensive\nanalysis of watermarking theory and practice from the perspective of\nintelligent identification. To bridge this gap, firstly, we explore how a\nrobust identity recognition system can be effectively implemented and managed\nwithin LLMs by various participants using watermarking technology. Secondly, we\npropose a mathematical framework based on mutual information theory, which\nsystematizes the identification process to achieve more precise and customized\nwatermarking. Additionally, we present a comprehensive evaluation of\nperformance metrics for LLM watermarking, reflecting participant preferences\nand advancing discussions on its identification applications. Lastly, we\noutline the existing challenges in current watermarking technologies and\ntheoretical frameworks, and provide directional guidance to address these\nchallenges. Our systematic classification and detailed exposition aim to\nenhance the comparison and evaluation of various methods, fostering further\nresearch and development toward a transparent, secure, and equitable LLM\necosystem.\n","authors":["Xuhong Wang","Haoyu Jiang","Yi Yu","Jingru Yu","Yilun Lin","Ping Yi","Yingchun Wang","Yu Qiao","Li Li","Fei-Yue Wang"],"pdf_url":"https://arxiv.org/pdf/2407.11100v3.pdf","comment":"59 pages, 7 figures"},{"id":"http://arxiv.org/abs/2407.17075v1","updated":"2024-07-24T08:04:00Z","published":"2024-07-24T08:04:00Z","title":"SAFETY-J: Evaluating Safety with Critique","summary":"  The deployment of Large Language Models (LLMs) in content generation raises\nsignificant safety concerns, particularly regarding the transparency and\ninterpretability of content evaluations. Current methods, primarily focused on\nbinary safety classifications, lack mechanisms for detailed critique, limiting\ntheir utility for model improvement and user trust. To address these\nlimitations, we introduce SAFETY-J, a bilingual generative safety evaluator for\nEnglish and Chinese with critique-based judgment. SAFETY-J utilizes a robust\ntraining dataset that includes diverse dialogues and augmented query-response\npairs to assess safety across various scenarios comprehensively. We establish\nan automated meta-evaluation benchmark that objectively assesses the quality of\ncritiques with minimal human intervention, facilitating scalable and continuous\nimprovement. Additionally, SAFETY-J employs an iterative preference learning\ntechnique to dynamically refine safety assessments based on meta-evaluations\nand critiques. Our evaluations demonstrate that SAFETY-J provides more nuanced\nand accurate safety evaluations, thereby enhancing both critique quality and\npredictive reliability in complex content scenarios. To facilitate further\nresearch and application, we will open-source SAFETY-J's training protocols,\ndatasets, and code.\n","authors":["Yixiu Liu","Yuxiang Zheng","Shijie Xia","Yuan Guo","Jiajun Li","Yi Tu","Chaoling Song","Pengfei Liu"],"pdf_url":"https://arxiv.org/pdf/2407.17075v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17060v1","updated":"2024-07-24T07:37:12Z","published":"2024-07-24T07:37:12Z","title":"High Efficiency Image Compression for Large Visual-Language Models","summary":"  In recent years, large visual language models (LVLMs) have shown impressive\nperformance and promising generalization capability in multi-modal tasks, thus\nreplacing humans as receivers of visual information in various application\nscenarios. In this paper, we pioneer to propose a variable bitrate image\ncompression framework consisting of a pre-editing module and an end-to-end\ncodec to achieve promising rate-accuracy performance for different LVLMs. In\nparticular, instead of optimizing an adaptive pre-editing network towards a\nparticular task or several representative tasks, we propose a new optimization\nstrategy tailored for LVLMs, which is designed based on the representation and\ndiscrimination capability with token-level distortion and rank. The pre-editing\nmodule and the variable bitrate end-to-end image codec are jointly trained by\nthe losses based on semantic tokens of the large model, which introduce\nenhanced generalization capability for various data and tasks. {Experimental\nresults demonstrate that the proposed framework could efficiently achieve much\nbetter rate-accuracy performance compared to the state-of-the-art coding\nstandard, Versatile Video Coding.} Meanwhile, experiments with multi-modal\ntasks have revealed the robustness and generalization capability of the\nproposed framework.\n","authors":["Binzhe Li","Shurun Wang","Shiqi Wang","Yan Ye"],"pdf_url":"https://arxiv.org/pdf/2407.17060v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16190v2","updated":"2024-07-24T07:32:25Z","published":"2024-07-23T05:32:00Z","title":"Artificial Agency and Large Language Models","summary":"  The arrival of Large Language Models (LLMs) has stirred up philosophical\ndebates about the possibility of realizing agency in an artificial manner. In\nthis work we contribute to the debate by presenting a theoretical model that\ncan be used as a threshold conception for artificial agents. The model defines\nagents as systems whose actions and goals are always influenced by a dynamic\nframework of factors that consists of the agent's accessible history, its\nadaptive repertoire and its external environment. This framework, in turn, is\ninfluenced by the actions that the agent takes and the goals that it forms. We\nshow with the help of the model that state-of-the-art LLMs are not agents yet,\nbut that there are elements to them that suggest a way forward. The paper\nargues that a combination of the agent architecture presented in Park et al.\n(2023) together with the use of modules like the Coscientist in Boiko et al.\n(2023) could potentially be a way to realize agency in an artificial manner. We\nend the paper by reflecting on the obstacles one might face in building such an\nartificial agent and by presenting possible directions for future research.\n","authors":["Maud van Lier","Gorka Muñoz-Gil"],"pdf_url":"https://arxiv.org/pdf/2407.16190v2.pdf","comment":"Accepted for publication in journal Intellectica, special issue\n  \"Philosophies of AI: thinking and writing with LLMs\" (Intellectica, issue 81)"},{"id":"http://arxiv.org/abs/2402.13463v4","updated":"2024-07-24T06:50:18Z","published":"2024-02-21T01:39:56Z","title":"RefuteBench: Evaluating Refuting Instruction-Following for Large\n  Language Models","summary":"  The application scope of large language models (LLMs) is increasingly\nexpanding. In practical use, users might provide feedback based on the model's\noutput, hoping for a responsive model that can complete responses according to\ntheir feedback. Whether the model can appropriately respond to users' refuting\nfeedback and consistently follow through with execution has not been thoroughly\nanalyzed. In light of this, this paper proposes a comprehensive benchmark,\nRefuteBench, covering tasks such as question answering, machine translation,\nand email writing. The evaluation aims to assess whether models can positively\naccept feedback in form of refuting instructions and whether they can\nconsistently adhere to user demands throughout the conversation. We conduct\nevaluations on numerous LLMs and find that LLMs are stubborn, i.e. exhibit\ninclination to their internal knowledge, often failing to comply with user\nfeedback. Additionally, as the length of the conversation increases, models\ngradually forget the user's stated feedback and roll back to their own\nresponses. We further propose a recall-and-repeat prompts as a simple and\neffective way to enhance the model's responsiveness to feedback.\n","authors":["Jianhao Yan","Yun Luo","Yue Zhang"],"pdf_url":"https://arxiv.org/pdf/2402.13463v4.pdf","comment":"ACL 2024 final version"},{"id":"http://arxiv.org/abs/2406.03855v3","updated":"2024-07-24T06:39:15Z","published":"2024-06-06T08:41:46Z","title":"Performance of large language models in numerical vs. semantic medical\n  knowledge: Benchmarking on evidence-based Q&As","summary":"  Clinical problem-solving requires processing of semantic medical knowledge\nsuch as illness scripts and numerical medical knowledge of diagnostic tests for\nevidence-based decision-making. As large language models (LLMs) show promising\nresults in many aspects of language-based clinical practice, their ability to\ngenerate non-language evidence-based answers to clinical questions is\ninherently limited by tokenization. Therefore, we evaluated LLMs' performance\non two question types: numeric (correlating findings) and semantic\n(differentiating entities) while examining differences within and between LLMs\nin medical aspects and comparing their performance to humans. To generate\nstraightforward multi-choice questions and answers (QAs) based on\nevidence-based medicine (EBM), we used a comprehensive medical knowledge graph\n(encompassed data from more than 50,00 peer-reviewed articles) and created the\n\"EBMQA\". EBMQA contains 105,000 QAs labeled with medical and non-medical topics\nand classified into numerical or semantic questions. We benchmarked this\ndataset using more than 24,500 QAs on two state-of-the-art LLMs: Chat-GPT4 and\nClaude3-Opus. We evaluated the LLMs accuracy on semantic and numerical question\ntypes and according to sub-labeled topics. For validation, six medical experts\nwere tested on 100 numerical EBMQA questions. We found that both LLMs excelled\nmore in semantic than numerical QAs, with Claude3 surpassing GPT4 in numerical\nQAs. However, both LLMs showed inter and intra gaps in different medical\naspects and remained inferior to humans. Thus, their medical advice should be\naddressed carefully.\n","authors":["Eden Avnat","Michal Levy","Daniel Herstain","Elia Yanko","Daniel Ben Joya","Michal Tzuchman Katz","Dafna Eshel","Sahar Laros","Yael Dagan","Shahar Barami","Joseph Mermelstein","Shahar Ovadia","Noam Shomron","Varda Shalev","Raja-Elie E. Abdulnour"],"pdf_url":"https://arxiv.org/pdf/2406.03855v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17023v1","updated":"2024-07-24T06:06:07Z","published":"2024-07-24T06:06:07Z","title":"From Internal Conflict to Contextual Adaptation of Language Models","summary":"  Knowledge-intensive language understanding tasks require Language Models\n(LMs) to integrate relevant context, mitigating their inherent weaknesses, such\nas incomplete or outdated knowledge. Nevertheless, studies indicate that LMs\noften ignore the provided context as it can conflict with the pre-existing LM's\nmemory learned during pre-training. Moreover, conflicting knowledge can already\nbe present in the LM's parameters, termed intra-memory conflict. Existing works\nhave studied the two types of knowledge conflicts only in isolation. We\nconjecture that the (degree of) intra-memory conflicts can in turn affect LM's\nhandling of context-memory conflicts. To study this, we introduce the DYNAMICQA\ndataset, which includes facts with a temporal dynamic nature where a fact can\nchange with a varying time frequency and disputable dynamic facts, which can\nchange depending on the viewpoint. DYNAMICQA is the first to include real-world\nknowledge conflicts and provide context to study the link between the different\ntypes of knowledge conflicts. With the proposed dataset, we assess the use of\nuncertainty for measuring the intra-memory conflict and introduce a novel\nCoherent Persuasion (CP) score to evaluate the context's ability to sway LM's\nsemantic output. Our extensive experiments reveal that static facts, which are\nunlikely to change, are more easily updated with additional context, relative\nto temporal and disputable facts.\n","authors":["Sara Vera Marjanović","Haeun Yu","Pepa Atanasova","Maria Maistro","Christina Lioma","Isabelle Augenstein"],"pdf_url":"https://arxiv.org/pdf/2407.17023v1.pdf","comment":"22 pages, 15 figures"},{"id":"http://arxiv.org/abs/2407.17022v1","updated":"2024-07-24T06:02:57Z","published":"2024-07-24T06:02:57Z","title":"Can Language Models Evaluate Human Written Text? Case Study on Korean\n  Student Writing for Education","summary":"  Large language model (LLM)-based evaluation pipelines have demonstrated their\ncapability to robustly evaluate machine-generated text. Extending this\nmethodology to assess human-written text could significantly benefit\neducational settings by providing direct feedback to enhance writing skills,\nalthough this application is not straightforward. In this paper, we investigate\nwhether LLMs can effectively assess human-written text for educational\npurposes. We collected 100 texts from 32 Korean students across 15 types of\nwriting and employed GPT-4-Turbo to evaluate them using grammaticality,\nfluency, coherence, consistency, and relevance as criteria. Our analyses\nindicate that LLM evaluators can reliably assess grammaticality and fluency, as\nwell as more objective types of writing, though they struggle with other\ncriteria and types of writing. We publicly release our dataset and feedback.\n","authors":["Seungyoon Kim","Seungone Kim"],"pdf_url":"https://arxiv.org/pdf/2407.17022v1.pdf","comment":"Work In Progress"},{"id":"http://arxiv.org/abs/2403.09092v2","updated":"2024-07-24T05:57:01Z","published":"2024-03-14T04:32:13Z","title":"MCFEND: A Multi-source Benchmark Dataset for Chinese Fake News Detection","summary":"  The prevalence of fake news across various online sources has had a\nsignificant influence on the public. Existing Chinese fake news detection\ndatasets are limited to news sourced solely from Weibo. However, fake news\noriginating from multiple sources exhibits diversity in various aspects,\nincluding its content and social context. Methods trained on purely one single\nnews source can hardly be applicable to real-world scenarios. Our pilot\nexperiment demonstrates that the F1 score of the state-of-the-art method that\nlearns from a large Chinese fake news detection dataset, Weibo-21, drops\nsignificantly from 0.943 to 0.470 when the test data is changed to multi-source\nnews data, failing to identify more than one-third of the multi-source fake\nnews. To address this limitation, we constructed the first multi-source\nbenchmark dataset for Chinese fake news detection, termed MCFEND, which is\ncomposed of news we collected from diverse sources such as social platforms,\nmessaging apps, and traditional online news outlets. Notably, such news has\nbeen fact-checked by 14 authoritative fact-checking agencies worldwide. In\naddition, various existing Chinese fake news detection methods are thoroughly\nevaluated on our proposed dataset in cross-source, multi-source, and unseen\nsource ways. MCFEND, as a benchmark dataset, aims to advance Chinese fake news\ndetection approaches in real-world scenarios.\n","authors":["Yupeng Li","Haorui He","Jin Bai","Dacheng Wen"],"pdf_url":"https://arxiv.org/pdf/2403.09092v2.pdf","comment":"Accepted by the ACM Web Conference 2024 (WWW 2024) oral, dataset\n  available: https://github.com/TrustworthyComp"},{"id":"http://arxiv.org/abs/2407.17011v1","updated":"2024-07-24T05:26:52Z","published":"2024-07-24T05:26:52Z","title":"Unveiling In-Context Learning: A Coordinate System to Understand Its\n  Working Mechanism","summary":"  Large language models (LLMs) exhibit remarkable in-context learning (ICL)\ncapabilities. However, the underlying working mechanism of ICL remains poorly\nunderstood. Recent research presents two conflicting views on ICL: One\nattributes it to LLMs' inherent ability of task recognition, deeming label\ncorrectness and shot numbers of demonstrations as not crucial; the other\nemphasizes the impact of similar examples in the demonstrations, stressing the\nneed for label correctness and more shots. In this work, we provide a\nTwo-Dimensional Coordinate System that unifies both views into a systematic\nframework. The framework explains the behavior of ICL through two orthogonal\nvariables: whether LLMs can recognize the task and whether similar examples are\npresented in the demonstrations. We propose the peak inverse rank metric to\ndetect the task recognition ability of LLMs and study LLMs' reactions to\ndifferent definitions of similarity. Based on these, we conduct extensive\nexperiments to elucidate how ICL functions across each quadrant on multiple\nrepresentative classification tasks. Finally, we extend our analyses to\ngeneration tasks, showing that our coordinate system can also be used to\ninterpret ICL for generation tasks effectively.\n","authors":["Anhao Zhao","Fanghua Ye","Jinlan Fu","Xiaoyu Shen"],"pdf_url":"https://arxiv.org/pdf/2407.17011v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.11233v2","updated":"2024-07-24T05:22:48Z","published":"2024-06-17T06:00:24Z","title":"Probing the Decision Boundaries of In-context Learning in Large Language\n  Models","summary":"  In-context learning is a key paradigm in large language models (LLMs) that\nenables them to generalize to new tasks and domains by simply prompting these\nmodels with a few exemplars without explicit parameter updates. Many attempts\nhave been made to understand in-context learning in LLMs as a function of model\nscale, pretraining data, and other factors. In this work, we propose a new\nmechanism to probe and understand in-context learning from the lens of decision\nboundaries for in-context binary classification. Decision boundaries are\nstraightforward to visualize and provide important information about the\nqualitative behavior of the inductive biases of standard classifiers. To our\nsurprise, we find that the decision boundaries learned by current LLMs in\nsimple binary classification tasks are often irregular and non-smooth,\nregardless of linear separability in the underlying task. This paper\ninvestigates the factors influencing these decision boundaries and explores\nmethods to enhance their generalizability. We assess various approaches,\nincluding training-free and fine-tuning methods for LLMs, the impact of model\narchitecture, and the effectiveness of active prompting techniques for\nsmoothing decision boundaries in a data-efficient manner. Our findings provide\na deeper understanding of in-context learning dynamics and offer practical\nimprovements for enhancing robustness and generalizability of in-context\nlearning.\n","authors":["Siyan Zhao","Tung Nguyen","Aditya Grover"],"pdf_url":"https://arxiv.org/pdf/2406.11233v2.pdf","comment":"18 pages, code at https://github.com/siyan-zhao/ICL_decision_boundary"},{"id":"http://arxiv.org/abs/2407.13787v2","updated":"2024-07-24T04:57:55Z","published":"2024-07-12T11:31:00Z","title":"The Honorific Effect: Exploring the Impact of Japanese Linguistic\n  Formalities on AI-Generated Physics Explanations","summary":"  This study investigates the influence of Japanese honorifics on the responses\nof large language models (LLMs) when explaining the law of conservation of\nmomentum. We analyzed the outputs of six state-of-the-art AI models, including\nvariations of ChatGPT, Coral, and Gemini, using 14 different honorific forms.\nOur findings reveal that honorifics significantly affect the quality,\nconsistency, and formality of AI-generated responses, demonstrating LLMs'\nability to interpret and adapt to social context cues embedded in language.\nNotable variations were observed across different models, with some emphasizing\nhistorical context and derivations, while others focused on intuitive\nexplanations. The study highlights the potential for using honorifics to adjust\nthe depth and complexity of AI-generated explanations in educational contexts.\nFurthermore, the responsiveness of AI models to cultural linguistic elements\nunderscores the importance of considering cultural factors in AI development\nfor educational applications. These results open new avenues for research in\nAI-assisted education and cultural adaptation in AI systems, with significant\nimplications for personalizing learning experiences and developing culturally\nsensitive AI tools for global education.\n","authors":["Keisuke Sato"],"pdf_url":"https://arxiv.org/pdf/2407.13787v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.17432v4","updated":"2024-07-24T04:44:11Z","published":"2023-12-29T01:56:17Z","title":"Video Understanding with Large Language Models: A Survey","summary":"  With the burgeoning growth of online video platforms and the escalating\nvolume of video content, the demand for proficient video understanding tools\nhas intensified markedly. Given the remarkable capabilities of large language\nmodels (LLMs) in language and multimodal tasks, this survey provides a detailed\noverview of recent advancements in video understanding that harness the power\nof LLMs (Vid-LLMs). The emergent capabilities of Vid-LLMs are surprisingly\nadvanced, particularly their ability for open-ended multi-granularity (general,\ntemporal, and spatiotemporal) reasoning combined with commonsense knowledge,\nsuggesting a promising path for future video understanding. We examine the\nunique characteristics and capabilities of Vid-LLMs, categorizing the\napproaches into three main types: Video Analyzer x LLM, Video Embedder x LLM,\nand (Analyzer + Embedder) x LLM. Furthermore, we identify five sub-types based\non the functions of LLMs in Vid-LLMs: LLM as Summarizer, LLM as Manager, LLM as\nText Decoder, LLM as Regressor, and LLM as Hidden Layer. Furthermore, this\nsurvey presents a comprehensive study of the tasks, datasets, benchmarks, and\nevaluation methodologies for Vid-LLMs. Additionally, it explores the expansive\napplications of Vid-LLMs across various domains, highlighting their remarkable\nscalability and versatility in real-world video understanding challenges.\nFinally, it summarizes the limitations of existing Vid-LLMs and outlines\ndirections for future research. For more information, readers are recommended\nto visit the repository at\nhttps://github.com/yunlong10/Awesome-LLMs-for-Video-Understanding.\n","authors":["Yunlong Tang","Jing Bi","Siting Xu","Luchuan Song","Susan Liang","Teng Wang","Daoan Zhang","Jie An","Jingyang Lin","Rongyi Zhu","Ali Vosoughi","Chao Huang","Zeliang Zhang","Pinxin Liu","Mingqian Feng","Feng Zheng","Jianguo Zhang","Ping Luo","Jiebo Luo","Chenliang Xu"],"pdf_url":"https://arxiv.org/pdf/2312.17432v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16997v1","updated":"2024-07-24T04:39:24Z","published":"2024-07-24T04:39:24Z","title":"Revisiting Who's Harry Potter: Towards Targeted Unlearning from a Causal\n  Intervention Perspective","summary":"  This paper investigates Who's Harry Potter (WHP), a pioneering yet\ninsufficiently understood method for LLM unlearning. We explore it in two\nsteps. First, we introduce a new task of LLM targeted unlearning, where given\nan unlearning target (e.g., a person) and some unlearning documents, we aim to\nunlearn only the information about the target, rather than everything in the\nunlearning documents. We further argue that a successful unlearning should\nsatisfy criteria such as not outputting gibberish, not fabricating facts about\nthe unlearning target, and not releasing factual information under jailbreak\nattacks. Second, we construct a causal intervention framework for targeted\nunlearning, where the knowledge of the unlearning target is modeled as a\nconfounder between LLM input and output, and the unlearning process as a\ndeconfounding process. This framework justifies and extends WHP, deriving a\nsimple unlearning algorithm that includes WHP as a special case. Experiments on\nexisting and new datasets show that our approach, without explicitly optimizing\nfor the aforementioned criteria, achieves competitive performance in all of\nthem. Our code is available at\nhttps://github.com/UCSB-NLP-Chang/causal_unlearn.git.\n","authors":["Yujian Liu","Yang Zhang","Tommi Jaakkola","Shiyu Chang"],"pdf_url":"https://arxiv.org/pdf/2407.16997v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16994v1","updated":"2024-07-24T04:27:55Z","published":"2024-07-24T04:27:55Z","title":"A Voter-Based Stochastic Rejection-Method Framework for Asymptotically\n  Safe Language Model Outputs","summary":"  This paper proposes a new method for preventing unsafe or otherwise low\nquality large language model (LLM) outputs, by leveraging the stochasticity of\nLLMs. We propose a system whereby LLM checkers vote on the acceptability of a\ngenerated output, regenerating it if a threshold of disapproval is reached,\nuntil sufficient checkers approve. We further propose estimators for cost and\nfailure rate, and based on those estimators and experimental data tailored to\nthe application, we propose an algorithm that achieves a desired failure rate\nat the least possible cost. We demonstrate that, under these models, failure\nrate decreases exponentially as a function of cost when voter count and\nthreshold are chosen according to the algorithm, and that the models reasonably\nestimate the actual performance of such a system in action, even with limited\ndata.\n","authors":["Jake R. Watts","Joel Sokol"],"pdf_url":"https://arxiv.org/pdf/2407.16994v1.pdf","comment":"7 pages, 2 figures"},{"id":"http://arxiv.org/abs/2407.12994v2","updated":"2024-07-24T03:53:41Z","published":"2024-07-17T20:23:19Z","title":"A Survey of Prompt Engineering Methods in Large Language Models for\n  Different NLP Tasks","summary":"  Large language models (LLMs) have shown remarkable performance on many\ndifferent Natural Language Processing (NLP) tasks. Prompt engineering plays a\nkey role in adding more to the already existing abilities of LLMs to achieve\nsignificant performance gains on various NLP tasks. Prompt engineering requires\ncomposing natural language instructions called prompts to elicit knowledge from\nLLMs in a structured way. Unlike previous state-of-the-art (SoTA) models,\nprompt engineering does not require extensive parameter re-training or\nfine-tuning based on the given NLP task and thus solely operates on the\nembedded knowledge of LLMs. Additionally, LLM enthusiasts can intelligently\nextract LLMs' knowledge through a basic natural language conversational\nexchange or prompt engineering, allowing more and more people even without deep\nmathematical machine learning background to experiment with LLMs. With prompt\nengineering gaining popularity in the last two years, researchers have come up\nwith numerous engineering techniques around designing prompts to improve\naccuracy of information extraction from the LLMs. In this paper, we summarize\ndifferent prompting techniques and club them together based on different NLP\ntasks that they have been used for. We further granularly highlight the\nperformance of these prompting strategies on various datasets belonging to that\nNLP task, talk about the corresponding LLMs used, present a taxonomy diagram\nand discuss the possible SoTA for specific datasets. In total, we read and\npresent a survey of 44 research papers which talk about 39 different prompting\nmethods on 29 different NLP tasks of which most of them have been published in\nthe last two years.\n","authors":["Shubham Vatsal","Harsh Dubey"],"pdf_url":"https://arxiv.org/pdf/2407.12994v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16970v1","updated":"2024-07-24T03:32:05Z","published":"2024-07-24T03:32:05Z","title":"Towards Aligning Language Models with Textual Feedback","summary":"  We present ALT (ALignment with Textual feedback), an approach that aligns\nlanguage models with user preferences expressed in text. We argue that text\noffers greater expressiveness, enabling users to provide richer feedback than\nsimple comparative preferences and this richer feedback can lead to more\nefficient and effective alignment. ALT aligns the model by conditioning its\ngeneration on the textual feedback. Our method relies solely on language\nmodeling techniques and requires minimal hyper-parameter tuning, though it\nstill presents the main benefits of RL-based alignment algorithms and can\neffectively learn from textual feedback. We explore the efficacy and efficiency\nof textual feedback across different tasks such as toxicity reduction,\nsummarization, and dialog response generation. We find that ALT outperforms PPO\nfor the task of toxicity reduction while being able to match its performance on\nsummarization with only 20% of the samples. We also explore how ALT can be used\nwith feedback provided by an existing LLM where we explore an LLM providing\nconstrained and unconstrained textual feedback. We also outline future\ndirections to align models with natural language feedback.\n","authors":["Saüc Abadal Lloret","Shehzaad Dhuliawala","Keerthiram Murugesan","Mrinmaya Sachan"],"pdf_url":"https://arxiv.org/pdf/2407.16970v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16951v1","updated":"2024-07-24T02:37:42Z","published":"2024-07-24T02:37:42Z","title":"Towards Transfer Unlearning: Empirical Evidence of Cross-Domain Bias\n  Mitigation","summary":"  Large language models (LLMs) often inherit biases from vast amounts of\ntraining corpora. Traditional debiasing methods, while effective to some\nextent, do not completely eliminate memorized biases and toxicity in LLMs. In\nthis paper, we study an unlearning-based approach to debiasing in LLMs by\nperforming gradient ascent on hate speech against minority groups, i.e.,\nminimizing the likelihood of biased or toxic content. Specifically, we propose\na mask language modeling unlearning technique, which unlearns the harmful part\nof the text. This method enables LLMs to selectively forget and disassociate\nfrom biased and harmful content. Experimental results demonstrate the\neffectiveness of our approach in diminishing bias while maintaining the\nlanguage modeling abilities. Surprisingly, the results also unveil an\nunexpected potential for cross-domain transfer unlearning: debiasing in one\nbias form (e.g. gender) may contribute to mitigating others (e.g. race and\nreligion).\n","authors":["Huimin Lu","Masaru Isonuma","Junichiro Mori","Ichiro Sakata"],"pdf_url":"https://arxiv.org/pdf/2407.16951v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.02363v2","updated":"2024-07-24T02:36:07Z","published":"2024-05-03T05:09:54Z","title":"LLM as Dataset Analyst: Subpopulation Structure Discovery with Large\n  Language Model","summary":"  The distribution of subpopulations is an important property hidden within a\ndataset. Uncovering and analyzing the subpopulation distribution within\ndatasets provides a comprehensive understanding of the datasets, standing as a\npowerful tool beneficial to various downstream tasks, including Dataset\nSubpopulation Organization, Subpopulation Shift, and Slice Discovery. Despite\nits importance, there has been no work that systematically explores the\nsubpopulation distribution of datasets to our knowledge. To address the\nlimitation and solve all the mentioned tasks in a unified way, we introduce a\nnovel concept of subpopulation structures to represent, analyze, and utilize\nsubpopulation distributions within datasets. To characterize the structures in\nan interpretable manner, we propose the Subpopulation Structure Discovery with\nLarge Language Models (SSD-LLM) framework, which employs world knowledge and\ninstruction-following capabilities of Large Language Models (LLMs) to\nlinguistically analyze informative image captions and summarize the structures.\nFurthermore, we propose complete workflows to address downstream tasks, named\nTask-specific Tuning, showcasing the application of the discovered structure to\na spectrum of subpopulation-related tasks, including dataset subpopulation\norganization, subpopulation shift, and slice discovery. Furthermore, we propose\ncomplete workflows to address downstream tasks, named Task-specific Tuning,\nshowcasing the application of the discovered structure to a spectrum of\nsubpopulation-related tasks, including dataset subpopulation organization,\nsubpopulation shift, and slice discovery.\n","authors":["Yulin Luo","Ruichuan An","Bocheng Zou","Yiming Tang","Jiaming Liu","Shanghang Zhang"],"pdf_url":"https://arxiv.org/pdf/2405.02363v2.pdf","comment":"ECCV24 Camera Ready"},{"id":"http://arxiv.org/abs/2407.16939v1","updated":"2024-07-24T02:17:10Z","published":"2024-07-24T02:17:10Z","title":"Early screening of potential breakthrough technologies with enhanced\n  interpretability: A patent-specific hierarchical attention network model","summary":"  Despite the usefulness of machine learning approaches for the early screening\nof potential breakthrough technologies, their practicality is often hindered by\nopaque models. To address this, we propose an interpretable machine learning\napproach to predicting future citation counts from patent texts using a\npatent-specific hierarchical attention network (PatentHAN) model. Central to\nthis approach are (1) a patent-specific pre-trained language model, capturing\nthe meanings of technical words in patent claims, (2) a hierarchical network\nstructure, enabling detailed analysis at the claim level, and (3) a claim-wise\nself-attention mechanism, revealing pivotal claims during the screening\nprocess. A case study of 35,376 pharmaceutical patents demonstrates the\neffectiveness of our approach in early screening of potential breakthrough\ntechnologies while ensuring interpretability. Furthermore, we conduct\nadditional analyses using different language models and claim types to examine\nthe robustness of the approach. It is expected that the proposed approach will\nenhance expert-machine collaboration in identifying breakthrough technologies,\nproviding new insight derived from text mining into technological value.\n","authors":["Jaewoong Choi","Janghyeok Yoon","Changyong Lee"],"pdf_url":"https://arxiv.org/pdf/2407.16939v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.14850v2","updated":"2024-07-24T02:11:47Z","published":"2024-02-20T01:59:11Z","title":"CHATATC: Large Language Model-Driven Conversational Agents for\n  Supporting Strategic Air Traffic Flow Management","summary":"  Generative artificial intelligence (AI) and large language models (LLMs) have\ngained rapid popularity through publicly available tools such as ChatGPT. The\nadoption of LLMs for personal and professional use is fueled by the natural\ninteractions between human users and computer applications such as ChatGPT,\nalong with powerful summarization and text generation capabilities. Given the\nwidespread use of such generative AI tools, in this work we investigate how\nthese tools can be deployed in a non-safety critical, strategic traffic flow\nmanagement setting. Specifically, we train an LLM, CHATATC, based on a large\nhistorical data set of Ground Delay Program (GDP) issuances, spanning 2000-2023\nand consisting of over 80,000 GDP implementations, revisions, and\ncancellations. We test the query and response capabilities of CHATATC,\ndocumenting successes (e.g., providing correct GDP rates, durations, and\nreason) and shortcomings (e.g,. superlative questions). We also detail the\ndesign of a graphical user interface for future users to interact and\ncollaborate with the CHATATC conversational agent.\n","authors":["Sinan Abdulhak","Wayne Hubbard","Karthik Gopalakrishnan","Max Z. Li"],"pdf_url":"https://arxiv.org/pdf/2402.14850v2.pdf","comment":"8 pages, 5 figures; minor revisions to address reviewer feedback for\n  final submission to the 11th International Conference on Research in Air\n  Transportation (ICRAT)"},{"id":"http://arxiv.org/abs/2407.03718v2","updated":"2024-07-24T02:03:47Z","published":"2024-07-04T08:08:12Z","title":"Multi-Convformer: Extending Conformer with Multiple Convolution Kernels","summary":"  Convolutions have become essential in state-of-the-art end-to-end Automatic\nSpeech Recognition~(ASR) systems due to their efficient modelling of local\ncontext. Notably, its use in Conformers has led to superior performance\ncompared to vanilla Transformer-based ASR systems. While components other than\nthe convolution module in the Conformer have been reexamined, altering the\nconvolution module itself has been far less explored. Towards this, we\nintroduce Multi-Convformer that uses multiple convolution kernels within the\nconvolution module of the Conformer in conjunction with gating. This helps in\nimproved modeling of local dependencies at varying granularities. Our model\nrivals existing Conformer variants such as CgMLP and E-Branchformer in\nperformance, while being more parameter efficient. We empirically compare our\napproach with Conformer and its variants across four different datasets and\nthree different modelling paradigms and show up to 8% relative word error\nrate~(WER) improvements.\n","authors":["Darshan Prabhu","Yifan Peng","Preethi Jyothi","Shinji Watanabe"],"pdf_url":"https://arxiv.org/pdf/2407.03718v2.pdf","comment":"Accepted to INTERSPEECH 2024"},{"id":"http://arxiv.org/abs/2407.16931v1","updated":"2024-07-24T01:46:55Z","published":"2024-07-24T01:46:55Z","title":"ScholarChemQA: Unveiling the Power of Language Models in Chemical\n  Research Question Answering","summary":"  Question Answering (QA) effectively evaluates language models' reasoning and\nknowledge depth. While QA datasets are plentiful in areas like general domain\nand biomedicine, academic chemistry is less explored. Chemical QA plays a\ncrucial role in both education and research by effectively translating complex\nchemical information into readily understandable format. Addressing this gap,\nwe introduce ScholarChemQA, a large-scale QA dataset constructed from chemical\npapers. This dataset reflects typical real-world challenges, including an\nimbalanced data distribution and a substantial amount of unlabeled data that\ncan be potentially useful. Correspondingly, we introduce a QAMatch model,\nspecifically designed to effectively answer chemical questions by fully\nleveraging our collected data. We first address the issue of imbalanced label\ndistribution by re-weighting the instance-wise loss based on the inverse\nfrequency of each class, ensuring minority classes are not dominated by\nmajority ones during optimization. Next, we utilize the unlabeled data to\nenrich the learning process, generating a variety of augmentations based on a\nSoftMix operation and ensuring their predictions align with the same target,\ni.e., pseudo-labels. To ensure the quality of the pseudo-labels, we propose a\ncalibration procedure aimed at closely aligning the pseudo-label estimates of\nindividual samples with a desired ground truth distribution. Experiments show\nthat our QAMatch significantly outperforms the recent similar-scale baselines\nand Large Language Models (LLMs) not only on our ScholarChemQA dataset but also\non four benchmark datasets. We hope our benchmark and model can facilitate and\npromote more research on chemical QA.\n","authors":["Xiuying Chen","Tairan Wang","Taicheng Guo","Kehan Guo","Juexiao Zhou","Haoyang Li","Mingchen Zhuge","Jürgen Schmidhuber","Xin Gao","Xiangliang Zhang"],"pdf_url":"https://arxiv.org/pdf/2407.16931v1.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2402.16568v2","updated":"2024-07-24T01:44:05Z","published":"2024-02-26T13:47:09Z","title":"Two-stage Generative Question Answering on Temporal Knowledge Graph\n  Using Large Language Models","summary":"  Temporal knowledge graph question answering (TKGQA) poses a significant\nchallenge task, due to the temporal constraints hidden in questions and the\nanswers sought from dynamic structured knowledge. Although large language\nmodels (LLMs) have made considerable progress in their reasoning ability over\nstructured data, their application to the TKGQA task is a relatively unexplored\narea. This paper first proposes a novel generative temporal knowledge graph\nquestion answering framework, GenTKGQA, which guides LLMs to answer temporal\nquestions through two phases: Subgraph Retrieval and Answer Generation. First,\nwe exploit LLM's intrinsic knowledge to mine temporal constraints and\nstructural links in the questions without extra training, thus narrowing down\nthe subgraph search space in both temporal and structural dimensions. Next, we\ndesign virtual knowledge indicators to fuse the graph neural network signals of\nthe subgraph and the text representations of the LLM in a non-shallow way,\nwhich helps the open-source LLM deeply understand the temporal order and\nstructural dependencies among the retrieved facts through instruction tuning.\nExperimental results on two widely used datasets demonstrate the superiority of\nour model.\n","authors":["Yifu Gao","Linbo Qiao","Zhigang Kan","Zhihua Wen","Yongquan He","Dongsheng Li"],"pdf_url":"https://arxiv.org/pdf/2402.16568v2.pdf","comment":"Accepted by ACL(Findings) 2024"},{"id":"http://arxiv.org/abs/2311.17086v2","updated":"2024-07-24T01:41:01Z","published":"2023-11-28T02:31:52Z","title":"PEA-Diffusion: Parameter-Efficient Adapter with Knowledge Distillation\n  in non-English Text-to-Image Generation","summary":"  Text-to-image diffusion models are well-known for their ability to generate\nrealistic images based on textual prompts. However, the existing works have\npredominantly focused on English, lacking support for non-English text-to-image\nmodels. The most commonly used translation methods cannot solve the generation\nproblem related to language culture, while training from scratch on a specific\nlanguage dataset is prohibitively expensive. In this paper, we are inspired to\npropose a simple plug-and-play language transfer method based on knowledge\ndistillation. All we need to do is train a lightweight MLP-like\nparameter-efficient adapter (PEA) with only 6M parameters under teacher\nknowledge distillation along with a small parallel data corpus. We are\nsurprised to find that freezing the parameters of UNet can still achieve\nremarkable performance on the language-specific prompt evaluation set,\ndemonstrating that PEA can stimulate the potential generation ability of the\noriginal UNet. Additionally, it closely approaches the performance of the\nEnglish text-to-image model on a general prompt evaluation set. Furthermore,\nour adapter can be used as a plugin to achieve significant results in\ndownstream tasks in cross-lingual text-to-image generation. Code will be\navailable at: https://github.com/OPPO-Mente-Lab/PEA-Diffusion\n","authors":["Jian Ma","Chen Chen","Qingsong Xie","Haonan Lu"],"pdf_url":"https://arxiv.org/pdf/2311.17086v2.pdf","comment":"ECCV 2024"},{"id":"http://arxiv.org/abs/2407.16920v1","updated":"2024-07-24T01:04:34Z","published":"2024-07-24T01:04:34Z","title":"Train-Attention: Meta-Learning Where to Focus in Continual Knowledge\n  Learning","summary":"  Previous studies on continual knowledge learning (CKL) in large language\nmodels (LLMs) have predominantly focused on approaches such as regularization,\narchitectural modifications, and rehearsal techniques to mitigate catastrophic\nforgetting. However, these methods naively inherit the inefficiencies of\nstandard training procedures, indiscriminately applying uniform weight across\nall tokens, which can lead to unnecessary parameter updates and increased\nforgetting. To address these shortcomings, we propose a novel CKL approach\ntermed Train-Attention-Augmented Language Model (TAALM), which enhances\nlearning efficiency by dynamically predicting and applying weights to tokens\nbased on their usefulness. This method employs a meta-learning framework that\noptimizes token importance predictions, facilitating targeted knowledge updates\nand minimizing forgetting. Also, we observe that existing benchmarks do not\nclearly exhibit the trade-off between learning and retaining, therefore we\npropose a new benchmark, \\textsc{LAMA-ckl}, to address this issue. Through\nexperiments conducted on both newly introduced and established CKL benchmarks,\nTAALM proves the state-of-the-art performance upon the baselines, and also\nshows synergistic compatibility when integrated with previous CKL approaches.\n","authors":["Yeongbin Seo","Dongha Lee","Jinyoung Yeo"],"pdf_url":"https://arxiv.org/pdf/2407.16920v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.10861v2","updated":"2024-07-24T00:10:04Z","published":"2024-05-17T15:48:30Z","title":"Tailoring Vaccine Messaging with Common-Ground Opinions","summary":"  One way to personalize chatbot interactions is by establishing common ground\nwith the intended reader. A domain where establishing mutual understanding\ncould be particularly impactful is vaccine concerns and misinformation. Vaccine\ninterventions are forms of messaging which aim to answer concerns expressed\nabout vaccination. Tailoring responses in this domain is difficult, since\nopinions often have seemingly little ideological overlap. We define the task of\ntailoring vaccine interventions to a Common-Ground Opinion (CGO). Tailoring\nresponses to a CGO involves meaningfully improving the answer by relating it to\nan opinion or belief the reader holds. In this paper we introduce TAILOR-CGO, a\ndataset for evaluating how well responses are tailored to provided CGOs. We\nbenchmark several major LLMs on this task; finding GPT-4-Turbo performs\nsignificantly better than others. We also build automatic evaluation metrics,\nincluding an efficient and accurate BERT model that outperforms finetuned LLMs,\ninvestigate how to successfully tailor vaccine messaging to CGOs, and provide\nactionable recommendations from this investigation.\n  Code and model weights: https://github.com/rickardstureborg/tailor-cgo\nDataset: https://huggingface.co/datasets/DukeNLP/tailor-cgo\n","authors":["Rickard Stureborg","Sanxing Chen","Ruoyu Xie","Aayushi Patel","Christopher Li","Chloe Qinyu Zhu","Tingnan Hu","Jun Yang","Bhuwan Dhingra"],"pdf_url":"https://arxiv.org/pdf/2405.10861v2.pdf","comment":"NAACL Findings 2024"},{"id":"http://arxiv.org/abs/2407.16607v2","updated":"2024-07-24T23:34:21Z","published":"2024-07-23T16:13:22Z","title":"Data Mixture Inference: What do BPE Tokenizers Reveal about their\n  Training Data?","summary":"  The pretraining data of today's strongest language models is opaque; in\nparticular, little is known about the proportions of various domains or\nlanguages represented. In this work, we tackle a task which we call data\nmixture inference, which aims to uncover the distributional make-up of training\ndata. We introduce a novel attack based on a previously overlooked source of\ninformation -- byte-pair encoding (BPE) tokenizers, used by the vast majority\nof modern language models. Our key insight is that the ordered list of merge\nrules learned by a BPE tokenizer naturally reveals information about the token\nfrequencies in its training data: the first merge is the most common byte pair,\nthe second is the most common pair after merging the first token, and so on.\nGiven a tokenizer's merge list along with data samples for each category of\ninterest, we formulate a linear program that solves for the proportion of each\ncategory in the tokenizer's training set. Importantly, to the extent to which\ntokenizer training data is representative of the pretraining data, we\nindirectly learn about pretraining data. In controlled experiments, we show\nthat our attack recovers mixture ratios with high precision for tokenizers\ntrained on known mixtures of natural languages, programming languages, and data\nsources. We then apply our approach to off-the-shelf tokenizers released with\nrecent LMs. We confirm much publicly disclosed information about these models,\nand also make several new inferences: GPT-4o's tokenizer is much more\nmultilingual than its predecessors, training on 39% non-English data; Llama3\nextends GPT-3.5's tokenizer primarily for multilingual (48%) use; GPT-3.5's and\nClaude's tokenizers are trained on predominantly code (~60%). We hope our work\nsheds light on current design practices for pretraining data, and inspires\ncontinued research into data mixture inference for LMs.\n","authors":["Jonathan Hayase","Alisa Liu","Yejin Choi","Sewoong Oh","Noah A. Smith"],"pdf_url":"https://arxiv.org/pdf/2407.16607v2.pdf","comment":"19 pages, 5 figures"},{"id":"http://arxiv.org/abs/2402.12784v2","updated":"2024-07-24T23:00:50Z","published":"2024-02-20T07:49:30Z","title":"Understanding and Mitigating the Threat of Vec2Text to Dense Retrieval\n  Systems","summary":"  The emergence of Vec2Text -- a method for text embedding inversion -- has\nraised serious privacy concerns for dense retrieval systems which use text\nembeddings, such as those offered by OpenAI and Cohere. This threat comes from\nthe ability for a malicious attacker with access to embeddings to reconstruct\nthe original text. In this paper, we investigate various factors related to\nembedding models that may impact text recoverability via Vec2Text. We explore\nfactors such as distance metrics, pooling functions, bottleneck pre-training,\ntraining with noise addition, embedding quantization, and embedding dimensions,\nwhich were not considered in the original Vec2Text paper. Through a\ncomprehensive analysis of these factors, our objective is to gain a deeper\nunderstanding of the key elements that affect the trade-offs between the text\nrecoverability and retrieval effectiveness of dense retrieval systems, offering\ninsights for practitioners designing privacy-aware dense retrieval systems. We\nalso propose a simple embedding transformation fix that guarantees equal\nranking effectiveness while mitigating the recoverability risk. Overall, this\nstudy reveals that Vec2Text could pose a threat to current dense retrieval\nsystems, but there are some effective methods to patch such systems.\n","authors":["Shengyao Zhuang","Bevan Koopman","Xiaoran Chu","Guido Zuccon"],"pdf_url":"https://arxiv.org/pdf/2402.12784v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15612v2","updated":"2024-07-24T21:10:24Z","published":"2024-07-22T13:14:27Z","title":"Can GPT-4 learn to analyze moves in research article abstracts?","summary":"  One of the most powerful and enduring ideas in written discourse analysis is\nthat genres can be described in terms of the moves which structure a writer's\npurpose. Considerable research has sought to identify these distinct\ncommunicative acts, but analyses have been beset by problems of subjectivity,\nreliability and the time-consuming need for multiple coders to confirm\nanalyses. In this paper we employ the affordances of GPT-4 to automate the\nannotation process by using natural language prompts. Focusing on abstracts\nfrom articles in four applied linguistics journals, we devise prompts which\nenable the model to identify moves effectively. The annotated outputs of these\nprompts were evaluated by two assessors with a third addressing disagreements.\nThe results show that an 8-shot prompt was more effective than one using two,\nconfirming that the inclusion of examples illustrating areas of variability can\nenhance GPT-4's ability to recognize multiple moves in a single sentence and\nreduce bias related to textual position. We suggest that GPT-4 offers\nconsiderable potential in automating this annotation process, when human actors\nwith domain specific linguistic expertise inform the prompting process.\n","authors":["Danni Yu","Marina Bondi","Ken Hyland"],"pdf_url":"https://arxiv.org/pdf/2407.15612v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17638v1","updated":"2024-07-24T21:06:40Z","published":"2024-07-24T21:06:40Z","title":"Time Matters: Examine Temporal Effects on Biomedical Language Models","summary":"  Time roots in applying language models for biomedical applications: models\nare trained on historical data and will be deployed for new or future data,\nwhich may vary from training data. While increasing biomedical tasks have\nemployed state-of-the-art language models, there are very few studies have\nexamined temporal effects on biomedical models when data usually shifts across\ndevelopment and deployment. This study fills the gap by statistically probing\nrelations between language model performance and data shifts across three\nbiomedical tasks. We deploy diverse metrics to evaluate model performance,\ndistance methods to measure data drifts, and statistical methods to quantify\ntemporal effects on biomedical language models. Our study shows that time\nmatters for deploying biomedical language models, while the degree of\nperformance degradation varies by biomedical tasks and statistical\nquantification approaches. We believe this study can establish a solid\nbenchmark to evaluate and assess temporal effects on deploying biomedical\nlanguage models.\n","authors":["Weisi Liu","Zhe He","Xiaolei Huang"],"pdf_url":"https://arxiv.org/pdf/2407.17638v1.pdf","comment":"Accept to AMIA 2024 Annual Symposium"},{"id":"http://arxiv.org/abs/2407.17636v1","updated":"2024-07-24T21:02:53Z","published":"2024-07-24T21:02:53Z","title":"IgnitionInnovators at \"Discharge Me!\": Chain-of-Thought Instruction\n  Finetuning Large Language Models for Discharge Summaries","summary":"  This paper presents our proposed approach to the Discharge Me! shared task,\ncollocated with the 23th Workshop on Biomedical Natural Language Processing\n(BioNLP). In this work, we develop an LLM-based framework for solving the\nDischarge Summary Documentation (DSD) task, i.e., generating the two critical\ntarget sections `Brief Hospital Course' and `Discharge Instructions' in the\ndischarge summary. By streamlining the recent instruction-finetuning process on\nLLMs, we explore several prompting strategies for optimally adapting LLMs to\nspecific generation task of DSD. Experimental results show that providing a\nclear output structure, complimented by a set of comprehensive\nChain-of-Thoughts (CoT) questions, effectively improves the model's reasoning\ncapability, and thereby, enhancing the structural correctness and faithfulness\nof clinical information in the generated text. Source code is available at:\nhttps://github.com/antangrocket1312/Discharge_LLM\n","authors":["An Quang Tang","Xiuzhen Zhang","Minh Ngoc Dinh"],"pdf_url":"https://arxiv.org/pdf/2407.17636v1.pdf","comment":"Accepted by BioNLP2024 Workshop"},{"id":"http://arxiv.org/abs/2401.07575v2","updated":"2024-07-24T20:50:04Z","published":"2024-01-15T10:18:08Z","title":"Cascaded Cross-Modal Transformer for Audio-Textual Classification","summary":"  Speech classification tasks often require powerful language understanding\nmodels to grasp useful features, which becomes problematic when limited\ntraining data is available. To attain superior classification performance, we\npropose to harness the inherent value of multimodal representations by\ntranscribing speech using automatic speech recognition (ASR) models and\ntranslating the transcripts into different languages via pretrained translation\nmodels. We thus obtain an audio-textual (multimodal) representation for each\ndata sample. Subsequently, we combine language-specific Bidirectional Encoder\nRepresentations from Transformers (BERT) with Wav2Vec2.0 audio features via a\nnovel cascaded cross-modal transformer (CCMT). Our model is based on two\ncascaded transformer blocks. The first one combines text-specific features from\ndistinct languages, while the second one combines acoustic features with\nmultilingual features previously learned by the first transformer block. We\nemployed our system in the Requests Sub-Challenge of the ACM Multimedia 2023\nComputational Paralinguistics Challenge. CCMT was declared the winning\nsolution, obtaining an unweighted average recall (UAR) of 65.41% and 85.87% for\ncomplaint and request detection, respectively. Moreover, we applied our\nframework on the Speech Commands v2 and HarperValleyBank dialog data sets,\nsurpassing previous studies reporting results on these benchmarks. Our code is\nfreely available for download at: https://github.com/ristea/ccmt.\n","authors":["Nicolae-Catalin Ristea","Andrei Anghel","Radu Tudor Ionescu"],"pdf_url":"https://arxiv.org/pdf/2401.07575v2.pdf","comment":"Accepted for publication in Artificial Intelligence Review"},{"id":"http://arxiv.org/abs/2407.17629v1","updated":"2024-07-24T20:38:13Z","published":"2024-07-24T20:38:13Z","title":"Papilusion at DAGPap24: Paper or Illusion? Detecting AI-generated\n  Scientific Papers","summary":"  This paper presents Papilusion, an AI-generated scientific text detector\ndeveloped within the DAGPap24 shared task on detecting automatically generated\nscientific papers. We propose an ensemble-based approach and conduct ablation\nstudies to analyze the effect of the detector configurations on the\nperformance. Papilusion is ranked 6th on the leaderboard, and we improve our\nperformance after the competition ended, achieving 99.46 (+9.63) of the\nF1-score on the official test set.\n","authors":["Nikita Andreev","Alexander Shirnin","Vladislav Mikhailov","Ekaterina Artemova"],"pdf_url":"https://arxiv.org/pdf/2407.17629v1.pdf","comment":"to appear in DAGPAP 2024 proceedings"},{"id":"http://arxiv.org/abs/2407.17624v1","updated":"2024-07-24T20:30:55Z","published":"2024-07-24T20:30:55Z","title":"Traditional Methods Outperform Generative LLMs at Forecasting Credit\n  Ratings","summary":"  Large Language Models (LLMs) have been shown to perform well for many\ndownstream tasks. Transfer learning can enable LLMs to acquire skills that were\nnot targeted during pre-training. In financial contexts, LLMs can sometimes\nbeat well-established benchmarks. This paper investigates how well LLMs perform\nin the task of forecasting corporate credit ratings. We show that while LLMs\nare very good at encoding textual information, traditional methods are still\nvery competitive when it comes to encoding numeric and multimodal data. For our\ntask, current LLMs perform worse than a more traditional XGBoost architecture\nthat combines fundamental and macroeconomic data with high-density text-based\nembedding features.\n","authors":["Felix Drinkall","Janet B. Pierrehumbert","Stefan Zohren"],"pdf_url":"https://arxiv.org/pdf/2407.17624v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17605v1","updated":"2024-07-24T19:29:13Z","published":"2024-07-24T19:29:13Z","title":"Coupling Speech Encoders with Downstream Text Models","summary":"  We present a modular approach to building cascade speech translation (AST)\nmodels that guarantees that the resulting model performs no worse than the\n1-best cascade baseline while preserving state-of-the-art speech recognition\n(ASR) and text translation (MT) performance for a given task. Our novel\ncontribution is the use of an ``exporter'' layer that is trained under L2-loss\nto ensure a strong match between ASR embeddings and the MT token embeddings for\nthe 1-best sequence. The ``exporter'' output embeddings are fed directly to the\nMT model in lieu of 1-best token embeddings, thus guaranteeing that the\nresulting model performs no worse than the 1-best cascade baseline, while\nallowing back-propagation gradient to flow from the MT model into the ASR\ncomponents. The matched-embeddings cascade architecture provide a significant\nimprovement over its 1-best counterpart in scenarios where incremental training\nof the MT model is not an option and yet we seek to improve quality by\nleveraging (speech, transcription, translated transcription) data provided with\nthe AST task. The gain disappears when the MT model is incrementally trained on\nthe parallel text data available with the AST task. The approach holds promise\nfor other scenarios that seek to couple ASR encoders and immutable text models,\nsuch at large language models (LLM).\n","authors":["Ciprian Chelba","Johan Schalkwyk"],"pdf_url":"https://arxiv.org/pdf/2407.17605v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.13067v3","updated":"2024-07-24T18:54:53Z","published":"2023-05-22T14:37:05Z","title":"Distilling Robustness into Natural Language Inference Models with\n  Domain-Targeted Augmentation","summary":"  Knowledge distillation optimises a smaller student model to behave similarly\nto a larger teacher model, retaining some of the performance benefits. While\nthis method can improve results on in-distribution examples, it does not\nnecessarily generalise to out-of-distribution (OOD) settings. We investigate\ntwo complementary methods for improving the robustness of the resulting student\nmodels on OOD domains. The first approach augments the distillation with\ngenerated unlabelled examples that match the target distribution. The second\nmethod upsamples data points among the training set that are similar to the\ntarget distribution. When applied on the task of natural language inference\n(NLI), our experiments on MNLI show that distillation with these modifications\noutperforms previous robustness solutions. We also find that these methods\nimprove performance on OOD domains even beyond the target domain.\n","authors":["Joe Stacey","Marek Rei"],"pdf_url":"https://arxiv.org/pdf/2305.13067v3.pdf","comment":"Accepted at ACL Findings 2024"},{"id":"http://arxiv.org/abs/2407.06023v3","updated":"2024-07-24T18:40:36Z","published":"2024-07-08T15:17:46Z","title":"Distilling System 2 into System 1","summary":"  Large language models (LLMs) can spend extra compute during inference to\ngenerate intermediate thoughts, which helps to produce better final responses.\nSince Chain-of-Thought (Wei et al., 2022), many such System 2 techniques have\nbeen proposed such as Rephrase and Respond (Deng et al., 2023a), System 2\nAttention (Weston and Sukhbaatar, 2023) and Branch-Solve-Merge (Saha et al.,\n2023). In this work we investigate self-supervised methods to ``compile''\n(distill) higher quality outputs from System 2 techniques back into LLM\ngenerations without intermediate reasoning token sequences, as this reasoning\nhas been distilled into System 1. We show that several such techniques can be\nsuccessfully distilled, resulting in improved results compared to the original\nSystem 1 performance, and with less inference cost than System 2. We posit that\nsuch System 2 distillation will be an important feature of future continually\nlearning AI systems, enabling them to focus System 2 capabilities on the\nreasoning tasks that they cannot yet do well.\n","authors":["Ping Yu","Jing Xu","Jason Weston","Ilia Kulikov"],"pdf_url":"https://arxiv.org/pdf/2407.06023v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17987v4","updated":"2024-07-24T18:38:51Z","published":"2024-06-26T00:00:45Z","title":"Multi-step Inference over Unstructured Data","summary":"  The advent of Large Language Models (LLMs) and Generative AI has\nrevolutionized natural language applications across various domains. However,\nhigh-stakes decision-making tasks in fields such as medical, legal and finance\nrequire a level of precision, comprehensiveness, and logical consistency that\npure LLM or Retrieval-Augmented-Generation (RAG) approaches often fail to\ndeliver. At Elemental Cognition (EC), we have developed a neuro-symbolic AI\nplatform to tackle these problems. The platform integrates fine-tuned LLMs for\nknowledge extraction and alignment with a robust symbolic reasoning engine for\nlogical inference, planning and interactive constraint solving. We describe\nCora, a Collaborative Research Assistant built on this platform, that is\ndesigned to perform complex research and discovery tasks in high-stakes\ndomains. This paper discusses the multi-step inference challenges inherent in\nsuch domains, critiques the limitations of existing LLM-based methods, and\ndemonstrates how Cora's neuro-symbolic approach effectively addresses these\nissues. We provide an overview of the system architecture, key algorithms for\nknowledge extraction and formal reasoning, and present preliminary evaluation\nresults that highlight Cora's superior performance compared to well-known LLM\nand RAG baselines.\n","authors":["Aditya Kalyanpur","Kailash Karthik Saravanakumar","Victor Barres","CJ McFate","Lori Moon","Nati Seifu","Maksim Eremeev","Jose Barrera","Abraham Bautista-Castillo","Eric Brown","David Ferrucci"],"pdf_url":"https://arxiv.org/pdf/2406.17987v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17546v1","updated":"2024-07-24T17:25:12Z","published":"2024-07-24T17:25:12Z","title":"Exploring Domain Robust Lightweight Reward Models based on Router\n  Mechanism","summary":"  Recent advancements in large language models have heavily relied on the large\nreward model from reinforcement learning from human feedback for fine-tuning.\nHowever, the use of a single reward model across various domains may not always\nbe optimal, often requiring retraining from scratch when new domain data is\nintroduced. To address these challenges, we explore the utilization of small\nlanguage models operating in a domain-specific manner based on router\nmechanisms. Our three approaches are: 1) utilize mixture of experts to form a\nsingle reward model by modularizing an internal router and experts, 2)\nemploying external router to select the appropriate reward model from multiple\ndomain-specific models, and 3) the framework reduces parameter size by loading\nreward models and router adapters onto a single small language model using\nadapters. Experimental validation underscores the effectiveness of our\napproach, demonstrating performance comparable to baseline methods while also\nreducing the total parameter size.\n","authors":["Hyuk Namgoong","Jeesu Jung","Sangkeun Jung","Yoonhyung Roh"],"pdf_url":"https://arxiv.org/pdf/2407.17546v1.pdf","comment":"This paper is accepted for ACL 2024"},{"id":"http://arxiv.org/abs/2407.17545v1","updated":"2024-07-24T16:33:04Z","published":"2024-07-24T16:33:04Z","title":"Large Language Models for Anomaly Detection in Computational Workflows:\n  from Supervised Fine-Tuning to In-Context Learning","summary":"  Anomaly detection in computational workflows is critical for ensuring system\nreliability and security. However, traditional rule-based methods struggle to\ndetect novel anomalies. This paper leverages large language models (LLMs) for\nworkflow anomaly detection by exploiting their ability to learn complex data\npatterns. Two approaches are investigated: 1) supervised fine-tuning (SFT),\nwhere pre-trained LLMs are fine-tuned on labeled data for sentence\nclassification to identify anomalies, and 2) in-context learning (ICL) where\nprompts containing task descriptions and examples guide LLMs in few-shot\nanomaly detection without fine-tuning. The paper evaluates the performance,\nefficiency, generalization of SFT models, and explores zero-shot and few-shot\nICL prompts and interpretability enhancement via chain-of-thought prompting.\nExperiments across multiple workflow datasets demonstrate the promising\npotential of LLMs for effective anomaly detection in complex executions.\n","authors":["Hongwei Jin","George Papadimitriou","Krishnan Raghavan","Pawel Zuk","Prasanna Balaprakash","Cong Wang","Anirban Mandal","Ewa Deelman"],"pdf_url":"https://arxiv.org/pdf/2407.17545v1.pdf","comment":"12 pages, 14 figures, paper is accepted by SC'24, source code, see:\n  https://github.com/PoSeiDon-Workflows/LLM_AD"},{"id":"http://arxiv.org/abs/2407.17532v1","updated":"2024-07-24T03:33:47Z","published":"2024-07-24T03:33:47Z","title":"Generative artificial intelligence in dentistry: Current approaches and\n  future challenges","summary":"  Artificial intelligence (AI) has become a commodity for people because of the\nadvent of generative AI (GenAI) models that bridge the usability gap of AI by\nproviding a natural language interface to interact with complex models. These\nGenAI models range from text generation - such as two-way chat systems - to the\ngeneration of image or video from textual descriptions input by a user. These\nadvancements in AI have impacted Dentistry in multiple aspects. In dental\neducation, the student now has the opportunity to solve a plethora of questions\nby only prompting a GenAI model and have the answer in a matter of seconds.\nGenAI models can help us deliver better patient healthcare by helping\npractitioners gather knowledge quickly and efficiently. Finally, GenAI can also\nbe used in dental research, where the applications range from new drug\ndiscovery to assistance in academic writing. In this review, we first define\nGenAI models and describe their multiple generation modalities; then, we\nexplain and discuss their current and potential applications in Dentistry; and\nfinally, we describe the challenges these new technologies impose in our area.\n","authors":["Fabián Villena","Claudia Véliz","Rosario García-Huidobro","Sebastián Aguayo"],"pdf_url":"https://arxiv.org/pdf/2407.17532v1.pdf","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2407.17470v1","updated":"2024-07-24T17:59:43Z","published":"2024-07-24T17:59:43Z","title":"SV4D: Dynamic 3D Content Generation with Multi-Frame and Multi-View\n  Consistency","summary":"  We present Stable Video 4D (SV4D), a latent video diffusion model for\nmulti-frame and multi-view consistent dynamic 3D content generation. Unlike\nprevious methods that rely on separately trained generative models for video\ngeneration and novel view synthesis, we design a unified diffusion model to\ngenerate novel view videos of dynamic 3D objects. Specifically, given a\nmonocular reference video, SV4D generates novel views for each video frame that\nare temporally consistent. We then use the generated novel view videos to\noptimize an implicit 4D representation (dynamic NeRF) efficiently, without the\nneed for cumbersome SDS-based optimization used in most prior works. To train\nour unified novel view video generation model, we curated a dynamic 3D object\ndataset from the existing Objaverse dataset. Extensive experimental results on\nmultiple datasets and user studies demonstrate SV4D's state-of-the-art\nperformance on novel-view video synthesis as well as 4D generation compared to\nprior works.\n","authors":["Yiming Xie","Chun-Han Yao","Vikram Voleti","Huaizu Jiang","Varun Jampani"],"pdf_url":"https://arxiv.org/pdf/2407.17470v1.pdf","comment":"Project page: https://sv4d.github.io/"},{"id":"http://arxiv.org/abs/2407.17460v1","updated":"2024-07-24T17:57:21Z","published":"2024-07-24T17:57:21Z","title":"SoNIC: Safe Social Navigation with Adaptive Conformal Inference and\n  Constrained Reinforcement Learning","summary":"  Reinforcement Learning (RL) has enabled social robots to generate\ntrajectories without human-designed rules or interventions, which makes it more\neffective than hard-coded systems for generalizing to complex real-world\nscenarios. However, social navigation is a safety-critical task that requires\nrobots to avoid collisions with pedestrians while previous RL-based solutions\nfall short in safety performance in complex environments. To enhance the safety\nof RL policies, to the best of our knowledge, we propose the first algorithm,\nSoNIC, that integrates adaptive conformal inference (ACI) with constrained\nreinforcement learning (CRL) to learn safe policies for social navigation. More\nspecifically, our method augments RL observations with ACI-generated\nnonconformity scores and provides explicit guidance for agents to leverage the\nuncertainty metrics to avoid safety-critical areas by incorporating safety\nconstraints with spatial relaxation. Our method outperforms state-of-the-art\nbaselines in terms of both safety and adherence to social norms by a large\nmargin and demonstrates much stronger robustness to out-of-distribution\nscenarios. Our code and video demos are available on our project website:\nhttps://sonic-social-nav.github.io/.\n","authors":["Jianpeng Yao","Xiaopan Zhang","Yu Xia","Zejin Wang","Amit K. Roy-Chowdhury","Jiachen Li"],"pdf_url":"https://arxiv.org/pdf/2407.17460v1.pdf","comment":"Project website: https://sonic-social-nav.github.io/"},{"id":"http://arxiv.org/abs/2407.17457v1","updated":"2024-07-24T17:50:00Z","published":"2024-07-24T17:50:00Z","title":"CSCPR: Cross-Source-Context Indoor RGB-D Place Recognition","summary":"  We present a new algorithm, Cross-Source-Context Place Recognition (CSCPR),\nfor RGB-D indoor place recognition that integrates global retrieval and\nreranking into a single end-to-end model. Unlike prior approaches that\nprimarily focus on the RGB domain, CSCPR is designed to handle the RGB-D data.\nWe extend the Context-of-Clusters (CoCs) for handling noisy colorized point\nclouds and introduce two novel modules for reranking: the Self-Context Cluster\n(SCC) and Cross Source Context Cluster (CSCC), which enhance feature\nrepresentation and match query-database pairs based on local features,\nrespectively. We also present two new datasets, ScanNetIPR and ARKitIPR. Our\nexperiments demonstrate that CSCPR significantly outperforms state-of-the-art\nmodels on these datasets by at least 36.5% in Recall@1 at ScanNet-PR dataset\nand 44% in new datasets. Code and datasets will be released.\n","authors":["Jing Liang","Zhuo Deng","Zheming Zhou","Min Sun","Omid Ghasemalizadeh","Cheng-Hao Kuo","Arnie Sen","Dinesh Manocha"],"pdf_url":"https://arxiv.org/pdf/2407.17457v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17453v1","updated":"2024-07-24T17:37:05Z","published":"2024-07-24T17:37:05Z","title":"$VILA^2$: VILA Augmented VILA","summary":"  Visual language models (VLMs) have rapidly progressed, driven by the success\nof large language models (LLMs). While model architectures and training\ninfrastructures advance rapidly, data curation remains under-explored. When\ndata quantity and quality become a bottleneck, existing work either directly\ncrawls more raw data from the Internet that does not have a guarantee of data\nquality or distills from black-box commercial models (e.g., GPT-4V / Gemini)\ncausing the performance upper bounded by that model. In this work, we introduce\na novel approach that includes a self-augment step and a specialist-augment\nstep to iteratively improve data quality and model performance. In the\nself-augment step, a VLM recaptions its own pretraining data to enhance data\nquality, and then retrains from scratch using this refined dataset to improve\nmodel performance. This process can iterate for several rounds. Once\nself-augmentation saturates, we employ several specialist VLMs finetuned from\nthe self-augmented VLM with domain-specific expertise, to further infuse\nspecialist knowledge into the generalist VLM through task-oriented recaptioning\nand retraining. With the combined self-augmented and specialist-augmented\ntraining, we introduce $VILA^2$ (VILA-augmented-VILA), a VLM family that\nconsistently improves the accuracy on a wide range of tasks over prior art, and\nachieves new state-of-the-art results on MMMU leaderboard among open-sourced\nmodels.\n","authors":["Yunhao Fang","Ligeng Zhu","Yao Lu","Yan Wang","Pavlo Molchanov","Jang Hyun Cho","Marco Pavone","Song Han","Hongxu Yin"],"pdf_url":"https://arxiv.org/pdf/2407.17453v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17449v1","updated":"2024-07-24T17:30:21Z","published":"2024-07-24T17:30:21Z","title":"Looking at Model Debiasing through the Lens of Anomaly Detection","summary":"  It is widely recognized that deep neural networks are sensitive to bias in\nthe data. This means that during training these models are likely to learn\nspurious correlations between data and labels, resulting in limited\ngeneralization abilities and low performance. In this context, model debiasing\napproaches can be devised aiming at reducing the model's dependency on such\nunwanted correlations, either leveraging the knowledge of bias information or\nnot. In this work, we focus on the latter and more realistic scenario, showing\nthe importance of accurately predicting the bias-conflicting and bias-aligned\nsamples to obtain compelling performance in bias mitigation. On this ground, we\npropose to conceive the problem of model bias from an out-of-distribution\nperspective, introducing a new bias identification method based on anomaly\ndetection. We claim that when data is mostly biased, bias-conflicting samples\ncan be regarded as outliers with respect to the bias-aligned distribution in\nthe feature space of a biased model, thus allowing for precisely detecting them\nwith an anomaly detection method. Coupling the proposed bias identification\napproach with bias-conflicting data upsampling and augmentation in a two-step\nstrategy, we reach state-of-the-art performance on synthetic and real benchmark\ndatasets. Ultimately, our proposed approach shows that the data bias issue does\nnot necessarily require complex debiasing methods, given that an accurate bias\nidentification procedure is defined.\n","authors":["Vito Paolo Pastore","Massimiliano Ciranni","Davide Marinelli","Francesca Odone","Vittorio Murino"],"pdf_url":"https://arxiv.org/pdf/2407.17449v1.pdf","comment":"15 pages, 7 figures"},{"id":"http://arxiv.org/abs/2407.17442v1","updated":"2024-07-24T17:19:58Z","published":"2024-07-24T17:19:58Z","title":"AHMF: Adaptive Hybrid-Memory-Fusion Model for Driver Attention\n  Prediction","summary":"  Accurate driver attention prediction can serve as a critical reference for\nintelligent vehicles in understanding traffic scenes and making informed\ndriving decisions. Though existing studies on driver attention prediction\nimproved performance by incorporating advanced saliency detection techniques,\nthey overlooked the opportunity to achieve human-inspired prediction by\nanalyzing driving tasks from a cognitive science perspective. During driving,\ndrivers' working memory and long-term memory play crucial roles in scene\ncomprehension and experience retrieval, respectively. Together, they form\nsituational awareness, facilitating drivers to quickly understand the current\ntraffic situation and make optimal decisions based on past driving experiences.\nTo explicitly integrate these two types of memory, this paper proposes an\nAdaptive Hybrid-Memory-Fusion (AHMF) driver attention prediction model to\nachieve more human-like predictions. Specifically, the model first encodes\ninformation about specific hazardous stimuli in the current scene to form\nworking memories. Then, it adaptively retrieves similar situational experiences\nfrom the long-term memory for final prediction. Utilizing domain adaptation\ntechniques, the model performs parallel training across multiple datasets,\nthereby enriching the accumulated driving experience within the long-term\nmemory module. Compared to existing models, our model demonstrates significant\nimprovements across various metrics on multiple public datasets, proving the\neffectiveness of integrating hybrid memories in driver attention prediction.\n","authors":["Dongyang Xu","Qingfan Wang","Ji Ma","Xiangyun Zeng","Lei Chen"],"pdf_url":"https://arxiv.org/pdf/2407.17442v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17438v1","updated":"2024-07-24T17:15:58Z","published":"2024-07-24T17:15:58Z","title":"HumanVid: Demystifying Training Data for Camera-controllable Human Image\n  Animation","summary":"  Human image animation involves generating videos from a character photo,\nallowing user control and unlocking potential for video and movie production.\nWhile recent approaches yield impressive results using high-quality training\ndata, the inaccessibility of these datasets hampers fair and transparent\nbenchmarking. Moreover, these approaches prioritize 2D human motion and\noverlook the significance of camera motions in videos, leading to limited\ncontrol and unstable video generation.To demystify the training data, we\npresent HumanVid, the first large-scale high-quality dataset tailored for human\nimage animation, which combines crafted real-world and synthetic data. For the\nreal-world data, we compile a vast collection of copyright-free real-world\nvideos from the internet. Through a carefully designed rule-based filtering\nstrategy, we ensure the inclusion of high-quality videos, resulting in a\ncollection of 20K human-centric videos in 1080P resolution. Human and camera\nmotion annotation is accomplished using a 2D pose estimator and a SLAM-based\nmethod. For the synthetic data, we gather 2,300 copyright-free 3D avatar assets\nto augment existing available 3D assets. Notably, we introduce a rule-based\ncamera trajectory generation method, enabling the synthetic pipeline to\nincorporate diverse and precise camera motion annotation, which can rarely be\nfound in real-world data. To verify the effectiveness of HumanVid, we establish\na baseline model named CamAnimate, short for Camera-controllable Human\nAnimation, that considers both human and camera motions as conditions. Through\nextensive experimentation, we demonstrate that such simple baseline training on\nour HumanVid achieves state-of-the-art performance in controlling both human\npose and camera motions, setting a new benchmark. Code and data will be\npublicly available at \\url{https://github.com/zhenzhiwang/HumanVid/}.\n","authors":["Zhenzhi Wang","Yixuan Li","Yanhong Zeng","Youqing Fang","Yuwei Guo","Wenran Liu","Jing Tan","Kai Chen","Tianfan Xue","Bo Dai","Dahua Lin"],"pdf_url":"https://arxiv.org/pdf/2407.17438v1.pdf","comment":"camera controllable human image animation, a dataset and a baseline"},{"id":"http://arxiv.org/abs/2407.15708v2","updated":"2024-07-24T16:55:08Z","published":"2024-07-22T15:17:39Z","title":"SwinSF: Image Reconstruction from Spatial-Temporal Spike Streams","summary":"  The spike camera, with its high temporal resolution, low latency, and high\ndynamic range, addresses high-speed imaging challenges like motion blur. It\ncaptures photons at each pixel independently, creating binary spike streams\nrich in temporal information but challenging for image reconstruction. Current\nalgorithms, both traditional and deep learning-based, still need to be improved\nin the utilization of the rich temporal detail and the restoration of the\ndetails of the reconstructed image. To overcome this, we introduce Swin\nSpikeformer (SwinSF), a novel model for dynamic scene reconstruction from spike\nstreams. SwinSF is composed of Spike Feature Extraction, Spatial-Temporal\nFeature Extraction, and Final Reconstruction Module. It combines shifted window\nself-attention and proposed temporal spike attention, ensuring a comprehensive\nfeature extraction that encapsulates both spatial and temporal dynamics,\nleading to a more robust and accurate reconstruction of spike streams.\nFurthermore, we build a new synthesized dataset for spike image reconstruction\nwhich matches the resolution of the latest spike camera, ensuring its relevance\nand applicability to the latest developments in spike camera imaging.\nExperimental results demonstrate that the proposed network SwinSF sets a new\nbenchmark, achieving state-of-the-art performance across a series of datasets,\nincluding both real-world and synthesized data across various resolutions. Our\ncodes and proposed dataset will be available soon.\n","authors":["Liangyan Jiang","Chuang Zhu","Yanxu Chen"],"pdf_url":"https://arxiv.org/pdf/2407.15708v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17418v1","updated":"2024-07-24T16:53:17Z","published":"2024-07-24T16:53:17Z","title":"3D Gaussian Splatting: Survey, Technologies, Challenges, and\n  Opportunities","summary":"  3D Gaussian Splatting (3DGS) has emerged as a prominent technique with the\npotential to become a mainstream method for 3D representations. It can\neffectively transform multi-view images into explicit 3D Gaussian\nrepresentations through efficient training, and achieve real-time rendering of\nnovel views. This survey aims to analyze existing 3DGS-related works from\nmultiple intersecting perspectives, including related tasks, technologies,\nchallenges, and opportunities. The primary objective is to provide newcomers\nwith a rapid understanding of the field and to assist researchers in\nmethodically organizing existing technologies and challenges. Specifically, we\ndelve into the optimization, application, and extension of 3DGS, categorizing\nthem based on their focuses or motivations. Additionally, we summarize and\nclassify nine types of technical modules and corresponding improvements\nidentified in existing works. Based on these analyses, we further examine the\ncommon challenges and technologies across various tasks, proposing potential\nresearch opportunities.\n","authors":["Yanqi Bao","Tianyu Ding","Jing Huo","Yaoli Liu","Yuxin Li","Wenbin Li","Yang Gao","Jiebo Luo"],"pdf_url":"https://arxiv.org/pdf/2407.17418v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17412v1","updated":"2024-07-24T16:47:45Z","published":"2024-07-24T16:47:45Z","title":"(PASS) Visual Prompt Locates Good Structure Sparsity through a Recurrent\n  HyperNetwork","summary":"  Large-scale neural networks have demonstrated remarkable performance in\ndifferent domains like vision and language processing, although at the cost of\nmassive computation resources. As illustrated by compression literature,\nstructural model pruning is a prominent algorithm to encourage model\nefficiency, thanks to its acceleration-friendly sparsity patterns. One of the\nkey questions of structural pruning is how to estimate the channel\nsignificance. In parallel, work on data-centric AI has shown that\nprompting-based techniques enable impressive generalization of large language\nmodels across diverse downstream tasks. In this paper, we investigate a\ncharming possibility - \\textit{leveraging visual prompts to capture the channel\nimportance and derive high-quality structural sparsity}. To this end, we\npropose a novel algorithmic framework, namely \\texttt{PASS}. It is a tailored\nhyper-network to take both visual prompts and network weight statistics as\ninput, and output layer-wise channel sparsity in a recurrent manner. Such\ndesigns consider the intrinsic channel dependency between layers. Comprehensive\nexperiments across multiple network architectures and six datasets demonstrate\nthe superiority of \\texttt{PASS} in locating good structural sparsity. For\nexample, at the same FLOPs level, \\texttt{PASS} subnetworks achieve $1\\%\\sim\n3\\%$ better accuracy on Food101 dataset; or with a similar performance of\n$80\\%$ accuracy, \\texttt{PASS} subnetworks obtain $0.35\\times$ more speedup\nthan the baselines.\n","authors":["Tianjin Huang","Fang Meng","Li Shen","Fan Liu","Yulong Pei","Mykola Pechenizkiy","Shiwei Liu","Tianlong Chen"],"pdf_url":"https://arxiv.org/pdf/2407.17412v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2407.17409v1","updated":"2024-07-24T16:43:38Z","published":"2024-07-24T16:43:38Z","title":"Generation of Training Data from HD Maps in the Lanelet2 Framework","summary":"  Using HD maps directly as training data for machine learning tasks has seen a\nmassive surge in popularity and shown promising results, e.g. in the field of\nmap perception. Despite that, a standardized HD map framework supporting all\nparts of map-based automated driving and training label generation from map\ndata does not exist. Furthermore, feeding map perception models with map data\nas part of the input during real-time inference is not addressed by the\nresearch community. In order to fill this gap, we presentlanelet2_ml_converter,\nan integrated extension to the HD map framework Lanelet2, widely used in\nautomated driving systems by academia and industry. With this addition Lanelet2\nunifies map based automated driving, machine learning inference and training,\nall from a single source of map data and format. Requirements for a unified\nframework are analyzed and the implementation of these requirements is\ndescribed. The usability of labels in state of the art machine learning is\ndemonstrated with application examples from the field of map perception. The\nsource code is available embedded in the Lanelet2 framework under\nhttps://github.com/fzi-forschungszentrum-informatik/Lanelet2/tree/feature_ml_converter\n","authors":["Fabian Immel","Richard Fehler","Frank Bieder","Christoph Stiller"],"pdf_url":"https://arxiv.org/pdf/2407.17409v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.03588v2","updated":"2024-07-24T16:26:41Z","published":"2024-07-04T02:45:29Z","title":"FDS: Feedback-guided Domain Synthesis with Multi-Source Conditional\n  Diffusion Models for Domain Generalization","summary":"  Domain Generalization techniques aim to enhance model robustness by\nsimulating novel data distributions during training, typically through various\naugmentation or stylization strategies. However, these methods frequently\nsuffer from limited control over the diversity of generated images and lack\nassurance that these images span distinct distributions. To address these\nchallenges, we propose FDS, Feedback-guided Domain Synthesis, a novel strategy\nthat employs diffusion models to synthesize novel, pseudo-domains by training a\nsingle model on all source domains and performing domain mixing based on\nlearned features. By incorporating images that pose classification challenges\nto models trained on original samples, alongside the original dataset, we\nensure the generation of a training set that spans a broad distribution\nspectrum. Our comprehensive evaluations demonstrate that this methodology sets\nnew benchmarks in domain generalization performance across a range of\nchallenging datasets, effectively managing diverse types of domain shifts. The\nimplementation is available at: \\url{https://github.com/Mehrdad-Noori/FDS.git}.\n","authors":["Mehrdad Noori","Milad Cheraghalikhani","Ali Bahri","Gustavo Adolfo Vargas Hakim","David Osowiechi","Moslem Yazdanpanah","Ismail Ben Ayed","Christian Desrosiers"],"pdf_url":"https://arxiv.org/pdf/2407.03588v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17399v1","updated":"2024-07-24T16:23:46Z","published":"2024-07-24T16:23:46Z","title":"Self-Calibrated Variance-Stabilizing Transformations for Real-World\n  Image Denoising","summary":"  Supervised deep learning has become the method of choice for image denoising.\nIt involves the training of neural networks on large datasets composed of pairs\nof noisy and clean images. However, the necessity of training data that are\nspecific to the targeted application constrains the widespread use of denoising\nnetworks. Recently, several approaches have been developed to overcome this\ndifficulty by whether artificially generating realistic clean/noisy image\npairs, or training exclusively on noisy images. In this paper, we show that,\ncontrary to popular belief, denoising networks specialized in the removal of\nGaussian noise can be efficiently leveraged in favor of real-world image\ndenoising, even without additional training. For this to happen, an appropriate\nvariance-stabilizing transform (VST) has to be applied beforehand. We propose\nan algorithm termed Noise2VST for the learning of such a model-free VST. Our\napproach requires only the input noisy image and an off-the-shelf Gaussian\ndenoiser. We demonstrate through extensive experiments the efficiency and\nsuperiority of Noise2VST in comparison to existing methods trained in the\nabsence of specific clean/noisy pairs.\n","authors":["Sébastien Herbreteau","Michael Unser"],"pdf_url":"https://arxiv.org/pdf/2407.17399v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17398v1","updated":"2024-07-24T16:22:27Z","published":"2024-07-24T16:22:27Z","title":"3D Question Answering for City Scene Understanding","summary":"  3D multimodal question answering (MQA) plays a crucial role in scene\nunderstanding by enabling intelligent agents to comprehend their surroundings\nin 3D environments. While existing research has primarily focused on indoor\nhousehold tasks and outdoor roadside autonomous driving tasks, there has been\nlimited exploration of city-level scene understanding tasks. Furthermore,\nexisting research faces challenges in understanding city scenes, due to the\nabsence of spatial semantic information and human-environment interaction\ninformation at the city level.To address these challenges, we investigate 3D\nMQA from both dataset and method perspectives. From the dataset perspective, we\nintroduce a novel 3D MQA dataset named City-3DQA for city-level scene\nunderstanding, which is the first dataset to incorporate scene semantic and\nhuman-environment interactive tasks within the city. From the method\nperspective, we propose a Scene graph enhanced City-level Understanding method\n(Sg-CityU), which utilizes the scene graph to introduce the spatial semantic. A\nnew benchmark is reported and our proposed Sg-CityU achieves accuracy of 63.94\n% and 63.76 % in different settings of City-3DQA. Compared to indoor 3D MQA\nmethods and zero-shot using advanced large language models (LLMs), Sg-CityU\ndemonstrates state-of-the-art (SOTA) performance in robustness and\ngeneralization.\n","authors":["Penglei Sun","Yaoxian Song","Xiang Liu","Xiaofei Yang","Qiang Wang","Tiefeng Li","Yang Yang","Xiaowen Chu"],"pdf_url":"https://arxiv.org/pdf/2407.17398v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17380v1","updated":"2024-07-24T16:04:18Z","published":"2024-07-24T16:04:18Z","title":"2D and 3D Deep Learning Models for MRI-based Parkinson's Disease\n  Classification: A Comparative Analysis of Convolutional Kolmogorov-Arnold\n  Networks, Convolutional Neural Networks, and Graph Convolutional Networks","summary":"  Early and accurate diagnosis of Parkinson's Disease (PD) remains challenging.\nThis study compares deep learning architectures for MRI-based PD\nclassification, introducing the first three-dimensional (3D) implementation of\nConvolutional Kolmogorov-Arnold Networks (ConvKANs), a new approach that\ncombines convolution layers with adaptive, spline-based activations. We\nevaluated Convolutional Neural Networks (CNNs), ConvKANs, and Graph\nConvolutional Networks (GCNs) using three open-source datasets; a total of 142\nparticipants (75 with PD and 67 age-matched healthy controls). For 2D analysis,\nwe extracted 100 axial slices centred on the midbrain from each T1-weighted\nscan. For 3D analysis, we used the entire volumetric scans. ConvKANs integrate\nlearnable B-spline functions with convolutional layers. GCNs represent MRI data\nas graphs, theoretically capturing structural relationships that may be\noverlooked by traditional approaches. Interpretability visualizations,\nincluding the first ConvKAN spline activation maps, and projections of graph\nnode embeddings, were depicted. ConvKANs demonstrated high performance across\ndatasets and dimensionalities, achieving the highest 2D AUROC (0.98) in one\ndataset and matching CNN peak 3D performance (1.00). CNN models performed well,\nwhile GCN models improved in 3D analyses, reaching up to 0.97 AUROC. 3D\nimplementations yielded higher AUROC values compared to 2D counterparts across\nall models. ConvKAN implementation shows promise for MRI analysis in PD\nclassification, particularly in the context of early diagnosis. The improvement\nin 3D analyses highlights the value of volumetric data in capturing subtle\nPD-related changes. While MRI is not currently used for PD diagnosis, these\nfindings suggest its potential as a component of a multimodal diagnostic\napproach, especially for early detection.\n","authors":["Salil B Patel","Vicky Goh","James F FitzGerald","Chrystalina A Antoniades"],"pdf_url":"https://arxiv.org/pdf/2407.17380v1.pdf","comment":"19 Pages, 5 figures"},{"id":"http://arxiv.org/abs/2403.18820v2","updated":"2024-07-24T16:04:02Z","published":"2024-03-27T17:59:54Z","title":"MetaCap: Meta-learning Priors from Multi-View Imagery for Sparse-view\n  Human Performance Capture and Rendering","summary":"  Faithful human performance capture and free-view rendering from sparse RGB\nobservations is a long-standing problem in Vision and Graphics. The main\nchallenges are the lack of observations and the inherent ambiguities of the\nsetting, e.g. occlusions and depth ambiguity. As a result, radiance fields,\nwhich have shown great promise in capturing high-frequency appearance and\ngeometry details in dense setups, perform poorly when naively supervising them\non sparse camera views, as the field simply overfits to the sparse-view inputs.\nTo address this, we propose MetaCap, a method for efficient and high-quality\ngeometry recovery and novel view synthesis given very sparse or even a single\nview of the human. Our key idea is to meta-learn the radiance field weights\nsolely from potentially sparse multi-view videos, which can serve as a prior\nwhen fine-tuning them on sparse imagery depicting the human. This prior\nprovides a good network weight initialization, thereby effectively addressing\nambiguities in sparse-view capture. Due to the articulated structure of the\nhuman body and motion-induced surface deformations, learning such a prior is\nnon-trivial. Therefore, we propose to meta-learn the field weights in a\npose-canonicalized space, which reduces the spatial feature range and makes\nfeature learning more effective. Consequently, one can fine-tune our field\nparameters to quickly generalize to unseen poses, novel illumination conditions\nas well as novel and sparse (even monocular) camera views. For evaluating our\nmethod under different scenarios, we collect a new dataset, WildDynaCap, which\ncontains subjects captured in, both, a dense camera dome and in-the-wild sparse\ncamera rigs, and demonstrate superior results compared to recent\nstate-of-the-art methods on, both, public and WildDynaCap dataset.\n","authors":["Guoxing Sun","Rishabh Dabral","Pascal Fua","Christian Theobalt","Marc Habermann"],"pdf_url":"https://arxiv.org/pdf/2403.18820v2.pdf","comment":"Project page: https://vcai.mpi-inf.mpg.de/projects/MetaCap/"},{"id":"http://arxiv.org/abs/2407.17379v1","updated":"2024-07-24T15:59:01Z","published":"2024-07-24T15:59:01Z","title":"MMRA: A Benchmark for Multi-granularity Multi-image Relational\n  Association","summary":"  Given the remarkable success that large visual language models (LVLMs) have\nachieved in image perception tasks, the endeavor to make LVMLs perceive the\nworld like humans is drawing increasing attention. Current multi-modal\nbenchmarks mainly focus on the objective fact or certain topic related\npotential knowledge within a image, but overlook the associative relations\nbetween multiple images. Therefore, we define a multi-image relation\nassociation task, and meticulously curate \\textbf{MMRA} benchmark, a\n\\textbf{M}ulti-granularity \\textbf{M}ulti-image \\textbf{R}elational\n\\textbf{A}ssociation benchmark, consisted of \\textbf{1026} samples. In order to\nsystematically and comprehensively evaluate mainstream LVLMs, we establish an\nassociational relation system among images that contain \\textbf{11 subtasks}\n(e.g, UsageSimilarity, SubEvent, etc.) at two granularity levels (i.e.,\n\"\\textbf{image}\" and \"\\textbf{entity}\") according to the relations in\nConceptNet. Our experiments demonstrate that, on our MMRA benchmark, current\nmainstream LVLMs all have their own advantages and disadvantages across\ndifferent subtasks. It is worth noting that, at the entity level, the\nperformance of all models is worse than that of them at the image level,\nindicating that the fine-grained multi-image perception task is still\nchallenging for LVLMs. The tasks related to spatial perception are relatively\ndifficult for LVLMs to handle. Furthermore, we find that LVMLs exhibit a good\nability to perceive image details, and the key to enhancing their multi-image\nassociation capability is to strengthen the reasoning ability of their language\nmodel component. All our codes and data are released at\nhtt\\url{https://github.com/Wusiwei0410/MMRA}.\n","authors":["Siwei Wu","Kang Zhu","Yu Bai","Yiming Liang","Yizhi Li","Haoning Wu","Jiaheng Liu","Ruibo Liu","Xingwei Qu","Xuxin Cheng","Ge Zhang","Wenhao Huang","Chenghua Lin"],"pdf_url":"https://arxiv.org/pdf/2407.17379v1.pdf","comment":"VLMS, Multi-Image Association"},{"id":"http://arxiv.org/abs/2407.17378v1","updated":"2024-07-24T15:58:24Z","published":"2024-07-24T15:58:24Z","title":"PrevPredMap: Exploring Temporal Modeling with Previous Predictions for\n  Online Vectorized HD Map Construction","summary":"  Temporal information is crucial for detecting occluded instances. Existing\ntemporal representations have progressed from BEV or PV features to more\ncompact query features. Compared to these aforementioned features, predictions\noffer the highest level of abstraction, providing explicit information. In the\ncontext of online vectorized HD map construction, this unique characteristic of\npredictions is potentially advantageous for long-term temporal modeling and the\nintegration of map priors. This paper introduces PrevPredMap, a pioneering\ntemporal modeling framework that leverages previous predictions for\nconstructing online vectorized HD maps. We have meticulously crafted two\nessential modules for PrevPredMap: the previous-predictions-based query\ngenerator and the dynamic-position-query decoder. Specifically, the\nprevious-predictions-based query generator is designed to separately encode\ndifferent types of information from previous predictions, which are then\neffectively utilized by the dynamic-position-query decoder to generate current\npredictions. Furthermore, we have developed a dual-mode strategy to ensure\nPrevPredMap's robust performance across both single-frame and temporal modes.\nExtensive experiments demonstrate that PrevPredMap achieves state-of-the-art\nperformance on the nuScenes and Argoverse2 datasets. Code will be available at\nhttps://github.com/pnnnnnnn/PrevPredMap.\n","authors":["Nan Peng","Xun Zhou","Mingming Wang","Xiaojun Yang","Songming Chen","Guisong Chen"],"pdf_url":"https://arxiv.org/pdf/2407.17378v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17365v1","updated":"2024-07-24T15:42:34Z","published":"2024-07-24T15:42:34Z","title":"ViPer: Visual Personalization of Generative Models via Individual\n  Preference Learning","summary":"  Different users find different images generated for the same prompt\ndesirable. This gives rise to personalized image generation which involves\ncreating images aligned with an individual's visual preference. Current\ngenerative models are, however, unpersonalized, as they are tuned to produce\noutputs that appeal to a broad audience. Using them to generate images aligned\nwith individual users relies on iterative manual prompt engineering by the user\nwhich is inefficient and undesirable. We propose to personalize the image\ngeneration process by first capturing the generic preferences of the user in a\none-time process by inviting them to comment on a small selection of images,\nexplaining why they like or dislike each. Based on these comments, we infer a\nuser's structured liked and disliked visual attributes, i.e., their visual\npreference, using a large language model. These attributes are used to guide a\ntext-to-image model toward producing images that are tuned towards the\nindividual user's visual preference. Through a series of user studies and large\nlanguage model guided evaluations, we demonstrate that the proposed method\nresults in generations that are well aligned with individual users' visual\npreferences.\n","authors":["Sogand Salehi","Mahdi Shafiei","Teresa Yeo","Roman Bachmann","Amir Zamir"],"pdf_url":"https://arxiv.org/pdf/2407.17365v1.pdf","comment":"Project page at https://viper.epfl.ch/"},{"id":"http://arxiv.org/abs/2407.17361v1","updated":"2024-07-24T15:38:20Z","published":"2024-07-24T15:38:20Z","title":"MuST: Multi-Scale Transformers for Surgical Phase Recognition","summary":"  Phase recognition in surgical videos is crucial for enhancing computer-aided\nsurgical systems as it enables automated understanding of sequential procedural\nstages. Existing methods often rely on fixed temporal windows for video\nanalysis to identify dynamic surgical phases. Thus, they struggle to\nsimultaneously capture short-, mid-, and long-term information necessary to\nfully understand complex surgical procedures. To address these issues, we\npropose Multi-Scale Transformers for Surgical Phase Recognition (MuST), a novel\nTransformer-based approach that combines a Multi-Term Frame encoder with a\nTemporal Consistency Module to capture information across multiple temporal\nscales of a surgical video. Our Multi-Term Frame Encoder computes\ninterdependencies across a hierarchy of temporal scales by sampling sequences\nat increasing strides around the frame of interest. Furthermore, we employ a\nlong-term Transformer encoder over the frame embeddings to further enhance\nlong-term reasoning. MuST achieves higher performance than previous\nstate-of-the-art methods on three different public benchmarks.\n","authors":["Alejandra Pérez","Santiago Rodríguez","Nicolás Ayobi","Nicolás Aparicio","Eugénie Dessevres","Pablo Arbeláez"],"pdf_url":"https://arxiv.org/pdf/2407.17361v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17354v1","updated":"2024-07-24T15:27:21Z","published":"2024-07-24T15:27:21Z","title":"Deep Spherical Superpixels","summary":"  Over the years, the use of superpixel segmentation has become very popular in\nvarious applications, serving as a preprocessing step to reduce data size by\nadapting to the content of the image, regardless of its semantic content. While\nthe superpixel segmentation of standard planar images, captured with a 90{\\deg}\nfield of view, has been extensively studied, there has been limited focus on\ndedicated methods to omnidirectional or spherical images, captured with a\n360{\\deg} field of view. In this study, we introduce the first deep\nlearning-based superpixel segmentation approach tailored for omnidirectional\nimages called DSS (for Deep Spherical Superpixels). Our methodology leverages\non spherical CNN architectures and the differentiable K-means clustering\nparadigm for superpixels, to generate superpixels that follow the spherical\ngeometry. Additionally, we propose to use data augmentation techniques\nspecifically designed for 360{\\deg} images, enabling our model to efficiently\nlearn from a limited set of annotated omnidirectional data. Our extensive\nvalidation across two datasets demonstrates that taking into account the\ninherent circular geometry of such images into our framework improves the\nsegmentation performance over traditional and deep learning-based superpixel\nmethods. Our code is available online.\n","authors":["Rémi Giraud","Michaël Clément"],"pdf_url":"https://arxiv.org/pdf/2407.17354v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.14154v2","updated":"2024-07-24T15:19:20Z","published":"2024-02-21T22:27:40Z","title":"MM-Soc: Benchmarking Multimodal Large Language Models in Social Media\n  Platforms","summary":"  Social media platforms are hubs for multimodal information exchange,\nencompassing text, images, and videos, making it challenging for machines to\ncomprehend the information or emotions associated with interactions in online\nspaces. Multimodal Large Language Models (MLLMs) have emerged as a promising\nsolution to these challenges, yet they struggle to accurately interpret human\nemotions and complex content such as misinformation. This paper introduces\nMM-Soc, a comprehensive benchmark designed to evaluate MLLMs' understanding of\nmultimodal social media content. MM-Soc compiles prominent multimodal datasets\nand incorporates a novel large-scale YouTube tagging dataset, targeting a range\nof tasks from misinformation detection, hate speech detection, and social\ncontext generation. Through our exhaustive evaluation on ten size-variants of\nfour open-source MLLMs, we have identified significant performance disparities,\nhighlighting the need for advancements in models' social understanding\ncapabilities. Our analysis reveals that, in a zero-shot setting, various types\nof MLLMs generally exhibit difficulties in handling social media tasks.\nHowever, MLLMs demonstrate performance improvements post fine-tuning,\nsuggesting potential pathways for improvement. Our code and data are available\nat https://github.com/claws-lab/MMSoc.git.\n","authors":["Yiqiao Jin","Minje Choi","Gaurav Verma","Jindong Wang","Srijan Kumar"],"pdf_url":"https://arxiv.org/pdf/2402.14154v2.pdf","comment":"In Proceedings of ACL 2024"},{"id":"http://arxiv.org/abs/2407.17339v1","updated":"2024-07-24T15:04:00Z","published":"2024-07-24T15:04:00Z","title":"Preliminary study on artificial intelligence methods for cybersecurity\n  threat detection in computer networks based on raw data packets","summary":"  Most of the intrusion detection methods in computer networks are based on\ntraffic flow characteristics. However, this approach may not fully exploit the\npotential of deep learning algorithms to directly extract features and patterns\nfrom raw packets. Moreover, it impedes real-time monitoring due to the\nnecessity of waiting for the processing pipeline to complete and introduces\ndependencies on additional software components.\n  In this paper, we investigate deep learning methodologies capable of\ndetecting attacks in real-time directly from raw packet data within network\ntraffic. We propose a novel approach where packets are stacked into windows and\nseparately recognised, with a 2D image representation suitable for processing\nwith computer vision models. Our investigation utilizes the CIC IDS-2017\ndataset, which includes both benign traffic and prevalent real-world attacks,\nproviding a comprehensive foundation for our research.\n","authors":["Aleksander Ogonowski","Michał Żebrowski","Arkadiusz Ćwiek","Tobiasz Jarosiewicz","Konrad Klimaszewski","Adam Padee","Piotr Wasiuk","Michał Wójcik"],"pdf_url":"https://arxiv.org/pdf/2407.17339v1.pdf","comment":"Submitted to Computer Science Journal"},{"id":"http://arxiv.org/abs/2407.17336v1","updated":"2024-07-24T15:02:09Z","published":"2024-07-24T15:02:09Z","title":"Cascaded Light Propagation Volumes using Spherical Radial Basis\n  Functions","summary":"  This paper introduces a contribution made to one of the newest methods for\nsimulating indirect lighting in dynamic scenes , the cascaded light propagation\nvolumes . Our contribution consists on using Spherical Radial Basis Functions\ninstead of Spherical Harmonic, since the first achieves much better results\nwhen many coefficients are used. We explain how to integrate the Spherical\nRadial Basis Functions with the cascaded light propagation volumes, and\nevaluate our technique against the same implementation, but with Spherical\nharmonics.\n","authors":["Ludovic Silvestre","João Pereira"],"pdf_url":"https://arxiv.org/pdf/2407.17336v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17331v1","updated":"2024-07-24T14:54:16Z","published":"2024-07-24T14:54:16Z","title":"Multi-label Cluster Discrimination for Visual Representation Learning","summary":"  Contrastive Language Image Pre-training (CLIP) has recently demonstrated\nsuccess across various tasks due to superior feature representation empowered\nby image-text contrastive learning. However, the instance discrimination method\nused by CLIP can hardly encode the semantic structure of training data. To\nhandle this limitation, cluster discrimination has been proposed through\niterative cluster assignment and classification. Nevertheless, most cluster\ndiscrimination approaches only define a single pseudo-label for each image,\nneglecting multi-label signals in the image. In this paper, we propose a novel\nMulti-Label Cluster Discrimination method named MLCD to enhance representation\nlearning. In the clustering step, we first cluster the large-scale LAION-400M\ndataset into one million centers based on off-the-shelf embedding features.\nConsidering that natural images frequently contain multiple visual objects or\nattributes, we select the multiple closest centers as auxiliary class labels.\nIn the discrimination step, we design a novel multi-label classification loss,\nwhich elegantly separates losses from positive classes and negative classes,\nand alleviates ambiguity on decision boundary. We validate the proposed\nmulti-label cluster discrimination method with experiments on different scales\nof models and pre-training datasets. Experimental results show that our method\nachieves state-of-the-art performance on multiple downstream tasks including\nlinear probe, zero-shot classification, and image-text retrieval.\n","authors":["Xiang An","Kaicheng Yang","Xiangzi Dai","Ziyong Feng","Jiankang Deng"],"pdf_url":"https://arxiv.org/pdf/2407.17331v1.pdf","comment":"Accepted by ECCV2024"},{"id":"http://arxiv.org/abs/2407.17328v1","updated":"2024-07-24T14:52:18Z","published":"2024-07-24T14:52:18Z","title":"DarSwin-Unet: Distortion Aware Encoder-Decoder Architecture","summary":"  Wide-angle fisheye images are becoming increasingly common for perception\ntasks in applications such as robotics, security, and mobility (e.g. drones,\navionics). However, current models often either ignore the distortions in\nwide-angle images or are not suitable to perform pixel-level tasks. In this\npaper, we present an encoder-decoder model based on a radial transformer\narchitecture that adapts to distortions in wide-angle lenses by leveraging the\nphysical characteristics defined by the radial distortion profile. In contrast\nto the original model, which only performs classification tasks, we introduce a\nU-Net architecture, DarSwin-Unet, designed for pixel level tasks. Furthermore,\nwe propose a novel strategy that minimizes sparsity when sampling the image for\ncreating its input tokens. Our approach enhances the model capability to handle\npixel-level tasks in wide-angle fisheye images, making it more effective for\nreal-world applications. Compared to other baselines, DarSwin-Unet achieves the\nbest results across different datasets, with significant gains when trained on\nbounded levels of distortions (very low, low, medium, and high) and tested on\nall, including out-of-distribution distortions. We demonstrate its performance\non depth estimation and show through extensive experiments that DarSwin-Unet\ncan perform zero-shot adaptation to unseen distortions of different wide-angle\nlenses.\n","authors":["Akshaya Athwale","Ichrak Shili","Émile Bergeron","Ola Ahmad","Jean-François Lalonde"],"pdf_url":"https://arxiv.org/pdf/2407.17328v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17324v1","updated":"2024-07-24T14:48:40Z","published":"2024-07-24T14:48:40Z","title":"Enhanced Deep Learning Methodologies and MRI Selection Techniques for\n  Dementia Diagnosis in the Elderly Population","summary":"  Dementia, a debilitating neurological condition affecting millions worldwide,\npresents significant diagnostic challenges. In this work, we introduce a novel\nmethodology for the classification of demented and non-demented elderly\npatients using 3D brain Magnetic Resonance Imaging (MRI) scans. Our approach\nfeatures a unique technique for selectively processing MRI slices, focusing on\nthe most relevant brain regions and excluding less informative sections. This\nmethodology is complemented by a confidence-based classification committee\ncomposed of three custom deep learning models: Dem3D ResNet, Dem3D CNN, and\nDem3D EfficientNet. These models work synergistically to enhance\ndecision-making accuracy, leveraging their collective strengths. Tested on the\nOpen Access Series of Imaging Studies(OASIS) dataset, our method achieved an\nimpressive accuracy of 94.12%, surpassing existing methodologies. Furthermore,\nvalidation on the Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset\nconfirmed the robustness and generalizability of our approach. The use of\nexplainable AI (XAI) techniques and comprehensive ablation studies further\nsubstantiate the effectiveness of our techniques, providing insights into the\ndecision-making process and the importance of our methodology. This research\noffers a significant advancement in dementia diagnosis, providing a highly\naccurate and efficient tool for clinical applications.\n","authors":["Nikolaos Ntampakis","Konstantinos Diamantaras","Ioanna Chouvarda","Vasileios Argyriou","Panagiotis Sarigianndis"],"pdf_url":"https://arxiv.org/pdf/2407.17324v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17312v1","updated":"2024-07-24T14:29:05Z","published":"2024-07-24T14:29:05Z","title":"Physical Adversarial Attack on Monocular Depth Estimation via\n  Shape-Varying Patches","summary":"  Adversarial attacks against monocular depth estimation (MDE) systems pose\nsignificant challenges, particularly in safety-critical applications such as\nautonomous driving. Existing patch-based adversarial attacks for MDE are\nconfined to the vicinity of the patch, making it difficult to affect the entire\ntarget. To address this limitation, we propose a physics-based adversarial\nattack on monocular depth estimation, employing a framework called Attack with\nShape-Varying Patches (ASP), aiming to optimize patch content, shape, and\nposition to maximize effectiveness. We introduce various mask shapes, including\nquadrilateral, rectangular, and circular masks, to enhance the flexibility and\nefficiency of the attack. Furthermore, we propose a new loss function to extend\nthe influence of the patch beyond the overlapping regions. Experimental results\ndemonstrate that our attack method generates an average depth error of 18\nmeters on the target car with a patch area of 1/9, affecting over 98\\% of the\ntarget area.\n","authors":["Chenxing Zhao","Yang Li","Shihao Wu","Wenyi Tan","Shuangju Zhou","Quan Pan"],"pdf_url":"https://arxiv.org/pdf/2407.17312v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17310v1","updated":"2024-07-24T14:22:55Z","published":"2024-07-24T14:22:55Z","title":"LangOcc: Self-Supervised Open Vocabulary Occupancy Estimation via Volume\n  Rendering","summary":"  Semantic occupancy has recently gained significant traction as a prominent\nmethod for 3D scene representation. However, most existing camera-based methods\nrely on costly datasets with fine-grained 3D voxel labels or LiDAR scans for\ntraining, which limits their practicality and scalability, raising the need for\nself-supervised approaches in this domain. Moreover, most methods are tied to a\npredefined set of classes which they can detect. In this work we present a\nnovel approach for open vocabulary occupancy estimation called\n\\textit{LangOcc}, that is trained only via camera images, and can detect\narbitrary semantics via vision-language alignment. In particular, we distill\nthe knowledge of the strong vision-language aligned encoder CLIP into a 3D\noccupancy model via differentiable volume rendering. Our model estimates\nvision-language aligned features in a 3D voxel grid using only images. It is\ntrained in a self-supervised manner by rendering our estimations back to 2D\nspace, where ground-truth features can be computed. This training mechanism\nautomatically supervises the scene geometry, allowing for a straight-forward\nand powerful training method without any explicit geometry supervision. LangOcc\noutperforms LiDAR-supervised competitors in open vocabulary occupancy by a\nlarge margin, solely relying on vision-based training. We also achieve\nstate-of-the-art results in self-supervised semantic occupancy estimation on\nthe Occ3D-nuScenes dataset, despite not being limited to a specific set of\ncategories, thus demonstrating the effectiveness of our proposed\nvision-language training.\n","authors":["Simon Boeder","Fabian Gigengack","Benjamin Risse"],"pdf_url":"https://arxiv.org/pdf/2407.17310v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.09920v2","updated":"2024-07-24T14:11:17Z","published":"2024-07-13T15:28:15Z","title":"MutDet: Mutually Optimizing Pre-training for Remote Sensing Object\n  Detection","summary":"  Detection pre-training methods for the DETR series detector have been\nextensively studied in natural scenes, e.g., DETReg. However, the detection\npre-training remains unexplored in remote sensing scenes. In existing\npre-training methods, alignment between object embeddings extracted from a\npre-trained backbone and detector features is significant. However, due to\ndifferences in feature extraction methods, a pronounced feature discrepancy\nstill exists and hinders the pre-training performance. The remote sensing\nimages with complex environments and more densely distributed objects\nexacerbate the discrepancy. In this work, we propose a novel Mutually\noptimizing pre-training framework for remote sensing object Detection, dubbed\nas MutDet. In MutDet, we propose a systemic solution against this challenge.\nFirstly, we propose a mutual enhancement module, which fuses the object\nembeddings and detector features bidirectionally in the last encoder layer,\nenhancing their information interaction.Secondly, contrastive alignment loss is\nemployed to guide this alignment process softly and simultaneously enhances\ndetector features' discriminativity. Finally, we design an auxiliary siamese\nhead to mitigate the task gap arising from the introduction of enhancement\nmodule. Comprehensive experiments on various settings show new state-of-the-art\ntransfer performance. The improvement is particularly pronounced when data\nquantity is limited. When using 10% of the DIOR-R data, MutDet improves DetReg\nby 6.1% in AP50. Codes and models are available at:\nhttps://github.com/floatingstarZ/MutDet.\n","authors":["Ziyue Huang","Yongchao Feng","Qingjie Liu","Yunhong Wang"],"pdf_url":"https://arxiv.org/pdf/2407.09920v2.pdf","comment":"14 pages, 4 figures; Accept to ECCV 2024"},{"id":"http://arxiv.org/abs/2407.17291v1","updated":"2024-07-24T14:02:20Z","published":"2024-07-24T14:02:20Z","title":"How Good (Or Bad) Are LLMs at Detecting Misleading Visualizations?","summary":"  In this study, we address the growing issue of misleading charts, a prevalent\nproblem that undermines the integrity of information dissemination. Misleading\ncharts can distort the viewer's perception of data, leading to\nmisinterpretations and decisions based on false information. The development of\neffective automatic detection methods for misleading charts is an urgent field\nof research. The recent advancement of multimodal Large Language Models (LLMs)\nhas introduced a promising direction for addressing this challenge. We explored\nthe capabilities of these models in analyzing complex charts and assessing the\nimpact of different prompting strategies on the models' analyses. We utilized a\ndataset of misleading charts collected from the internet by prior research and\ncrafted nine distinct prompts, ranging from simple to complex, to test the\nability of four different multimodal LLMs in detecting over 21 different chart\nissues. Through three experiments--from initial exploration to detailed\nanalysis--we progressively gained insights into how to effectively prompt LLMs\nto identify misleading charts and developed strategies to address the\nscalability challenges encountered as we expanded our detection range from the\ninitial five issues to 21 issues in the final experiment. Our findings reveal\nthat multimodal LLMs possess a strong capability for chart comprehension and\ncritical thinking in data interpretation. There is significant potential in\nemploying multimodal LLMs to counter misleading information by supporting\ncritical thinking and enhancing visualization literacy. This study demonstrates\nthe applicability of LLMs in addressing the pressing concern of misleading\ncharts.\n","authors":["Leo Yu-Ho Lo","Huamin Qu"],"pdf_url":"https://arxiv.org/pdf/2407.17291v1.pdf","comment":"To be presented at IEEE VIS 2024"},{"id":"http://arxiv.org/abs/2212.00749v2","updated":"2024-07-24T14:00:50Z","published":"2022-12-01T18:35:03Z","title":"Multimodal Query-guided Object Localization","summary":"  Consider a scenario in one-shot query-guided object localization where\nneither an image of the object nor the object category name is available as a\nquery. In such a scenario, a hand-drawn sketch of the object could be a choice\nfor a query. However, hand-drawn crude sketches alone, when used as queries,\nmight be ambiguous for object localization, e.g., a sketch of a laptop could be\nconfused for a sofa. On the other hand, a linguistic definition of the\ncategory, e.g., a small portable computer small enough to use in your lap\"\nalong with the sketch query, gives better visual and semantic cues for object\nlocalization. In this work, we present a multimodal query-guided object\nlocalization approach under the challenging open-set setting. In particular, we\nuse queries from two modalities, namely, hand-drawn sketch and description of\nthe object (also known as gloss), to perform object localization. Multimodal\nquery-guided object localization is a challenging task, especially when a large\ndomain gap exists between the queries and the natural images, as well as due to\nthe challenge of combining the complementary and minimal information present\nacross the queries. For example, hand-drawn crude sketches contain abstract\nshape information of an object, while the text descriptions often capture\npartial semantic information about a given object category. To address the\naforementioned challenges, we present a novel cross-modal attention scheme that\nguides the region proposal network to generate object proposals relevant to the\ninput queries and a novel orthogonal projection-based proposal scoring\ntechnique that scores each proposal with respect to the queries, thereby\nyielding the final localization results. ...\n","authors":["Aditay Tripathi","Rajath R Dani","Anand Mishra","Anirban Chakraborty"],"pdf_url":"https://arxiv.org/pdf/2212.00749v2.pdf","comment":"Accepted to MMTA"},{"id":"http://arxiv.org/abs/2311.17135v4","updated":"2024-07-24T13:55:48Z","published":"2023-11-28T18:54:16Z","title":"TLControl: Trajectory and Language Control for Human Motion Synthesis","summary":"  Controllable human motion synthesis is essential for applications in AR/VR,\ngaming and embodied AI. Existing methods often focus solely on either language\nor full trajectory control, lacking precision in synthesizing motions aligned\nwith user-specified trajectories, especially for multi-joint control. To\naddress these issues, we present TLControl, a novel method for realistic human\nmotion synthesis, incorporating both low-level Trajectory and high-level\nLanguage semantics controls, through the integration of neural-based and\noptimization-based techniques. Specifically, we begin with training a VQ-VAE\nfor a compact and well-structured latent motion space organized by body parts.\nWe then propose a Masked Trajectories Transformer (MTT) for predicting a motion\ndistribution conditioned on language and trajectory. Once trained, we use MTT\nto sample initial motion predictions given user-specified partial trajectories\nand text descriptions as conditioning. Finally, we introduce a test-time\noptimization to refine these coarse predictions for precise trajectory control,\nwhich offers flexibility by allowing users to specify various optimization\ngoals and ensures high runtime efficiency. Comprehensive experiments show that\nTLControl significantly outperforms the state-of-the-art in trajectory accuracy\nand time efficiency, making it practical for interactive and high-quality\nanimation generation.\n","authors":["Weilin Wan","Zhiyang Dou","Taku Komura","Wenping Wang","Dinesh Jayaraman","Lingjie Liu"],"pdf_url":"https://arxiv.org/pdf/2311.17135v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.03135v6","updated":"2024-07-24T13:50:56Z","published":"2023-08-06T15:05:42Z","title":"EventBind: Learning a Unified Representation to Bind Them All for\n  Event-based Open-world Understanding","summary":"  In this paper, we propose EventBind, a novel and effective framework that\nunleashes the potential of vision-language models (VLMs) for event-based\nrecognition to compensate for the lack of large-scale event-based datasets. In\nparticular, due to the distinct modality gap with the image-text data and the\nlack of large-scale datasets, learning a common representation space for\nimages, texts, and events is non-trivial.Intuitively, we need to address two\nkey challenges: 1) how to generalize CLIP's visual encoder to event data while\nfully leveraging events' unique properties, e.g., sparsity and high temporal\nresolution; 2) how to effectively align the multi-modal embeddings, i.e.,\nimage, text, and events. Accordingly, we first introduce a novel event encoder\nthat subtly models the temporal information from events and meanwhile,\ngenerates event prompts for modality bridging. We then design a text encoder\nthat generates content prompts and utilizes hybrid text prompts to enhance\nEventBind's generalization ability across diverse datasets.With the proposed\nevent encoder, text encoder, and image encoder, a novel Hierarchical Triple\nContrastive Alignment (HTCA) module is introduced to jointly optimize the\ncorrelation and enable efficient knowledge transfer among the three modalities.\nWe evaluate various settings, including fine-tuning and few-shot on three\nbenchmarks, and our EventBind achieves new state-of-the-art accuracy compared\nwith the previous methods, such as on N-Caltech101 (+5.34% and +1.70%) and\nN-Imagenet (+5.65% and +1.99%) with fine-tuning and 20-shot settings,\nrespectively. Moreover, our EventBind can be flexibly extended to the event\nretrieval task using text or image queries, showing plausible performance.\nProject page:https://vlislab22.github.io/EventBind/.\n","authors":["Jiazhou Zhou","Xu Zheng","Yuanhuiyi Lyu","Lin Wang"],"pdf_url":"https://arxiv.org/pdf/2308.03135v6.pdf","comment":"ECCV 2024 Accepted. Camera-ready version with supplementary"},{"id":"http://arxiv.org/abs/2407.17274v1","updated":"2024-07-24T13:39:51Z","published":"2024-07-24T13:39:51Z","title":"Revolutionizing Text-to-Image Retrieval as Autoregressive Token-to-Voken\n  Generation","summary":"  Text-to-image retrieval is a fundamental task in multimedia processing,\naiming to retrieve semantically relevant cross-modal content. Traditional\nstudies have typically approached this task as a discriminative problem,\nmatching the text and image via the cross-attention mechanism (one-tower\nframework) or in a common embedding space (two-tower framework). Recently,\ngenerative cross-modal retrieval has emerged as a new research line, which\nassigns images with unique string identifiers and generates the target\nidentifier as the retrieval target. Despite its great potential, existing\ngenerative approaches are limited due to the following issues: insufficient\nvisual information in identifiers, misalignment with high-level semantics, and\nlearning gap towards the retrieval target. To address the above issues, we\npropose an autoregressive voken generation method, named AVG. AVG tokenizes\nimages into vokens, i.e., visual tokens, and innovatively formulates the\ntext-to-image retrieval task as a token-to-voken generation problem. AVG\ndiscretizes an image into a sequence of vokens as the identifier of the image,\nwhile maintaining the alignment with both the visual information and high-level\nsemantics of the image. Additionally, to bridge the learning gap between\ngenerative training and the retrieval target, we incorporate discriminative\ntraining to modify the learning direction during token-to-voken training.\nExtensive experiments demonstrate that AVG achieves superior results in both\neffectiveness and efficiency.\n","authors":["Yongqi Li","Hongru Cai","Wenjie Wang","Leigang Qu","Yinwei Wei","Wenjie Li","Liqiang Nie","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2407.17274v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2407.17272v1","updated":"2024-07-24T13:39:07Z","published":"2024-07-24T13:39:07Z","title":"DenseTrack: Drone-based Crowd Tracking via Density-aware\n  Motion-appearance Synergy","summary":"  Drone-based crowd tracking faces difficulties in accurately identifying and\nmonitoring objects from an aerial perspective, largely due to their small size\nand close proximity to each other, which complicates both localization and\ntracking. To address these challenges, we present the Density-aware Tracking\n(DenseTrack) framework. DenseTrack capitalizes on crowd counting to precisely\ndetermine object locations, blending visual and motion cues to improve the\ntracking of small-scale objects. It specifically addresses the problem of\ncross-frame motion to enhance tracking accuracy and dependability. DenseTrack\nemploys crowd density estimates as anchors for exact object localization within\nvideo frames. These estimates are merged with motion and position information\nfrom the tracking network, with motion offsets serving as key tracking cues.\nMoreover, DenseTrack enhances the ability to distinguish small-scale objects\nusing insights from the visual-language model, integrating appearance with\nmotion cues. The framework utilizes the Hungarian algorithm to ensure the\naccurate matching of individuals across frames. Demonstrated on DroneCrowd\ndataset, our approach exhibits superior performance, confirming its\neffectiveness in scenarios captured by drones.\n","authors":["Yi Lei","Huilin Zhu","Jingling Yuan","Guangli Xiang","Xian Zhong","Shengfeng He"],"pdf_url":"https://arxiv.org/pdf/2407.17272v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.12842v2","updated":"2024-07-24T13:38:33Z","published":"2023-09-22T12:59:39Z","title":"SRFNet: Monocular Depth Estimation with Fine-grained Structure via\n  Spatial Reliability-oriented Fusion of Frames and Events","summary":"  Monocular depth estimation is a crucial task to measure distance relative to\na camera, which is important for applications, such as robot navigation and\nself-driving. Traditional frame-based methods suffer from performance drops due\nto the limited dynamic range and motion blur. Therefore, recent works leverage\nnovel event cameras to complement or guide the frame modality via frame-event\nfeature fusion. However, event streams exhibit spatial sparsity, leaving some\nareas unperceived, especially in regions with marginal light changes.\nTherefore, direct fusion methods, e.g., RAMNet, often ignore the contribution\nof the most confident regions of each modality. This leads to structural\nambiguity in the modality fusion process, thus degrading the depth estimation\nperformance. In this paper, we propose a novel Spatial Reliability-oriented\nFusion Network (SRFNet), that can estimate depth with fine-grained structure at\nboth daytime and nighttime. Our method consists of two key technical\ncomponents. Firstly, we propose an attention-based interactive fusion (AIF)\nmodule that applies spatial priors of events and frames as the initial masks\nand learns the consensus regions to guide the inter-modal feature fusion. The\nfused feature are then fed back to enhance the frame and event feature\nlearning. Meanwhile, it utilizes an output head to generate a fused mask, which\nis iteratively updated for learning consensual spatial priors. Secondly, we\npropose the Reliability-oriented Depth Refinement (RDR) module to estimate\ndense depth with the fine-grained structure based on the fused features and\nmasks. We evaluate the effectiveness of our method on the synthetic and\nreal-world datasets, which shows that, even without pretraining, our method\noutperforms the prior methods, e.g., RAMNet, especially in night scenes. Our\nproject homepage: https://vlislab22.github.io/SRFNet.\n","authors":["Tianbo Pan","Zidong Cao","Lin Wang"],"pdf_url":"https://arxiv.org/pdf/2309.12842v2.pdf","comment":"Accepted to ICRA 2024"},{"id":"http://arxiv.org/abs/2407.17267v1","updated":"2024-07-24T13:30:46Z","published":"2024-07-24T13:30:46Z","title":"M4: Multi-Proxy Multi-Gate Mixture of Experts Network for Multiple\n  Instance Learning in Histopathology Image Analysis","summary":"  Multiple instance learning (MIL) has been successfully applied for whole\nslide images (WSIs) analysis in computational pathology, enabling a wide range\nof prediction tasks from tumor subtyping to inferring genetic mutations and\nmulti-omics biomarkers. However, existing MIL methods predominantly focus on\nsingle-task learning, resulting in not only overall low efficiency but also the\noverlook of inter-task relatedness. To address these issues, we proposed an\nadapted architecture of Multi-gate Mixture-of-experts with Multi-proxy for\nMultiple instance learning (M4), and applied this framework for simultaneous\nprediction of multiple genetic mutations from WSIs. The proposed M4 model has\ntwo main innovations: (1) utilizing a mixture of experts with multiple gating\nstrategies for multi-genetic mutation prediction on a single pathological\nslide; (2) constructing multi-proxy expert network and gate network for\ncomprehensive and effective modeling of pathological image information. Our\nmodel achieved significant improvements across five tested TCGA datasets in\ncomparison to current state-of-the-art single-task methods. The code is\navailable at:https://github.com/Bigyehahaha/M4.\n","authors":["Junyu Li","Ye Zhang","Wen Shu","Xiaobing Feng","Yingchun Wang","Pengju Yan","Xiaolin Li","Chulin Sha","Min He"],"pdf_url":"https://arxiv.org/pdf/2407.17267v1.pdf","comment":"25pages,5figures"},{"id":"http://arxiv.org/abs/2407.17265v1","updated":"2024-07-24T13:29:17Z","published":"2024-07-24T13:29:17Z","title":"SCIsegV2: A Universal Tool for Segmentation of Intramedullary Lesions in\n  Spinal Cord Injury","summary":"  Spinal cord injury (SCI) is a devastating incidence leading to permanent\nparalysis and loss of sensory-motor functions potentially resulting in the\nformation of lesions within the spinal cord. Imaging biomarkers obtained from\nmagnetic resonance imaging (MRI) scans can predict the functional recovery of\nindividuals with SCI and help choose the optimal treatment strategy. Currently,\nmost studies employ manual quantification of these MRI-derived biomarkers,\nwhich is a subjective and tedious task. In this work, we propose (i) a\nuniversal tool for the automatic segmentation of intramedullary SCI lesions,\ndubbed \\texttt{SCIsegV2}, and (ii) a method to automatically compute the width\nof the tissue bridges from the segmented lesion. Tissue bridges represent the\nspared spinal tissue adjacent to the lesion, which is associated with\nfunctional recovery in SCI patients. The tool was trained and validated on a\nheterogeneous dataset from 7 sites comprising patients from different SCI\nphases (acute, sub-acute, and chronic) and etiologies (traumatic SCI, ischemic\nSCI, and degenerative cervical myelopathy). Tissue bridges quantified\nautomatically did not significantly differ from those computed manually,\nsuggesting that the proposed automatic tool can be used to derive relevant MRI\nbiomarkers. \\texttt{SCIsegV2} and the automatic tissue bridges computation are\nopen-source and available in Spinal Cord Toolbox (v6.4 and above) via the\n\\texttt{sct\\_deepseg -task seg\\_sc\\_lesion\\_t2w\\_sci} and\n\\texttt{sct\\_analyze\\_lesion} functions, respectively.\n","authors":["Enamundram Naga Karthik","Jan Valošek","Lynn Farner","Dario Pfyffer","Simon Schading-Sassenhausen","Anna Lebret","Gergely David","Andrew C. Smith","Kenneth A. Weber II","Maryam Seif","RHSCIR Network Imaging Group","Patrick Freund","Julien Cohen-Adad"],"pdf_url":"https://arxiv.org/pdf/2407.17265v1.pdf","comment":"Accepted at MICCAI AMAI 2024 workshop"},{"id":"http://arxiv.org/abs/2407.17261v1","updated":"2024-07-24T13:24:25Z","published":"2024-07-24T13:24:25Z","title":"Embedding-Free Transformer with Inference Spatial Reduction for\n  Efficient Semantic Segmentation","summary":"  We present an Encoder-Decoder Attention Transformer, EDAFormer, which\nconsists of the Embedding-Free Transformer (EFT) encoder and the all-attention\ndecoder leveraging our Embedding-Free Attention (EFA) structure. The proposed\nEFA is a novel global context modeling mechanism that focuses on functioning\nthe global non-linearity, not the specific roles of the query, key and value.\nFor the decoder, we explore the optimized structure for considering the\nglobality, which can improve the semantic segmentation performance. In\naddition, we propose a novel Inference Spatial Reduction (ISR) method for the\ncomputational efficiency. Different from the previous spatial reduction\nattention methods, our ISR method further reduces the key-value resolution at\nthe inference phase, which can mitigate the computation-performance trade-off\ngap for the efficient semantic segmentation. Our EDAFormer shows the\nstate-of-the-art performance with the efficient computation compared to the\nexisting transformer-based semantic segmentation models in three public\nbenchmarks, including ADE20K, Cityscapes and COCO-Stuff. Furthermore, our ISR\nmethod reduces the computational cost by up to 61% with minimal mIoU\nperformance degradation on Cityscapes dataset. The code is available at\nhttps://github.com/hyunwoo137/EDAFormer.\n","authors":["Hyunwoo Yu","Yubin Cho","Beoungwoo Kang","Seunghun Moon","Kyeongbo Kong","Suk-Ju Kang"],"pdf_url":"https://arxiv.org/pdf/2407.17261v1.pdf","comment":"Accepted by ECCV 2024"},{"id":"http://arxiv.org/abs/2405.15700v2","updated":"2024-07-24T13:17:10Z","published":"2024-05-24T16:44:22Z","title":"Trackastra: Transformer-based cell tracking for live-cell microscopy","summary":"  Cell tracking is a ubiquitous image analysis task in live-cell microscopy.\nUnlike multiple object tracking (MOT) for natural images, cell tracking\ntypically involves hundreds of similar-looking objects that can divide in each\nframe, making it a particularly challenging problem. Current state-of-the-art\napproaches follow the tracking-by-detection paradigm, i.e. first all cells are\ndetected per frame and successively linked in a second step to form\nbiologically consistent cell tracks. Linking is commonly solved via discrete\noptimization methods, which require manual tuning of hyperparameters for each\ndataset and are therefore cumbersome to use in practice. Here we propose\nTrackastra, a general purpose cell tracking approach that uses a simple\ntransformer architecture to directly learn pairwise associations of cells\nwithin a temporal window from annotated data. Importantly, unlike existing\ntransformer-based MOT pipelines, our learning architecture also accounts for\ndividing objects such as cells and allows for accurate tracking even with\nsimple greedy linking, thus making strides towards removing the requirement for\na complex linking step. The proposed architecture operates on the full\nspatio-temporal context of detections within a time window by avoiding the\ncomputational burden of processing dense images. We show that our tracking\napproach performs on par with or better than highly tuned state-of-the-art cell\ntracking algorithms for various biological datasets, such as bacteria, cell\ncultures and fluorescent particles. We provide code at\nhttps://github.com/weigertlab/trackastra.\n","authors":["Benjamin Gallusser","Martin Weigert"],"pdf_url":"https://arxiv.org/pdf/2405.15700v2.pdf","comment":"Accepted at ECCV 2024"},{"id":"http://arxiv.org/abs/2304.09691v5","updated":"2024-07-24T13:17:07Z","published":"2023-04-19T14:32:56Z","title":"DarSwin: Distortion Aware Radial Swin Transformer","summary":"  Wide-angle lenses are commonly used in perception tasks requiring a large\nfield of view. Unfortunately, these lenses produce significant distortions,\nmaking conventional models that ignore the distortion effects unable to adapt\nto wide-angle images. In this paper, we present a novel transformer-based model\nthat automatically adapts to the distortion produced by wide-angle lenses. Our\nproposed image encoder architecture, dubbed DarSwin, leverages the physical\ncharacteristics of such lenses analytically defined by the radial distortion\nprofile. In contrast to conventional transformer-based architectures, DarSwin\ncomprises a radial patch partitioning, a distortion-based sampling technique\nfor creating token embeddings, and an angular position encoding for radial\npatch merging. Compared to other baselines, DarSwin achieves the best results\non different datasets with significant gains when trained on bounded levels of\ndistortions (very low, low, medium, and high) and tested on all, including\nout-of-distribution distortions. While the base DarSwin architecture requires\nknowledge of the radial distortion profile, we show it can be combined with a\nself-calibration network that estimates such a profile from the input image\nitself, resulting in a completely uncalibrated pipeline. Finally, we also\npresent DarSwin-Unet, which extends DarSwin, to an encoder-decoder architecture\nsuitable for pixel-level tasks. We demonstrate its performance on depth\nestimation and show through extensive experiments that DarSwin-Unet can perform\nzero-shot adaptation to unseen distortions of different wide-angle lenses. The\ncode and models are publicly available at https://lvsn.github.io/darswin/\n","authors":["Akshaya Athwale","Arman Afrasiyabi","Justin Lagüe","Ichrak Shili","Ola Ahmad","Jean-François Lalonde"],"pdf_url":"https://arxiv.org/pdf/2304.09691v5.pdf","comment":"18 pages, 12 figures"},{"id":"http://arxiv.org/abs/2311.16037v2","updated":"2024-07-24T13:16:38Z","published":"2023-11-27T17:58:21Z","title":"GaussianEditor: Editing 3D Gaussians Delicately with Text Instructions","summary":"  Recently, impressive results have been achieved in 3D scene editing with text\ninstructions based on a 2D diffusion model. However, current diffusion models\nprimarily generate images by predicting noise in the latent space, and the\nediting is usually applied to the whole image, which makes it challenging to\nperform delicate, especially localized, editing for 3D scenes. Inspired by\nrecent 3D Gaussian splatting, we propose a systematic framework, named\nGaussianEditor, to edit 3D scenes delicately via 3D Gaussians with text\ninstructions. Benefiting from the explicit property of 3D Gaussians, we design\na series of techniques to achieve delicate editing. Specifically, we first\nextract the region of interest (RoI) corresponding to the text instruction,\naligning it to 3D Gaussians. The Gaussian RoI is further used to control the\nediting process. Our framework can achieve more delicate and precise editing of\n3D scenes than previous methods while enjoying much faster training speed, i.e.\nwithin 20 minutes on a single V100 GPU, more than twice as fast as\nInstruct-NeRF2NeRF (45 minutes -- 2 hours).\n","authors":["Junjie Wang","Jiemin Fang","Xiaopeng Zhang","Lingxi Xie","Qi Tian"],"pdf_url":"https://arxiv.org/pdf/2311.16037v2.pdf","comment":"CVPR 2024, Project page: https://GaussianEditor.github.io"},{"id":"http://arxiv.org/abs/2407.13421v2","updated":"2024-07-24T13:09:22Z","published":"2024-07-18T11:43:26Z","title":"CycleMix: Mixing Source Domains for Domain Generalization in\n  Style-Dependent Data","summary":"  As deep learning-based systems have become an integral part of everyday life,\nlimitations in their generalization ability have begun to emerge. Machine\nlearning algorithms typically rely on the i.i.d. assumption, meaning that their\ntraining and validation data are expected to follow the same distribution,\nwhich does not necessarily hold in practice. In the case of image\nclassification, one frequent reason that algorithms fail to generalize is that\nthey rely on spurious correlations present in training data, such as\nassociating image styles with target classes. These associations may not be\npresent in the unseen test data, leading to significant degradation of their\neffectiveness. In this work, we attempt to mitigate this Domain Generalization\n(DG) problem by training a robust feature extractor which disregards features\nattributed to image-style but infers based on style-invariant image\nrepresentations. To achieve this, we train CycleGAN models to learn the\ndifferent styles present in the training data and randomly mix them together to\ncreate samples with novel style attributes to improve generalization.\nExperimental results on the PACS DG benchmark validate the proposed method.\n","authors":["Aristotelis Ballas","Christos Diou"],"pdf_url":"https://arxiv.org/pdf/2407.13421v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07503v3","updated":"2024-07-24T13:07:26Z","published":"2024-07-10T09:41:36Z","title":"Inter and Intra Prior Learning-based Hyperspectral Image Reconstruction\n  Using Snapshot SWIR Metasurface","summary":"  Shortwave-infrared(SWIR) spectral information, ranging from 1 {\\mu}m to\n2.5{\\mu}m, overcomes the limitations of traditional color cameras in acquiring\nscene information. However, conventional SWIR hyperspectral imaging systems\nface challenges due to their bulky setups and low acquisition speeds. This work\nintroduces a snapshot SWIR hyperspectral imaging system based on a metasurface\nfilter and a corresponding filter selection method to achieve the lowest\ncorrelation coefficient among these filters. This system offers the advantages\nof compact size and snapshot imaging. We propose a novel inter and intra prior\nlearning unfolding framework to achieve high-quality SWIR hyperspectral image\nreconstruction, which bridges the gap between prior learning and cross-stage\ninformation interaction. Additionally, We design an adaptive feature transfer\nmechanism to adaptively transfer the contextual correlation of multi-scale\nencoder features to prevent detailed information loss in the decoder.\nExperiment results demonstrate that our method can reconstruct hyperspectral\nimages with high speed and superior performance over existing methods.\n","authors":["Linqiang Li","Jinglei Hao","Yongqiang Zhao","Pan Liu","Haofang Yan","Ziqin Zhang","Seong G. Kong"],"pdf_url":"https://arxiv.org/pdf/2407.07503v3.pdf","comment":"12 pages,9 figures"},{"id":"http://arxiv.org/abs/2407.16636v2","updated":"2024-07-24T13:04:19Z","published":"2024-07-23T16:52:42Z","title":"Velocity Driven Vision: Asynchronous Sensor Fusion Birds Eye View Models\n  for Autonomous Vehicles","summary":"  Fusing different sensor modalities can be a difficult task, particularly if\nthey are asynchronous. Asynchronisation may arise due to long processing times\nor improper synchronisation during calibration, and there must exist a way to\nstill utilise this previous information for the purpose of safe driving, and\nobject detection in ego vehicle/ multi-agent trajectory prediction.\nDifficulties arise in the fact that the sensor modalities have captured\ninformation at different times and also at different positions in space.\nTherefore, they are not spatially nor temporally aligned. This paper will\ninvestigate the challenge of radar and LiDAR sensors being asynchronous\nrelative to the camera sensors, for various time latencies. The spatial\nalignment will be resolved before lifting into BEV space via the transformation\nof the radar/LiDAR point clouds into the new ego frame coordinate system. Only\nafter this can we concatenate the radar/LiDAR point cloud and lifted camera\nfeatures. Temporal alignment will be remedied for radar data only, we will\nimplement a novel method of inferring the future radar point positions using\nthe velocity information. Our approach to resolving the issue of sensor\nasynchrony yields promising results. We demonstrate velocity information can\ndrastically improve IoU for asynchronous datasets, as for a time latency of 360\nmilliseconds (ms), IoU improves from 49.54 to 53.63. Additionally, for a time\nlatency of 550ms, the camera+radar (C+R) model outperforms the camera+LiDAR\n(C+L) model by 0.18 IoU. This is an advancement in utilising the\noften-neglected radar sensor modality, which is less favoured than LiDAR for\nautonomous driving purposes.\n","authors":["Seamie Hayes","Sushil Sharma","Ciarán Eising"],"pdf_url":"https://arxiv.org/pdf/2407.16636v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.04557v2","updated":"2024-07-24T12:48:47Z","published":"2023-06-07T16:04:08Z","title":"PhenoBench -- A Large Dataset and Benchmarks for Semantic Image\n  Interpretation in the Agricultural Domain","summary":"  The production of food, feed, fiber, and fuel is a key task of agriculture,\nwhich has to cope with many challenges in the upcoming decades, e.g., a higher\ndemand, climate change, lack of workers, and the availability of arable land.\nVision systems can support making better and more sustainable field management\ndecisions, but also support the breeding of new crop varieties by allowing\ntemporally dense and reproducible measurements. Recently, agricultural robotics\ngot an increasing interest in the vision and robotics communities since it is a\npromising avenue for coping with the aforementioned lack of workers and\nenabling more sustainable production. While large datasets and benchmarks in\nother domains are readily available and enable significant progress,\nagricultural datasets and benchmarks are comparably rare. We present an\nannotated dataset and benchmarks for the semantic interpretation of real\nagricultural fields. Our dataset recorded with a UAV provides high-quality,\npixel-wise annotations of crops and weeds, but also crop leaf instances at the\nsame time. Furthermore, we provide benchmarks for various tasks on a hidden\ntest set comprised of different fields: known fields covered by the training\ndata and a completely unseen field. Our dataset, benchmarks, and code are\navailable at \\url{https://www.phenobench.org}.\n","authors":["Jan Weyler","Federico Magistri","Elias Marks","Yue Linn Chong","Matteo Sodano","Gianmarco Roggiolani","Nived Chebrolu","Cyrill Stachniss","Jens Behley"],"pdf_url":"https://arxiv.org/pdf/2306.04557v2.pdf","comment":"Accepted by IEEE Transactions on Pattern Analysis and Machine\n  Intelligence (T-PAMI)"},{"id":"http://arxiv.org/abs/2407.17229v1","updated":"2024-07-24T12:32:24Z","published":"2024-07-24T12:32:24Z","title":"LPGen: Enhancing High-Fidelity Landscape Painting Generation through\n  Diffusion Model","summary":"  Generating landscape paintings expands the possibilities of artistic\ncreativity and imagination. Traditional landscape painting methods involve\nusing ink or colored ink on rice paper, which requires substantial time and\neffort. These methods are susceptible to errors and inconsistencies and lack\nprecise control over lines and colors. This paper presents LPGen, a\nhigh-fidelity, controllable model for landscape painting generation,\nintroducing a novel multi-modal framework that integrates image prompts into\nthe diffusion model. We extract its edges and contours by computing canny edges\nfrom the target landscape image. These, along with natural language text\nprompts and drawing style references, are fed into the latent diffusion model\nas conditions. We implement a decoupled cross-attention strategy to ensure\ncompatibility between image and text prompts, facilitating multi-modal image\ngeneration. A decoder generates the final image. Quantitative and qualitative\nanalyses demonstrate that our method outperforms existing approaches in\nlandscape painting generation and exceeds the current state-of-the-art. The\nLPGen network effectively controls the composition and color of landscape\npaintings, generates more accurate images, and supports further research in\ndeep learning-based landscape painting generation.\n","authors":["Wanggong Yang","Xiaona Wang","Yingrui Qiu","Yifei Zhao"],"pdf_url":"https://arxiv.org/pdf/2407.17229v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.03678v3","updated":"2024-07-24T12:23:52Z","published":"2023-12-06T18:41:01Z","title":"Hybrid Functional Maps for Crease-Aware Non-Isometric Shape Matching","summary":"  Non-isometric shape correspondence remains a fundamental challenge in\ncomputer vision. Traditional methods using Laplace-Beltrami operator (LBO)\neigenmodes face limitations in characterizing high-frequency extrinsic shape\nchanges like bending and creases. We propose a novel approach of combining the\nnon-orthogonal extrinsic basis of eigenfunctions of the elastic thin-shell\nhessian with the intrinsic ones of the LBO, creating a hybrid spectral space in\nwhich we construct functional maps. To this end, we present a theoretical\nframework to effectively integrate non-orthogonal basis functions into\ndescriptor- and learning-based functional map methods. Our approach can be\nincorporated easily into existing functional map pipelines across varying\napplications and is able to handle complex deformations beyond isometries. We\nshow extensive evaluations across various supervised and unsupervised settings\nand demonstrate significant improvements. Notably, our approach achieves up to\n15% better mean geodesic error for non-isometric correspondence settings and up\nto 45% improvement in scenarios with topological noise.\n","authors":["Lennart Bastian","Yizheng Xie","Nassir Navab","Zorah Lähner"],"pdf_url":"https://arxiv.org/pdf/2312.03678v3.pdf","comment":"Presented at CVPR 2024. This version contains two additional figures\n  in the main paper and generalization experiments in the appendix. Please cite\n  the official IEEE CVPR publication"},{"id":"http://arxiv.org/abs/2402.02333v2","updated":"2024-07-24T12:23:41Z","published":"2024-02-04T04:00:33Z","title":"Copyright Protection in Generative AI: A Technical Perspective","summary":"  Generative AI has witnessed rapid advancement in recent years, expanding\ntheir capabilities to create synthesized content such as text, images, audio,\nand code. The high fidelity and authenticity of contents generated by these\nDeep Generative Models (DGMs) have sparked significant copyright concerns.\nThere have been various legal debates on how to effectively safeguard\ncopyrights in DGMs. This work delves into this issue by providing a\ncomprehensive overview of copyright protection from a technical perspective. We\nexamine from two distinct viewpoints: the copyrights pertaining to the source\ndata held by the data owners and those of the generative models maintained by\nthe model builders. For data copyright, we delve into methods data owners can\nprotect their content and DGMs can be utilized without infringing upon these\nrights. For model copyright, our discussion extends to strategies for\npreventing model theft and identifying outputs generated by specific models.\nFinally, we highlight the limitations of existing techniques and identify areas\nthat remain unexplored. Furthermore, we discuss prospective directions for the\nfuture of copyright protection, underscoring its importance for the sustainable\nand ethical development of Generative AI.\n","authors":["Jie Ren","Han Xu","Pengfei He","Yingqian Cui","Shenglai Zeng","Jiankun Zhang","Hongzhi Wen","Jiayuan Ding","Pei Huang","Lingjuan Lyu","Hui Liu","Yi Chang","Jiliang Tang"],"pdf_url":"https://arxiv.org/pdf/2402.02333v2.pdf","comment":"26 pages"},{"id":"http://arxiv.org/abs/2407.17219v1","updated":"2024-07-24T12:19:39Z","published":"2024-07-24T12:19:39Z","title":"Graph Neural Networks: A suitable Alternative to MLPs in Latent 3D\n  Medical Image Classification?","summary":"  Recent studies have underscored the capabilities of natural imaging\nfoundation models to serve as powerful feature extractors, even in a zero-shot\nsetting for medical imaging data. Most commonly, a shallow multi-layer\nperceptron (MLP) is appended to the feature extractor to facilitate end-to-end\nlearning and downstream prediction tasks such as classification, thus\nrepresenting the de facto standard. However, as graph neural networks (GNNs)\nhave become a practicable choice for various tasks in medical research in the\nrecent past, we direct attention to the question of how effective GNNs are\ncompared to MLP prediction heads for the task of 3D medical image\nclassification, proposing them as a potential alternative. In our experiments,\nwe devise a subject-level graph for each volumetric dataset instance. Therein\nlatent representations of all slices in the volume, encoded through a DINOv2\npretrained vision transformer (ViT), constitute the nodes and their respective\nnode features. We use public datasets to compare the classification heads\nnumerically and evaluate various graph construction and graph convolution\nmethods in our experiments. Our findings show enhancements of the GNN in\nclassification performance and substantial improvements in runtime compared to\nan MLP prediction head. Additional robustness evaluations further validate the\npromising performance of the GNN, promoting them as a suitable alternative to\ntraditional MLP classification heads. Our code is publicly available at:\nhttps://github.com/compai-lab/2024-miccai-grail-kiechle\n","authors":["Johannes Kiechle","Daniel M. Lang","Stefan M. Fischer","Lina Felsner","Jan C. Peeken","Julia A. Schnabel"],"pdf_url":"https://arxiv.org/pdf/2407.17219v1.pdf","comment":"Accepted at MICCAI 2024 - GRAIL Workshop"},{"id":"http://arxiv.org/abs/2407.17209v1","updated":"2024-07-24T12:09:07Z","published":"2024-07-24T12:09:07Z","title":"Nonverbal Immediacy Analysis in Education: A Multimodal Computational\n  Model","summary":"  This paper introduces a novel computational approach for analyzing nonverbal\nsocial behavior in educational settings. Integrating multimodal behavioral\ncues, including facial expressions, gesture intensity, and spatial dynamics,\nthe model assesses the nonverbal immediacy (NVI) of teachers from RGB classroom\nvideos. A dataset of 400 30-second video segments from German classrooms was\nconstructed for model training and validation. The gesture intensity regressor\nachieved a correlation of 0.84, the perceived distance regressor 0.55, and the\nNVI model 0.44 with median human ratings. The model demonstrates the potential\nto provide a valuable support in nonverbal behavior assessment, approximating\nthe accuracy of individual human raters. Validated against both questionnaire\ndata and trained observer ratings, our models show moderate to strong\ncorrelations with relevant educational outcomes, indicating their efficacy in\nreflecting effective teaching behaviors. This research advances the objective\nassessment of nonverbal communication behaviors, opening new pathways for\neducational research.\n","authors":["Uroš Petković","Jonas Frenkel","Olaf Hellwich","Rebecca Lazarides"],"pdf_url":"https://arxiv.org/pdf/2407.17209v1.pdf","comment":"12 pages, 3 figures. Camera-ready version for the SAB 2024: 17th\n  International Conference on the Simulation of Adaptive Behavior"},{"id":"http://arxiv.org/abs/2407.17197v1","updated":"2024-07-24T11:58:31Z","published":"2024-07-24T11:58:31Z","title":"ALPI: Auto-Labeller with Proxy Injection for 3D Object Detection using\n  2D Labels Only","summary":"  3D object detection plays a crucial role in various applications such as\nautonomous vehicles, robotics and augmented reality. However, training 3D\ndetectors requires a costly precise annotation, which is a hindrance to scaling\nannotation to large datasets. To address this challenge, we propose a weakly\nsupervised 3D annotator that relies solely on 2D bounding box annotations from\nimages, along with size priors. One major problem is that supervising a 3D\ndetection model using only 2D boxes is not reliable due to ambiguities between\ndifferent 3D poses and their identical 2D projection. We introduce a simple yet\neffective and generic solution: we build 3D proxy objects with annotations by\nconstruction and add them to the training dataset. Our method requires only\nsize priors to adapt to new classes. To better align 2D supervision with 3D\ndetection, our method ensures depth invariance with a novel expression of the\n2D losses. Finally, to detect more challenging instances, our annotator follows\nan offline pseudo-labelling scheme which gradually improves its 3D\npseudo-labels. Extensive experiments on the KITTI dataset demonstrate that our\nmethod not only performs on-par or above previous works on the Car category,\nbut also achieves performance close to fully supervised methods on more\nchallenging classes. We further demonstrate the effectiveness and robustness of\nour method by being the first to experiment on the more challenging nuScenes\ndataset. We additionally propose a setting where weak labels are obtained from\na 2D detector pre-trained on MS-COCO instead of human annotations.\n","authors":["Saad Lahlali","Nicolas Granger","Hervé Le Borgne","Quoc-Cuong Pham"],"pdf_url":"https://arxiv.org/pdf/2407.17197v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17193v1","updated":"2024-07-24T11:51:47Z","published":"2024-07-24T11:51:47Z","title":"Unpaired Photo-realistic Image Deraining with Energy-informed Diffusion\n  Model","summary":"  Existing unpaired image deraining approaches face challenges in accurately\ncapture the distinguishing characteristics between the rainy and clean domains,\nresulting in residual degradation and color distortion within the reconstructed\nimages. To this end, we propose an energy-informed diffusion model for unpaired\nphoto-realistic image deraining (UPID-EDM). Initially, we delve into the\nintricate visual-language priors embedded within the contrastive language-image\npre-training model (CLIP), and demonstrate that the CLIP priors aid in the\ndiscrimination of rainy and clean images. Furthermore, we introduce a\ndual-consistent energy function (DEF) that retains the rain-irrelevant\ncharacteristics while eliminating the rain-relevant features. This energy\nfunction is trained by the non-corresponding rainy and clean images. In\naddition, we employ the rain-relevance discarding energy function (RDEF) and\nthe rain-irrelevance preserving energy function (RPEF) to direct the reverse\nsampling procedure of a pre-trained diffusion model, effectively removing the\nrain streaks while preserving the image contents. Extensive experiments\ndemonstrate that our energy-informed model surpasses the existing unpaired\nlearning approaches in terms of both supervised and no-reference metrics.\n","authors":["Yuanbo Wen","Tao Gao","Ting Chen"],"pdf_url":"https://arxiv.org/pdf/2407.17193v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.02776v2","updated":"2024-07-24T11:48:28Z","published":"2024-06-04T20:45:53Z","title":"MeshVPR: Citywide Visual Place Recognition Using 3D Meshes","summary":"  Mesh-based scene representation offers a promising direction for simplifying\nlarge-scale hierarchical visual localization pipelines, combining a visual\nplace recognition step based on global features (retrieval) and a visual\nlocalization step based on local features. While existing work demonstrates the\nviability of meshes for visual localization, the impact of using synthetic\ndatabases rendered from them in visual place recognition remains largely\nunexplored. In this work we investigate using dense 3D textured meshes for\nlarge-scale Visual Place Recognition (VPR). We identify a significant\nperformance drop when using synthetic mesh-based image databases compared to\nreal-world images for retrieval. To address this, we propose MeshVPR, a novel\nVPR pipeline that utilizes a lightweight features alignment framework to bridge\nthe gap between real-world and synthetic domains. MeshVPR leverages pre-trained\nVPR models and is efficient and scalable for city-wide deployments. We\nintroduce novel datasets with freely available 3D meshes and manually collected\nqueries from Berlin, Paris, and Melbourne. Extensive evaluations demonstrate\nthat MeshVPR achieves competitive performance with standard VPR pipelines,\npaving the way for mesh-based localization systems. Data, code, and interactive\nvisualizations are available at https://meshvpr.github.io/\n","authors":["Gabriele Berton","Lorenz Junglas","Riccardo Zaccone","Thomas Pollok","Barbara Caputo","Carlo Masone"],"pdf_url":"https://arxiv.org/pdf/2406.02776v2.pdf","comment":"Website: https://mesh-vpr.github.io/"},{"id":"http://arxiv.org/abs/2407.17181v1","updated":"2024-07-24T11:32:33Z","published":"2024-07-24T11:32:33Z","title":"Trans2Unet: Neural fusion for Nuclei Semantic Segmentation","summary":"  Nuclei segmentation, despite its fundamental role in histopathological image\nanalysis, is still a challenge work. The main challenge of this task is the\nexistence of overlapping areas, which makes separating independent nuclei more\ncomplicated. In this paper, we propose a new two-branch architecture by\ncombining the Unet and TransUnet networks for nuclei segmentation task. In the\nproposed architecture, namely Trans2Unet, the input image is first sent into\nthe Unet branch whose the last convolution layer is removed. This branch makes\nthe network combine features from different spatial regions of the input image\nand localizes more precisely the regions of interest. The input image is also\nfed into the second branch. In the second branch, which is called TransUnet\nbranch, the input image will be divided into patches of images. With Vision\ntransformer (ViT) in architecture, TransUnet can serve as a powerful encoder\nfor medical image segmentation tasks and enhance image details by recovering\nlocalized spatial information. To boost up Trans2Unet efficiency and\nperformance, we proposed to infuse TransUnet with a computational-efficient\nvariation called \"Waterfall\" Atrous Spatial Pooling with Skip Connection\n(WASP-KC) module, which is inspired by the \"Waterfall\" Atrous Spatial Pooling\n(WASP) module. Experiment results on the 2018 Data Science Bowl benchmark show\nthe effectiveness and performance of the proposed architecture while compared\nwith previous segmentation models.\n","authors":["Dinh-Phu Tran","Quoc-Anh Nguyen","Van-Truong Pham","Thi-Thao Tran"],"pdf_url":"https://arxiv.org/pdf/2407.17181v1.pdf","comment":"ICCAIS 2022"},{"id":"http://arxiv.org/abs/2407.17170v1","updated":"2024-07-24T11:22:02Z","published":"2024-07-24T11:22:02Z","title":"Domain Generalized Recaptured Screen Image Identification Using SWIN\n  Transformer","summary":"  An increasing number of classification approaches have been developed to\naddress the issue of image rebroadcast and recapturing, a standard attack\nstrategy in insurance frauds, face spoofing, and video piracy. However, most of\nthem neglected scale variations and domain generalization scenarios, performing\npoorly in instances involving domain shifts, typically made worse by\ninter-domain and cross-domain scale variances. To overcome these issues, we\npropose a cascaded data augmentation and SWIN transformer domain generalization\nframework (DAST-DG) in the current research work Initially, we examine the\ndisparity in dataset representation. A feature generator is trained to make\nauthentic images from various domains indistinguishable. This process is then\napplied to recaptured images, creating a dual adversarial learning setup.\nExtensive experiments demonstrate that our approach is practical and surpasses\nstate-of-the-art methods across different databases. Our model achieves an\naccuracy of approximately 82\\% with a precision of 95\\% on high-variance\ndatasets.\n","authors":["Preeti Mehta","Aman Sagar","Suchi Kumari"],"pdf_url":"https://arxiv.org/pdf/2407.17170v1.pdf","comment":"11 pages, 10 figures, 9 tables"},{"id":"http://arxiv.org/abs/2407.17162v1","updated":"2024-07-24T11:06:47Z","published":"2024-07-24T11:06:47Z","title":"Context-aware Multi-task Learning for Pedestrian Intent and Trajectory\n  Prediction","summary":"  The advancement of socially-aware autonomous vehicles hinges on precise\nmodeling of human behavior. Within this broad paradigm, the specific challenge\nlies in accurately predicting pedestrian's trajectory and intention.\nTraditional methodologies have leaned heavily on historical trajectory data,\nfrequently overlooking vital contextual cues such as pedestrian-specific traits\nand environmental factors. Furthermore, there's a notable knowledge gap as\ntrajectory and intention prediction have largely been approached as separate\nproblems, despite their mutual dependence. To bridge this gap, we introduce\nPTINet (Pedestrian Trajectory and Intention Prediction Network), which jointly\nlearns the trajectory and intention prediction by combining past trajectory\nobservations, local contextual features (individual pedestrian behaviors), and\nglobal features (signs, markings etc.). The efficacy of our approach is\nevaluated on widely used public datasets: JAAD and PIE, where it has\ndemonstrated superior performance over existing state-of-the-art models in\ntrajectory and intention prediction. The results from our experiments and\nablation studies robustly validate PTINet's effectiveness in jointly exploring\nintention and trajectory prediction for pedestrian behaviour modelling. The\nexperimental evaluation indicates the advantage of using global and local\ncontextual features for pedestrian trajectory and intention prediction. The\neffectiveness of PTINet in predicting pedestrian behavior paves the way for the\ndevelopment of automated systems capable of seamlessly interacting with\npedestrians in urban settings.\n","authors":["Farzeen Munir","Tomasz Piotr Kucner"],"pdf_url":"https://arxiv.org/pdf/2407.17162v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17157v1","updated":"2024-07-24T11:00:08Z","published":"2024-07-24T11:00:08Z","title":"Establishing Truly Causal Relationship Between Whole Slide Image\n  Predictions and Diagnostic Evidence Subregions in Deep Learning","summary":"  In the field of deep learning-driven Whole Slide Image (WSI) classification,\nMultiple Instance Learning (MIL) has gained significant attention due to its\nability to be trained using only slide-level diagnostic labels. Previous MIL\nresearches have primarily focused on enhancing feature aggregators for globally\nanalyzing WSIs, but overlook a causal relationship in diagnosis: model's\nprediction should ideally stem solely from regions of the image that contain\ndiagnostic evidence (such as tumor cells), which usually occupy relatively\nsmall areas. To address this limitation and establish the truly causal\nrelationship between model predictions and diagnostic evidence regions, we\npropose Causal Inference Multiple Instance Learning (CI-MIL). CI-MIL integrates\nfeature distillation with a novel patch decorrelation mechanism, employing a\ntwo-stage causal inference approach to distill and process patches with high\ndiagnostic value. Initially, CI-MIL leverages feature distillation to identify\npatches likely containing tumor cells and extracts their corresponding feature\nrepresentations. These features are then mapped to random Fourier feature\nspace, where a learnable weighting scheme is employed to minimize inter-feature\ncorrelations, effectively reducing redundancy from homogenous patches and\nmitigating data bias. These processes strengthen the causal relationship\nbetween model predictions and diagnostically relevant regions, making the\nprediction more direct and reliable. Experimental results demonstrate that\nCI-MIL outperforms state-of-the-art methods. Additionally, CI-MIL exhibits\nsuperior interpretability, as its selected regions demonstrate high consistency\nwith ground truth annotations, promising more reliable diagnostic assistance\nfor pathologists.\n","authors":["Tianhang Nan","Yong Ding","Hao Quan","Deliang Li","Mingchen Zou","Xiaoyu Cui"],"pdf_url":"https://arxiv.org/pdf/2407.17157v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17155v1","updated":"2024-07-24T10:53:14Z","published":"2024-07-24T10:53:14Z","title":"FIIH: Fully Invertible Image Hiding for Secure and Robust","summary":"  Image hiding is the study of techniques for covert storage and transmission,\nwhich embeds a secret image into a container image and generates stego image to\nmake it similar in appearance to a normal image. However, existing image hiding\nmethods have a serious problem that the hiding and revealing process cannot be\nfully invertible, which results in the revealing network not being able to\nrecover the secret image losslessly, which makes it impossible to\nsimultaneously achieve high fidelity and secure transmission of the secret\nimage in an insecure network environment. To solve this problem,this paper\nproposes a fully invertible image hiding architecture based on invertible\nneural network,aiming to realize invertible hiding of secret images,which is\ninvertible on both data and network. Based on this ingenious architecture, the\nmethod can withstand deep learning based image steganalysis. In addition, we\npropose a new method for enhancing the robustness of stego images after\ninterference during transmission. Experiments demonstrate that the FIIH\nproposed in this paper significantly outperforms other state-of-the-art image\nhiding methods in hiding a single image, and also significantly outperforms\nother state-of-the-art methods in robustness and security.\n","authors":["Lang Huang","Lin Huo","Zheng Gan","Xinrong He"],"pdf_url":"https://arxiv.org/pdf/2407.17155v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17152v1","updated":"2024-07-24T10:51:46Z","published":"2024-07-24T10:51:46Z","title":"XMeCap: Meme Caption Generation with Sub-Image Adaptability","summary":"  Humor, deeply rooted in societal meanings and cultural details, poses a\nunique challenge for machines. While advances have been made in natural\nlanguage processing, real-world humor often thrives in a multi-modal context,\nencapsulated distinctively by memes. This paper poses a particular emphasis on\nthe impact of multi-images on meme captioning. After that, we introduce the\n\\textsc{XMeCap} framework, a novel approach that adopts supervised fine-tuning\nand reinforcement learning based on an innovative reward model, which factors\nin both global and local similarities between visuals and text. Our results,\nbenchmarked against contemporary models, manifest a marked improvement in\ncaption generation for both single-image and multi-image memes, as well as\ndifferent meme categories. \\textsc{XMeCap} achieves an average evaluation score\nof 75.85 for single-image memes and 66.32 for multi-image memes, outperforming\nthe best baseline by 3.71\\% and 4.82\\%, respectively. This research not only\nestablishes a new frontier in meme-related studies but also underscores the\npotential of machines in understanding and generating humor in a multi-modal\nsetting.\n","authors":["Yuyan Chen","Songzhou Yan","Zhihong Zhu","Zhixu Li","Yanghua Xiao"],"pdf_url":"https://arxiv.org/pdf/2407.17152v1.pdf","comment":"Accepted to MM 2024"},{"id":"http://arxiv.org/abs/2205.09615v5","updated":"2024-07-24T10:49:23Z","published":"2022-05-19T15:13:00Z","title":"EXACT: How to Train Your Accuracy","summary":"  Classification tasks are usually evaluated in terms of accuracy. However,\naccuracy is discontinuous and cannot be directly optimized using gradient\nascent. Popular methods minimize cross-entropy, hinge loss, or other surrogate\nlosses, which can lead to suboptimal results. In this paper, we propose a new\noptimization framework by introducing stochasticity to a model's output and\noptimizing expected accuracy, i.e. accuracy of the stochastic model. Extensive\nexperiments on linear models and deep image classification show that the\nproposed optimization method is a powerful alternative to widely used\nclassification losses.\n","authors":["Ivan Karpukhin","Stanislav Dereka","Sergey Kolesnikov"],"pdf_url":"https://arxiv.org/pdf/2205.09615v5.pdf","comment":"Pattern Recognition Letters (2024)"},{"id":"http://arxiv.org/abs/2404.12867v2","updated":"2024-07-24T10:33:11Z","published":"2024-04-19T13:08:43Z","title":"FipTR: A Simple yet Effective Transformer Framework for Future Instance\n  Prediction in Autonomous Driving","summary":"  The future instance prediction from a Bird's Eye View(BEV) perspective is a\nvital component in autonomous driving, which involves future instance\nsegmentation and instance motion prediction. Existing methods usually rely on a\nredundant and complex pipeline which requires multiple auxiliary outputs and\npost-processing procedures. Moreover, estimated errors on each of the auxiliary\npredictions will lead to degradation of the prediction performance. In this\npaper, we propose a simple yet effective fully end-to-end framework named\nFuture Instance Prediction Transformer(FipTR), which views the task as BEV\ninstance segmentation and prediction for future frames. We propose to adopt\ninstance queries representing specific traffic participants to directly\nestimate the corresponding future occupied masks, and thus get rid of complex\npost-processing procedures. Besides, we devise a flow-aware BEV predictor for\nfuture BEV feature prediction composed of a flow-aware deformable attention\nthat takes backward flow guiding the offset sampling. A novel future instance\nmatching strategy is also proposed to further improve the temporal coherence.\nExtensive experiments demonstrate the superiority of FipTR and its\neffectiveness under different temporal BEV encoders. The code is available at\nhttps://github.com/TabGuigui/FipTR .\n","authors":["Xingtai Gui","Tengteng Huang","Haonan Shao","Haotian Yao","Chi Zhang"],"pdf_url":"https://arxiv.org/pdf/2404.12867v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17140v1","updated":"2024-07-24T10:20:19Z","published":"2024-07-24T10:20:19Z","title":"RT-DETRv2: Improved Baseline with Bag-of-Freebies for Real-Time\n  Detection Transformer","summary":"  In this report, we present RT-DETRv2, an improved Real-Time DEtection\nTRansformer (RT-DETR). RT-DETRv2 builds upon the previous state-of-the-art\nreal-time detector, RT-DETR, and opens up a set of bag-of-freebies for\nflexibility and practicality, as well as optimizing the training strategy to\nachieve enhanced performance. To improve the flexibility, we suggest setting a\ndistinct number of sampling points for features at different scales in the\ndeformable attention to achieve selective multi-scale feature extraction by the\ndecoder. To enhance practicality, we propose an optional discrete sampling\noperator to replace the grid_sample operator that is specific to RT-DETR\ncompared to YOLOs. This removes the deployment constraints typically associated\nwith DETRs. For the training strategy, we propose dynamic data augmentation and\nscale-adaptive hyperparameters customization to improve performance without\nloss of speed. Source code and pre-trained models will be available at\nhttps://github.com/lyuwenyu/RT-DETR.\n","authors":["Wenyu Lv","Yian Zhao","Qinyao Chang","Kui Huang","Guanzhong Wang","Yi Liu"],"pdf_url":"https://arxiv.org/pdf/2407.17140v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.04413v2","updated":"2024-07-24T10:16:33Z","published":"2024-06-06T18:01:30Z","title":"Efficient 3D-Aware Facial Image Editing via Attribute-Specific Prompt\n  Learning","summary":"  Drawing upon StyleGAN's expressivity and disentangled latent space, existing\n2D approaches employ textual prompting to edit facial images with different\nattributes. In contrast, 3D-aware approaches that generate faces at different\ntarget poses require attribute-specific classifiers, learning separate model\nweights for each attribute, and are not scalable for novel attributes. In this\nwork, we propose an efficient, plug-and-play, 3D-aware face editing framework\nbased on attribute-specific prompt learning, enabling the generation of facial\nimages with controllable attributes across various target poses. To this end,\nwe introduce a text-driven learnable style token-based latent attribute editor\n(LAE). The LAE harnesses a pre-trained vision-language model to find\ntext-guided attribute-specific editing direction in the latent space of any\npre-trained 3D-aware GAN. It utilizes learnable style tokens and style mappers\nto learn and transform this editing direction to 3D latent space. To train LAE\nwith multiple attributes, we use directional contrastive loss and style token\nloss. Furthermore, to ensure view consistency and identity preservation across\ndifferent poses and attributes, we employ several 3D-aware identity and pose\npreservation losses. Our experiments show that our proposed framework generates\nhigh-quality images with 3D awareness and view consistency while maintaining\nattribute-specific features. We demonstrate the effectiveness of our method on\ndifferent facial attributes, including hair color and style, expression, and\nothers.\n","authors":["Amandeep Kumar","Muhammad Awais","Sanath Narayan","Hisham Cholakkal","Salman Khan","Rao Muhammad Anwer"],"pdf_url":"https://arxiv.org/pdf/2406.04413v2.pdf","comment":"Accepted at ECCV, 2024. Amandeep Kumar and Muhammad Awais are joint\n  first authors. More details are available at\n  https://awaisrauf.github.io/3d_face_editing"},{"id":"http://arxiv.org/abs/2402.14654v2","updated":"2024-07-24T09:55:25Z","published":"2024-02-22T16:05:13Z","title":"Multi-HMR: Multi-Person Whole-Body Human Mesh Recovery in a Single Shot","summary":"  We present Multi-HMR, a strong sigle-shot model for multi-person 3D human\nmesh recovery from a single RGB image. Predictions encompass the whole body,\ni.e., including hands and facial expressions, using the SMPL-X parametric model\nand 3D location in the camera coordinate system. Our model detects people by\npredicting coarse 2D heatmaps of person locations, using features produced by a\nstandard Vision Transformer (ViT) backbone. It then predicts their whole-body\npose, shape and 3D location using a new cross-attention module called the Human\nPrediction Head (HPH), with one query attending to the entire set of features\nfor each detected person. As direct prediction of fine-grained hands and facial\nposes in a single shot, i.e., without relying on explicit crops around body\nparts, is hard to learn from existing data, we introduce CUFFS, the Close-Up\nFrames of Full-Body Subjects dataset, containing humans close to the camera\nwith diverse hand poses. We show that incorporating it into the training data\nfurther enhances predictions, particularly for hands. Multi-HMR also optionally\naccounts for camera intrinsics, if available, by encoding camera ray directions\nfor each image token. This simple design achieves strong performance on\nwhole-body and body-only benchmarks simultaneously: a ViT-S backbone on\n$448{\\times}448$ images already yields a fast and competitive model, while\nlarger models and higher resolutions obtain state-of-the-art results.\n","authors":["Fabien Baradel","Matthieu Armando","Salma Galaaoui","Romain Brégier","Philippe Weinzaepfel","Grégory Rogez","Thomas Lucas"],"pdf_url":"https://arxiv.org/pdf/2402.14654v2.pdf","comment":"Accepted at ECCV'24 - Code: https://github.com/naver/multi-hmr"},{"id":"http://arxiv.org/abs/2404.17147v3","updated":"2024-07-24T09:28:11Z","published":"2024-04-26T04:34:45Z","title":"On the Federated Learning Framework for Cooperative Perception","summary":"  Cooperative perception is essential to enhance the efficiency and safety of\nfuture transportation systems, requiring extensive data sharing among vehicles\non the road, which raises significant privacy concerns. Federated learning\noffers a promising solution by enabling data privacy-preserving collaborative\nenhancements in perception, decision-making, and planning among connected and\nautonomous vehicles (CAVs). However, federated learning is impeded by\nsignificant challenges arising from data heterogeneity across diverse clients,\npotentially diminishing model accuracy and prolonging convergence periods. This\nstudy introduces a specialized federated learning framework for CP, termed the\nfederated dynamic weighted aggregation (FedDWA) algorithm, facilitated by\ndynamic adjusting loss (DALoss) function. This framework employs dynamic client\nweighting to direct model convergence and integrates a novel loss function that\nutilizes Kullback-Leibler divergence (KLD) to counteract the detrimental\neffects of non-independently and identically distributed (Non-IID) and\nunbalanced data. Utilizing the BEV transformer as the primary model, our\nrigorous testing on the OpenV2V dataset, augmented with FedBEVT data,\ndemonstrates significant improvements in the average intersection over union\n(IoU). These results highlight the substantial potential of our federated\nlearning framework to address data heterogeneity challenges in CP, thereby\nenhancing the accuracy of environmental perception models and facilitating more\nrobust and efficient collaborative learning solutions in the transportation\nsector.\n","authors":["Zhenrong Zhang","Jianan Liu","Xi Zhou","Tao Huang","Qing-Long Han","Jingxin Liu","Hongbin Liu"],"pdf_url":"https://arxiv.org/pdf/2404.17147v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17114v1","updated":"2024-07-24T09:24:39Z","published":"2024-07-24T09:24:39Z","title":"A Self-Supervised Image Registration Approach for Measuring Local\n  Response Patterns in Metastatic Ovarian Cancer","summary":"  High-grade serous ovarian carcinoma (HGSOC) is characterised by significant\nspatial and temporal heterogeneity, typically manifesting at an advanced\nmetastatic stage. A major challenge in treating advanced HGSOC is effectively\nmonitoring localised change in tumour burden across multiple sites during\nneoadjuvant chemotherapy (NACT) and predicting long-term pathological response\nand overall patient survival. In this work, we propose a self-supervised\ndeformable image registration algorithm that utilises a general-purpose image\nencoder for image feature extraction to co-register contrast-enhanced\ncomputerised tomography scan images acquired before and after neoadjuvant\nchemotherapy. This approach addresses challenges posed by highly complex tumour\ndeformations and longitudinal lesion matching during treatment. Localised\ntumour changes are calculated using the Jacobian determinant maps of the\nregistration deformation at multiple disease sites and their macroscopic areas,\nincluding hypo-dense (i.e., cystic/necrotic), hyper-dense (i.e., calcified),\nand intermediate density (i.e., soft tissue) portions. A series of experiments\nis conducted to understand the role of a general-purpose image encoder and its\napplication in quantifying change in tumour burden during neoadjuvant\nchemotherapy in HGSOC. This work is the first to demonstrate the feasibility of\na self-supervised image registration approach in quantifying NACT-induced\nlocalised tumour changes across the whole disease burden of patients with\ncomplex multi-site HGSOC, which could be used as a potential marker for ovarian\ncancer patient's long-term pathological response and survival.\n","authors":["Inês P. Machado","Anna Reithmeir","Fryderyk Kogl","Leonardo Rundo","Gabriel Funingana","Marika Reinius","Gift Mungmeeprued","Zeyu Gao","Cathal McCague","Eric Kerfoot","Ramona Woitek","Evis Sala","Yangming Ou","James Brenton","Julia Schnabel","Mireia Crispin"],"pdf_url":"https://arxiv.org/pdf/2407.17114v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16344v2","updated":"2024-07-24T08:57:32Z","published":"2024-07-23T09:45:25Z","title":"SOAP: Enhancing Spatio-Temporal Relation and Motion Information\n  Capturing for Few-Shot Action Recognition","summary":"  High frame-rate (HFR) videos of action recognition improve fine-grained\nexpression while reducing the spatio-temporal relation and motion information\ndensity. Thus, large amounts of video samples are continuously required for\ntraditional data-driven training. However, samples are not always sufficient in\nreal-world scenarios, promoting few-shot action recognition (FSAR) research. We\nobserve that most recent FSAR works build spatio-temporal relation of video\nsamples via temporal alignment after spatial feature extraction, cutting apart\nspatial and temporal features within samples. They also capture motion\ninformation via narrow perspectives between adjacent frames without considering\ndensity, leading to insufficient motion information capturing. Therefore, we\npropose a novel plug-and-play architecture for FSAR called Spatio-tempOral\nfrAme tuPle enhancer (SOAP) in this paper. The model we designed with such\narchitecture refers to SOAP-Net. Temporal connections between different feature\nchannels and spatio-temporal relation of features are considered instead of\nsimple feature extraction. Comprehensive motion information is also captured,\nusing frame tuples with multiple frames containing more motion information than\nadjacent frames. Combining frame tuples of diverse frame counts further\nprovides a broader perspective. SOAP-Net achieves new state-of-the-art\nperformance across well-known benchmarks such as SthSthV2, Kinetics, UCF101,\nand HMDB51. Extensive empirical evaluations underscore the competitiveness,\npluggability, generalization, and robustness of SOAP. The code is released at\nhttps://github.com/wenbohuang1002/SOAP.\n","authors":["Wenbo Huang","Jinghui Zhang","Xuwei Qian","Zhen Wu","Meng Wang","Lei Zhang"],"pdf_url":"https://arxiv.org/pdf/2407.16344v2.pdf","comment":"Accepted by ACM MM 2024"},{"id":"http://arxiv.org/abs/2407.17101v1","updated":"2024-07-24T08:53:29Z","published":"2024-07-24T08:53:29Z","title":"PiPa++: Towards Unification of Domain Adaptive Semantic Segmentation via\n  Self-supervised Learning","summary":"  Unsupervised domain adaptive segmentation aims to improve the segmentation\naccuracy of models on target domains without relying on labeled data from those\ndomains. This approach is crucial when labeled target domain data is scarce or\nunavailable. It seeks to align the feature representations of the source domain\n(where labeled data is available) and the target domain (where only unlabeled\ndata is present), thus enabling the model to generalize well to the target\ndomain. Current image- and video-level domain adaptation have been addressed\nusing different and specialized frameworks, training strategies and\noptimizations despite their underlying connections. In this paper, we propose a\nunified framework PiPa++, which leverages the core idea of ``comparing'' to (1)\nexplicitly encourage learning of discriminative pixel-wise features with\nintraclass compactness and inter-class separability, (2) promote the robust\nfeature learning of the identical patch against different contexts or\nfluctuations, and (3) enable the learning of temporal continuity under dynamic\nenvironments. With the designed task-smart contrastive sampling strategy,\nPiPa++ enables the mining of more informative training samples according to the\ntask demand. Extensive experiments demonstrate the effectiveness of our method\non both image-level and video-level domain adaption benchmarks. Moreover, the\nproposed method is compatible with other UDA approaches to further improve the\nperformance without introducing extra parameters.\n","authors":["Mu Chen","Zhedong Zheng","Yi Yang"],"pdf_url":"https://arxiv.org/pdf/2407.17101v1.pdf","comment":"This study is under IEEE TMM review. arXiv admin note: substantial\n  text overlap with arXiv:2211.07609"},{"id":"http://arxiv.org/abs/2407.17095v1","updated":"2024-07-24T08:46:58Z","published":"2024-07-24T08:46:58Z","title":"MemBench: Memorized Image Trigger Prompt Dataset for Diffusion Models","summary":"  Diffusion models have achieved remarkable success in Text-to-Image generation\ntasks, leading to the development of many commercial models. However, recent\nstudies have reported that diffusion models often generate replicated images in\ntrain data when triggered by specific prompts, potentially raising social\nissues ranging from copyright to privacy concerns. To sidestep the\nmemorization, there have been recent studies for developing memorization\nmitigation methods for diffusion models. Nevertheless, the lack of benchmarks\nimpedes the assessment of the true effectiveness of these methods. In this\nwork, we present MemBench, the first benchmark for evaluating image\nmemorization mitigation methods. Our benchmark includes a large number of\nmemorized image trigger prompts in Stable Diffusion, the most popularly used\nmodel nowadays. Furthermore, in contrast to the prior work evaluating\nmitigation performance only on trigger prompts, we present metrics evaluating\non both trigger prompts and general prompts, so that we can see whether\nmitigation methods address the memorization issue while maintaining performance\nfor general prompts. This is an important development considering the practical\napplications which previous works have overlooked. Through evaluation on\nMemBench, we verify that the performance of existing image memorization\nmitigation methods is still insufficient for application to diffusion models.\n","authors":["Chunsan Hong","Tae-Hyun Oh","Minhyuk Sung"],"pdf_url":"https://arxiv.org/pdf/2407.17095v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09308v2","updated":"2024-07-24T08:43:36Z","published":"2024-04-14T17:33:33Z","title":"In My Perspective, In My Hands: Accurate Egocentric 2D Hand Pose and\n  Action Recognition","summary":"  Action recognition is essential for egocentric video understanding, allowing\nautomatic and continuous monitoring of Activities of Daily Living (ADLs)\nwithout user effort. Existing literature focuses on 3D hand pose input, which\nrequires computationally intensive depth estimation networks or wearing an\nuncomfortable depth sensor. In contrast, there has been insufficient research\nin understanding 2D hand pose for egocentric action recognition, despite the\navailability of user-friendly smart glasses in the market capable of capturing\na single RGB image. Our study aims to fill this research gap by exploring the\nfield of 2D hand pose estimation for egocentric action recognition, making two\ncontributions. Firstly, we introduce two novel approaches for 2D hand pose\nestimation, namely EffHandNet for single-hand estimation and EffHandEgoNet,\ntailored for an egocentric perspective, capturing interactions between hands\nand objects. Both methods outperform state-of-the-art models on H2O and FPHA\npublic benchmarks. Secondly, we present a robust action recognition\narchitecture from 2D hand and object poses. This method incorporates\nEffHandEgoNet, and a transformer-based action recognition method. Evaluated on\nH2O and FPHA datasets, our architecture has a faster inference time and\nachieves an accuracy of 91.32% and 94.43%, respectively, surpassing state of\nthe art, including 3D-based methods. Our work demonstrates that using 2D\nskeletal data is a robust approach for egocentric action understanding.\nExtensive evaluation and ablation studies show the impact of the hand pose\nestimation approach, and how each input affects the overall performance.\n","authors":["Wiktor Mucha","Martin Kampel"],"pdf_url":"https://arxiv.org/pdf/2404.09308v2.pdf","comment":"Accepted at: The 18th IEEE International Conference on Automatic Face\n  and Gesture Recognition"},{"id":"http://arxiv.org/abs/2401.03407v6","updated":"2024-07-24T08:27:47Z","published":"2024-01-07T07:56:47Z","title":"Bilateral Reference for High-Resolution Dichotomous Image Segmentation","summary":"  We introduce a novel bilateral reference framework (BiRefNet) for\nhigh-resolution dichotomous image segmentation (DIS). It comprises two\nessential components: the localization module (LM) and the reconstruction\nmodule (RM) with our proposed bilateral reference (BiRef). The LM aids in\nobject localization using global semantic information. Within the RM, we\nutilize BiRef for the reconstruction process, where hierarchical patches of\nimages provide the source reference and gradient maps serve as the target\nreference. These components collaborate to generate the final predicted maps.\nWe also introduce auxiliary gradient supervision to enhance focus on regions\nwith finer details. Furthermore, we outline practical training strategies\ntailored for DIS to improve map quality and training process. To validate the\ngeneral applicability of our approach, we conduct extensive experiments on four\ntasks to evince that BiRefNet exhibits remarkable performance, outperforming\ntask-specific cutting-edge methods across all benchmarks. Our codes are\navailable at https://github.com/ZhengPeng7/BiRefNet.\n","authors":["Peng Zheng","Dehong Gao","Deng-Ping Fan","Li Liu","Jorma Laaksonen","Wanli Ouyang","Nicu Sebe"],"pdf_url":"https://arxiv.org/pdf/2401.03407v6.pdf","comment":"Version 6, the final version of the journal with a fixed institute"},{"id":"http://arxiv.org/abs/2407.13519v2","updated":"2024-07-24T08:23:26Z","published":"2024-07-18T13:53:15Z","title":"GPSFormer: A Global Perception and Local Structure Fitting-based\n  Transformer for Point Cloud Understanding","summary":"  Despite the significant advancements in pre-training methods for point cloud\nunderstanding, directly capturing intricate shape information from irregular\npoint clouds without reliance on external data remains a formidable challenge.\nTo address this problem, we propose GPSFormer, an innovative Global Perception\nand Local Structure Fitting-based Transformer, which learns detailed shape\ninformation from point clouds with remarkable precision. The core of GPSFormer\nis the Global Perception Module (GPM) and the Local Structure Fitting\nConvolution (LSFConv). Specifically, GPM utilizes Adaptive Deformable Graph\nConvolution (ADGConv) to identify short-range dependencies among similar\nfeatures in the feature space and employs Multi-Head Attention (MHA) to learn\nlong-range dependencies across all positions within the feature space,\nultimately enabling flexible learning of contextual representations. Inspired\nby Taylor series, we design LSFConv, which learns both low-order fundamental\nand high-order refinement information from explicitly encoded local geometric\nstructures. Integrating the GPM and LSFConv as fundamental components, we\nconstruct GPSFormer, a cutting-edge Transformer that effectively captures\nglobal and local structures of point clouds. Extensive experiments validate\nGPSFormer's effectiveness in three point cloud tasks: shape classification,\npart segmentation, and few-shot learning. The code of GPSFormer is available at\n\\url{https://github.com/changshuowang/GPSFormer}.\n","authors":["Changshuo Wang","Meiqing Wu","Siew-Kei Lam","Xin Ning","Shangshu Yu","Ruiping Wang","Weijun Li","Thambipillai Srikanthan"],"pdf_url":"https://arxiv.org/pdf/2407.13519v2.pdf","comment":"Accepted by ECCV 2024"},{"id":"http://arxiv.org/abs/2407.17085v1","updated":"2024-07-24T08:22:49Z","published":"2024-07-24T08:22:49Z","title":"OVR: A Dataset for Open Vocabulary Temporal Repetition Counting in\n  Videos","summary":"  We introduce a dataset of annotations of temporal repetitions in videos. The\ndataset, OVR (pronounced as over), contains annotations for over 72K videos,\nwith each annotation specifying the number of repetitions, the start and end\ntime of the repetitions, and also a free-form description of what is repeating.\nThe annotations are provided for videos sourced from Kinetics and Ego4D, and\nconsequently cover both Exo and Ego viewing conditions, with a huge variety of\nactions and activities. Moreover, OVR is almost an order of magnitude larger\nthan previous datasets for video repetition. We also propose a baseline\ntransformer-based counting model, OVRCounter, that can localise and count\nrepetitions in videos that are up to 320 frames long. The model is trained and\nevaluated on the OVR dataset, and its performance assessed with and without\nusing text to specify the target class to count. The performance is also\ncompared to a prior repetition counting model. The dataset is available for\ndownload at: https://sites.google.com/view/openvocabreps/\n","authors":["Debidatta Dwibedi","Yusuf Aytar","Jonathan Tompson","Andrew Zisserman"],"pdf_url":"https://arxiv.org/pdf/2407.17085v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17083v1","updated":"2024-07-24T08:20:02Z","published":"2024-07-24T08:20:02Z","title":"When Text and Images Don't Mix: Bias-Correcting Language-Image\n  Similarity Scores for Anomaly Detection","summary":"  Contrastive Language-Image Pre-training (CLIP) achieves remarkable\nperformance in various downstream tasks through the alignment of image and text\ninput embeddings and holds great promise for anomaly detection. However, our\nempirical experiments show that the embeddings of text inputs unexpectedly\ntightly cluster together, far away from image embeddings, contrary to the\nmodel's contrastive training objective to align image-text input pairs. We show\nthat this phenomenon induces a `similarity bias' - in which false negative and\nfalse positive errors occur due to bias in the similarities between images and\nthe normal label text embeddings. To address this bias, we propose a novel\nmethodology called BLISS which directly accounts for this similarity bias\nthrough the use of an auxiliary, external set of text inputs. BLISS is simple,\nit does not require strong inductive biases about anomalous behaviour nor an\nexpensive training process, and it significantly outperforms baseline methods\non benchmark image datasets, even when access to normal data is extremely\nlimited.\n","authors":["Adam Goodge","Bryan Hooi","Wee Siong Ng"],"pdf_url":"https://arxiv.org/pdf/2407.17083v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.14077v2","updated":"2024-07-24T07:51:18Z","published":"2024-05-23T00:46:53Z","title":"Learning to Transform Dynamically for Better Adversarial Transferability","summary":"  Adversarial examples, crafted by adding perturbations imperceptible to\nhumans, can deceive neural networks. Recent studies identify the adversarial\ntransferability across various models, \\textit{i.e.}, the cross-model attack\nability of adversarial samples. To enhance such adversarial transferability,\nexisting input transformation-based methods diversify input data with\ntransformation augmentation. However, their effectiveness is limited by the\nfinite number of available transformations. In our study, we introduce a novel\napproach named Learning to Transform (L2T). L2T increases the diversity of\ntransformed images by selecting the optimal combination of operations from a\npool of candidates, consequently improving adversarial transferability. We\nconceptualize the selection of optimal transformation combinations as a\ntrajectory optimization problem and employ a reinforcement learning strategy to\neffectively solve the problem. Comprehensive experiments on the ImageNet\ndataset, as well as practical tests with Google Vision and GPT-4V, reveal that\nL2T surpasses current methodologies in enhancing adversarial transferability,\nthereby confirming its effectiveness and practical significance. The code is\navailable at https://github.com/RongyiZhu/L2T.\n","authors":["Rongyi Zhu","Zeliang Zhang","Susan Liang","Zhuo Liu","Chenliang Xu"],"pdf_url":"https://arxiv.org/pdf/2405.14077v2.pdf","comment":"accepted as a poster in CVPR 2024"},{"id":"http://arxiv.org/abs/2407.04833v2","updated":"2024-07-24T07:49:00Z","published":"2024-07-05T19:38:10Z","title":"3D Adaptive Structural Convolution Network for Domain-Invariant Point\n  Cloud Recognition","summary":"  Adapting deep learning networks for point cloud data recognition in\nself-driving vehicles faces challenges due to the variability in datasets and\nsensor technologies, emphasizing the need for adaptive techniques to maintain\naccuracy across different conditions. In this paper, we introduce the 3D\nAdaptive Structural Convolution Network (3D-ASCN), a cutting-edge framework for\n3D point cloud recognition. It combines 3D convolution kernels, a structural\ntree structure, and adaptive neighborhood sampling for effective geometric\nfeature extraction. This method obtains domain-invariant features and\ndemonstrates robust, adaptable performance on a variety of point cloud\ndatasets, ensuring compatibility across diverse sensor configurations without\nthe need for parameter adjustments. This highlights its potential to\nsignificantly enhance the reliability and efficiency of self-driving vehicle\ntechnology.\n","authors":["Younggun Kim","Beomsik Cho","Seonghoon Ryoo","Soomok Lee"],"pdf_url":"https://arxiv.org/pdf/2407.04833v2.pdf","comment":"11 pages, 3 figures"},{"id":"http://arxiv.org/abs/2407.17064v1","updated":"2024-07-24T07:45:37Z","published":"2024-07-24T07:45:37Z","title":"AI-based Density Recognition","summary":"  Learning-based analysis of images is commonly used in the fields of mobility\nand robotics for safe environmental motion and interaction. This requires not\nonly object recognition but also the assignment of certain properties to them.\nWith the help of this information, causally related actions can be adapted to\ndifferent circumstances. Such logical interactions can be optimized by\nrecognizing object-assigned properties. Density as a physical property offers\nthe possibility to recognize how heavy an object is, which material it is made\nof, which forces are at work, and consequently which influence it has on its\nenvironment. Our approach introduces an AI-based concept for assigning physical\nproperties to objects through the use of associated images. Based on\nsynthesized data, we derive specific patterns from 2D images using a neural\nnetwork to extract further information such as volume, material, or density.\nAccordingly, we discuss the possibilities of property-based feature extraction\nto improve causally related logics.\n","authors":["Simone Müller","Daniel Kolb","Matthias Müller","Dieter Kranzlmüller"],"pdf_url":"https://arxiv.org/pdf/2407.17064v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17060v1","updated":"2024-07-24T07:37:12Z","published":"2024-07-24T07:37:12Z","title":"High Efficiency Image Compression for Large Visual-Language Models","summary":"  In recent years, large visual language models (LVLMs) have shown impressive\nperformance and promising generalization capability in multi-modal tasks, thus\nreplacing humans as receivers of visual information in various application\nscenarios. In this paper, we pioneer to propose a variable bitrate image\ncompression framework consisting of a pre-editing module and an end-to-end\ncodec to achieve promising rate-accuracy performance for different LVLMs. In\nparticular, instead of optimizing an adaptive pre-editing network towards a\nparticular task or several representative tasks, we propose a new optimization\nstrategy tailored for LVLMs, which is designed based on the representation and\ndiscrimination capability with token-level distortion and rank. The pre-editing\nmodule and the variable bitrate end-to-end image codec are jointly trained by\nthe losses based on semantic tokens of the large model, which introduce\nenhanced generalization capability for various data and tasks. {Experimental\nresults demonstrate that the proposed framework could efficiently achieve much\nbetter rate-accuracy performance compared to the state-of-the-art coding\nstandard, Versatile Video Coding.} Meanwhile, experiments with multi-modal\ntasks have revealed the robustness and generalization capability of the\nproposed framework.\n","authors":["Binzhe Li","Shurun Wang","Shiqi Wang","Yan Ye"],"pdf_url":"https://arxiv.org/pdf/2407.17060v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17058v1","updated":"2024-07-24T07:36:33Z","published":"2024-07-24T07:36:33Z","title":"DiffCD: A Symmetric Differentiable Chamfer Distance for Neural Implicit\n  Surface Fitting","summary":"  Neural implicit surfaces can be used to recover accurate 3D geometry from\nimperfect point clouds. In this work, we show that state-of-the-art techniques\nwork by minimizing an approximation of a one-sided Chamfer distance. This shape\nmetric is not symmetric, as it only ensures that the point cloud is near the\nsurface but not vice versa. As a consequence, existing methods can produce\ninaccurate reconstructions with spurious surfaces. Although one approach\nagainst spurious surfaces has been widely used in the literature, we\ntheoretically and experimentally show that it is equivalent to regularizing the\nsurface area, resulting in over-smoothing. As a more appealing alternative, we\npropose DiffCD, a novel loss function corresponding to the symmetric Chamfer\ndistance. In contrast to previous work, DiffCD also assures that the surface is\nnear the point cloud, which eliminates spurious surfaces without the need for\nadditional regularization. We experimentally show that DiffCD reliably recovers\na high degree of shape detail, substantially outperforming existing work across\nvarying surface complexity and noise levels. Project code is available at\nhttps://github.com/linusnie/diffcd.\n","authors":["Linus Härenstam-Nielsen","Lu Sang","Abhishek Saroha","Nikita Araslanov","Daniel Cremers"],"pdf_url":"https://arxiv.org/pdf/2407.17058v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10121v3","updated":"2024-07-24T07:35:32Z","published":"2024-07-14T08:51:25Z","title":"MSD: A Benchmark Dataset for Floor Plan Generation of Building Complexes","summary":"  Diverse and realistic floor plan data are essential for the development of\nuseful computer-aided methods in architectural design. Today's large-scale\nfloor plan datasets predominantly feature simple floor plan layouts, typically\nrepresenting single-apartment dwellings only. To compensate for the mismatch\nbetween current datasets and the real world, we develop \\textbf{Modified Swiss\nDwellings} (MSD) -- the first large-scale floor plan dataset that contains a\nsignificant share of layouts of multi-apartment dwellings. MSD features over\n5.3K floor plans of medium- to large-scale building complexes, covering over\n18.9K distinct apartments. We validate that existing approaches for floor plan\ngeneration, while effective in simpler scenarios, cannot yet seamlessly address\nthe challenges posed by MSD. Our benchmark calls for new research in floor plan\nmachine understanding. Code and data are open.\n","authors":["Casper van Engelenburg","Fatemeh Mostafavi","Emanuel Kuhn","Yuntae Jeon","Michael Franzen","Matthias Standfest","Jan van Gemert","Seyran Khademi"],"pdf_url":"https://arxiv.org/pdf/2407.10121v3.pdf","comment":"ECCV 2024 (incl. Suppl. Mat.)"},{"id":"http://arxiv.org/abs/2405.20443v2","updated":"2024-07-24T07:34:35Z","published":"2024-05-30T19:40:08Z","title":"P-MSDiff: Parallel Multi-Scale Diffusion for Remote Sensing Image\n  Segmentation","summary":"  Diffusion models and multi-scale features are essential components in\nsemantic segmentation tasks that deal with remote-sensing images. They\ncontribute to improved segmentation boundaries and offer significant contextual\ninformation. U-net-like architectures are frequently employed in diffusion\nmodels for segmentation tasks. These architectural designs include dense skip\nconnections that may pose challenges for interpreting intermediate features.\nConsequently, they might not efficiently convey semantic information throughout\nvarious layers of the encoder-decoder architecture. To address these\nchallenges, we propose a new model for semantic segmentation known as the\ndiffusion model with parallel multi-scale branches. This model consists of\nParallel Multiscale Diffusion modules (P-MSDiff) and a Cross-Bridge Linear\nAttention mechanism (CBLA). P-MSDiff enhances the understanding of semantic\ninformation across multiple levels of granularity and detects repetitive\ndistribution data through the integration of recursive denoising branches. It\nfurther facilitates the amalgamation of data by connecting relevant branches to\nthe primary framework to enable concurrent denoising. Furthermore, within the\ninterconnected transformer architecture, the LA module has been substituted\nwith the CBLA module. This module integrates a semidefinite matrix linked to\nthe query into the dot product computation of keys and values. This integration\nenables the adaptation of queries within the LA framework. This adjustment\nenhances the structure for multi-head attention computation, leading to\nenhanced network performance and CBLA is a plug-and-play module. Our model\ndemonstrates superior performance based on the J1 metric on both the UAVid and\nVaihingen Building datasets, showing improvements of 1.60% and 1.40% over\nstrong baseline models, respectively.\n","authors":["Qi Zhang","Guohua Geng","Longquan Yan","Pengbo Zhou","Zhaodi Li","Kang Li","Qinglin Liu"],"pdf_url":"https://arxiv.org/pdf/2405.20443v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.11442v2","updated":"2024-07-24T07:31:37Z","published":"2024-05-19T04:35:05Z","title":"Unifying 3D Vision-Language Understanding via Promptable Queries","summary":"  A unified model for 3D vision-language (3D-VL) understanding is expected to\ntake various scene representations and perform a wide range of tasks in a 3D\nscene. However, a considerable gap exists between existing methods and such a\nunified model, due to the independent application of representation and\ninsufficient exploration of 3D multi-task training. In this paper, we introduce\nPQ3D, a unified model capable of using Promptable Queries to tackle a wide\nrange of 3D-VL tasks, from low-level instance segmentation to high-level\nreasoning and planning. This is achieved through three key innovations: (1)\nunifying various 3D scene representations (i.e., voxels, point clouds,\nmulti-view images) into a shared 3D coordinate space by segment-level grouping,\n(2) an attention-based query decoder for task-specific information retrieval\nguided by prompts, and (3) universal output heads for different tasks to\nsupport multi-task training. Tested across ten diverse 3D-VL datasets, PQ3D\ndemonstrates impressive performance on these tasks, setting new records on most\nbenchmarks. Particularly, PQ3D improves the state-of-the-art on ScanNet200 by\n4.9% (AP25), ScanRefer by 5.4% (acc@0.5), Multi3DRefer by 11.7% (F1@0.5), and\nScan2Cap by 13.4% (CIDEr@0.5). Moreover, PQ3D supports flexible inference with\nindividual or combined forms of available 3D representations, e.g., solely\nvoxel input.\n","authors":["Ziyu Zhu","Zhuofan Zhang","Xiaojian Ma","Xuesong Niu","Yixin Chen","Baoxiong Jia","Zhidong Deng","Siyuan Huang","Qing Li"],"pdf_url":"https://arxiv.org/pdf/2405.11442v2.pdf","comment":"ECCV 2024. Project page: https://pq3d.github.io"},{"id":"http://arxiv.org/abs/2312.06193v2","updated":"2024-07-24T07:21:35Z","published":"2023-12-11T08:16:55Z","title":"DisControlFace: Adding Disentangled Control to Diffusion Autoencoder for\n  One-shot Explicit Facial Image Editing","summary":"  In this work, we focus on exploring explicit fine-grained control of\ngenerative facial image editing, all while generating faithful facial\nappearances and consistent semantic details, which however, is quite\nchallenging and has not been extensively explored, especially under an one-shot\nscenario. We identify the key challenge as the exploration of disentangled\nconditional control between high-level semantics and explicit parameters (e.g.,\n3DMM) in the generation process, and accordingly propose a novel\ndiffusion-based editing framework, named DisControlFace. Specifically, we\nleverage a Diffusion Autoencoder (Diff-AE) as the semantic reconstruction\nbackbone. To enable explicit face editing, we construct an Exp-FaceNet that is\ncompatible with Diff-AE to generate spatial-wise explicit control conditions\nbased on estimated 3DMM parameters. Different from current diffusion-based\nediting methods that train the whole conditional generative model from scratch,\nwe freeze the pre-trained weights of the Diff-AE to maintain its semantically\ndeterministic conditioning capability and accordingly propose a random semantic\nmasking (RSM) strategy to effectively achieve an independent training of\nExp-FaceNet. This setting endows the model with disentangled face control\nmeanwhile reducing semantic information shift in editing. Our model can be\ntrained using 2D in-the-wild portrait images without requiring 3D or video data\nand perform robust editing on any new facial image through a simple one-shot\nfine-tuning. Comprehensive experiments demonstrate that DisControlFace can\ngenerate realistic facial images with better editing accuracy and identity\npreservation over state-of-the-art methods. Project page:\nhttps://discontrolface.github.io/\n","authors":["Haozhe Jia","Yan Li","Hengfei Cui","Di Xu","Yuwang Wang","Tao Yu"],"pdf_url":"https://arxiv.org/pdf/2312.06193v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16653v2","updated":"2024-07-24T07:18:46Z","published":"2024-07-23T17:14:01Z","title":"Aggregated Attributions for Explanatory Analysis of 3D Segmentation\n  Models","summary":"  Analysis of 3D segmentation models, especially in the context of medical\nimaging, is often limited to segmentation performance metrics that overlook the\ncrucial aspect of explainability and bias. Currently, effectively explaining\nthese models with saliency maps is challenging due to the high dimensions of\ninput images multiplied by the ever-growing number of segmented class labels.\nTo this end, we introduce Agg^2Exp, a methodology for aggregating fine-grained\nvoxel attributions of the segmentation model's predictions. Unlike classical\nexplanation methods that primarily focus on the local feature attribution,\nAgg^2Exp enables a more comprehensive global view on the importance of\npredicted segments in 3D images. Our benchmarking experiments show that\ngradient-based voxel attributions are more faithful to the model's predictions\nthan perturbation-based explanations. As a concrete use-case, we apply Agg^2Exp\nto discover knowledge acquired by the Swin UNEt TRansformer model trained on\nthe TotalSegmentator v2 dataset for segmenting anatomical structures in\ncomputed tomography medical images. Agg^2Exp facilitates the explanatory\nanalysis of large segmentation models beyond their predictive performance.\n","authors":["Maciej Chrabaszcz","Hubert Baniecki","Piotr Komorowski","Szymon Płotka","Przemyslaw Biecek"],"pdf_url":"https://arxiv.org/pdf/2407.16653v2.pdf","comment":"Added Acknowledgments"},{"id":"http://arxiv.org/abs/2405.20091v3","updated":"2024-07-24T07:06:43Z","published":"2024-05-30T14:27:40Z","title":"VAAD: Visual Attention Analysis Dashboard applied to e-Learning","summary":"  In this paper, we present an approach in the Multimodal Learning Analytics\nfield. Within this approach, we have developed a tool to visualize and analyze\neye movement data collected during learning sessions in online courses. The\ntool is named VAAD, an acronym for Visual Attention Analysis Dashboard. These\neye movement data have been gathered using an eye-tracker and subsequently\nprocessed and visualized for interpretation. The purpose of the tool is to\nconduct a descriptive analysis of the data by facilitating its visualization,\nenabling the identification of differences and learning patterns among various\nlearner populations. Additionally, it integrates a predictive module capable of\nanticipating learner activities during a learning session. Consequently, VAAD\nholds the potential to offer valuable insights into online learning behaviors\nfrom both descriptive and predictive perspectives.\n","authors":["Miriam Navarro","Álvaro Becerra","Roberto Daza","Ruth Cobos","Aythami Morales","Julian Fierrez"],"pdf_url":"https://arxiv.org/pdf/2405.20091v3.pdf","comment":"Accepted in CEDI 2024 (VII Congreso Espa\\~nol de Inform\\'atica), A\n  Coru\\~na, Spain"},{"id":"http://arxiv.org/abs/2406.14556v3","updated":"2024-07-24T07:03:29Z","published":"2024-06-20T17:59:03Z","title":"Asynchronous Large Language Model Enhanced Planner for Autonomous\n  Driving","summary":"  Despite real-time planners exhibiting remarkable performance in autonomous\ndriving, the growing exploration of Large Language Models (LLMs) has opened\navenues for enhancing the interpretability and controllability of motion\nplanning. Nevertheless, LLM-based planners continue to encounter significant\nchallenges, including elevated resource consumption and extended inference\ntimes, which pose substantial obstacles to practical deployment. In light of\nthese challenges, we introduce AsyncDriver, a new asynchronous LLM-enhanced\nclosed-loop framework designed to leverage scene-associated instruction\nfeatures produced by LLM to guide real-time planners in making precise and\ncontrollable trajectory predictions. On one hand, our method highlights the\nprowess of LLMs in comprehending and reasoning with vectorized scene data and a\nseries of routing instructions, demonstrating its effective assistance to\nreal-time planners. On the other hand, the proposed framework decouples the\ninference processes of the LLM and real-time planners. By capitalizing on the\nasynchronous nature of their inference frequencies, our approach have\nsuccessfully reduced the computational cost introduced by LLM, while\nmaintaining comparable performance. Experiments show that our approach achieves\nsuperior closed-loop evaluation performance on nuPlan's challenging scenarios.\n","authors":["Yuan Chen","Zi-han Ding","Ziqin Wang","Yan Wang","Lijun Zhang","Si Liu"],"pdf_url":"https://arxiv.org/pdf/2406.14556v3.pdf","comment":"ECCV 2024"},{"id":"http://arxiv.org/abs/2306.08326v3","updated":"2024-07-24T06:53:19Z","published":"2023-06-14T07:58:14Z","title":"Early Detection of Late Blight Tomato Disease using Histogram Oriented\n  Gradient based Support Vector Machine","summary":"  The tomato is one of the most important fruits on earth. It plays an\nimportant and useful role in the agricultural production of any country. This\nresearch propose a novel smart technique for early detection of late blight\ndiseases in tomatoes. This work improve the dataset with an increase in images\nfrom the field (the Plant Village dataset) and proposed a hybrid algorithm\ncomposed of support vector machines (SVM) and histogram-oriented gradients\n(HOG) for real-time detection of late blight tomato disease. To propose a\nHOG-based SVM model for early detection of late blight tomato leaf disease. To\ncheck the performance of the proposed model in terms of MSE, accuracy,\nprecision, and recall as compared to Decision Tree and KNN. The integration of\nadvanced technology in agriculture has the potential to revolutionize the\nindustry, making it more efficient, sustainable, and profitable. This research\nwork on the early detection of tomato diseases contributes to the growing\nimportance of smart farming, the need for climate-smart agriculture, the rising\nneed to more efficiently utilize natural resources, and the demand for higher\ncrop yields. The proposed hybrid algorithm of SVM and HOG has significant\npotential for the early detection of late blight disease in tomato plants. The\nperformance of the proposed model against decision tree and KNN algorithms and\nthe results may assist in selecting the best algorithm for future applications.\nThe research work can help farmers make data-driven decisions to optimize crop\nyield and quality while also reducing the environmental impact of farming\npractices.\n","authors":["Yousef Alhwaiti","Muhammad Ishaq","Muhammad Hameed Siddiqi","Muhammad Waqas","Madallah Alruwaili","Saad Alanazi","Asfandyar Khan","Faheem Khan"],"pdf_url":"https://arxiv.org/pdf/2306.08326v3.pdf","comment":"The article titled \"Early Detection of Late Blight Tomato Disease\n  using Histogram Oriented Gradient based Support Vector Machine\" need to be\n  withdrawn there are other contributors in the improvement of this article"},{"id":"http://arxiv.org/abs/2311.17050v3","updated":"2024-07-24T06:49:30Z","published":"2023-11-28T18:56:01Z","title":"Surf-D: Generating High-Quality Surfaces of Arbitrary Topologies Using\n  Diffusion Models","summary":"  We present Surf-D, a novel method for generating high-quality 3D shapes as\nSurfaces with arbitrary topologies using Diffusion models. Previous methods\nexplored shape generation with different representations and they suffer from\nlimited topologies and poor geometry details. To generate high-quality surfaces\nof arbitrary topologies, we use the Unsigned Distance Field (UDF) as our\nsurface representation to accommodate arbitrary topologies. Furthermore, we\npropose a new pipeline that employs a point-based AutoEncoder to learn a\ncompact and continuous latent space for accurately encoding UDF and support\nhigh-resolution mesh extraction. We further show that our new pipeline\nsignificantly outperforms the prior approaches to learning the distance fields,\nsuch as the grid-based AutoEncoder, which is not scalable and incapable of\nlearning accurate UDF. In addition, we adopt a curriculum learning strategy to\nefficiently embed various surfaces. With the pretrained shape latent space, we\nemploy a latent diffusion model to acquire the distribution of various shapes.\nExtensive experiments are presented on using Surf-D for unconditional\ngeneration, category conditional generation, image conditional generation, and\ntext-to-shape tasks. The experiments demonstrate the superior performance of\nSurf-D in shape generation across multiple modalities as conditions. Visit our\nproject page at https://yzmblog.github.io/projects/SurfD/.\n","authors":["Zhengming Yu","Zhiyang Dou","Xiaoxiao Long","Cheng Lin","Zekun Li","Yuan Liu","Norman Müller","Taku Komura","Marc Habermann","Christian Theobalt","Xin Li","Wenping Wang"],"pdf_url":"https://arxiv.org/pdf/2311.17050v3.pdf","comment":"Accepted to ECCV 2024. Project Page:\n  https://yzmblog.github.io/projects/SurfD/"},{"id":"http://arxiv.org/abs/2406.18113v2","updated":"2024-07-24T06:43:07Z","published":"2024-06-26T06:59:09Z","title":"The Surprising Effectiveness of Multimodal Large Language Models for\n  Video Moment Retrieval","summary":"  Recent studies have shown promising results in utilizing multimodal large\nlanguage models (MLLMs) for computer vision tasks such as object detection and\nsemantic segmentation. However, many challenging video tasks remain\nunder-explored. Video-language tasks necessitate spatial and temporal\ncomprehension and require significant compute. Therefore, prior works have\ndeveloped complex, highly specialized architectures or leveraged additional\ninput signals such as video transcripts to best encode contextual and temporal\ninformation, which limits their generality and can be impractical. One\nparticularly challenging task is video moment retrieval, which requires precise\ntemporal and contextual grounding. This work demonstrates the surprising\neffectiveness of leveraging image-text pretrained MLLMs for moment retrieval.\nWe introduce Mr. BLIP (Mr. as in Moment Retrieval), a multimodal, single-stage\nmodel that requires no expensive video-language pretraining, no additional\ninput signal (e.g., no transcript or audio), and has a simpler and more\nversatile design than prior state-of-the-art methods. We achieve a new\nstate-of-the-art in moment retrieval on the widely used benchmarks\nCharades-STA, QVHighlights, and ActivityNet Captions. Notably, we attain over\n9% (absolute) higher Recall (at 0.5 and 0.7 IoU) on the challenging long-video\nmulti-moment QVHighlights benchmark. Our code is publicly available.\n","authors":["Meinardus Boris","Batra Anil","Rohrbach Anna","Rohrbach Marcus"],"pdf_url":"https://arxiv.org/pdf/2406.18113v2.pdf","comment":"Code: https://github.com/sudo-Boris/mr-Blip"},{"id":"http://arxiv.org/abs/2407.17035v1","updated":"2024-07-24T06:42:46Z","published":"2024-07-24T06:42:46Z","title":"Q-Ground: Image Quality Grounding with Large Multi-modality Models","summary":"  Recent advances of large multi-modality models (LMM) have greatly improved\nthe ability of image quality assessment (IQA) method to evaluate and explain\nthe quality of visual content. However, these advancements are mostly focused\non overall quality assessment, and the detailed examination of local quality,\nwhich is crucial for comprehensive visual understanding, is still largely\nunexplored. In this work, we introduce Q-Ground, the first framework aimed at\ntackling fine-scale visual quality grounding by combining large multi-modality\nmodels with detailed visual quality analysis. Central to our contribution is\nthe introduction of the QGround-100K dataset, a novel resource containing 100k\ntriplets of (image, quality text, distortion segmentation) to facilitate deep\ninvestigations into visual quality. The dataset comprises two parts: one with\nhuman-labeled annotations for accurate quality assessment, and another labeled\nautomatically by LMMs such as GPT4V, which helps improve the robustness of\nmodel training while also reducing the costs of data collection. With the\nQGround-100K dataset, we propose a LMM-based method equipped with multi-scale\nfeature learning to learn models capable of performing both image quality\nanswering and distortion segmentation based on text prompts. This\ndual-capability approach not only refines the model's understanding of\nregion-aware image quality but also enables it to interactively respond to\ncomplex, text-based queries about image quality and specific distortions.\nQ-Ground takes a step towards sophisticated visual quality analysis in a finer\nscale, establishing a new benchmark for future research in the area. Codes and\ndataset are available at https://github.com/Q-Future/Q-Ground.\n","authors":["Chaofeng Chen","Sensen Yang","Haoning Wu","Liang Liao","Zicheng Zhang","Annan Wang","Wenxiu Sun","Qiong Yan","Weisi Lin"],"pdf_url":"https://arxiv.org/pdf/2407.17035v1.pdf","comment":"ACM Multimedia 2024 (Oral)"},{"id":"http://arxiv.org/abs/2407.17028v1","updated":"2024-07-24T06:15:28Z","published":"2024-07-24T06:15:28Z","title":"Enhancing Environmental Monitoring through Multispectral Imaging: The\n  WasteMS Dataset for Semantic Segmentation of Lakeside Waste","summary":"  Environmental monitoring of lakeside green areas is crucial for environmental\nprotection. Compared to manual inspections, computer vision technologies offer\na more efficient solution when deployed on-site. Multispectral imaging provides\ndiverse information about objects under different spectrums, aiding in the\ndifferentiation between waste and lakeside lawn environments. This study\nintroduces WasteMS, the first multispectral dataset established for the\nsemantic segmentation of lakeside waste. WasteMS includes a diverse range of\nwaste types in lawn environments, captured under various lighting conditions.\nWe implemented a rigorous annotation process to label waste in images.\nRepresentative semantic segmentation frameworks were used to evaluate\nsegmentation accuracy using WasteMS. Challenges encountered when using WasteMS\nfor segmenting waste on lakeside lawns were discussed. The WasteMS dataset is\navailable at https://github.com/zhuqinfeng1999/WasteMS.\n","authors":["Qinfeng Zhu","Ningxin Weng","Lei Fan","Yuanzhi Cai"],"pdf_url":"https://arxiv.org/pdf/2407.17028v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.09084v4","updated":"2024-07-24T06:07:28Z","published":"2023-08-17T16:23:52Z","title":"MovePose: A High-performance Human Pose Estimation Algorithm on Mobile\n  and Edge Devices","summary":"  We present MovePose, an optimized lightweight convolutional neural network\ndesigned specifically for real-time body pose estimation on CPU-based mobile\ndevices. The current solutions do not provide satisfactory accuracy and speed\nfor human posture estimation, and MovePose addresses this gap. It aims to\nmaintain real-time performance while improving the accuracy of human posture\nestimation for mobile devices. Our MovePose algorithm has attained an Mean\nAverage Precision (mAP) score of 68.0 on the COCO \\cite{cocodata} validation\ndataset. The MovePose algorithm displayed efficiency with a performance of 69+\nframes per second (fps) when run on an Intel i9-10920x CPU. Additionally, it\nshowcased an increased performance of 452+ fps on an NVIDIA RTX3090 GPU. On an\nAndroid phone equipped with a Snapdragon 8 + 4G processor, the fps reached\nabove 11. To enhance accuracy, we incorporated three techniques: deconvolution,\nlarge kernel convolution, and coordinate classification methods. Compared to\nbasic upsampling, deconvolution is trainable, improves model capacity, and\nenhances the receptive field. Large kernel convolution strengthens these\nproperties at a decreased computational cost. In summary, MovePose provides\nhigh accuracy and real-time performance, marking it a potential tool for a\nvariety of applications, including those focused on mobile-side human posture\nestimation. The code and models for this algorithm will be made publicly\naccessible.\n","authors":["Dongyang Yu","Haoyue Zhang","Ruisheng Zhao","Guoqi Chen","Wangpeng An","Yanhong Yang"],"pdf_url":"https://arxiv.org/pdf/2308.09084v4.pdf","comment":"This paper has been accepted by ICANN 2024 and is an oral\n  presentation"},{"id":"http://arxiv.org/abs/2407.17020v1","updated":"2024-07-24T06:00:33Z","published":"2024-07-24T06:00:33Z","title":"EAFormer: Scene Text Segmentation with Edge-Aware Transformers","summary":"  Scene text segmentation aims at cropping texts from scene images, which is\nusually used to help generative models edit or remove texts. The existing text\nsegmentation methods tend to involve various text-related supervisions for\nbetter performance. However, most of them ignore the importance of text edges,\nwhich are significant for downstream applications. In this paper, we propose\nEdge-Aware Transformers, termed EAFormer, to segment texts more accurately,\nespecially at the edge of texts. Specifically, we first design a text edge\nextractor to detect edges and filter out edges of non-text areas. Then, we\npropose an edge-guided encoder to make the model focus more on text edges.\nFinally, an MLP-based decoder is employed to predict text masks. We have\nconducted extensive experiments on commonly-used benchmarks to verify the\neffectiveness of EAFormer. The experimental results demonstrate that the\nproposed method can perform better than previous methods, especially on the\nsegmentation of text edges. Considering that the annotations of several\nbenchmarks (e.g., COCO_TS and MLT_S) are not accurate enough to fairly evaluate\nour methods, we have relabeled these datasets. Through experiments, we observe\nthat our method can achieve a higher performance improvement when more accurate\nannotations are used for training.\n","authors":["Haiyang Yu","Teng Fu","Bin Li","Xiangyang Xue"],"pdf_url":"https://arxiv.org/pdf/2407.17020v1.pdf","comment":"ECCV 2024"},{"id":"http://arxiv.org/abs/2407.16248v2","updated":"2024-07-24T05:56:55Z","published":"2024-07-23T07:36:54Z","title":"Spatiotemporal Graph Guided Multi-modal Network for Livestreaming\n  Product Retrieval","summary":"  With the rapid expansion of e-commerce, more consumers have become accustomed\nto making purchases via livestreaming. Accurately identifying the products\nbeing sold by salespeople, i.e., livestreaming product retrieval (LPR), poses a\nfundamental and daunting challenge. The LPR task encompasses three primary\ndilemmas in real-world scenarios: 1) the recognition of intended products from\ndistractor products present in the background; 2) the video-image heterogeneity\nthat the appearance of products showcased in live streams often deviates\nsubstantially from standardized product images in stores; 3) there are numerous\nconfusing products with subtle visual nuances in the shop. To tackle these\nchallenges, we propose the Spatiotemporal Graphing Multi-modal Network (SGMN).\nFirst, we employ a text-guided attention mechanism that leverages the spoken\ncontent of salespeople to guide the model to focus toward intended products,\nemphasizing their salience over cluttered background products. Second, a\nlong-range spatiotemporal graph network is further designed to achieve both\ninstance-level interaction and frame-level matching, solving the misalignment\ncaused by video-image heterogeneity. Third, we propose a multi-modal hard\nexample mining, assisting the model in distinguishing highly similar products\nwith fine-grained features across the video-image-text domain. Through\nextensive quantitative and qualitative experiments, we demonstrate the superior\nperformance of our proposed SGMN model, surpassing the state-of-the-art methods\nby a substantial margin. The code is available at\nhttps://github.com/Huxiaowan/SGMN.\n","authors":["Xiaowan Hu","Yiyi Chen","Yan Li","Minquan Wang","Haoqian Wang","Quan Chen","Han Li","Peng Jiang"],"pdf_url":"https://arxiv.org/pdf/2407.16248v2.pdf","comment":"9 pages, 12 figures"},{"id":"http://arxiv.org/abs/2308.03725v2","updated":"2024-07-24T05:49:27Z","published":"2023-08-07T17:07:48Z","title":"Efficient Temporal Sentence Grounding in Videos with Multi-Teacher\n  Knowledge Distillation","summary":"  Temporal Sentence Grounding in Videos (TSGV) aims to detect the event\ntimestamps described by the natural language query from untrimmed videos. This\npaper discusses the challenge of achieving efficient computation in TSGV models\nwhile maintaining high performance. Most existing approaches exquisitely design\ncomplex architectures to improve accuracy with extra layers and loss, suffering\nfrom inefficiency and heaviness. Although some works have noticed that, they\nonly make an issue of feature fusion layers, which can hardly enjoy the\nhighspeed merit in the whole clunky network. To tackle this problem, we propose\na novel efficient multi-teacher model (EMTM) based on knowledge distillation to\ntransfer diverse knowledge from both heterogeneous and isomorphic networks.\nSpecifically, We first unify different outputs of the heterogeneous models into\none single form. Next, a Knowledge Aggregation Unit (KAU) is built to acquire\nhigh-quality integrated soft labels from multiple teachers. After that, the KAU\nmodule leverages the multi-scale video and global query information to\nadaptively determine the weights of different teachers. A Shared Encoder\nstrategy is then proposed to solve the problem that the student shallow layers\nhardly benefit from teachers, in which an isomorphic teacher is collaboratively\ntrained with the student to align their hidden states. Extensive experimental\nresults on three popular TSGV benchmarks demonstrate that our method is both\neffective and efficient without bells and whistles.\n","authors":["Renjie Liang","Yiming Yang","Hui Lu","Li Li"],"pdf_url":"https://arxiv.org/pdf/2308.03725v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09624v3","updated":"2024-07-24T05:28:49Z","published":"2024-04-15T09:56:20Z","title":"AesExpert: Towards Multi-modality Foundation Model for Image Aesthetics\n  Perception","summary":"  The highly abstract nature of image aesthetics perception (IAP) poses\nsignificant challenge for current multimodal large language models (MLLMs). The\nlack of human-annotated multi-modality aesthetic data further exacerbates this\ndilemma, resulting in MLLMs falling short of aesthetics perception\ncapabilities. To address the above challenge, we first introduce a\ncomprehensively annotated Aesthetic Multi-Modality Instruction Tuning (AesMMIT)\ndataset, which serves as the footstone for building multi-modality aesthetics\nfoundation models. Specifically, to align MLLMs with human aesthetics\nperception, we construct a corpus-rich aesthetic critique database with 21,904\ndiverse-sourced images and 88K human natural language feedbacks, which are\ncollected via progressive questions, ranging from coarse-grained aesthetic\ngrades to fine-grained aesthetic descriptions. To ensure that MLLMs can handle\ndiverse queries, we further prompt GPT to refine the aesthetic critiques and\nassemble the large-scale aesthetic instruction tuning dataset, i.e. AesMMIT,\nwhich consists of 409K multi-typed instructions to activate stronger aesthetic\ncapabilities. Based on the AesMMIT database, we fine-tune the open-sourced\ngeneral foundation models, achieving multi-modality Aesthetic Expert models,\ndubbed AesExpert. Extensive experiments demonstrate that the proposed AesExpert\nmodels deliver significantly better aesthetic perception performances than the\nstate-of-the-art MLLMs, including the most advanced GPT-4V and\nGemini-Pro-Vision. Project homepage: https://yipoh.github.io/aes-expert/.\n","authors":["Yipo Huang","Xiangfei Sheng","Zhichao Yang","Quan Yuan","Zhichao Duan","Pengfei Chen","Leida Li","Weisi Lin","Guangming Shi"],"pdf_url":"https://arxiv.org/pdf/2404.09624v3.pdf","comment":"Accepted by ACMMM24"},{"id":"http://arxiv.org/abs/2404.09490v2","updated":"2024-07-24T05:08:08Z","published":"2024-04-15T06:24:56Z","title":"Leveraging Temporal Contextualization for Video Action Recognition","summary":"  We propose a novel framework for video understanding, called Temporally\nContextualized CLIP (TC-CLIP), which leverages essential temporal information\nthrough global interactions in a spatio-temporal domain within a video. To be\nspecific, we introduce Temporal Contextualization (TC), a layer-wise temporal\ninformation infusion mechanism for videos, which 1) extracts core information\nfrom each frame, 2) connects relevant information across frames for the\nsummarization into context tokens, and 3) leverages the context tokens for\nfeature encoding. Furthermore, the Video-conditional Prompting (VP) module\nprocesses context tokens to generate informative prompts in the text modality.\nExtensive experiments in zero-shot, few-shot, base-to-novel, and\nfully-supervised action recognition validate the effectiveness of our model.\nAblation studies for TC and VP support our design choices. Our project page\nwith the source code is available at https://github.com/naver-ai/tc-clip\n","authors":["Minji Kim","Dongyoon Han","Taekyung Kim","Bohyung Han"],"pdf_url":"https://arxiv.org/pdf/2404.09490v2.pdf","comment":"26 pages, 11 figures, 16 tables. To be presented at ECCV'24"},{"id":"http://arxiv.org/abs/2405.07987v4","updated":"2024-07-24T05:01:21Z","published":"2024-05-13T17:58:30Z","title":"The Platonic Representation Hypothesis","summary":"  We argue that representations in AI models, particularly deep networks, are\nconverging. First, we survey many examples of convergence in the literature:\nover time and across multiple domains, the ways by which different neural\nnetworks represent data are becoming more aligned. Next, we demonstrate\nconvergence across data modalities: as vision models and language models get\nlarger, they measure distance between datapoints in a more and more alike way.\nWe hypothesize that this convergence is driving toward a shared statistical\nmodel of reality, akin to Plato's concept of an ideal reality. We term such a\nrepresentation the platonic representation and discuss several possible\nselective pressures toward it. Finally, we discuss the implications of these\ntrends, their limitations, and counterexamples to our analysis.\n","authors":["Minyoung Huh","Brian Cheung","Tongzhou Wang","Phillip Isola"],"pdf_url":"https://arxiv.org/pdf/2405.07987v4.pdf","comment":"Equal contributions. Project: https://phillipi.github.io/prh/ Code:\n  https://github.com/minyoungg/platonic-rep"},{"id":"http://arxiv.org/abs/2407.17003v1","updated":"2024-07-24T05:00:31Z","published":"2024-07-24T05:00:31Z","title":"Progressive Query Refinement Framework for Bird's-Eye-View Semantic\n  Segmentation from Surrounding Images","summary":"  Expressing images with Multi-Resolution (MR) features has been widely adopted\nin many computer vision tasks. In this paper, we introduce the MR concept into\nBird's-Eye-View (BEV) semantic segmentation for autonomous driving. This\nintroduction enhances our model's ability to capture both global and local\ncharacteristics of driving scenes through our proposed residual learning.\nSpecifically, given a set of MR BEV query maps, the lowest resolution query map\nis initially updated using a View Transformation (VT) encoder. This updated\nquery map is then upscaled and merged with a higher resolution query map to\nundergo further updates in a subsequent VT encoder. This process is repeated\nuntil the resolution of the updated query map reaches the target. Finally, the\nlowest resolution map is added to the target resolution to generate the final\nquery map. During training, we enforce both the lowest and final query maps to\nalign with the ground-truth BEV semantic map to help our model effectively\ncapture the global and local characteristics. We also propose a visual feature\ninteraction network that promotes interactions between features across images\nand across feature levels, thus highly contributing to the performance\nimprovement. We evaluate our model on a large-scale real-world dataset. The\nexperimental results show that our model outperforms the SOTA models in terms\nof IoU metric. Codes are available at\nhttps://github.com/d1024choi/ProgressiveQueryRefineNet\n","authors":["Dooseop Choi","Jungyu Kang","Taeghyun An","Kyounghwan Ahn","KyoungWook Min"],"pdf_url":"https://arxiv.org/pdf/2407.17003v1.pdf","comment":"IROS 2024"},{"id":"http://arxiv.org/abs/2312.17432v4","updated":"2024-07-24T04:44:11Z","published":"2023-12-29T01:56:17Z","title":"Video Understanding with Large Language Models: A Survey","summary":"  With the burgeoning growth of online video platforms and the escalating\nvolume of video content, the demand for proficient video understanding tools\nhas intensified markedly. Given the remarkable capabilities of large language\nmodels (LLMs) in language and multimodal tasks, this survey provides a detailed\noverview of recent advancements in video understanding that harness the power\nof LLMs (Vid-LLMs). The emergent capabilities of Vid-LLMs are surprisingly\nadvanced, particularly their ability for open-ended multi-granularity (general,\ntemporal, and spatiotemporal) reasoning combined with commonsense knowledge,\nsuggesting a promising path for future video understanding. We examine the\nunique characteristics and capabilities of Vid-LLMs, categorizing the\napproaches into three main types: Video Analyzer x LLM, Video Embedder x LLM,\nand (Analyzer + Embedder) x LLM. Furthermore, we identify five sub-types based\non the functions of LLMs in Vid-LLMs: LLM as Summarizer, LLM as Manager, LLM as\nText Decoder, LLM as Regressor, and LLM as Hidden Layer. Furthermore, this\nsurvey presents a comprehensive study of the tasks, datasets, benchmarks, and\nevaluation methodologies for Vid-LLMs. Additionally, it explores the expansive\napplications of Vid-LLMs across various domains, highlighting their remarkable\nscalability and versatility in real-world video understanding challenges.\nFinally, it summarizes the limitations of existing Vid-LLMs and outlines\ndirections for future research. For more information, readers are recommended\nto visit the repository at\nhttps://github.com/yunlong10/Awesome-LLMs-for-Video-Understanding.\n","authors":["Yunlong Tang","Jing Bi","Siting Xu","Luchuan Song","Susan Liang","Teng Wang","Daoan Zhang","Jie An","Jingyang Lin","Rongyi Zhu","Ali Vosoughi","Chao Huang","Zeliang Zhang","Pinxin Liu","Mingqian Feng","Feng Zheng","Jianguo Zhang","Ping Luo","Jiebo Luo","Chenliang Xu"],"pdf_url":"https://arxiv.org/pdf/2312.17432v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.18293v3","updated":"2024-07-24T04:43:30Z","published":"2024-02-28T12:38:44Z","title":"Continuous Memory Representation for Anomaly Detection","summary":"  There have been significant advancements in anomaly detection in an\nunsupervised manner, where only normal images are available for training.\nSeveral recent methods aim to detect anomalies based on a memory, comparing or\nreconstructing the input with directly stored normal features (or trained\nfeatures with normal images). However, such memory-based approaches operate on\na discrete feature space implemented by the nearest neighbor or attention\nmechanism, suffering from poor generalization or an identity shortcut issue\noutputting the same as input, respectively. Furthermore, the majority of\nexisting methods are designed to detect single-class anomalies, resulting in\nunsatisfactory performance when presented with multiple classes of objects. To\ntackle all of the above challenges, we propose CRAD, a novel anomaly detection\nmethod for representing normal features within a \"continuous\" memory, enabled\nby transforming spatial features into coordinates and mapping them to\ncontinuous grids. Furthermore, we carefully design the grids tailored for\nanomaly detection, representing both local and global normal features and\nfusing them effectively. Our extensive experiments demonstrate that CRAD\nsuccessfully generalizes the normal features and mitigates the identity\nshortcut, furthermore, CRAD effectively handles diverse classes in a single\nmodel thanks to the high-granularity continuous representation. In an\nevaluation using the MVTec AD dataset, CRAD significantly outperforms the\nprevious state-of-the-art method by reducing 65.0% of the error for multi-class\nunified anomaly detection. The project page is available at\nhttps://tae-mo.github.io/crad/.\n","authors":["Joo Chan Lee","Taejune Kim","Eunbyung Park","Simon S. Woo","Jong Hwan Ko"],"pdf_url":"https://arxiv.org/pdf/2402.18293v3.pdf","comment":"Project page: https://tae-mo.github.io/crad/"},{"id":"http://arxiv.org/abs/2407.16993v1","updated":"2024-07-24T04:27:03Z","published":"2024-07-24T04:27:03Z","title":"LoFormer: Local Frequency Transformer for Image Deblurring","summary":"  Due to the computational complexity of self-attention (SA), prevalent\ntechniques for image deblurring often resort to either adopting localized SA or\nemploying coarse-grained global SA methods, both of which exhibit drawbacks\nsuch as compromising global modeling or lacking fine-grained correlation. In\norder to address this issue by effectively modeling long-range dependencies\nwithout sacrificing fine-grained details, we introduce a novel approach termed\nLocal Frequency Transformer (LoFormer). Within each unit of LoFormer, we\nincorporate a Local Channel-wise SA in the frequency domain (Freq-LC) to\nsimultaneously capture cross-covariance within low- and high-frequency local\nwindows. These operations offer the advantage of (1) ensuring equitable\nlearning opportunities for both coarse-grained structures and fine-grained\ndetails, and (2) exploring a broader range of representational properties\ncompared to coarse-grained global SA methods. Additionally, we introduce an MLP\nGating mechanism complementary to Freq-LC, which serves to filter out\nirrelevant features while enhancing global learning capabilities. Our\nexperiments demonstrate that LoFormer significantly improves performance in the\nimage deblurring task, achieving a PSNR of 34.09 dB on the GoPro dataset with\n126G FLOPs. https://github.com/DeepMed-Lab-ECNU/Single-Image-Deblur\n","authors":["Xintian Mao","Jiansheng Wang","Xingran Xie","Qingli Li","Yan Wang"],"pdf_url":"https://arxiv.org/pdf/2407.16993v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16988v1","updated":"2024-07-24T04:13:43Z","published":"2024-07-24T04:13:43Z","title":"DreamCar: Leveraging Car-specific Prior for in-the-wild 3D Car\n  Reconstruction","summary":"  Self-driving industries usually employ professional artists to build\nexquisite 3D cars. However, it is expensive to craft large-scale digital\nassets. Since there are already numerous datasets available that contain a vast\nnumber of images of cars, we focus on reconstructing high-quality 3D car models\nfrom these datasets. However, these datasets only contain one side of cars in\nthe forward-moving scene. We try to use the existing generative models to\nprovide more supervision information, but they struggle to generalize well in\ncars since they are trained on synthetic datasets not car-specific. In\naddition, The reconstructed 3D car texture misaligns due to a large error in\ncamera pose estimation when dealing with in-the-wild images. These restrictions\nmake it challenging for previous methods to reconstruct complete 3D cars. To\naddress these problems, we propose a novel method, named DreamCar, which can\nreconstruct high-quality 3D cars given a few images even a single image. To\ngeneralize the generative model, we collect a car dataset, named Car360, with\nover 5,600 vehicles. With this dataset, we make the generative model more\nrobust to cars. We use this generative prior specific to the car to guide its\nreconstruction via Score Distillation Sampling. To further complement the\nsupervision information, we utilize the geometric and appearance symmetry of\ncars. Finally, we propose a pose optimization method that rectifies poses to\ntackle texture misalignment. Extensive experiments demonstrate that our method\nsignificantly outperforms existing methods in reconstructing high-quality 3D\ncars. \\href{https://xiaobiaodu.github.io/dreamcar-project/}{Our code is\navailable.}\n","authors":["Xiaobiao Du","Haiyang Sun","Ming Lu","Tianqing Zhu","Xin Yu"],"pdf_url":"https://arxiv.org/pdf/2407.16988v1.pdf","comment":"Projet Page: https://xiaobiaodu.github.io/dreamcar-project/"},{"id":"http://arxiv.org/abs/2404.09512v2","updated":"2024-07-24T04:06:12Z","published":"2024-04-15T07:15:39Z","title":"Magic Clothing: Controllable Garment-Driven Image Synthesis","summary":"  We propose Magic Clothing, a latent diffusion model (LDM)-based network\narchitecture for an unexplored garment-driven image synthesis task. Aiming at\ngenerating customized characters wearing the target garments with diverse text\nprompts, the image controllability is the most critical issue, i.e., to\npreserve the garment details and maintain faithfulness to the text prompts. To\nthis end, we introduce a garment extractor to capture the detailed garment\nfeatures, and employ self-attention fusion to incorporate them into the\npretrained LDMs, ensuring that the garment details remain unchanged on the\ntarget character. Then, we leverage the joint classifier-free guidance to\nbalance the control of garment features and text prompts over the generated\nresults. Meanwhile, the proposed garment extractor is a plug-in module\napplicable to various finetuned LDMs, and it can be combined with other\nextensions like ControlNet and IP-Adapter to enhance the diversity and\ncontrollability of the generated characters. Furthermore, we design\nMatched-Points-LPIPS (MP-LPIPS), a robust metric for evaluating the consistency\nof the target image to the source garment. Extensive experiments demonstrate\nthat our Magic Clothing achieves state-of-the-art results under various\nconditional controls for garment-driven image synthesis. Our source code is\navailable at https://github.com/ShineChen1024/MagicClothing.\n","authors":["Weifeng Chen","Tao Gu","Yuhao Xu","Chengcai Chen"],"pdf_url":"https://arxiv.org/pdf/2404.09512v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16982v1","updated":"2024-07-24T03:58:58Z","published":"2024-07-24T03:58:58Z","title":"Diffree: Text-Guided Shape Free Object Inpainting with Diffusion Model","summary":"  This paper addresses an important problem of object addition for images with\nonly text guidance. It is challenging because the new object must be integrated\nseamlessly into the image with consistent visual context, such as lighting,\ntexture, and spatial location. While existing text-guided image inpainting\nmethods can add objects, they either fail to preserve the background\nconsistency or involve cumbersome human intervention in specifying bounding\nboxes or user-scribbled masks. To tackle this challenge, we introduce Diffree,\na Text-to-Image (T2I) model that facilitates text-guided object addition with\nonly text control. To this end, we curate OABench, an exquisite synthetic\ndataset by removing objects with advanced image inpainting techniques. OABench\ncomprises 74K real-world tuples of an original image, an inpainted image with\nthe object removed, an object mask, and object descriptions. Trained on OABench\nusing the Stable Diffusion model with an additional mask prediction module,\nDiffree uniquely predicts the position of the new object and achieves object\naddition with guidance from only text. Extensive experiments demonstrate that\nDiffree excels in adding new objects with a high success rate while maintaining\nbackground consistency, spatial appropriateness, and object relevance and\nquality.\n","authors":["Lirui Zhao","Tianshuo Yang","Wenqi Shao","Yuxin Zhang","Yu Qiao","Ping Luo","Kaipeng Zhang","Rongrong Ji"],"pdf_url":"https://arxiv.org/pdf/2407.16982v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16981v1","updated":"2024-07-24T03:58:07Z","published":"2024-07-24T03:58:07Z","title":"Case-Enhanced Vision Transformer: Improving Explanations of Image\n  Similarity with a ViT-based Similarity Metric","summary":"  This short paper presents preliminary research on the Case-Enhanced Vision\nTransformer (CEViT), a similarity measurement method aimed at improving the\nexplainability of similarity assessments for image data. Initial experimental\nresults suggest that integrating CEViT into k-Nearest Neighbor (k-NN)\nclassification yields classification accuracy comparable to state-of-the-art\ncomputer vision models, while adding capabilities for illustrating differences\nbetween classes. CEViT explanations can be influenced by prior cases, to\nillustrate aspects of similarity relevant to those cases.\n","authors":["Ziwei Zhao","David Leake","Xiaomeng Ye","David Crandall"],"pdf_url":"https://arxiv.org/pdf/2407.16981v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16977v1","updated":"2024-07-24T03:45:35Z","published":"2024-07-24T03:45:35Z","title":"Selective Vision-Language Subspace Projection for Few-shot CLIP","summary":"  Vision-language models such as CLIP are capable of mapping the different\nmodality data into a unified feature space, enabling zero/few-shot inference by\nmeasuring the similarity of given images and texts. However, most existing\nmethods overlook modality gaps in CLIP's encoded features, which is shown as\nthe text and image features lie far apart from each other, resulting in limited\nclassification performance. To tackle this issue, we introduce a method called\nSelective Vision-Language Subspace Projection (SSP), which incorporates local\nimage features and utilizes them as a bridge to enhance the alignment between\nimage-text pairs. Specifically, our SSP framework comprises two parallel\nmodules: a vision projector and a language projector. Both projectors utilize\nlocal image features to span the respective subspaces for image and texts,\nthereby projecting the image and text features into their respective subspaces\nto achieve alignment. Moreover, our approach entails only training-free matrix\ncalculations and can be seamlessly integrated into advanced CLIP-based few-shot\nlearning frameworks. Extensive experiments on 11 datasets have demonstrated\nSSP's superior text-image alignment capabilities, outperforming the\nstate-of-the-art alignment methods. The code is available at\nhttps://github.com/zhuhsingyuu/SSP\n","authors":["Xingyu Zhu","Beier Zhu","Yi Tan","Shuo Wang","Yanbin Hao","Hanwang Zhang"],"pdf_url":"https://arxiv.org/pdf/2407.16977v1.pdf","comment":"Accepted to ACM MultiMedia 2024"},{"id":"http://arxiv.org/abs/2407.16962v1","updated":"2024-07-24T03:01:55Z","published":"2024-07-24T03:01:55Z","title":"Toward an Integrated Decision Making Framework for Optimized Stroke\n  Diagnosis with DSA and Treatment under Uncertainty","summary":"  This study addresses the challenge of stroke diagnosis and treatment under\nuncertainty, a critical issue given the rapid progression and severe\nconsequences of stroke conditions such as aneurysms, arteriovenous\nmalformations (AVM), and occlusions. Current diagnostic methods, including\nDigital Subtraction Angiography (DSA), face limitations due to high costs and\nits invasive nature. To overcome these challenges, we propose a novel approach\nusing a Partially Observable Markov Decision Process (POMDP) framework. Our\nmodel integrates advanced diagnostic tools and treatment approaches with a\ndecision-making algorithm that accounts for the inherent uncertainties in\nstroke diagnosis. Our approach combines noisy observations from CT scans,\nSiriraj scores, and DSA reports to inform the subsequent treatment options. We\nutilize the online solver DESPOT, which employs tree-search methods and\nparticle filters, to simulate potential future scenarios and guide our\nstrategies. The results indicate that our POMDP framework balances diagnostic\nand treatment objectives, striking a tradeoff between the need for precise\nstroke identification via invasive procedures like DSA and the constraints of\nlimited healthcare resources that necessitate more cost-effective strategies,\nsuch as in-hospital or at-home observation, by relying only relying on\nsimulation rollouts and not imposing any prior knowledge. Our study offers a\nsignificant contribution by presenting a systematic framework that optimally\nintegrates diagnostic and treatment processes for stroke and accounting for\nvarious uncertainties, thereby improving care and outcomes in stroke\nmanagement.\n","authors":["Nur Ahmad Khatim","Ahmad Azmul Asmar Irfan","Amaliya Mata'ul Hayah","Mansur M. Arief"],"pdf_url":"https://arxiv.org/pdf/2407.16962v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16961v1","updated":"2024-07-24T03:00:53Z","published":"2024-07-24T03:00:53Z","title":"Pose Estimation from Camera Images for Underwater Inspection","summary":"  High-precision localization is pivotal in underwater reinspection missions.\nTraditional localization methods like inertial navigation systems, Doppler\nvelocity loggers, and acoustic positioning face significant challenges and are\nnot cost-effective for some applications. Visual localization is a\ncost-effective alternative in such cases, leveraging the cameras already\nequipped on inspection vehicles to estimate poses from images of the\nsurrounding scene. Amongst these, machine learning-based pose estimation from\nimages shows promise in underwater environments, performing efficient\nrelocalization using models trained based on previously mapped scenes. We\nexplore the efficacy of learning-based pose estimators in both clear and turbid\nwater inspection missions, assessing the impact of image formats, model\narchitectures and training data diversity. We innovate by employing novel view\nsynthesis models to generate augmented training data, significantly enhancing\npose estimation in unexplored regions. Moreover, we enhance localization\naccuracy by integrating pose estimator outputs with sensor data via an extended\nKalman filter, demonstrating improved trajectory smoothness and accuracy.\n","authors":["Luyuan Peng","Hari Vishnu","Mandar Chitre","Yuen Min Too","Bharath Kalyan","Rajat Mishra","Soo Pieng Tan"],"pdf_url":"https://arxiv.org/pdf/2407.16961v1.pdf","comment":"Submitted to IEEE Journal of Oceanic Engineering"},{"id":"http://arxiv.org/abs/2406.11271v2","updated":"2024-07-24T02:59:40Z","published":"2024-06-17T07:21:36Z","title":"MINT-1T: Scaling Open-Source Multimodal Data by 10x: A Multimodal\n  Dataset with One Trillion Tokens","summary":"  Multimodal interleaved datasets featuring free-form interleaved sequences of\nimages and text are crucial for training frontier large multimodal models\n(LMMs). Despite the rapid progression of open-source LMMs, there remains a\npronounced scarcity of large-scale, diverse open-source multimodal interleaved\ndatasets. In response, we introduce MINT-1T, the most extensive and diverse\nopen-source Multimodal INTerleaved dataset to date. MINT-1T comprises one\ntrillion text tokens and 3.4 billion images, a 10x scale-up from existing\nopen-source datasets. Additionally, we include previously untapped sources such\nas PDFs and ArXiv papers. As scaling multimodal interleaved datasets requires\nsubstantial engineering effort, sharing the data curation process and releasing\nthe dataset greatly benefits the community. Our experiments show that LMMs\ntrained on MINT-1T rival the performance of models trained on the previous\nleading dataset, OBELICS. Our data and code will be released at\nhttps://github.com/mlfoundations/MINT-1T.\n","authors":["Anas Awadalla","Le Xue","Oscar Lo","Manli Shu","Hannah Lee","Etash Kumar Guha","Matt Jordan","Sheng Shen","Mohamed Awadalla","Silvio Savarese","Caiming Xiong","Ran Xu","Yejin Choi","Ludwig Schmidt"],"pdf_url":"https://arxiv.org/pdf/2406.11271v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07720v2","updated":"2024-07-24T02:55:17Z","published":"2024-07-10T14:53:37Z","title":"SvANet: A Scale-variant Attention-based Network for Small Medical Object\n  Segmentation","summary":"  Early detection and accurate diagnosis can predict the risk of malignant\ndisease transformation, thereby increasing the probability of effective\ntreatment. A mild syndrome with small infected regions is an ominous warning\nand is foremost in the early diagnosis of diseases. Deep learning algorithms,\nsuch as convolutional neural networks (CNNs), have been used to segment natural\nor medical objects, showing promising results. However, analyzing medical\nobjects of small areas in images remains a challenge due to information losses\nand compression defects caused by convolution and pooling operations in CNNs.\nThese losses and defects become increasingly significant as the network\ndeepens, particularly for small medical objects. To address these challenges,\nwe propose a novel scale-variant attention-based network (SvANet) for accurate\nsmall-scale object segmentation in medical images. The SvANet consists of Monte\nCarlo attention, scale-variant attention, and vision transformer, which\nincorporates cross-scale features and alleviates compression artifacts for\nenhancing the discrimination of small medical objects. Quantitative\nexperimental results demonstrate the superior performance of SvANet, achieving\n96.12%, 96.11%, 89.79%, 84.15%, 80.25%, 73.05%, and 72.58% in mean Dice\ncoefficient for segmenting kidney tumors, skin lesions, hepatic tumors, polyps,\nsurgical excision cells, retinal vasculatures, and sperms, which occupy less\nthan 1% of the image areas in KiTS23, ISIC 2018, ATLAS, PolypGen, TissueNet,\nFIVES, and SpermHealth datasets, respectively.\n","authors":["Wei Dai"],"pdf_url":"https://arxiv.org/pdf/2407.07720v2.pdf","comment":"14 pages, 9 figures, under review"},{"id":"http://arxiv.org/abs/2402.16479v2","updated":"2024-07-24T02:53:00Z","published":"2024-02-26T10:54:26Z","title":"Edge Detectors Can Make Deep Convolutional Neural Networks More Robust","summary":"  Deep convolutional neural networks (DCNN for short) are vulnerable to\nexamples with small perturbations. Improving DCNN's robustness is of great\nsignificance to the safety-critical applications, such as autonomous driving\nand industry automation. Inspired by the principal way that human eyes\nrecognize objects, i.e., largely relying on the shape features, this paper\nfirst employs the edge detectors as layer kernels and designs a binary edge\nfeature branch (BEFB for short) to learn the binary edge features, which can be\neasily integrated into any popular backbone. The four edge detectors can learn\nthe horizontal, vertical, positive diagonal, and negative diagonal edge\nfeatures, respectively, and the branch is stacked by multiple Sobel layers\n(using edge detectors as kernels) and one threshold layer. The binary edge\nfeatures learned by the branch, concatenated with the texture features learned\nby the backbone, are fed into the fully connected layers for classification. We\nintegrate the proposed branch into VGG16 and ResNet34, respectively, and\nconduct experiments on multiple datasets. Experimental results demonstrate the\nBEFB is lightweight and has no side effects on training. And the accuracy of\nthe BEFB integrated models is better than the original ones on all datasets\nwhen facing FGSM, PGD, and C\\&W attacks. Besides, BEFB integrated models\nequipped with the robustness enhancing techniques can achieve better\nclassification accuracy compared to the original models. The work in this paper\nfor the first time shows it is feasible to enhance the robustness of DCNNs\nthrough combining both shape-like features and texture features.\n","authors":["Jin Ding","Jie-Chao Zhao","Yong-Zhi Sun","Ping Tan","Jia-Wei Wang","Ji-En Ma","You-Tong Fang"],"pdf_url":"https://arxiv.org/pdf/2402.16479v2.pdf","comment":"26 pages, 18 figures, 7 tables"},{"id":"http://arxiv.org/abs/2407.16957v1","updated":"2024-07-24T02:48:30Z","published":"2024-07-24T02:48:30Z","title":"Raindrop Clarity: A Dual-Focused Dataset for Day and Night Raindrop\n  Removal","summary":"  Existing raindrop removal datasets have two shortcomings. First, they consist\nof images captured by cameras with a focus on the background, leading to the\npresence of blurry raindrops. To our knowledge, none of these datasets include\nimages where the focus is specifically on raindrops, which results in a blurry\nbackground. Second, these datasets predominantly consist of daytime images,\nthereby lacking nighttime raindrop scenarios. Consequently, algorithms trained\non these datasets may struggle to perform effectively in raindrop-focused or\nnighttime scenarios. The absence of datasets specifically designed for\nraindrop-focused and nighttime raindrops constrains research in this area. In\nthis paper, we introduce a large-scale, real-world raindrop removal dataset\ncalled Raindrop Clarity. Raindrop Clarity comprises 15,186 high-quality\npairs/triplets (raindrops, blur, and background) of images with raindrops and\nthe corresponding clear background images. There are 5,442 daytime raindrop\nimages and 9,744 nighttime raindrop images. Specifically, the 5,442 daytime\nimages include 3,606 raindrop- and 1,836 background-focused images. While the\n9,744 nighttime images contain 4,838 raindrop- and 4,906 background-focused\nimages. Our dataset will enable the community to explore background-focused and\nraindrop-focused images, including challenges unique to daytime and nighttime\nconditions. Our data and code are available at:\n\\url{https://github.com/jinyeying/RaindropClarity}\n","authors":["Yeying Jin","Xin Li","Jiadong Wang","Yan Zhang","Malu Zhang"],"pdf_url":"https://arxiv.org/pdf/2407.16957v1.pdf","comment":"Accepted to ECCV2024, dataset and benchmark at:\n  \\url{https://github.com/jinyeying/RaindropClarity}"},{"id":"http://arxiv.org/abs/2407.11219v2","updated":"2024-07-24T02:45:37Z","published":"2024-07-15T20:07:45Z","title":"TLRN: Temporal Latent Residual Networks For Large Deformation Image\n  Registration","summary":"  This paper presents a novel approach, termed {\\em Temporal Latent Residual\nNetwork (TLRN)}, to predict a sequence of deformation fields in time-series\nimage registration. The challenge of registering time-series images often lies\nin the occurrence of large motions, especially when images differ significantly\nfrom a reference (e.g., the start of a cardiac cycle compared to the peak\nstretching phase). To achieve accurate and robust registration results, we\nleverage the nature of motion continuity and exploit the temporal smoothness in\nconsecutive image frames. Our proposed TLRN highlights a temporal residual\nnetwork with residual blocks carefully designed in latent deformation spaces,\nwhich are parameterized by time-sequential initial velocity fields. We treat a\nsequence of residual blocks over time as a dynamic training system, where each\nblock is designed to learn the residual function between desired deformation\nfeatures and current input accumulated from previous time frames. We validate\nthe effectivenss of TLRN on both synthetic data and real-world cine cardiac\nmagnetic resonance (CMR) image videos. Our experimental results shows that TLRN\nis able to achieve substantially improved registration accuracy compared to the\nstate-of-the-art. Our code is publicly available at\nhttps://github.com/nellie689/TLRN.\n","authors":["Nian Wu","Jiarui Xing","Miaomiao Zhang"],"pdf_url":"https://arxiv.org/pdf/2407.11219v2.pdf","comment":"10 pages. Accepted by MICCAI 2024"},{"id":"http://arxiv.org/abs/2407.16955v1","updated":"2024-07-24T02:44:41Z","published":"2024-07-24T02:44:41Z","title":"DVPE: Divided View Position Embedding for Multi-View 3D Object Detection","summary":"  Sparse query-based paradigms have achieved significant success in multi-view\n3D detection for autonomous vehicles. Current research faces challenges in\nbalancing between enlarging receptive fields and reducing interference when\naggregating multi-view features. Moreover, different poses of cameras present\nchallenges in training global attention models. To address these problems, this\npaper proposes a divided view method, in which features are modeled globally\nvia the visibility crossattention mechanism, but interact only with partial\nfeatures in a divided local virtual space. This effectively reduces\ninterference from other irrelevant features and alleviates the training\ndifficulties of the transformer by decoupling the position embedding from\ncamera poses. Additionally, 2D historical RoI features are incorporated into\nthe object-centric temporal modeling to utilize highlevel visual semantic\ninformation. The model is trained using a one-to-many assignment strategy to\nfacilitate stability. Our framework, named DVPE, achieves state-of-the-art\nperformance (57.2% mAP and 64.5% NDS) on the nuScenes test set. Codes will be\navailable at https://github.com/dop0/DVPE.\n","authors":["Jiasen Wang","Zhenglin Li","Ke Sun","Xianyuan Liu","Yang Zhou"],"pdf_url":"https://arxiv.org/pdf/2407.16955v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16953v1","updated":"2024-07-24T02:41:19Z","published":"2024-07-24T02:41:19Z","title":"Open Challenges on Fairness of Artificial Intelligence in Medical\n  Imaging Applications","summary":"  Recently, the research community of computerized medical imaging has started\nto discuss and address potential fairness issues that may emerge when\ndeveloping and deploying AI systems for medical image analysis. This chapter\ncovers some of the pressing challenges encountered when doing research in this\narea, and it is intended to raise questions and provide food for thought for\nthose aiming to enter this research field. The chapter first discusses various\nsources of bias, including data collection, model training, and clinical\ndeployment, and their impact on the fairness of machine learning algorithms in\nmedical image computing. We then turn to discussing open challenges that we\nbelieve require attention from researchers and practitioners, as well as\npotential pitfalls of naive application of common methods in the field. We\ncover a variety of topics including the impact of biased metrics when auditing\nfor fairness, the leveling down effect, task difficulty variations among\nsubgroups, discovering biases in unseen populations, and explaining biases\nbeyond standard demographic attributes.\n","authors":["Enzo Ferrante","Rodrigo Echeveste"],"pdf_url":"https://arxiv.org/pdf/2407.16953v1.pdf","comment":"Published as part of the book \"Trustworthy AI in Medical Imaging\"\n  (Elsevier, 2024) available at\n  https://shop.elsevier.com/books/trustworthy-ai-in-medical-imaging/lorenzi/978-0-443-23761-4"},{"id":"http://arxiv.org/abs/2405.02363v2","updated":"2024-07-24T02:36:07Z","published":"2024-05-03T05:09:54Z","title":"LLM as Dataset Analyst: Subpopulation Structure Discovery with Large\n  Language Model","summary":"  The distribution of subpopulations is an important property hidden within a\ndataset. Uncovering and analyzing the subpopulation distribution within\ndatasets provides a comprehensive understanding of the datasets, standing as a\npowerful tool beneficial to various downstream tasks, including Dataset\nSubpopulation Organization, Subpopulation Shift, and Slice Discovery. Despite\nits importance, there has been no work that systematically explores the\nsubpopulation distribution of datasets to our knowledge. To address the\nlimitation and solve all the mentioned tasks in a unified way, we introduce a\nnovel concept of subpopulation structures to represent, analyze, and utilize\nsubpopulation distributions within datasets. To characterize the structures in\nan interpretable manner, we propose the Subpopulation Structure Discovery with\nLarge Language Models (SSD-LLM) framework, which employs world knowledge and\ninstruction-following capabilities of Large Language Models (LLMs) to\nlinguistically analyze informative image captions and summarize the structures.\nFurthermore, we propose complete workflows to address downstream tasks, named\nTask-specific Tuning, showcasing the application of the discovered structure to\na spectrum of subpopulation-related tasks, including dataset subpopulation\norganization, subpopulation shift, and slice discovery. Furthermore, we propose\ncomplete workflows to address downstream tasks, named Task-specific Tuning,\nshowcasing the application of the discovered structure to a spectrum of\nsubpopulation-related tasks, including dataset subpopulation organization,\nsubpopulation shift, and slice discovery.\n","authors":["Yulin Luo","Ruichuan An","Bocheng Zou","Yiming Tang","Jiaming Liu","Shanghang Zhang"],"pdf_url":"https://arxiv.org/pdf/2405.02363v2.pdf","comment":"ECCV24 Camera Ready"},{"id":"http://arxiv.org/abs/2407.16945v1","updated":"2024-07-24T02:24:21Z","published":"2024-07-24T02:24:21Z","title":"Affective Behaviour Analysis via Progressive Learning","summary":"  Affective Behavior Analysis aims to develop emotionally intelligent\ntechnology that can recognize and respond to human emotions. To advance this,\nthe 7th Affective Behavior Analysis in-the-wild (ABAW) competition establishes\ntwo tracks: i.e., the Multi-task Learning (MTL) Challenge and the Compound\nExpression (CE) challenge based on Aff-Wild2 and C-EXPR-DB datasets. In this\npaper, we present our methods and experimental results for the two competition\ntracks. Specifically, it can be summarized in the following four aspects: 1) To\nattain high-quality facial features, we train a Masked-Auto Encoder in a\nself-supervised manner. 2) We devise a temporal convergence module to capture\nthe temporal information between video frames and explore the impact of window\nsize and sequence length on each sub-task. 3) To facilitate the joint\noptimization of various sub-tasks, we explore the impact of sub-task joint\ntraining and feature fusion from individual tasks on each task performance\nimprovement. 4) We utilize curriculum learning to transition the model from\nrecognizing single expressions to recognizing compound expressions, thereby\nimproving the accuracy of compound expression recognition. Extensive\nexperiments demonstrate the superiority of our designs.\n","authors":["Chen Liu","Wei Zhang","Feng Qiu","Lincheng Li","Xin Yu"],"pdf_url":"https://arxiv.org/pdf/2407.16945v1.pdf","comment":"Techical Report for 7th ABAW Competition"},{"id":"http://arxiv.org/abs/2407.16943v1","updated":"2024-07-24T02:23:02Z","published":"2024-07-24T02:23:02Z","title":"McGAN: Generating Manufacturable Designs by Embedding Manufacturing\n  Rules into Conditional Generative Adversarial Network","summary":"  Generative design (GD) methods aim to automatically generate a wide variety\nof designs that satisfy functional or aesthetic design requirements. However,\nresearch to date generally lacks considerations of manufacturability of the\ngenerated designs. To this end, we propose a novel GD approach by using deep\nneural networks to encode design for manufacturing (DFM) rules, thereby\nmodifying part designs to make them manufacturable by a given manufacturing\nprocess. Specifically, a three-step approach is proposed: first, an instance\nsegmentation method, Mask R-CNN, is used to decompose a part design into\nsubregions. Second, a conditional generative adversarial neural network (cGAN),\nPix2Pix, transforms unmanufacturable decomposed subregions into manufacturable\nsubregions. The transformed subregions of designs are subsequently reintegrated\ninto a unified manufacturable design. These three steps, Mask-RCNN, Pix2Pix,\nand reintegration, form the basis of the proposed Manufacturable conditional\nGAN (McGAN) framework. Experimental results show that McGAN can transform\nexisting unmanufacturable designs to generate their corresponding\nmanufacturable counterparts automatically that realize the specified\nmanufacturing rules in an efficient and robust manner. The effectiveness of\nMcGAN is demonstrated through two-dimensional design case studies of an\ninjection molding process.\n","authors":["Zhichao Wang","Xiaoliang Yan","Shreyes Melkote","David Rosen"],"pdf_url":"https://arxiv.org/pdf/2407.16943v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.17086v2","updated":"2024-07-24T01:41:01Z","published":"2023-11-28T02:31:52Z","title":"PEA-Diffusion: Parameter-Efficient Adapter with Knowledge Distillation\n  in non-English Text-to-Image Generation","summary":"  Text-to-image diffusion models are well-known for their ability to generate\nrealistic images based on textual prompts. However, the existing works have\npredominantly focused on English, lacking support for non-English text-to-image\nmodels. The most commonly used translation methods cannot solve the generation\nproblem related to language culture, while training from scratch on a specific\nlanguage dataset is prohibitively expensive. In this paper, we are inspired to\npropose a simple plug-and-play language transfer method based on knowledge\ndistillation. All we need to do is train a lightweight MLP-like\nparameter-efficient adapter (PEA) with only 6M parameters under teacher\nknowledge distillation along with a small parallel data corpus. We are\nsurprised to find that freezing the parameters of UNet can still achieve\nremarkable performance on the language-specific prompt evaluation set,\ndemonstrating that PEA can stimulate the potential generation ability of the\noriginal UNet. Additionally, it closely approaches the performance of the\nEnglish text-to-image model on a general prompt evaluation set. Furthermore,\nour adapter can be used as a plugin to achieve significant results in\ndownstream tasks in cross-lingual text-to-image generation. Code will be\navailable at: https://github.com/OPPO-Mente-Lab/PEA-Diffusion\n","authors":["Jian Ma","Chen Chen","Qingsong Xie","Haonan Lu"],"pdf_url":"https://arxiv.org/pdf/2311.17086v2.pdf","comment":"ECCV 2024"},{"id":"http://arxiv.org/abs/2407.16921v1","updated":"2024-07-24T01:11:28Z","published":"2024-07-24T01:11:28Z","title":"SAR to Optical Image Translation with Color Supervised Diffusion Model","summary":"  Synthetic Aperture Radar (SAR) offers all-weather, high-resolution imaging\ncapabilities, but its complex imaging mechanism often poses challenges for\ninterpretation. In response to these limitations, this paper introduces an\ninnovative generative model designed to transform SAR images into more\nintelligible optical images, thereby enhancing the interpretability of SAR\nimages. Specifically, our model backbone is based on the recent diffusion\nmodels, which have powerful generative capabilities. We employ SAR images as\nconditional guides in the sampling process and integrate color supervision to\ncounteract color shift issues effectively. We conducted experiments on the\nSEN12 dataset and employed quantitative evaluations using peak signal-to-noise\nratio, structural similarity, and fr\\'echet inception distance. The results\ndemonstrate that our model not only surpasses previous methods in quantitative\nassessments but also significantly enhances the visual quality of the generated\nimages.\n","authors":["Xinyu Bai","Feng Xu"],"pdf_url":"https://arxiv.org/pdf/2407.16921v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16552v2","updated":"2024-07-24T01:09:36Z","published":"2024-07-23T15:05:55Z","title":"MicroEmo: Time-Sensitive Multimodal Emotion Recognition with\n  Micro-Expression Dynamics in Video Dialogues","summary":"  Multimodal Large Language Models (MLLMs) have demonstrated remarkable\nmultimodal emotion recognition capabilities, integrating multimodal cues from\nvisual, acoustic, and linguistic contexts in the video to recognize human\nemotional states. However, existing methods ignore capturing local facial\nfeatures of temporal dynamics of micro-expressions and do not leverage the\ncontextual dependencies of the utterance-aware temporal segments in the video,\nthereby limiting their expected effectiveness to a certain extent. In this\nwork, we propose MicroEmo, a time-sensitive MLLM aimed at directing attention\nto the local facial micro-expression dynamics and the contextual dependencies\nof utterance-aware video clips. Our model incorporates two key architectural\ncontributions: (1) a global-local attention visual encoder that integrates\nglobal frame-level timestamp-bound image features with local facial features of\ntemporal dynamics of micro-expressions; (2) an utterance-aware video Q-Former\nthat captures multi-scale and contextual dependencies by generating visual\ntoken sequences for each utterance segment and for the entire video then\ncombining them. Preliminary qualitative experiments demonstrate that in a new\nExplainable Multimodal Emotion Recognition (EMER) task that exploits\nmulti-modal and multi-faceted clues to predict emotions in an open-vocabulary\n(OV) manner, MicroEmo demonstrates its effectiveness compared with the latest\nmethods.\n","authors":["Liyun Zhang"],"pdf_url":"https://arxiv.org/pdf/2407.16552v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.02508v4","updated":"2024-07-24T00:53:26Z","published":"2024-05-03T22:42:00Z","title":"Rasterized Edge Gradients: Handling Discontinuities Differentiably","summary":"  Computing the gradients of a rendering process is paramount for diverse\napplications in computer vision and graphics. However, accurate computation of\nthese gradients is challenging due to discontinuities and rendering\napproximations, particularly for surface-based representations and\nrasterization-based rendering. We present a novel method for computing\ngradients at visibility discontinuities for rasterization-based differentiable\nrenderers. Our method elegantly simplifies the traditionally complex problem\nthrough a carefully designed approximation strategy, allowing for a\nstraightforward, effective, and performant solution. We introduce a novel\nconcept of micro-edges, which allows us to treat the rasterized images as\noutcomes of a differentiable, continuous process aligned with the inherently\nnon-differentiable, discrete-pixel rasterization. This technique eliminates the\nnecessity for rendering approximations or other modifications to the forward\npass, preserving the integrity of the rendered image, which makes it applicable\nto rasterized masks, depth, and normals images where filtering is prohibitive.\nUtilizing micro-edges simplifies gradient interpretation at discontinuities and\nenables handling of geometry intersections, offering an advantage over the\nprior art. We showcase our method in dynamic human head scene reconstruction,\ndemonstrating effective handling of camera images and segmentation masks.\n","authors":["Stanislav Pidhorskyi","Tomas Simon","Gabriel Schwartz","He Wen","Yaser Sheikh","Jason Saragih"],"pdf_url":"https://arxiv.org/pdf/2405.02508v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16369v2","updated":"2024-07-24T00:49:00Z","published":"2024-07-23T10:34:02Z","title":"FCNR: Fast Compressive Neural Representation of Visualization Images","summary":"  We present FCNR, a fast compressive neural representation for tens of\nthousands of visualization images under varying viewpoints and timesteps. The\nexisting NeRVI solution, albeit enjoying a high compression ratio, incurs slow\nspeeds in encoding and decoding. Built on the recent advances in stereo image\ncompression, FCNR assimilates stereo context modules and joint context transfer\nmodules to compress image pairs. Our solution significantly improves encoding\nand decoding speed while maintaining high reconstruction quality and satisfying\ncompression ratio. To demonstrate its effectiveness, we compare FCNR with\nstate-of-the-art neural compression methods, including E-NeRV, HNeRV, NeRVI,\nand ECSIC. The source code can be found at\nhttps://github.com/YunfeiLu0112/FCNR.\n","authors":["Yunfei Lu","Pengfei Gu","Chaoli Wang"],"pdf_url":"https://arxiv.org/pdf/2407.16369v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17673v1","updated":"2024-07-24T23:39:10Z","published":"2024-07-24T23:39:10Z","title":"CRASAR-U-DROIDs: A Large Scale Benchmark Dataset for Building Alignment\n  and Damage Assessment in Georectified sUAS Imagery","summary":"  This document presents the Center for Robot Assisted Search And Rescue -\nUncrewed Aerial Systems - Disaster Response Overhead Inspection Dataset\n(CRASAR-U-DROIDs) for building damage assessment and spatial alignment\ncollected from small uncrewed aerial systems (sUAS) geospatial imagery. This\ndataset is motivated by the increasing use of sUAS in disaster response and the\nlack of previous work in utilizing high-resolution geospatial sUAS imagery for\nmachine learning and computer vision models, the lack of alignment with\noperational use cases, and with hopes of enabling further investigations\nbetween sUAS and satellite imagery. The CRASAR-U-DRIODs dataset consists of\nfifty-two (52) orthomosaics from ten (10) federally declared disasters\n(Hurricane Ian, Hurricane Ida, Hurricane Harvey, Hurricane Idalia, Hurricane\nLaura, Hurricane Michael, Musset Bayou Fire, Mayfield Tornado, Kilauea\nEruption, and Champlain Towers Collapse) spanning 67.98 square kilometers\n(26.245 square miles), containing 21,716 building polygons and damage labels,\nand 7,880 adjustment annotations. The imagery was tiled and presented in\nconjunction with overlaid building polygons to a pool of 130 annotators who\nprovided human judgments of damage according to the Joint Damage Scale. These\nannotations were then reviewed via a two-stage review process in which building\npolygon damage labels were first reviewed individually and then again by\ncommittee. Additionally, the building polygons have been aligned spatially to\nprecisely overlap with the imagery to enable more performant machine learning\nmodels to be trained. It appears that CRASAR-U-DRIODs is the largest labeled\ndataset of sUAS orthomosaic imagery.\n","authors":["Thomas Manzini","Priyankari Perali","Raisa Karnik","Robin Murphy"],"pdf_url":"https://arxiv.org/pdf/2407.17673v1.pdf","comment":"16 Pages, 7 Figures, 6 Tables"},{"id":"http://arxiv.org/abs/2407.17671v1","updated":"2024-07-24T23:23:38Z","published":"2024-07-24T23:23:38Z","title":"Unsqueeze [CLS] Bottleneck to Learn Rich Representations","summary":"  Distillation-based self-supervised learning typically leads to more\ncompressed representations due to its radical clustering process and the\nimplementation of a sharper target distribution. To overcome this limitation\nand preserve more information from input, we introduce UDI, conceptualized as\nUnsqueezed Distillation-based self-supervised learning (SSL). UDI enriches the\nlearned representation by encouraging multimodal prediction distilled from a\nconsolidated profile of local predictions that are derived via stratified\nsampling. Our evaluations show that UDI not only promotes semantically\nmeaningful representations at instance level, delivering superior or\ncompetitive results to state-of-the-art SSL methods in image classification,\nbut also effectively preserves the nuisance of input, which yields significant\nimprovement in dense prediction tasks, including object detection and\nsegmentation. Additionally, UDI performs competitively in low-shot image\nclassification, improving the scalability of joint-embedding pipelines. Various\nvisualizations and ablation studies are presented to further elucidate the\nmechanisms behind UDI. Our source code is available at\nhttps://github.com/ISL-CV/udi.\n","authors":["Qing Su","Shihao Ji"],"pdf_url":"https://arxiv.org/pdf/2407.17671v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.03411v2","updated":"2024-07-24T23:03:01Z","published":"2024-06-05T16:09:01Z","title":"Interactive Text-to-Image Retrieval with Large Language Models: A\n  Plug-and-Play Approach","summary":"  In this paper, we primarily address the issue of dialogue-form context query\nwithin the interactive text-to-image retrieval task. Our methodology, PlugIR,\nactively utilizes the general instruction-following capability of LLMs in two\nways. First, by reformulating the dialogue-form context, we eliminate the\nnecessity of fine-tuning a retrieval model on existing visual dialogue data,\nthereby enabling the use of any arbitrary black-box model. Second, we construct\nthe LLM questioner to generate non-redundant questions about the attributes of\nthe target image, based on the information of retrieval candidate images in the\ncurrent context. This approach mitigates the issues of noisiness and redundancy\nin the generated questions. Beyond our methodology, we propose a novel\nevaluation metric, Best log Rank Integral (BRI), for a comprehensive assessment\nof the interactive retrieval system. PlugIR demonstrates superior performance\ncompared to both zero-shot and fine-tuned baselines in various benchmarks.\nAdditionally, the two methodologies comprising PlugIR can be flexibly applied\ntogether or separately in various situations. Our codes are available at\nhttps://github.com/Saehyung-Lee/PlugIR.\n","authors":["Saehyung Lee","Sangwon Yu","Junsung Park","Jihun Yi","Sungroh Yoon"],"pdf_url":"https://arxiv.org/pdf/2406.03411v2.pdf","comment":"ACL 2024 Oral"},{"id":"http://arxiv.org/abs/2312.13317v2","updated":"2024-07-24T22:46:33Z","published":"2023-12-20T11:02:59Z","title":"Deep Hybrid Camera Deblurring for Smartphone Cameras","summary":"  Mobile cameras, despite their significant advancements, still have difficulty\nin low-light imaging due to compact sensors and lenses, leading to longer\nexposures and motion blur. Traditional blind deconvolution methods and\nlearning-based deblurring methods can be potential solutions to remove blur.\nHowever, achieving practical performance still remains a challenge. To address\nthis, we propose a learning-based deblurring framework for smartphones,\nutilizing wide and ultra-wide cameras as a hybrid camera system. We\nsimultaneously capture a long-exposure wide image and short-exposure burst\nultra-wide images, and utilize the burst images to deblur the wide image. To\nfully exploit burst ultra-wide images, we present HCDeblur, a practical\ndeblurring framework that includes novel deblurring networks, HC-DNet and\nHC-FNet. HC-DNet utilizes motion information extracted from burst images to\ndeblur a wide image, and HC-FNet leverages burst images as reference images to\nfurther enhance a deblurred output. For training and evaluating the proposed\nmethod, we introduce the HCBlur dataset, which consists of synthetic and\nreal-world datasets. Our experiments demonstrate that HCDeblur achieves\nstate-of-the-art deblurring quality. Code and datasets are available at\nhttps://cg.postech.ac.kr/research/HCDeblur.\n","authors":["Jaesung Rim","Junyong Lee","Heemin Yang","Sunghyun Cho"],"pdf_url":"https://arxiv.org/pdf/2312.13317v2.pdf","comment":"SIGGRAPH 2024, Project page:\n  http://cg.postech.ac.kr/research/HCDeblur"},{"id":"http://arxiv.org/abs/2407.17664v1","updated":"2024-07-24T22:21:35Z","published":"2024-07-24T22:21:35Z","title":"SDLNet: Statistical Deep Learning Network for Co-Occurring Object\n  Detection and Identification","summary":"  With the growing advances in deep learning based technologies the detection\nand identification of co-occurring objects is a challenging task which has many\napplications in areas such as, security and surveillance. In this paper, we\npropose a novel framework called SDLNet- Statistical analysis with Deep\nLearning Network that identifies co-occurring objects in conjunction with base\nobjects in multilabel object categories. The pipeline of proposed work is\nimplemented in two stages: in the first stage of SDLNet we deal with multilabel\ndetectors for discovering labels, and in the second stage we perform\nco-occurrence matrix analysis. In co-occurrence matrix analysis, we learn\nco-occurrence statistics by setting base classes and frequently occurring\nclasses, following this we build association rules and generate frequent\npatterns. The crucial part of SDLNet is recognizing base classes and making\nconsideration for co-occurring classes. Finally, the generated co-occurrence\nmatrix based on frequent patterns will show base classes and their\ncorresponding co-occurring classes. SDLNet is evaluated on two publicly\navailable datasets: Pascal VOC and MS-COCO. The experimental results on these\nbenchmark datasets are reported in Sec 4.\n","authors":["Binay Kumar Singh","Niels Da Vitoria Lobo"],"pdf_url":"https://arxiv.org/pdf/2407.17664v1.pdf","comment":"8 pages, 3 figures, ICMLT-2024. arXiv admin note: text overlap with\n  arXiv:2403.17223"},{"id":"http://arxiv.org/abs/2402.14566v2","updated":"2024-07-24T21:52:29Z","published":"2024-02-22T14:04:41Z","title":"Self-supervised Visualisation of Medical Image Datasets","summary":"  Self-supervised learning methods based on data augmentations, such as SimCLR,\nBYOL, or DINO, allow obtaining semantically meaningful representations of image\ndatasets and are widely used prior to supervised fine-tuning. A recent\nself-supervised learning method, $t$-SimCNE, uses contrastive learning to\ndirectly train a 2D representation suitable for visualisation. When applied to\nnatural image datasets, $t$-SimCNE yields 2D visualisations with semantically\nmeaningful clusters. In this work, we used $t$-SimCNE to visualise medical\nimage datasets, including examples from dermatology, histology, and blood\nmicroscopy. We found that increasing the set of data augmentations to include\narbitrary rotations improved the results in terms of class separability,\ncompared to data augmentations used for natural images. Our 2D representations\nshow medically relevant structures and can be used to aid data exploration and\nannotation, improving on common approaches for data visualisation.\n","authors":["Ifeoma Veronica Nwabufo","Jan Niklas Böhm","Philipp Berens","Dmitry Kobak"],"pdf_url":"https://arxiv.org/pdf/2402.14566v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07673v3","updated":"2024-07-24T20:39:47Z","published":"2024-07-10T14:00:19Z","title":"Towards Adaptive Pseudo-label Learning for Semi-Supervised Temporal\n  Action Localization","summary":"  Alleviating noisy pseudo labels remains a key challenge in Semi-Supervised\nTemporal Action Localization (SS-TAL). Existing methods often filter pseudo\nlabels based on strict conditions, but they typically assess classification and\nlocalization quality separately, leading to suboptimal pseudo-label ranking and\nselection. In particular, there might be inaccurate pseudo labels within\nselected positives, alongside reliable counterparts erroneously assigned to\nnegatives. To tackle these problems, we propose a novel Adaptive Pseudo-label\nLearning (APL) framework to facilitate better pseudo-label selection.\nSpecifically, to improve the ranking quality, Adaptive Label Quality Assessment\n(ALQA) is proposed to jointly learn classification confidence and localization\nreliability, followed by dynamically selecting pseudo labels based on the joint\nscore. Additionally, we propose an Instance-level Consistency Discriminator\n(ICD) for eliminating ambiguous positives and mining potential positives\nsimultaneously based on inter-instance intrinsic consistency, thereby leading\nto a more precise selection. We further introduce a general unsupervised\nAction-aware Contrastive Pre-training (ACP) to enhance the discrimination both\nwithin actions and between actions and backgrounds, which benefits SS-TAL.\nExtensive experiments on THUMOS14 and ActivityNet v1.3 demonstrate that our\nmethod achieves state-of-the-art performance under various semi-supervised\nsettings.\n","authors":["Feixiang Zhou","Bryan Williams","Hossein Rahmani"],"pdf_url":"https://arxiv.org/pdf/2407.07673v3.pdf","comment":"Accepted to ECCV 2024"},{"id":"http://arxiv.org/abs/2407.17630v1","updated":"2024-07-24T20:39:17Z","published":"2024-07-24T20:39:17Z","title":"Revising the Problem of Partial Labels from the Perspective of CNNs'\n  Robustness","summary":"  Convolutional neural networks (CNNs) have gained increasing popularity and\nversatility in recent decades, finding applications in diverse domains. These\nremarkable achievements are greatly attributed to the support of extensive\ndatasets with precise labels. However, annotating image datasets is intricate\nand complex, particularly in the case of multi-label datasets. Hence, the\nconcept of partial-label setting has been proposed to reduce annotation costs,\nand numerous corresponding solutions have been introduced. The evaluation\nmethods for these existing solutions have been primarily based on accuracy.\nThat is, their performance is assessed by their predictive accuracy on the test\nset. However, we insist that such an evaluation is insufficient and one-sided.\nOn one hand, since the quality of the test set has not been evaluated, the\nassessment results are unreliable. On the other hand, the partial-label problem\nmay also be raised by undergoing adversarial attacks. Therefore, incorporating\nrobustness into the evaluation system is crucial. For this purpose, we first\npropose two attack models to generate multiple partial-label datasets with\nvarying degrees of label missing rates. Subsequently, we introduce a\nlightweight partial-label solution using pseudo-labeling techniques and a\ndesigned loss function. Then, we employ D-Score to analyze both the proposed\nand existing methods to determine whether they can enhance robustness while\nimproving accuracy. Extensive experimental results demonstrate that while\ncertain methods may improve accuracy, the enhancement in robustness is not\nsignificant, and in some cases, it even diminishes.\n","authors":["Xin Zhang","Yuqi Song","Wyatt McCurdy","Xiaofeng Wang","Fei Zuo"],"pdf_url":"https://arxiv.org/pdf/2407.17630v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17628v1","updated":"2024-07-24T20:35:20Z","published":"2024-07-24T20:35:20Z","title":"PEEKABOO: Hiding parts of an image for unsupervised object localization","summary":"  Localizing objects in an unsupervised manner poses significant challenges due\nto the absence of key visual information such as the appearance, type and\nnumber of objects, as well as the lack of labeled object classes typically\navailable in supervised settings. While recent approaches to unsupervised\nobject localization have demonstrated significant progress by leveraging\nself-supervised visual representations, they often require computationally\nintensive training processes, resulting in high resource demands in terms of\ncomputation, learnable parameters, and data. They also lack explicit modeling\nof visual context, potentially limiting their accuracy in object localization.\nTo tackle these challenges, we propose a single-stage learning framework,\ndubbed PEEKABOO, for unsupervised object localization by learning context-based\nrepresentations at both the pixel- and shape-level of the localized objects\nthrough image masking. The key idea is to selectively hide parts of an image\nand leverage the remaining image information to infer the location of objects\nwithout explicit supervision. The experimental results, both quantitative and\nqualitative, across various benchmark datasets, demonstrate the simplicity,\neffectiveness and competitive performance of our approach compared to\nstate-of-the-art methods in both single object discovery and unsupervised\nsalient object detection tasks. Code and pre-trained models are available at:\nhttps://github.com/hasibzunair/peekaboo\n","authors":["Hasib Zunair","A. Ben Hamza"],"pdf_url":"https://arxiv.org/pdf/2407.17628v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17620v1","updated":"2024-07-24T20:17:05Z","published":"2024-07-24T20:17:05Z","title":"CoMoTo: Unpaired Cross-Modal Lesion Distillation Improves Breast Lesion\n  Detection in Tomosynthesis","summary":"  Digital Breast Tomosynthesis (DBT) is an advanced breast imaging modality\nthat offers superior lesion detection accuracy compared to conventional\nmammography, albeit at the trade-off of longer reading time. Accelerating\nlesion detection from DBT using deep learning is hindered by limited data\navailability and huge annotation costs. A possible solution to this issue could\nbe to leverage the information provided by a more widely available modality,\nsuch as mammography, to enhance DBT lesion detection. In this paper, we present\na novel framework, CoMoTo, for improving lesion detection in DBT. Our framework\nleverages unpaired mammography data to enhance the training of a DBT model,\nimproving practicality by eliminating the need for mammography during\ninference. Specifically, we propose two novel components, Lesion-specific\nKnowledge Distillation (LsKD) and Intra-modal Point Alignment (ImPA). LsKD\nselectively distills lesion features from a mammography teacher model to a DBT\nstudent model, disregarding background features. ImPA further enriches LsKD by\nensuring the alignment of lesion features within the teacher before distilling\nknowledge to the student. Our comprehensive evaluation shows that CoMoTo is\nsuperior to traditional pretraining and image-level KD, improving performance\nby 7% Mean Sensitivity under low-data setting. Our code is available at\nhttps://github.com/Muhammad-Al-Barbary/CoMoTo .\n","authors":["Muhammad Alberb","Marawan Elbatel","Aya Elgebaly","Ricardo Montoya-del-Angel","Xiaomeng Li","Robert Martí"],"pdf_url":"https://arxiv.org/pdf/2407.17620v1.pdf","comment":"ADSMI @ MICCAI 2024"},{"id":"http://arxiv.org/abs/2311.13976v2","updated":"2024-07-24T20:01:08Z","published":"2023-11-23T12:42:52Z","title":"Low Latency Instance Segmentation by Continuous Clustering for LiDAR\n  Sensors","summary":"  Low-latency instance segmentation of LiDAR point clouds is crucial in\nreal-world applications because it serves as an initial and frequently-used\nbuilding block in a robot's perception pipeline, where every task adds further\ndelay. Particularly in dynamic environments, this total delay can result in\nsignificant positional offsets of dynamic objects, as seen in highway\nscenarios. To address this issue, we employ a new technique, which we call\ncontinuous clustering. Unlike most existing clustering approaches, which use a\nfull revolution of the LiDAR sensor, we process the data stream in a continuous\nand seamless fashion. Our approach does not rely on the concept of complete or\npartial sensor rotations with multiple discrete range images; instead, it views\nthe range image as a single and infinitely horizontally growing entity. Each\nnew column of this continuous range image is processed as soon it is available.\nObstacle points are clustered to existing instances in real-time and it is\nchecked at a high-frequency which instances are completed in order to publish\nthem without waiting for the completion of the revolution or some other\nintegration period. In the case of rotating sensors, no problematic\ndiscontinuities between the points of the end and the start of a scan are\nobserved. In this work we describe the two-layered data structure and the\ncorresponding algorithm for continuous clustering. It is able to achieve an\naverage latency of just 5 ms with respect to the latest timestamp of all points\nin the cluster. We are publishing the source code at\nhttps://github.com/UniBwTAS/continuous_clustering.\n","authors":["Andreas Reich","Mirko Maehlisch"],"pdf_url":"https://arxiv.org/pdf/2311.13976v2.pdf","comment":"Accompanying Video: https://www.youtube.com/watch?v=ex4qcR2bkWs"},{"id":"http://arxiv.org/abs/2407.17596v1","updated":"2024-07-24T19:02:01Z","published":"2024-07-24T19:02:01Z","title":"Quality Assured: Rethinking Annotation Strategies in Imaging AI","summary":"  This paper does not describe a novel method. Instead, it studies an essential\nfoundation for reliable benchmarking and ultimately real-world application of\nAI-based image analysis: generating high-quality reference annotations.\nPrevious research has focused on crowdsourcing as a means of outsourcing\nannotations. However, little attention has so far been given to annotation\ncompanies, specifically regarding their internal quality assurance (QA)\nprocesses. Therefore, our aim is to evaluate the influence of QA employed by\nannotation companies on annotation quality and devise methodologies for\nmaximizing data annotation efficacy. Based on a total of 57,648 instance\nsegmented images obtained from a total of 924 annotators and 34 QA workers from\nfour annotation companies and Amazon Mechanical Turk (MTurk), we derived the\nfollowing insights: (1) Annotation companies perform better both in terms of\nquantity and quality compared to the widely used platform MTurk. (2) Annotation\ncompanies' internal QA only provides marginal improvements, if any. However,\nimproving labeling instructions instead of investing in QA can substantially\nboost annotation performance. (3) The benefit of internal QA depends on\nspecific image characteristics. Our work could enable researchers to derive\nsubstantially more value from a fixed annotation budget and change the way\nannotation companies conduct internal QA.\n","authors":["Tim Rädsch","Annika Reinke","Vivienn Weru","Minu D. Tizabi","Nicholas Heller","Fabian Isensee","Annette Kopp-Schneider","Lena Maier-Hein"],"pdf_url":"https://arxiv.org/pdf/2407.17596v1.pdf","comment":"Accepted at ECCV 2024, preprint, Computer Vision, Data Annotation"},{"id":"http://arxiv.org/abs/2406.03556v2","updated":"2024-07-24T18:50:51Z","published":"2024-06-05T18:10:49Z","title":"Npix2Cpix: A GAN-based Image-to-Image Translation Network with\n  Retrieval-Classification Integration for Watermark Retrieval from Historical\n  Document Images","summary":"  The identification and restoration of ancient watermarks have long been a\nmajor topic in codicology and history. Classifying historical documents based\non watermarks is challenging due to their diversity, noisy samples, multiple\nrepresentation modes, and minor distinctions between classes and intra-class\nvariations. This paper proposes a modified U-net-based conditional generative\nadversarial network (GAN) named Npix2Cpix to translate noisy raw historical\nwatermarked images into clean, handwriting-free watermarked images by\nperforming image translation from degraded (noisy) pixels to clean pixels.\nUsing image-to-image translation and adversarial learning, the network creates\nclutter-free images for watermark restoration and categorization. The generator\nand discriminator of the proposed GAN are trained using two separate loss\nfunctions, each based on the distance between images, to learn the mapping from\nthe input noisy image to the output clean image. After using the proposed GAN\nto pre-process noisy watermarked images, Siamese-based one-shot learning is\nemployed for watermark classification. Experimental results on a large-scale\nhistorical watermark dataset demonstrate that cleaning the noisy watermarked\nimages can help to achieve high one-shot classification accuracy. The\nqualitative and quantitative evaluation of the retrieved watermarked image\nhighlights the effectiveness of the proposed approach.\n","authors":["Utsab Saha","Sawradip Saha","Shaikh Anowarul Fattah","Mohammad Saquib"],"pdf_url":"https://arxiv.org/pdf/2406.03556v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.06863v3","updated":"2024-07-24T18:09:48Z","published":"2024-07-09T13:50:43Z","title":"Beyond Aesthetics: Cultural Competence in Text-to-Image Models","summary":"  Text-to-Image (T2I) models are being increasingly adopted in diverse global\ncommunities where they create visual representations of their unique cultures.\nCurrent T2I benchmarks primarily focus on faithfulness, aesthetics, and realism\nof generated images, overlooking the critical dimension of cultural competence.\nIn this work, we introduce a framework to evaluate cultural competence of T2I\nmodels along two crucial dimensions: cultural awareness and cultural diversity,\nand present a scalable approach using a combination of structured knowledge\nbases and large language models to build a large dataset of cultural artifacts\nto enable this evaluation. In particular, we apply this approach to build CUBE\n(CUltural BEnchmark for Text-to-Image models), a first-of-its-kind benchmark to\nevaluate cultural competence of T2I models. CUBE covers cultural artifacts\nassociated with 8 countries across different geo-cultural regions and along 3\nconcepts: cuisine, landmarks, and art. CUBE consists of 1) CUBE-1K, a set of\nhigh-quality prompts that enable the evaluation of cultural awareness, and 2)\nCUBE-CSpace, a larger dataset of cultural artifacts that serves as grounding to\nevaluate cultural diversity. We also introduce cultural diversity as a novel\nT2I evaluation component, leveraging quality-weighted Vendi score. Our\nevaluations reveal significant gaps in the cultural awareness of existing\nmodels across countries and provide valuable insights into the cultural\ndiversity of T2I outputs for under-specified prompts. Our methodology is\nextendable to other cultural regions and concepts, and can facilitate the\ndevelopment of T2I models that better cater to the global population.\n","authors":["Nithish Kannen","Arif Ahmad","Marco Andreetto","Vinodkumar Prabhakaran","Utsav Prabhu","Adji Bousso Dieng","Pushpak Bhattacharyya","Shachi Dave"],"pdf_url":"https://arxiv.org/pdf/2407.06863v3.pdf","comment":"30 pages, 10 figures, preprint"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2407.17451v1","updated":"2024-07-24T17:31:48Z","published":"2024-07-24T17:31:48Z","title":"BlueTempNet: A Temporal Multi-network Dataset of Social Interactions in\n  Bluesky Social","summary":"  Decentralized social media platforms like Bluesky Social (Bluesky) have made\nit possible to publicly disclose some user behaviors with millisecond-level\nprecision. Embracing Bluesky's principles of open-source and open-data, we\npresent the first collection of the temporal dynamics of user-driven social\ninteractions. BlueTempNet integrates multiple types of networks into a single\nmulti-network, including user-to-user interactions (following and blocking\nusers) and user-to-community interactions (creating and joining communities).\nCommunities are user-formed groups in custom Feeds, where users subscribe to\nposts aligned with their interests. Following Bluesky's public data policy, we\ncollect existing Bluesky Feeds, including the users who liked and generated\nthese Feeds, and provide tools to gather users' social interactions within a\ndate range. This data-collection strategy captures past user behaviors and\nsupports the future data collection of user behavior.\n","authors":["Ujun Jeong","Bohan Jiang","Zhen Tan","H. Russell Bernard","Huan Liu"],"pdf_url":"https://arxiv.org/pdf/2407.17451v1.pdf","comment":"to appear in IEEE Data Description"},{"id":"http://arxiv.org/abs/2406.08461v2","updated":"2024-07-24T16:40:41Z","published":"2024-06-12T17:51:47Z","title":"Bridging the Gap: Unravelling Local Government Data Sharing Barriers in\n  Estonia and Beyond","summary":"  Open Government Data (OGD) plays a crucial role in transforming smart cities\ninto sustainable and intelligent entities by providing data for analytics,\nreal-time monitoring, and informed decision-making. This data is increasingly\nused in urban digital twins, enhancing city management through stakeholder\ncollaboration. However, local administrative data remains underutilized even in\ndigitally advanced countries like Estonia. This study explores barriers\npreventing Estonian municipalities from sharing OGD, using a qualitative\napproach through interviews with Estonian municipalities and drawing on the\nOGD-adapted Innovation Resistance Theory model (IRT). Interviews with local\ngovernment officials highlight ongoing is-sues in data provision and quality.\nBy addressing overlooked weaknesses in the Estonian open data ecosystem and\nproviding actionable recommendations, this research contributes to a more\nresilient and sustainable open data ecosystem. Additionally, by validating the\nOGD-adapted Innovation Resistance Theory model and proposing a revised version\ntailored for local government contexts, the study advances theoretical\nframeworks for understanding data sharing resistance. Ultimately, this study\nserves as a call to action for policymakers and practitioners to prioritize\nlocal OGD initiatives, ensuring the full utilization of OGD in smart city\ndevelopment.\n","authors":["Katrin Rajamäe Soosaar","Anastasija Nikiforova"],"pdf_url":"https://arxiv.org/pdf/2406.08461v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.12517v5","updated":"2024-07-24T15:10:41Z","published":"2023-05-21T17:14:31Z","title":"Description-Based Text Similarity","summary":"  Identifying texts with a given semantics is central for many information\nseeking scenarios. Similarity search over vector embeddings appear to be\ncentral to this ability, yet the similarity reflected in current text\nembeddings is corpus-driven, and is inconsistent and sub-optimal for many use\ncases. What, then, is a good notion of similarity for effective retrieval of\ntext?\n  We identify the need to search for texts based on abstract descriptions of\ntheir content, and the corresponding notion of \\emph{description based\nsimilarity}. We demonstrate the inadequacy of current text embeddings and\npropose an alternative model that significantly improves when used in standard\nnearest neighbor search. The model is trained using positive and negative pairs\nsourced through prompting a LLM, demonstrating how data from LLMs can be used\nfor creating new capabilities not immediately possible using the original\nmodel.\n","authors":["Shauli Ravfogel","Valentina Pyatkin","Amir DN Cohen","Avshalom Manevich","Yoav Goldberg"],"pdf_url":"https://arxiv.org/pdf/2305.12517v5.pdf","comment":"Accepted in COLM 2024"},{"id":"http://arxiv.org/abs/2407.17284v1","updated":"2024-07-24T13:50:21Z","published":"2024-07-24T13:50:21Z","title":"A Novel Two-Step Fine-Tuning Pipeline for Cold-Start Active Learning in\n  Text Classification Tasks","summary":"  This is the first work to investigate the effectiveness of BERT-based\ncontextual embeddings in active learning (AL) tasks on cold-start scenarios,\nwhere traditional fine-tuning is infeasible due to the absence of labeled data.\nOur primary contribution is the proposal of a more robust fine-tuning pipeline\n- DoTCAL - that diminishes the reliance on labeled data in AL using two steps:\n(1) fully leveraging unlabeled data through domain adaptation of the embeddings\nvia masked language modeling and (2) further adjusting model weights using\nlabeled data selected by AL. Our evaluation contrasts BERT-based embeddings\nwith other prevalent text representation paradigms, including Bag of Words\n(BoW), Latent Semantic Indexing (LSI), and FastText, at two critical stages of\nthe AL process: instance selection and classification. Experiments conducted on\neight ATC benchmarks with varying AL budgets (number of labeled instances) and\nnumber of instances (about 5,000 to 300,000) demonstrate DoTCAL's superior\neffectiveness, achieving up to a 33% improvement in Macro-F1 while reducing\nlabeling efforts by half compared to the traditional one-step method. We also\nfound that in several tasks, BoW and LSI (due to information aggregation)\nproduce results superior (up to 59% ) to BERT, especially in low-budget\nscenarios and hard-to-classify tasks, which is quite surprising.\n","authors":["Fabiano Belém","Washington Cunha","Celso França","Claudio Andrade","Leonardo Rocha","Marcos André Gonçalves"],"pdf_url":"https://arxiv.org/pdf/2407.17284v1.pdf","comment":"11 pages, 4 figures, 2 Tables, and 1 algorithm"},{"id":"http://arxiv.org/abs/2407.07503v3","updated":"2024-07-24T13:07:26Z","published":"2024-07-10T09:41:36Z","title":"Inter and Intra Prior Learning-based Hyperspectral Image Reconstruction\n  Using Snapshot SWIR Metasurface","summary":"  Shortwave-infrared(SWIR) spectral information, ranging from 1 {\\mu}m to\n2.5{\\mu}m, overcomes the limitations of traditional color cameras in acquiring\nscene information. However, conventional SWIR hyperspectral imaging systems\nface challenges due to their bulky setups and low acquisition speeds. This work\nintroduces a snapshot SWIR hyperspectral imaging system based on a metasurface\nfilter and a corresponding filter selection method to achieve the lowest\ncorrelation coefficient among these filters. This system offers the advantages\nof compact size and snapshot imaging. We propose a novel inter and intra prior\nlearning unfolding framework to achieve high-quality SWIR hyperspectral image\nreconstruction, which bridges the gap between prior learning and cross-stage\ninformation interaction. Additionally, We design an adaptive feature transfer\nmechanism to adaptively transfer the contextual correlation of multi-scale\nencoder features to prevent detailed information loss in the decoder.\nExperiment results demonstrate that our method can reconstruct hyperspectral\nimages with high speed and superior performance over existing methods.\n","authors":["Linqiang Li","Jinglei Hao","Yongqiang Zhao","Pan Liu","Haofang Yan","Ziqin Zhang","Seong G. Kong"],"pdf_url":"https://arxiv.org/pdf/2407.07503v3.pdf","comment":"12 pages,9 figures"},{"id":"http://arxiv.org/abs/2407.17234v1","updated":"2024-07-24T12:42:41Z","published":"2024-07-24T12:42:41Z","title":"Intent-Guided Heterogeneous Graph Contrastive Learning for\n  Recommendation","summary":"  Contrastive Learning (CL)-based recommender systems have gained prominence in\nthe context of Heterogeneous Graph (HG) due to their capacity to enhance the\nconsistency of representations across different views. Nonetheless, existing\nframeworks often neglect the fact that user-item interactions within HG are\ngoverned by diverse latent intents (for instance, preferences towards specific\nbrands or the demographic characteristics of item audiences), which are pivotal\nin capturing fine-grained relations. The exploration of these underlying\nintents, particularly through the lens of meta-paths in HGs, presents us with\ntwo principal challenges: i) How to integrate CL mechanisms with latent\nintents; ii) How to mitigate the noise associated with these complicated\nintents.To address these challenges, we propose an innovative framework termed\nIntent-Guided Heterogeneous Graph Contrastive Learning (IHGCL), which designed\nto enhance CL-based recommendation by capturing the intents contained within\nmeta-paths. Specifically, the IHGCL framework includes: i) it employs a\nmeta-path-based dual contrastive learning approach to effectively integrate\nintents into the recommendation, constructing meta-path contrast and view\ncontrast; ii) it uses an bottlenecked autoencoder that combines mask\npropagation with the information bottleneck principle to significantly reduce\nnoise perturbations introduced by meta-paths. Empirical evaluations conducted\nacross six distinct datasets demonstrate the superior performance of our IHGCL\nframework relative to conventional baseline methods. Our model implementation\nis available at https://github.com/wangyu0627/IHGCL.\n","authors":["Lei Sang","Yu Wang","Yi Zhang","Yiwen Zhang","Xindong Wu"],"pdf_url":"https://arxiv.org/pdf/2407.17234v1.pdf","comment":"14pages, 11figures"},{"id":"http://arxiv.org/abs/2407.11245v2","updated":"2024-07-24T11:54:26Z","published":"2024-07-15T21:14:13Z","title":"Pacer and Runner: Cooperative Learning Framework between Single- and\n  Cross-Domain Sequential Recommendation","summary":"  Cross-Domain Sequential Recommendation (CDSR) improves recommendation\nperformance by utilizing information from multiple domains, which contrasts\nwith Single-Domain Sequential Recommendation (SDSR) that relies on a historical\ninteraction within a specific domain. However, CDSR may underperform compared\nto the SDSR approach in certain domains due to negative transfer, which occurs\nwhen there is a lack of relation between domains or different levels of data\nsparsity. To address the issue of negative transfer, our proposed CDSR model\nestimates the degree of negative transfer of each domain and adaptively assigns\nit as a weight factor to the prediction loss, to control gradient flows through\ndomains with significant negative transfer. To this end, our model compares the\nperformance of a model trained on multiple domains (CDSR) with a model trained\nsolely on the specific domain (SDSR) to evaluate the negative transfer of each\ndomain using our asymmetric cooperative network. In addition, to facilitate the\ntransfer of valuable cues between the SDSR and CDSR tasks, we developed an\nauxiliary loss that maximizes the mutual information between the representation\npairs from both tasks on a per-domain basis. This cooperative learning between\nSDSR and CDSR tasks is similar to the collaborative dynamics between pacers and\nrunners in a marathon. Our model outperformed numerous previous works in\nextensive experiments on two real-world industrial datasets across ten service\ndomains. We also have deployed our model in the recommendation system of our\npersonal assistant app service, resulting in 21.4% increase in click-through\nrate compared to existing models, which is valuable to real-world business.\n","authors":["Chung Park","Taesan Kim","Hyungjun Yoon","Junui Hong","Yelim Yu","Mincheol Cho","Minsung Choi","Jaegul Choo"],"pdf_url":"https://arxiv.org/pdf/2407.11245v2.pdf","comment":"Accepted at SIGIR'24 (Best Paper Honorable Mention)"},{"id":"http://arxiv.org/abs/2407.17115v1","updated":"2024-07-24T09:24:49Z","published":"2024-07-24T09:24:49Z","title":"Reinforced Prompt Personalization for Recommendation with Large Language\n  Models","summary":"  Designing effective prompts can empower LLMs to understand user preferences\nand provide recommendations by leveraging LLMs' intent comprehension and\nknowledge utilization capabilities. However, existing research predominantly\nconcentrates on task-wise prompting, developing fixed prompt templates composed\nof four patterns (i.e., role-playing, history records, reasoning guidance, and\noutput format) and applying them to all users for a given task. Although\nconvenient, task-wise prompting overlooks individual user differences, leading\nto potential mismatches in capturing user preferences. To address it, we\nintroduce the concept of instance-wise prompting to personalize discrete\nprompts for individual users and propose Reinforced Prompt Personalization\n(RPP) to optimize the four patterns in prompts using multi-agent reinforcement\nlearning (MARL). To boost efficiency, RPP formulates prompt personalization as\nselecting optimal sentences holistically across the four patterns, rather than\noptimizing word-by-word. To ensure the quality of prompts, RPP meticulously\ncrafts diverse expressions for each of the four patterns, considering multiple\nanalytical perspectives for specific recommendation tasks. In addition to RPP,\nour proposal of RPP+ aims to enhance the scalability of action space by\ndynamically refining actions with LLMs throughout the iterative process. We\nevaluate the effectiveness of RPP/RPP+ in ranking tasks over various datasets.\nExperimental results demonstrate the superiority of RPP/RPP+ over traditional\nrecommender models, few-shot methods, and other prompt-based methods,\nunderscoring the significance of instance-wise prompting for LLMs in\nrecommendation tasks and validating the effectiveness of RPP/RPP+. Our code is\navailable at https://github.com/maowenyu-11/RPP.\n","authors":["Wenyu Mao","Jiancan Wu","Weijian Chen","Chongming Gao","Xiang Wang","Xiangnan He"],"pdf_url":"https://arxiv.org/pdf/2407.17115v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.03365v2","updated":"2024-07-24T08:48:38Z","published":"2024-01-31T11:03:58Z","title":"Heterophily-Aware Fair Recommendation using Graph Convolutional Networks","summary":"  In recent years, graph neural networks (GNNs) have become a popular tool to\nimprove the accuracy and performance of recommender systems. Modern recommender\nsystems are not only designed to serve end users, but also to benefit other\nparticipants, such as items and items providers. These participants may have\ndifferent or conflicting goals and interests, which raise the need for fairness\nand popularity bias considerations. GNN-based recommendation methods also face\nthe challenges of unfairness and popularity bias and their normalization and\naggregation processes suffer from these challenges. In this paper, we propose a\nfair GNN-based recommender system, called HetroFair, to improve items' side\nfairness. HetroFair uses two separate components to generate fairness-aware\nembeddings: i) fairnessaware attention which incorporates dot product in the\nnormalization process of GNNs, to decrease the effect of nodes' degrees, and\nii) heterophily feature weighting to assign distinct weights to different\nfeatures during the aggregation process. In order to evaluate the effectiveness\nof HetroFair, we conduct extensive experiments over six real-world datasets.\nOur experimental results reveal that HetroFair not only alleviates the\nunfairness and popularity bias on items' side, but also achieves superior\naccuracy on users' side. Our implementation is publicly available at\nhttps://github.com/NematGH/HetroFair.\n","authors":["Nemat Gholinejad","Mostafa Haghir Chehreghani"],"pdf_url":"https://arxiv.org/pdf/2402.03365v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16984v1","updated":"2024-07-24T04:01:09Z","published":"2024-07-24T04:01:09Z","title":"scGHSOM: Hierarchical clustering and visualization of single-cell and\n  CRISPR data using growing hierarchical SOM","summary":"  High-dimensional single-cell data poses significant challenges in identifying\nunderlying biological patterns due to the complexity and heterogeneity of\ncellular states. We propose a comprehensive gene-cell dependency visualization\nvia unsupervised clustering, Growing Hierarchical Self-Organizing Map (GHSOM),\nspecifically designed for analyzing high-dimensional single-cell data like\nsingle-cell sequencing and CRISPR screens. GHSOM is applied to cluster samples\nin a hierarchical structure such that the self-growth structure of clusters\nsatisfies the required variations between and within. We propose a novel\nSignificant Attributes Identification Algorithm to identify features that\ndistinguish clusters. This algorithm pinpoints attributes with minimal\nvariation within a cluster but substantial variation between clusters. These\nkey attributes can then be used for targeted data retrieval and downstream\nanalysis. Furthermore, we present two innovative visualization tools: Cluster\nFeature Map and Cluster Distribution Map. The Cluster Feature Map highlights\nthe distribution of specific features across the hierarchical structure of\nGHSOM clusters. This allows for rapid visual assessment of cluster uniqueness\nbased on chosen features. The Cluster Distribution Map depicts leaf clusters as\ncircles on the GHSOM grid, with circle size reflecting cluster data size and\ncolor customizable to visualize features like cell type or other attributes. We\napply our analysis to three single-cell datasets and one CRISPR dataset\n(cell-gene database) and evaluate clustering methods with internal and external\nCH and ARI scores. GHSOM performs well, being the best performer in internal\nevaluation (CH=4.2). In external evaluation, GHSOM has the third-best\nperformance of all methods.\n","authors":["Shang-Jung Wen","Jia-Ming Chang","Fang Yu"],"pdf_url":"https://arxiv.org/pdf/2407.16984v1.pdf","comment":"Abstract presentation at BIOKDD@ACM KDD 2024"},{"id":"http://arxiv.org/abs/2407.08108v2","updated":"2024-07-24T03:37:17Z","published":"2024-07-11T00:54:56Z","title":"CADC: Encoding User-Item Interactions for Compressing Recommendation\n  Model Training Data","summary":"  Deep learning recommendation models (DLRMs) are at the heart of the current\ne-commerce industry. However, the amount of training data used to train these\nlarge models is growing exponentially, leading to substantial training hurdles.\nThe training dataset contains two primary types of information: content-based\ninformation (features of users and items) and collaborative information\n(interactions between users and items). One approach to reduce the training\ndataset is to remove user-item interactions. But that significantly diminishes\ncollaborative information, which is crucial for maintaining accuracy due to its\ninclusion of interaction histories. This loss profoundly impacts DLRM\nperformance.\n  This paper makes an important observation that if one can capture the\nuser-item interaction history to enrich the user and item embeddings, then the\ninteraction history can be compressed without losing model accuracy. Thus, this\nwork, Collaborative Aware Data Compression (CADC), takes a two-step approach to\ntraining dataset compression. In the first step, we use matrix factorization of\nthe user-item interaction matrix to create a novel embedding representation for\nboth the users and items. Once the user and item embeddings are enriched by the\ninteraction history information the approach then applies uniform random\nsampling of the training dataset to drastically reduce the training dataset\nsize while minimizing model accuracy drop. The source code of CADC is available\nat\n\\href{https://anonymous.4open.science/r/DSS-RM-8C1D/README.md}{https://anonymous.4open.science/r/DSS-RM-8C1D/README.md}.\n","authors":["Hossein Entezari Zarch","Abdulla Alshabanah","Chaoyi Jiang","Murali Annavaram"],"pdf_url":"https://arxiv.org/pdf/2407.08108v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17688v2","updated":"2024-07-24T01:35:07Z","published":"2024-03-26T13:31:33Z","title":"Large Language Models Enhanced Collaborative Filtering","summary":"  Recent advancements in Large Language Models (LLMs) have attracted\nconsiderable interest among researchers to leverage these models to enhance\nRecommender Systems (RSs). Existing work predominantly utilizes LLMs to\ngenerate knowledge-rich texts or utilizes LLM-derived embeddings as features to\nimprove RSs. Although the extensive world knowledge embedded in LLMs generally\nbenefits RSs, the application can only take limited number of users and items\nas inputs, without adequately exploiting collaborative filtering information.\nConsidering its crucial role in RSs, one key challenge in enhancing RSs with\nLLMs lies in providing better collaborative filtering information through LLMs.\nIn this paper, drawing inspiration from the in-context learning and chain of\nthought reasoning in LLMs, we propose the Large Language Models enhanced\nCollaborative Filtering (LLM-CF) framework, which distils the world knowledge\nand reasoning capabilities of LLMs into collaborative filtering. We also\nexplored a concise and efficient instruction-tuning method, which improves the\nrecommendation capabilities of LLMs while preserving their general\nfunctionalities (e.g., not decreasing on the LLM benchmark). Comprehensive\nexperiments on three real-world datasets demonstrate that LLM-CF significantly\nenhances several backbone recommendation models and consistently outperforms\ncompetitive baselines, showcasing its effectiveness in distilling the world\nknowledge and reasoning capabilities of LLM into collaborative filtering.\n","authors":["Zhongxiang Sun","Zihua Si","Xiaoxue Zang","Kai Zheng","Yang Song","Xiao Zhang","Jun Xu"],"pdf_url":"https://arxiv.org/pdf/2403.17688v2.pdf","comment":"Accepted by CIKM 2024"},{"id":"http://arxiv.org/abs/2407.09017v3","updated":"2024-07-24T01:15:20Z","published":"2024-07-12T06:10:01Z","title":"AI-Driven Guided Response for Security Operation Centers with Microsoft\n  Copilot for Security","summary":"  Security operation centers contend with a constant stream of security\nincidents, ranging from straightforward to highly complex. To address this, we\ndeveloped Copilot Guided Response (CGR), an industry-scale ML architecture that\nguides security analysts across three key tasks -- (1) investigation, providing\nessential historical context by identifying similar incidents; (2) triaging to\nascertain the nature of the incident -- whether it is a true positive, false\npositive, or benign positive; and (3) remediation, recommending tailored\ncontainment actions. CGR is integrated into the Microsoft Defender XDR product\nand deployed worldwide, generating millions of recommendations across thousands\nof customers. Our extensive evaluation, incorporating internal evaluation,\ncollaboration with security experts, and customer feedback, demonstrates that\nCGR delivers high-quality recommendations across all three tasks. We provide a\ncomprehensive overview of the CGR architecture, setting a precedent as the\nfirst cybersecurity company to openly discuss these capabilities in such depth.\nAdditionally, we GUIDE, the largest public collection of real-world security\nincidents, spanning 13M evidences across 1M annotated incidents. By enabling\nresearchers and practitioners to conduct research on real-world data, GUIDE\nadvances the state of cybersecurity and supports the development of\nnext-generation machine learning systems.\n","authors":["Scott Freitas","Jovan Kalajdjieski","Amir Gharib","Robert McCann"],"pdf_url":"https://arxiv.org/pdf/2407.09017v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.12784v2","updated":"2024-07-24T23:00:50Z","published":"2024-02-20T07:49:30Z","title":"Understanding and Mitigating the Threat of Vec2Text to Dense Retrieval\n  Systems","summary":"  The emergence of Vec2Text -- a method for text embedding inversion -- has\nraised serious privacy concerns for dense retrieval systems which use text\nembeddings, such as those offered by OpenAI and Cohere. This threat comes from\nthe ability for a malicious attacker with access to embeddings to reconstruct\nthe original text. In this paper, we investigate various factors related to\nembedding models that may impact text recoverability via Vec2Text. We explore\nfactors such as distance metrics, pooling functions, bottleneck pre-training,\ntraining with noise addition, embedding quantization, and embedding dimensions,\nwhich were not considered in the original Vec2Text paper. Through a\ncomprehensive analysis of these factors, our objective is to gain a deeper\nunderstanding of the key elements that affect the trade-offs between the text\nrecoverability and retrieval effectiveness of dense retrieval systems, offering\ninsights for practitioners designing privacy-aware dense retrieval systems. We\nalso propose a simple embedding transformation fix that guarantees equal\nranking effectiveness while mitigating the recoverability risk. Overall, this\nstudy reveals that Vec2Text could pose a threat to current dense retrieval\nsystems, but there are some effective methods to patch such systems.\n","authors":["Shengyao Zhuang","Bevan Koopman","Xiaoran Chu","Guido Zuccon"],"pdf_url":"https://arxiv.org/pdf/2402.12784v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17631v1","updated":"2024-07-24T20:44:36Z","published":"2024-07-24T20:44:36Z","title":"BLAZE: Cross-Language and Cross-Project Bug Localization via Dynamic\n  Chunking and Hard Example Learning","summary":"  Software bugs require developers to exert significant effort to identify and\nresolve them, often consuming about one-third of their time. Bug localization,\nthe process of pinpointing the exact source code files that need modification,\nis crucial in reducing this effort. Existing bug localization tools, typically\nreliant on deep learning techniques, face limitations in cross-project\napplicability and effectiveness in multi-language environments. Recent\nadvancements with Large Language Models (LLMs) offer detailed representations\nfor bug localization. However, they encounter challenges with limited context\nwindows and mapping accuracy. To address these issues, we propose BLAZE, an\napproach that employs dynamic chunking and hard example learning. First, BLAZE\ndynamically segments source code to minimize continuity loss. Then, BLAZE\nfine-tunes a GPT-based model using challenging bug cases, in order to enhance\ncross-project and cross-language bug localization. To support the capability of\nBLAZE, we create the BEETLEBOX dataset, which comprises 26,321 bugs from 29\nlarge and thriving open-source projects across five different programming\nlanguages (Java, C++, Python, Go, and JavaScript). Our evaluations of BLAZE on\nthree benchmark datasets BEETLEBOX, SWE-Bench, and Ye et al. demonstrate\nsubstantial improvements compared to six state-of-the-art baselines.\nSpecifically, BLAZE achieves up to an increase of 120% in Top 1 accuracy, 144%\nin Mean Average Precision (MAP), and 100% in Mean Reciprocal Rank (MRR). An\nextensive ablation study confirms the contributions of our pipeline components\nto the overall performance enhancement.\n","authors":["Partha Chakraborty","Mahmoud Alfadel","Meiyappan Nagappan"],"pdf_url":"https://arxiv.org/pdf/2407.17631v1.pdf","comment":null}],"Machine Learning":[{"id":"http://arxiv.org/abs/2407.17467v1","updated":"2024-07-24T17:59:02Z","published":"2024-07-24T17:59:02Z","title":"CMR Scaling Law: Predicting Critical Mixture Ratios for Continual\n  Pre-training of Language Models","summary":"  Large Language Models (LLMs) excel in diverse tasks but often underperform in\nspecialized fields due to limited domain-specific or proprietary corpus.\nContinual pre-training (CPT) enhances LLM capabilities by imbuing new\ndomain-specific or proprietary knowledge while replaying general corpus to\nprevent catastrophic forgetting. The data mixture ratio of general corpus and\ndomain-specific corpus, however, has been chosen heuristically, leading to\nsub-optimal training efficiency in practice. In this context, we attempt to\nre-visit the scaling behavior of LLMs under the hood of CPT, and discover a\npower-law relationship between loss, mixture ratio, and training tokens scale.\nWe formalize the trade-off between general and domain-specific capabilities,\nleading to a well-defined Critical Mixture Ratio (CMR) of general and domain\ndata. By striking the balance, CMR maintains the model's general ability and\nachieves the desired domain transfer, ensuring the highest utilization of\navailable resources. Therefore, if we value the balance between efficiency and\neffectiveness, CMR can be consider as the optimal mixture ratio.Through\nextensive experiments, we ascertain the predictability of CMR, and propose CMR\nscaling law and have substantiated its generalization. These findings offer\npractical guidelines for optimizing LLM training in specialized domains,\nensuring both general and domain-specific performance while efficiently\nmanaging training resources.\n","authors":["Jiawei Gu","Zacc Yang","Chuanghao Ding","Rui Zhao","Fei Tan"],"pdf_url":"https://arxiv.org/pdf/2407.17467v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17466v1","updated":"2024-07-24T17:58:49Z","published":"2024-07-24T17:58:49Z","title":"Traversing Pareto Optimal Policies: Provably Efficient Multi-Objective\n  Reinforcement Learning","summary":"  This paper investigates multi-objective reinforcement learning (MORL), which\nfocuses on learning Pareto optimal policies in the presence of multiple reward\nfunctions. Despite MORL's significant empirical success, there is still a lack\nof satisfactory understanding of various MORL optimization targets and\nefficient learning algorithms. Our work offers a systematic analysis of several\noptimization targets to assess their abilities to find all Pareto optimal\npolicies and controllability over learned policies by the preferences for\ndifferent objectives. We then identify Tchebycheff scalarization as a favorable\nscalarization method for MORL. Considering the non-smoothness of Tchebycheff\nscalarization, we reformulate its minimization problem into a new min-max-max\noptimization problem. Then, for the stochastic policy class, we propose\nefficient algorithms using this reformulation to learn Pareto optimal policies.\nWe first propose an online UCB-based algorithm to achieve an $\\varepsilon$\nlearning error with an $\\tilde{\\mathcal{O}}(\\varepsilon^{-2})$ sample\ncomplexity for a single given preference. To further reduce the cost of\nenvironment exploration under different preferences, we propose a\npreference-free framework that first explores the environment without\npre-defined preferences and then generates solutions for any number of\npreferences. We prove that it only requires an\n$\\tilde{\\mathcal{O}}(\\varepsilon^{-2})$ exploration complexity in the\nexploration phase and demands no additional exploration afterward. Lastly, we\nanalyze the smooth Tchebycheff scalarization, an extension of Tchebycheff\nscalarization, which is proved to be more advantageous in distinguishing the\nPareto optimal policies from other weakly Pareto optimal policies based on\nentry values of preference vectors. Furthermore, we extend our algorithms and\ntheoretical analysis to accommodate this optimization target.\n","authors":["Shuang Qiu","Dake Zhang","Rui Yang","Boxiang Lyu","Tong Zhang"],"pdf_url":"https://arxiv.org/pdf/2407.17466v1.pdf","comment":"Initially submitted in May 2024"},{"id":"http://arxiv.org/abs/2407.17465v1","updated":"2024-07-24T17:58:42Z","published":"2024-07-24T17:58:42Z","title":"u-$μ$P: The Unit-Scaled Maximal Update Parametrization","summary":"  The Maximal Update Parametrization ($\\mu$P) aims to make the optimal\nhyperparameters (HPs) of a model independent of its size, allowing them to be\nswept using a cheap proxy model rather than the full-size target model. We\npresent a new scheme, u-$\\mu$P, which improves upon $\\mu$P by combining it with\nUnit Scaling, a method for designing models that makes them easy to train in\nlow-precision. The two techniques have a natural affinity: $\\mu$P ensures that\nthe scale of activations is independent of model size, and Unit Scaling ensures\nthat activations, weights and gradients begin training with a scale of one.\nThis synthesis opens the door to a simpler scheme, whose default values are\nnear-optimal. This in turn facilitates a more efficient sweeping strategy, with\nu-$\\mu$P models reaching a lower loss than comparable $\\mu$P models and working\nout-of-the-box in FP8.\n","authors":["Charlie Blake","Constantin Eichenberg","Josef Dean","Lukas Balles","Luke Y. Prince","Björn Deiseroth","Andres Felipe Cruz-Salinas","Carlo Luschi","Samuel Weinbach","Douglas Orr"],"pdf_url":"https://arxiv.org/pdf/2407.17465v1.pdf","comment":"48 pages"},{"id":"http://arxiv.org/abs/2407.17460v1","updated":"2024-07-24T17:57:21Z","published":"2024-07-24T17:57:21Z","title":"SoNIC: Safe Social Navigation with Adaptive Conformal Inference and\n  Constrained Reinforcement Learning","summary":"  Reinforcement Learning (RL) has enabled social robots to generate\ntrajectories without human-designed rules or interventions, which makes it more\neffective than hard-coded systems for generalizing to complex real-world\nscenarios. However, social navigation is a safety-critical task that requires\nrobots to avoid collisions with pedestrians while previous RL-based solutions\nfall short in safety performance in complex environments. To enhance the safety\nof RL policies, to the best of our knowledge, we propose the first algorithm,\nSoNIC, that integrates adaptive conformal inference (ACI) with constrained\nreinforcement learning (CRL) to learn safe policies for social navigation. More\nspecifically, our method augments RL observations with ACI-generated\nnonconformity scores and provides explicit guidance for agents to leverage the\nuncertainty metrics to avoid safety-critical areas by incorporating safety\nconstraints with spatial relaxation. Our method outperforms state-of-the-art\nbaselines in terms of both safety and adherence to social norms by a large\nmargin and demonstrates much stronger robustness to out-of-distribution\nscenarios. Our code and video demos are available on our project website:\nhttps://sonic-social-nav.github.io/.\n","authors":["Jianpeng Yao","Xiaopan Zhang","Yu Xia","Zejin Wang","Amit K. Roy-Chowdhury","Jiachen Li"],"pdf_url":"https://arxiv.org/pdf/2407.17460v1.pdf","comment":"Project website: https://sonic-social-nav.github.io/"},{"id":"http://arxiv.org/abs/2403.14236v3","updated":"2024-07-24T17:56:32Z","published":"2024-03-21T08:54:24Z","title":"A Unified Framework for Model Editing","summary":"  ROME and MEMIT are largely believed to be two different model editing\nalgorithms, with the major difference between them being the ability to perform\nbatched edits. In this paper, we unify these two algorithms under a single\nconceptual umbrella, optimizing for the same goal, which we call the\npreservation-memorization objective. ROME uses an equality constraint to\noptimize this objective to perform one edit at a time, whereas MEMIT employs a\nmore flexible least-square constraint that allows for batched edits. We\ngeneralize ROME and enable batched editing with equality constraint in the form\nof EMMET - an Equality-constrained Mass Model Editing algorithm for\nTransformers, a new batched memory-editing algorithm. EMMET can perform\nbatched-edits up to a batch-size of 10,000, with very similar performance to\nMEMIT across multiple dimensions. With the introduction of EMMET, we truly\nunify ROME and MEMIT and show that both algorithms are equivalent in terms of\ntheir optimization objective, their abilities (singular and batched editing),\ntheir model editing performance and their limitations.\n","authors":["Akshat Gupta","Dev Sajnani","Gopala Anumanchipalli"],"pdf_url":"https://arxiv.org/pdf/2403.14236v3.pdf","comment":"Under review. To appear as poster at KnowledgeableLM Workshop\n  co-located with ACL 2024"},{"id":"http://arxiv.org/abs/2407.17459v1","updated":"2024-07-24T17:54:07Z","published":"2024-07-24T17:54:07Z","title":"Hidden or Inferred: Fair Learning-To-Rank with Unknown Demographics","summary":"  As learning-to-rank models are increasingly deployed for decision-making in\nareas with profound life implications, the FairML community has been developing\nfair learning-to-rank (LTR) models. These models rely on the availability of\nsensitive demographic features such as race or sex. However, in practice,\nregulatory obstacles and privacy concerns protect this data from collection and\nuse. As a result, practitioners may either need to promote fairness despite the\nabsence of these features or turn to demographic inference tools to attempt to\ninfer them. Given that these tools are fallible, this paper aims to further\nunderstand how errors in demographic inference impact the fairness performance\nof popular fair LTR strategies. In which cases would it be better to keep such\ndemographic attributes hidden from models versus infer them? We examine a\nspectrum of fair LTR strategies ranging from fair LTR with and without\ndemographic features hidden versus inferred to fairness-unaware LTR followed by\nfair re-ranking. We conduct a controlled empirical investigation modeling\ndifferent levels of inference errors by systematically perturbing the inferred\nsensitive attribute. We also perform three case studies with real-world\ndatasets and popular open-source inference methods. Our findings reveal that as\ninference noise grows, LTR-based methods that incorporate fairness\nconsiderations into the learning process may increase bias. In contrast, fair\nre-ranking strategies are more robust to inference errors. All source code,\ndata, and experimental artifacts of our experimental study are available here:\nhttps://github.com/sewen007/hoiltr.git\n","authors":["Oluseun Olulana","Kathleen Cachel","Fabricio Murai","Elke Rundensteiner"],"pdf_url":"https://arxiv.org/pdf/2407.17459v1.pdf","comment":"This paper has been accepted by AAAI/AIES to the AIES 2024 conference"},{"id":"http://arxiv.org/abs/2407.17458v1","updated":"2024-07-24T17:50:54Z","published":"2024-07-24T17:50:54Z","title":"EuroCropsML: A Time Series Benchmark Dataset For Few-Shot Crop Type\n  Classification","summary":"  We introduce EuroCropsML, an analysis-ready remote sensing machine learning\ndataset for time series crop type classification of agricultural parcels in\nEurope. It is the first dataset designed to benchmark transnational few-shot\ncrop type classification algorithms that supports advancements in algorithmic\ndevelopment and research comparability. It comprises 706 683 multi-class\nlabeled data points across 176 classes, featuring annual time series of\nper-parcel median pixel values from Sentinel-2 L1C data for 2021, along with\ncrop type labels and spatial coordinates. Based on the open-source EuroCrops\ncollection, EuroCropsML is publicly available on Zenodo.\n","authors":["Joana Reuss","Jan Macdonald","Simon Becker","Lorenz Richter","Marco Körner"],"pdf_url":"https://arxiv.org/pdf/2407.17458v1.pdf","comment":"5 pages, 5 figures"},{"id":"http://arxiv.org/abs/2407.17449v1","updated":"2024-07-24T17:30:21Z","published":"2024-07-24T17:30:21Z","title":"Looking at Model Debiasing through the Lens of Anomaly Detection","summary":"  It is widely recognized that deep neural networks are sensitive to bias in\nthe data. This means that during training these models are likely to learn\nspurious correlations between data and labels, resulting in limited\ngeneralization abilities and low performance. In this context, model debiasing\napproaches can be devised aiming at reducing the model's dependency on such\nunwanted correlations, either leveraging the knowledge of bias information or\nnot. In this work, we focus on the latter and more realistic scenario, showing\nthe importance of accurately predicting the bias-conflicting and bias-aligned\nsamples to obtain compelling performance in bias mitigation. On this ground, we\npropose to conceive the problem of model bias from an out-of-distribution\nperspective, introducing a new bias identification method based on anomaly\ndetection. We claim that when data is mostly biased, bias-conflicting samples\ncan be regarded as outliers with respect to the bias-aligned distribution in\nthe feature space of a biased model, thus allowing for precisely detecting them\nwith an anomaly detection method. Coupling the proposed bias identification\napproach with bias-conflicting data upsampling and augmentation in a two-step\nstrategy, we reach state-of-the-art performance on synthetic and real benchmark\ndatasets. Ultimately, our proposed approach shows that the data bias issue does\nnot necessarily require complex debiasing methods, given that an accurate bias\nidentification procedure is defined.\n","authors":["Vito Paolo Pastore","Massimiliano Ciranni","Davide Marinelli","Francesca Odone","Vittorio Murino"],"pdf_url":"https://arxiv.org/pdf/2407.17449v1.pdf","comment":"15 pages, 7 figures"},{"id":"http://arxiv.org/abs/2404.14436v2","updated":"2024-07-24T17:26:07Z","published":"2024-04-19T20:03:30Z","title":"Investigating Resource-efficient Neutron/Gamma Classification ML Models\n  Targeting eFPGAs","summary":"  There has been considerable interest and resulting progress in implementing\nmachine learning (ML) models in hardware over the last several years from the\nparticle and nuclear physics communities. A big driver has been the release of\nthe Python package, hls4ml, which has enabled porting models specified and\ntrained using Python ML libraries to register transfer level (RTL) code. So\nfar, the primary end targets have been commercial FPGAs or synthesized custom\nblocks on ASICs. However, recent developments in open-source embedded FPGA\n(eFPGA) frameworks now provide an alternate, more flexible pathway for\nimplementing ML models in hardware. These customized eFPGA fabrics can be\nintegrated as part of an overall chip design. In general, the decision between\na fully custom, eFPGA, or commercial FPGA ML implementation will depend on the\ndetails of the end-use application. In this work, we explored the parameter\nspace for eFPGA implementations of fully-connected neural network (fcNN) and\nboosted decision tree (BDT) models using the task of neutron/gamma\nclassification with a specific focus on resource efficiency. We used data\ncollected using an AmBe sealed source incident on Stilbene, which was optically\ncoupled to an OnSemi J-series SiPM to generate training and test data for this\nstudy. We investigated relevant input features and the effects of\nbit-resolution and sampling rate as well as trade-offs in hyperparameters for\nboth ML architectures while tracking total resource usage. The performance\nmetric used to track model performance was the calculated neutron efficiency at\na gamma leakage of 10$^{-3}$. The results of the study will be used to aid the\nspecification of an eFPGA fabric, which will be integrated as part of a test\nchip.\n","authors":["Jyothisraj Johnson","Billy Boxer","Tarun Prakash","Carl Grace","Peter Sorensen","Mani Tripathi"],"pdf_url":"https://arxiv.org/pdf/2404.14436v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17446v1","updated":"2024-07-24T17:23:14Z","published":"2024-07-24T17:23:14Z","title":"Fractional signature: a generalisation of the signature inspired by\n  fractional calculus","summary":"  In this paper, we propose a novel generalisation of the signature of a path,\nmotivated by fractional calculus, which is able to describe the solutions of\nlinear Caputo controlled FDEs. We also propose another generalisation of the\nsignature, inspired by the previous one, but more convenient to use in machine\nlearning. Finally, we test this last signature in a toy application to the\nproblem of handwritten digit recognition, where significant improvements in\naccuracy rates are observed compared to those of the original signature.\n","authors":["José Manuel Corcuera","Rubén Jiménez"],"pdf_url":"https://arxiv.org/pdf/2407.17446v1.pdf","comment":"9 pages, 1 figure"},{"id":"http://arxiv.org/abs/2407.17438v1","updated":"2024-07-24T17:15:58Z","published":"2024-07-24T17:15:58Z","title":"HumanVid: Demystifying Training Data for Camera-controllable Human Image\n  Animation","summary":"  Human image animation involves generating videos from a character photo,\nallowing user control and unlocking potential for video and movie production.\nWhile recent approaches yield impressive results using high-quality training\ndata, the inaccessibility of these datasets hampers fair and transparent\nbenchmarking. Moreover, these approaches prioritize 2D human motion and\noverlook the significance of camera motions in videos, leading to limited\ncontrol and unstable video generation.To demystify the training data, we\npresent HumanVid, the first large-scale high-quality dataset tailored for human\nimage animation, which combines crafted real-world and synthetic data. For the\nreal-world data, we compile a vast collection of copyright-free real-world\nvideos from the internet. Through a carefully designed rule-based filtering\nstrategy, we ensure the inclusion of high-quality videos, resulting in a\ncollection of 20K human-centric videos in 1080P resolution. Human and camera\nmotion annotation is accomplished using a 2D pose estimator and a SLAM-based\nmethod. For the synthetic data, we gather 2,300 copyright-free 3D avatar assets\nto augment existing available 3D assets. Notably, we introduce a rule-based\ncamera trajectory generation method, enabling the synthetic pipeline to\nincorporate diverse and precise camera motion annotation, which can rarely be\nfound in real-world data. To verify the effectiveness of HumanVid, we establish\na baseline model named CamAnimate, short for Camera-controllable Human\nAnimation, that considers both human and camera motions as conditions. Through\nextensive experimentation, we demonstrate that such simple baseline training on\nour HumanVid achieves state-of-the-art performance in controlling both human\npose and camera motions, setting a new benchmark. Code and data will be\npublicly available at \\url{https://github.com/zhenzhiwang/HumanVid/}.\n","authors":["Zhenzhi Wang","Yixuan Li","Yanhong Zeng","Youqing Fang","Yuwei Guo","Wenran Liu","Jing Tan","Kai Chen","Tianfan Xue","Bo Dai","Dahua Lin"],"pdf_url":"https://arxiv.org/pdf/2407.17438v1.pdf","comment":"camera controllable human image animation, a dataset and a baseline"},{"id":"http://arxiv.org/abs/2402.06912v2","updated":"2024-07-24T17:15:44Z","published":"2024-02-10T09:15:21Z","title":"Solving Deep Reinforcement Learning Tasks with Evolution Strategies and\n  Linear Policy Networks","summary":"  Although deep reinforcement learning methods can learn effective policies for\nchallenging problems such as Atari games and robotics tasks, algorithms are\ncomplex, and training times are often long. This study investigates how\nEvolution Strategies perform compared to gradient-based deep reinforcement\nlearning methods. We use Evolution Strategies to optimize the weights of a\nneural network via neuroevolution, performing direct policy search. We\nbenchmark both deep policy networks and networks consisting of a single linear\nlayer from observations to actions for three gradient-based methods, such as\nProximal Policy Optimization. These methods are evaluated against three\nclassical Evolution Strategies and Augmented Random Search, which all use\nlinear policy networks. Our results reveal that Evolution Strategies can find\neffective linear policies for many reinforcement learning benchmark tasks,\nunlike deep reinforcement learning methods that can only find successful\npolicies using much larger networks, suggesting that current benchmarks are\neasier to solve than previously assumed. Interestingly, Evolution Strategies\nalso achieve results comparable to gradient-based deep reinforcement learning\nalgorithms for higher-complexity tasks. Furthermore, we find that by directly\naccessing the memory state of the game, Evolution Strategies can find\nsuccessful policies in Atari that outperform the policies found by Deep\nQ-Learning. Evolution Strategies also outperform Augmented Random Search in\nmost benchmarks, demonstrating superior sample efficiency and robustness in\ntraining linear policy networks.\n","authors":["Annie Wong","Jacob de Nobel","Thomas Bäck","Aske Plaat","Anna V. Kononova"],"pdf_url":"https://arxiv.org/pdf/2402.06912v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.01267v2","updated":"2024-07-24T17:13:55Z","published":"2024-03-02T17:10:44Z","title":"Dissecting Language Models: Machine Unlearning via Selective Pruning","summary":"  Understanding and shaping the behaviour of Large Language Models (LLMs) is\nincreasingly important as applications become more powerful and more frequently\nadopted. This paper introduces a machine unlearning method specifically\ndesigned for LLMs. We introduce a selective pruning method for LLMs that\nremoves neurons based on their relative importance on a targeted capability\ncompared to overall network performance. This approach is a compute- and\ndata-efficient method for identifying and removing neurons that enable specific\nbehaviours. Our findings reveal that both feed-forward and attention neurons in\nLLMs are specialized; that is, for specific tasks, certain neurons are more\ncrucial than others. Code from all experiments is available at\nhttps://github.com/nickypro/selective-pruning\n","authors":["Nicholas Pochinkov","Nandi Schoots"],"pdf_url":"https://arxiv.org/pdf/2403.01267v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17437v1","updated":"2024-07-24T17:13:31Z","published":"2024-07-24T17:13:31Z","title":"Nerva: a Truly Sparse Implementation of Neural Networks","summary":"  We introduce Nerva, a fast neural network library under development in C++.\nIt supports sparsity by using the sparse matrix operations of Intel's Math\nKernel Library (MKL), which eliminates the need for binary masks. We show that\nNerva significantly decreases training time and memory usage while reaching\nequivalent accuracy to PyTorch. We run static sparse experiments with an MLP on\nCIFAR-10. On high sparsity levels like $99\\%$, the runtime is reduced by a\nfactor of $4\\times$ compared to a PyTorch model using masks. Similar to other\npopular frameworks such as PyTorch and Keras, Nerva offers a Python interface\nfor users to work with.\n","authors":["Wieger Wesselink","Bram Grooten","Qiao Xiao","Cassio de Campos","Mykola Pechenizkiy"],"pdf_url":"https://arxiv.org/pdf/2407.17437v1.pdf","comment":"The Nerva library is available at https://github.com/wiegerw/nerva"},{"id":"http://arxiv.org/abs/2407.13018v2","updated":"2024-07-24T17:04:35Z","published":"2024-07-17T21:14:05Z","title":"Proof-of-Collaborative-Learning: A Multi-winner Federated Learning\n  Consensus Algorithm","summary":"  Regardless of their variations, blockchains require a consensus mechanism to\nvalidate transactions, supervise added blocks, maintain network security,\nsynchronize the network state, and distribute incentives. Proof-of-Work (PoW),\none of the most influential implementations of consensus mechanisms, consumes\nan extraordinary amount of energy for a task that lacks direct productive\noutput. In this paper, we propose Proof-of-Collaborative-Learning (PoCL), a\nmulti-winner federated learning validated consensus mechanism that redirects\nthe computation power of blockchains to train federated learning models. In\naddition, we present a novel evaluation mechanism to ensure the efficiency of\nthe locally trained models of miners. We evaluated the security of our\nevaluation mechanism by introducing and conducting probable attacks. Moreover,\nwe present a novel reward distribution mechanism to incentivize winning miners\nfairly, and demonstrate that our reward system is fair both within and across\nall rounds.\n","authors":["Amirreza Sokhankhosh","Sara Rouhani"],"pdf_url":"https://arxiv.org/pdf/2407.13018v2.pdf","comment":"8 pages. Accepted at the 7th IEEE International Conference on\n  Blockchain (Blockchain 2024)"},{"id":"http://arxiv.org/abs/2403.14606v2","updated":"2024-07-24T16:56:17Z","published":"2024-03-21T17:55:16Z","title":"The Elements of Differentiable Programming","summary":"  Artificial intelligence has recently experienced remarkable advances, fueled\nby large models, vast datasets, accelerated hardware, and, last but not least,\nthe transformative power of differentiable programming. This new programming\nparadigm enables end-to-end differentiation of complex computer programs\n(including those with control flows and data structures), making gradient-based\noptimization of program parameters possible. As an emerging paradigm,\ndifferentiable programming builds upon several areas of computer science and\napplied mathematics, including automatic differentiation, graphical models,\noptimization and statistics. This book presents a comprehensive review of the\nfundamental concepts useful for differentiable programming. We adopt two main\nperspectives, that of optimization and that of probability, with clear\nanalogies between the two. Differentiable programming is not merely the\ndifferentiation of programs, but also the thoughtful design of programs\nintended for differentiation. By making programs differentiable, we inherently\nintroduce probability distributions over their execution, providing a means to\nquantify the uncertainty associated with program outputs.\n","authors":["Mathieu Blondel","Vincent Roulet"],"pdf_url":"https://arxiv.org/pdf/2403.14606v2.pdf","comment":"Draft version 2"},{"id":"http://arxiv.org/abs/2402.14925v2","updated":"2024-07-24T16:54:33Z","published":"2024-02-22T19:15:50Z","title":"Efficient Unbiased Sparsification","summary":"  An unbiased $m$-sparsification of a vector $p\\in \\mathbb{R}^n$ is a random\nvector $Q\\in \\mathbb{R}^n$ with mean $p$ that has at most $m<n$ nonzero\ncoordinates. Unbiased sparsification compresses the original vector without\nintroducing bias; it arises in various contexts, such as in federated learning\nand sampling sparse probability distributions. Ideally, unbiased sparsification\nshould also minimize the expected value of a divergence function\n$\\mathsf{Div}(Q,p)$ that measures how far away $Q$ is from the original $p$. If\n$Q$ is optimal in this sense, then we call it efficient. Our main results\ndescribe efficient unbiased sparsifications for divergences that are either\npermutation-invariant or additively separable. Surprisingly, the\ncharacterization for permutation-invariant divergences is robust to the choice\nof divergence function, in the sense that our class of optimal $Q$ for squared\nEuclidean distance coincides with our class of optimal $Q$ for Kullback-Leibler\ndivergence, or indeed any of a wide variety of divergences.\n","authors":["Leighton Barnes","Stephen Cameron","Timothy Chow","Emma Cohen","Keith Frankston","Benjamin Howard","Fred Kochman","Daniel Scheinerman","Jeffrey VanderKam"],"pdf_url":"https://arxiv.org/pdf/2402.14925v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17417v1","updated":"2024-07-24T16:53:09Z","published":"2024-07-24T16:53:09Z","title":"Can Watermarking Large Language Models Prevent Copyrighted Text\n  Generation and Hide Training Data?","summary":"  Large Language Models (LLMs) have demonstrated impressive capabilities in\ngenerating diverse and contextually rich text. However, concerns regarding\ncopyright infringement arise as LLMs may inadvertently produce copyrighted\nmaterial. In this paper, we first investigate the effectiveness of watermarking\nLLMs as a deterrent against the generation of copyrighted texts. Through\ntheoretical analysis and empirical evaluation, we demonstrate that\nincorporating watermarks into LLMs significantly reduces the likelihood of\ngenerating copyrighted content, thereby addressing a critical concern in the\ndeployment of LLMs. Additionally, we explore the impact of watermarking on\nMembership Inference Attacks (MIAs), which aim to discern whether a sample was\npart of the pretraining dataset and may be used to detect copyright violations.\nSurprisingly, we find that watermarking adversely affects the success rate of\nMIAs, complicating the task of detecting copyrighted text in the pretraining\ndataset. Finally, we propose an adaptive technique to improve the success rate\nof a recent MIA under watermarking. Our findings underscore the importance of\ndeveloping adaptive methods to study critical problems in LLMs with potential\nlegal implications.\n","authors":["Michael-Andrei Panaitescu-Liess","Zora Che","Bang An","Yuancheng Xu","Pankayaraj Pathmanathan","Souradip Chakraborty","Sicheng Zhu","Tom Goldstein","Furong Huang"],"pdf_url":"https://arxiv.org/pdf/2407.17417v1.pdf","comment":"21 pages, 6 figures"},{"id":"http://arxiv.org/abs/2407.14933v2","updated":"2024-07-24T16:52:51Z","published":"2024-07-20T16:50:18Z","title":"Consent in Crisis: The Rapid Decline of the AI Data Commons","summary":"  General-purpose artificial intelligence (AI) systems are built on massive\nswathes of public web data, assembled into corpora such as C4, RefinedWeb, and\nDolma. To our knowledge, we conduct the first, large-scale, longitudinal audit\nof the consent protocols for the web domains underlying AI training corpora.\nOur audit of 14,000 web domains provides an expansive view of crawlable web\ndata and how codified data use preferences are changing over time. We observe a\nproliferation of AI-specific clauses to limit use, acute differences in\nrestrictions on AI developers, as well as general inconsistencies between\nwebsites' expressed intentions in their Terms of Service and their robots.txt.\nWe diagnose these as symptoms of ineffective web protocols, not designed to\ncope with the widespread re-purposing of the internet for AI. Our longitudinal\nanalyses show that in a single year (2023-2024) there has been a rapid\ncrescendo of data restrictions from web sources, rendering ~5%+ of all tokens\nin C4, or 28%+ of the most actively maintained, critical sources in C4, fully\nrestricted from use. For Terms of Service crawling restrictions, a full 45% of\nC4 is now restricted. If respected or enforced, these restrictions are rapidly\nbiasing the diversity, freshness, and scaling laws for general-purpose AI\nsystems. We hope to illustrate the emerging crises in data consent, for both\ndevelopers and creators. The foreclosure of much of the open web will impact\nnot only commercial AI, but also non-commercial AI and academic research.\n","authors":["Shayne Longpre","Robert Mahari","Ariel Lee","Campbell Lund","Hamidah Oderinwale","William Brannon","Nayan Saxena","Naana Obeng-Marnu","Tobin South","Cole Hunter","Kevin Klyman","Christopher Klamm","Hailey Schoelkopf","Nikhil Singh","Manuel Cherep","Ahmad Anis","An Dinh","Caroline Chitongo","Da Yin","Damien Sileo","Deividas Mataciunas","Diganta Misra","Emad Alghamdi","Enrico Shippole","Jianguo Zhang","Joanna Materzynska","Kun Qian","Kush Tiwary","Lester Miranda","Manan Dey","Minnie Liang","Mohammed Hamdy","Niklas Muennighoff","Seonghyeon Ye","Seungone Kim","Shrestha Mohanty","Vipul Gupta","Vivek Sharma","Vu Minh Chien","Xuhui Zhou","Yizhi Li","Caiming Xiong","Luis Villa","Stella Biderman","Hanlin Li","Daphne Ippolito","Sara Hooker","Jad Kabbara","Sandy Pentland"],"pdf_url":"https://arxiv.org/pdf/2407.14933v2.pdf","comment":"41 pages (13 main), 5 figures, 9 tables"},{"id":"http://arxiv.org/abs/2407.16083v2","updated":"2024-07-24T16:45:29Z","published":"2024-07-22T23:04:49Z","title":"Self-driving lab discovers principles for steering spontaneous emission","summary":"  We developed an autonomous experimentation platform to accelerate\ninterpretable scientific discovery in ultrafast nanophotonics, targeting a\nnovel method to steer spontaneous emission from reconfigurable semiconductor\nmetasurfaces. Controlling spontaneous emission is crucial for clean-energy\nsolutions in illumination, thermal radiation engineering, and remote sensing.\nDespite the potential of reconfigurable semiconductor metasurfaces with\nembedded sources for spatiotemporal control, achieving arbitrary far-field\ncontrol remains challenging. Here, we present a self-driving lab (SDL) platform\nthat addresses this challenge by discovering the governing equations for\npredicting the far-field emission profile from light-emitting metasurfaces. We\ndiscover that both the spatial gradient (grating-like) and the curvature\n(lens-like) of the local refractive index are key factors in steering\nspontaneous emission. The SDL employs a machine-learning framework comprising:\n(1) a variational autoencoder for generating complex spatial refractive index\nprofiles, (2) an active learning agent for guiding experiments with real-time\nclosed-loop feedback, and (3) a neural network-based equation learner to\nuncover structure-property relationships. The SDL demonstrated a four-fold\nenhancement in peak emission directivity (up to 77%) over a 72{\\deg} field of\nview within ~300 experiments. Our findings reveal that combinations of positive\ngratings and lenses are as effective as negative lenses and gratings for all\nemission angles, offering a novel strategy for controlling spontaneous emission\nbeyond conventional Fourier optics.\n","authors":["Saaketh Desai","Sadhvikas Addamane","Jeffery Y. Tsao","Igal Brener","Remi Dingreville","Prasad P. Iyer"],"pdf_url":"https://arxiv.org/pdf/2407.16083v2.pdf","comment":"25 pages, 4 figures in main text, 5 figures in supplementary\n  information"},{"id":"http://arxiv.org/abs/2403.12844v3","updated":"2024-07-24T16:17:22Z","published":"2024-03-19T15:51:21Z","title":"MELTing point: Mobile Evaluation of Language Transformers","summary":"  Transformers have revolutionized the machine learning landscape, gradually\nmaking their way into everyday tasks and equipping our computers with \"sparks\nof intelligence\". However, their runtime requirements have prevented them from\nbeing broadly deployed on mobile. As personal devices become increasingly\npowerful and prompt privacy becomes an ever more pressing issue, we explore the\ncurrent state of mobile execution of Large Language Models (LLMs). To achieve\nthis, we have created our own automation infrastructure, MELT, which supports\nthe headless execution and benchmarking of LLMs on device, supporting different\nmodels, devices and frameworks, including Android, iOS and Nvidia Jetson\ndevices. We evaluate popular instruction fine-tuned LLMs and leverage different\nframeworks to measure their end-to-end and granular performance, tracing their\nmemory and energy requirements along the way. Our analysis is the first\nsystematic study of on-device LLM execution, quantifying performance, energy\nefficiency and accuracy across various state-of-the-art models and showcases\nthe state of on-device intelligence in the era of hyperscale models. Results\nhighlight the performance heterogeneity across targets and corroborates that\nLLM inference is largely memory-bound. Quantization drastically reduces memory\nrequirements and renders execution viable, but at a non-negligible accuracy\ncost. Drawing from its energy footprint and thermal behavior, the continuous\nexecution of LLMs remains elusive, as both factors negatively affect user\nexperience. Last, our experience shows that the ecosystem is still in its\ninfancy, and algorithmic as well as hardware breakthroughs can significantly\nshift the execution cost. We expect NPU acceleration, and framework-hardware\nco-design to be the biggest bet towards efficient standalone execution, with\nthe alternative of offloading tailored towards edge deployments.\n","authors":["Stefanos Laskaridis","Kleomenis Katevas","Lorenzo Minto","Hamed Haddadi"],"pdf_url":"https://arxiv.org/pdf/2403.12844v3.pdf","comment":"Accepted at the 30th Annual International Conference On Mobile\n  Computing And Networking (MobiCom 2024)"},{"id":"http://arxiv.org/abs/2407.17396v1","updated":"2024-07-24T16:17:15Z","published":"2024-07-24T16:17:15Z","title":"Systematic Reasoning About Relational Domains With Graph Neural Networks","summary":"  Developing models that can learn to reason is a notoriously challenging\nproblem. We focus on reasoning in relational domains, where the use of Graph\nNeural Networks (GNNs) seems like a natural choice. However, previous work on\nreasoning with GNNs has shown that such models tend to fail when presented with\ntest examples that require longer inference chains than those seen during\ntraining. This suggests that GNNs lack the ability to generalize from training\nexamples in a systematic way, which would fundamentally limit their reasoning\nabilities. A common solution is to instead rely on neuro-symbolic methods,\nwhich are capable of reasoning in a systematic way by design. Unfortunately,\nthe scalability of such methods is often limited and they tend to rely on\noverly strong assumptions, e.g.\\ that queries can be answered by inspecting a\nsingle relational path. In this paper, we revisit the idea of reasoning with\nGNNs, showing that systematic generalization is possible as long as the right\ninductive bias is provided. In particular, we argue that node embeddings should\nbe treated as epistemic states and that GNN should be parameterised\naccordingly. We propose a simple GNN architecture which is based on this view\nand show that it is capable of achieving state-of-the-art results. We\nfurthermore introduce a benchmark which requires models to aggregate evidence\nfrom multiple relational paths. We show that existing neuro-symbolic approaches\nfail on this benchmark, whereas our considered GNN model learns to reason\naccurately.\n","authors":["Irtaza Khalid","Steven Schockaert"],"pdf_url":"https://arxiv.org/pdf/2407.17396v1.pdf","comment":"10+16 pages, 2+7 figures, 4+9 tables. Preprint under review. Comments\n  welcome"},{"id":"http://arxiv.org/abs/2407.17395v1","updated":"2024-07-24T16:17:14Z","published":"2024-07-24T16:17:14Z","title":"Five reasons against assuming a data-generating distribution in Machine\n  Learning","summary":"  Machine Learning research, as most of Statistics, heavily relies on the\nconcept of a data-generating probability distribution. As data points are\nthought to be sampled from such a distribution, we can learn from observed data\nabout this distribution and, thus, predict future data points drawn from it\n(with some probability of success). Drawing on scholarship across disciplines,\nwe here argue that this framework is not always a good model. Not only do such\ntrue probability distributions not exist; the framework can also be misleading\nand obscure both the choices made and the goals pursued in machine learning\npractice. We suggest an alternative framework that focuses on finite\npopulations rather than abstract distributions; while classical learning theory\ncan be left almost unchanged, it opens new opportunities, especially to model\nsampling. We compile these considerations into five reasons for modelling\nmachine learning -- in some settings -- with finite distributions rather than\ngenerative distributions, both to be more faithful to practice and to provide\nnovel theoretical insights.\n","authors":["Benedikt Höltgen","Robert C. Williamson"],"pdf_url":"https://arxiv.org/pdf/2407.17395v1.pdf","comment":"Presented at the Humans, Algorithmic Decision-Making and Society\n  Workshop at ICML 2024"},{"id":"http://arxiv.org/abs/2307.09230v2","updated":"2024-07-24T16:15:44Z","published":"2023-07-18T13:06:17Z","title":"Detecting Throat Cancer from Speech Signals using Machine Learning: A\n  Scoping Literature Review","summary":"  Introduction: Cases of throat cancer are rising worldwide. With survival\ndecreasing significantly at later stages, early detection is vital. Artificial\nintelligence (AI) and machine learning (ML) have the potential to detect throat\ncancer from patient speech, facilitating earlier diagnosis and reducing the\nburden on overstretched healthcare systems. However, no comprehensive review\nhas explored the use of AI and ML for detecting throat cancer from speech. This\nreview aims to fill this gap by evaluating how these technologies perform and\nidentifying issues that need to be addressed in future research. Materials and\nMethods: We conducted a scoping literature review across three databases:\nScopus,Web of Science, and PubMed. We included articles that classified speech\nusing machine learning and specified the inclusion of throat cancer patients in\ntheir data. Articles were categorized based on whether they performed binary or\nmulti-class classification. Results: We found 27 articles fitting our inclusion\ncriteria, 12 performing binary classification, 13 performing multi-class\nclassification, and two that do both binary and multiclass classification. The\nmost common classification method used was neural networks, and the most\nfrequently extracted feature was mel-spectrograms. We also documented\npre-processing methods and classifier performance. We compared each article\nagainst the TRIPOD-AI checklist, which showed a significant lack of open\nscience, with only one article sharing code and only three using open-access\ndata. Conclusion: Open-source code is essential for external validation and\nfurther development in this field. Our review indicates that no single method\nor specific feature consistently outperforms others in detecting throat cancer\nfrom speech. Future research should focus on standardizing methodologies and\nimproving the reproducibility of results.\n","authors":["Mary Paterson","James Moor","Luisa Cutillo"],"pdf_url":"https://arxiv.org/pdf/2307.09230v2.pdf","comment":"15 pages, 10 figures, 5 tables"},{"id":"http://arxiv.org/abs/2406.06348v2","updated":"2024-07-24T16:13:45Z","published":"2024-06-10T15:08:14Z","title":"Causal Discovery over High-Dimensional Structured Hypothesis Spaces with\n  Causal Graph Partitioning","summary":"  The aim in many sciences is to understand the mechanisms that underlie the\nobserved distribution of variables, starting from a set of initial hypotheses.\nCausal discovery allows us to infer mechanisms as sets of cause and effect\nrelationships in a generalized way -- without necessarily tailoring to a\nspecific domain. Causal discovery algorithms search over a structured\nhypothesis space, defined by the set of directed acyclic graphs, to find the\ngraph that best explains the data. For high-dimensional problems, however, this\nsearch becomes intractable and scalable algorithms for causal discovery are\nneeded to bridge the gap. In this paper, we define a novel causal graph\npartition that allows for divide-and-conquer causal discovery with theoretical\nguarantees. We leverage the idea of a superstructure -- a set of learned or\nexisting candidate hypotheses -- to partition the search space. We prove under\ncertain assumptions that learning with a causal graph partition always yields\nthe Markov Equivalence Class of the true causal graph. We show our algorithm\nachieves comparable accuracy and a faster time to solution for\nbiologically-tuned synthetic networks and networks up to ${10^4}$ variables.\nThis makes our method applicable to gene regulatory network inference and other\ndomains with high-dimensional structured hypothesis spaces.\n","authors":["Ashka Shah","Adela DePavia","Nathaniel Hudson","Ian Foster","Rick Stevens"],"pdf_url":"https://arxiv.org/pdf/2406.06348v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17385v1","updated":"2024-07-24T16:07:57Z","published":"2024-07-24T16:07:57Z","title":"Causal modelling without counterfactuals and individualised effects","summary":"  The most common approach to causal modelling is the potential outcomes\nframework due to Neyman and Rubin. In this framework, outcomes of\ncounterfactual treatments are assumed to be well-defined. This metaphysical\nassumption is often thought to be problematic yet indispensable. The\nconventional approach relies not only on counterfactuals, but also on abstract\nnotions of distributions and assumptions of independence that are not directly\ntestable. In this paper, we construe causal inference as treatment-wise\npredictions for finite populations where all assumptions are testable; this\nmeans that one can not only test predictions themselves (without any\nfundamental problem), but also investigate sources of error when they fail. The\nnew framework highlights the model-dependence of causal claims as well as the\ndifference between statistical and scientific inference.\n","authors":["Benedikt Höltgen","Robert C. Williamson"],"pdf_url":"https://arxiv.org/pdf/2407.17385v1.pdf","comment":"Presented at the Humans, Algorithmic Decision-Making and Society\n  Workshop at ICML 2024"},{"id":"http://arxiv.org/abs/2407.17383v1","updated":"2024-07-24T16:07:11Z","published":"2024-07-24T16:07:11Z","title":"A Comprehensive Approach to Misspelling Correction with BERT and\n  Levenshtein Distance","summary":"  Writing, as an omnipresent form of human communication, permeates nearly\nevery aspect of contemporary life. Consequently, inaccuracies or errors in\nwritten communication can lead to profound consequences, ranging from financial\nlosses to potentially life-threatening situations. Spelling mistakes, among the\nmost prevalent writing errors, are frequently encountered due to various\nfactors. This research aims to identify and rectify diverse spelling errors in\ntext using neural networks, specifically leveraging the Bidirectional Encoder\nRepresentations from Transformers (BERT) masked language model. To achieve this\ngoal, we compiled a comprehensive dataset encompassing both non-real-word and\nreal-word errors after categorizing different types of spelling mistakes.\nSubsequently, multiple pre-trained BERT models were employed. To ensure optimal\nperformance in correcting misspelling errors, we propose a combined approach\nutilizing the BERT masked language model and Levenshtein distance. The results\nfrom our evaluation data demonstrate that the system presented herein exhibits\nremarkable capabilities in identifying and rectifying spelling mistakes, often\nsurpassing existing systems tailored for the Persian language.\n","authors":["Amirreza Naziri","Hossein Zeinali"],"pdf_url":"https://arxiv.org/pdf/2407.17383v1.pdf","comment":"12 pages, 9 figures, 5 tables"},{"id":"http://arxiv.org/abs/2407.17377v1","updated":"2024-07-24T15:57:55Z","published":"2024-07-24T15:57:55Z","title":"Entropy Reweighted Conformal Classification","summary":"  Conformal Prediction (CP) is a powerful framework for constructing prediction\nsets with guaranteed coverage. However, recent studies have shown that\nintegrating confidence calibration with CP can lead to a degradation in\nefficiency. In this paper, We propose an adaptive approach that considers the\nclassifier's uncertainty and employs entropy-based reweighting to enhance the\nefficiency of prediction sets for conformal classification. Our experimental\nresults demonstrate that this method significantly improves efficiency.\n","authors":["Rui Luo","Nicolo Colombo"],"pdf_url":"https://arxiv.org/pdf/2407.17377v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.16534v3","updated":"2024-07-24T15:45:58Z","published":"2023-05-25T23:32:10Z","title":"Variation Spaces for Multi-Output Neural Networks: Insights on\n  Multi-Task Learning and Network Compression","summary":"  This paper introduces a novel theoretical framework for the analysis of\nvector-valued neural networks through the development of vector-valued\nvariation spaces, a new class of reproducing kernel Banach spaces. These spaces\nemerge from studying the regularization effect of weight decay in training\nnetworks with activations like the rectified linear unit (ReLU). This framework\noffers a deeper understanding of multi-output networks and their function-space\ncharacteristics. A key contribution of this work is the development of a\nrepresenter theorem for the vector-valued variation spaces. This representer\ntheorem establishes that shallow vector-valued neural networks are the\nsolutions to data-fitting problems over these infinite-dimensional spaces,\nwhere the network widths are bounded by the square of the number of training\ndata. This observation reveals that the norm associated with these\nvector-valued variation spaces encourages the learning of features that are\nuseful for multiple tasks, shedding new light on multi-task learning with\nneural networks. Finally, this paper develops a connection between weight-decay\nregularization and the multi-task lasso problem. This connection leads to novel\nbounds for layer widths in deep networks that depend on the intrinsic\ndimensions of the training data representations. This insight not only deepens\nthe understanding of the deep network architectural requirements, but also\nyields a simple convex optimization method for deep neural network compression.\nThe performance of this compression procedure is evaluated on various\narchitectures.\n","authors":["Joseph Shenouda","Rahul Parhi","Kangwook Lee","Robert D. Nowak"],"pdf_url":"https://arxiv.org/pdf/2305.16534v3.pdf","comment":"Updated to version published in JMLR"},{"id":"http://arxiv.org/abs/2405.01557v4","updated":"2024-07-24T15:43:49Z","published":"2024-03-22T13:08:22Z","title":"An Experimental Study on the Rashomon Effect of Balancing Methods in\n  Imbalanced Classification","summary":"  Predictive models may generate biased predictions when classifying imbalanced\ndatasets. This happens when the model favors the majority class, leading to low\nperformance in accurately predicting the minority class. To address this issue,\nbalancing or resampling methods are critical data-centric AI approaches in the\nmodeling process to improve prediction performance. However, there have been\ndebates and questions about the functionality of these methods in recent years.\nIn particular, many candidate models may exhibit very similar predictive\nperformance, called the Rashomon effect, in model selection, and they may even\nproduce different predictions for the same observations. Selecting one of these\nmodels without considering the predictive multiplicity -- which is the case of\nyielding conflicting models' predictions for any sample -- can result in blind\nselection. In this paper, the impact of balancing methods on predictive\nmultiplicity is examined using the Rashomon effect. It is crucial because the\nblind model selection in data-centric AI is risky from a set of approximately\nequally accurate models. This may lead to severe problems in model selection,\nvalidation, and explanation. To tackle this matter, we conducted real dataset\nexperiments to observe the impact of balancing methods on predictive\nmultiplicity through the Rashomon effect by using a newly proposed metric\nobscurity in addition to the existing ones: ambiguity and discrepancy. Our\nfindings showed that balancing methods inflate the predictive multiplicity and\nyield varying results. To monitor the trade-off between the prediction\nperformance and predictive multiplicity for conducting the modeling process\nresponsibly, we proposed using the extended version of the performance-gain\nplot when balancing the training data.\n","authors":["Mustafa Cavus","Przemysław Biecek"],"pdf_url":"https://arxiv.org/pdf/2405.01557v4.pdf","comment":"16 pages, 6 figures"},{"id":"http://arxiv.org/abs/2407.17358v1","updated":"2024-07-24T15:30:12Z","published":"2024-07-24T15:30:12Z","title":"Quantile Learn-Then-Test: Quantile-Based Risk Control for Hyperparameter\n  Optimization","summary":"  The increasing adoption of Artificial Intelligence (AI) in engineering\nproblems calls for the development of calibration methods capable of offering\nrobust statistical reliability guarantees. The calibration of black box AI\nmodels is carried out via the optimization of hyperparameters dictating\narchitecture, optimization, and/or inference configuration. Prior work has\nintroduced learn-then-test (LTT), a calibration procedure for hyperparameter\noptimization (HPO) that provides statistical guarantees on average performance\nmeasures. Recognizing the importance of controlling risk-aware objectives in\nengineering contexts, this work introduces a variant of LTT that is designed to\nprovide statistical guarantees on quantiles of a risk measure. We illustrate\nthe practical advantages of this approach by applying the proposed algorithm to\na radio access scheduling problem.\n","authors":["Amirmohammad Farzaneh","Sangwoo Park","Osvaldo Simeone"],"pdf_url":"https://arxiv.org/pdf/2407.17358v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.14040v3","updated":"2024-07-24T15:29:46Z","published":"2023-03-24T14:51:06Z","title":"Euler Characteristic Tools For Topological Data Analysis","summary":"  In this article, we study Euler characteristic techniques in topological data\nanalysis. Pointwise computing the Euler characteristic of a family of\nsimplicial complexes built from data gives rise to the so-called Euler\ncharacteristic profile. We show that this simple descriptor achieve\nstate-of-the-art performance in supervised tasks at a very low computational\ncost. Inspired by signal analysis, we compute hybrid transforms of Euler\ncharacteristic profiles. These integral transforms mix Euler characteristic\ntechniques with Lebesgue integration to provide highly efficient compressors of\ntopological signals. As a consequence, they show remarkable performances in\nunsupervised settings. On the qualitative side, we provide numerous heuristics\non the topological and geometric information captured by Euler profiles and\ntheir hybrid transforms. Finally, we prove stability results for these\ndescriptors as well as asymptotic guarantees in random settings.\n","authors":["Olympio Hacquard","Vadim Lebovici"],"pdf_url":"https://arxiv.org/pdf/2303.14040v3.pdf","comment":"39 pages - Version accepted in JMLR"},{"id":"http://arxiv.org/abs/2407.17356v1","updated":"2024-07-24T15:28:08Z","published":"2024-07-24T15:28:08Z","title":"Gradient-based inference of abstract task representations for\n  generalization in neural networks","summary":"  Humans and many animals show remarkably adaptive behavior and can respond\ndifferently to the same input depending on their internal goals. The brain not\nonly represents the intermediate abstractions needed to perform a computation\nbut also actively maintains a representation of the computation itself (task\nabstraction). Such separation of the computation and its abstraction is\nassociated with faster learning, flexible decision-making, and broad\ngeneralization capacity. We investigate if such benefits might extend to neural\nnetworks trained with task abstractions. For such benefits to emerge, one needs\na task inference mechanism that possesses two crucial abilities: First, the\nability to infer abstract task representations when no longer explicitly\nprovided (task inference), and second, manipulate task representations to adapt\nto novel problems (task recomposition). To tackle this, we cast task inference\nas an optimization problem from a variational inference perspective and ground\nour approach in an expectation-maximization framework. We show that gradients\nbackpropagated through a neural network to a task representation layer are an\nefficient heuristic to infer current task demands, a process we refer to as\ngradient-based inference (GBI). Further iterative optimization of the task\nrepresentation layer allows for recomposing abstractions to adapt to novel\nsituations. Using a toy example, a novel image classifier, and a language\nmodel, we demonstrate that GBI provides higher learning efficiency and\ngeneralization to novel tasks and limits forgetting. Moreover, we show that GBI\nhas unique advantages such as preserving information for uncertainty estimation\nand detecting out-of-distribution samples.\n","authors":["Ali Hummos","Felipe del Río","Brabeeba Mien Wang","Julio Hurtado","Cristian B. Calderon","Guangyu Robert Yang"],"pdf_url":"https://arxiv.org/pdf/2407.17356v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17353v1","updated":"2024-07-24T15:26:01Z","published":"2024-07-24T15:26:01Z","title":"Scalify: scale propagation for efficient low-precision LLM training","summary":"  Low-precision formats such as float8 have been introduced in machine learning\naccelerated hardware to improve computational efficiency for large language\nmodels training and inference. Nevertheless, adoption by the ML community has\nbeen slowed down by the complex, and sometimes brittle, techniques required to\nmatch higher precision training accuracy. In this work, we present Scalify, a\nend-to-end scale propagation paradigm for computational graphs, generalizing\nand formalizing existing tensor scaling methods. Experiment results show that\nScalify supports out-of-the-box float8 matrix multiplication and gradients\nrepresentation, as well as float16 optimizer state storage. Our JAX\nimplementation of Scalify is open-sourced at\nhttps://github.com/graphcore-research/jax-scalify\n","authors":["Paul Balança","Sam Hosegood","Carlo Luschi","Andrew Fitzgibbon"],"pdf_url":"https://arxiv.org/pdf/2407.17353v1.pdf","comment":"11 pages, 5 figures, ICML 2024 WANT workshop"},{"id":"http://arxiv.org/abs/2305.12517v5","updated":"2024-07-24T15:10:41Z","published":"2023-05-21T17:14:31Z","title":"Description-Based Text Similarity","summary":"  Identifying texts with a given semantics is central for many information\nseeking scenarios. Similarity search over vector embeddings appear to be\ncentral to this ability, yet the similarity reflected in current text\nembeddings is corpus-driven, and is inconsistent and sub-optimal for many use\ncases. What, then, is a good notion of similarity for effective retrieval of\ntext?\n  We identify the need to search for texts based on abstract descriptions of\ntheir content, and the corresponding notion of \\emph{description based\nsimilarity}. We demonstrate the inadequacy of current text embeddings and\npropose an alternative model that significantly improves when used in standard\nnearest neighbor search. The model is trained using positive and negative pairs\nsourced through prompting a LLM, demonstrating how data from LLMs can be used\nfor creating new capabilities not immediately possible using the original\nmodel.\n","authors":["Shauli Ravfogel","Valentina Pyatkin","Amir DN Cohen","Avshalom Manevich","Yoav Goldberg"],"pdf_url":"https://arxiv.org/pdf/2305.12517v5.pdf","comment":"Accepted in COLM 2024"},{"id":"http://arxiv.org/abs/2407.17341v1","updated":"2024-07-24T15:08:52Z","published":"2024-07-24T15:08:52Z","title":"Mathematical programming algorithms for convex hull approximation with a\n  hyperplane budget","summary":"  We consider the following problem in computational geometry: given, in the\nd-dimensional real space, a set of points marked as positive and a set of\npoints marked as negative, such that the convex hull of the positive set does\nnot intersect the negative set, find K hyperplanes that separate, if possible,\nall the positive points from the negative ones. That is, we search for a convex\npolyhedron with at most K faces, containing all the positive points and no\nnegative point. The problem is known in the literature for pure convex\npolyhedral approximation; our interest stems from its possible applications in\nconstraint learning, where points are feasible or infeasible solutions of a\nMixed Integer Program, and the K hyperplanes are linear constraints to be\nfound. We cast the problem as an optimization one, minimizing the number of\nnegative points inside the convex polyhedron, whenever exact separation cannot\nbe achieved. We introduce models inspired by support vector machines and we\ndesign two mathematical programming formulations with binary variables. We\nexploit Dantzig-Wolfe decomposition to obtain extended formulations, and we\ndevise column generation algorithms with ad-hoc pricing routines. We compare\ncomputing time and separation error values obtained by all our approaches on\nsynthetic datasets, with number of points from hundreds up to a few thousands,\nshowing our approaches to perform better than existing ones from the\nliterature. Furthermore, we observe that key computational differences arise,\ndepending on whether the budget K is sufficient to completely separate the\npositive points from the negative ones or not. On 8-dimensional instances (and\nover), existing convex hull algorithms become computational inapplicable, while\nour algorithms allow to identify good convex hull approximations in minutes of\ncomputation.\n","authors":["Michele Barbato","Alberto Ceselli","Rosario Messana"],"pdf_url":"https://arxiv.org/pdf/2407.17341v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.00304v2","updated":"2024-07-24T15:01:45Z","published":"2024-05-01T04:00:09Z","title":"QUACK: Quantum Aligned Centroid Kernel","summary":"  Quantum computing (QC) seems to show potential for application in machine\nlearning (ML). In particular quantum kernel methods (QKM) exhibit promising\nproperties for use in supervised ML tasks. However, a major disadvantage of\nkernel methods is their unfavorable quadratic scaling with the number of\ntraining samples. Together with the limits imposed by currently available\nquantum hardware (NISQ devices) with their low qubit coherence times, small\nnumber of qubits, and high error rates, the use of QC in ML at an industrially\nrelevant scale is currently impossible. As a small step in improving the\npotential applications of QKMs, we introduce QUACK, a quantum kernel algorithm\nwhose time complexity scales linear with the number of samples during training,\nand independent of the number of training samples in the inference stage. In\nthe training process, only the kernel entries for the samples and the centers\nof the classes are calculated, i.e. the maximum shape of the kernel for n\nsamples and c classes is (n, c). During training, the parameters of the quantum\nkernel and the positions of the centroids are optimized iteratively. In the\ninference stage, for every new sample the circuit is only evaluated for every\ncentroid, i.e. c times. We show that the QUACK algorithm nevertheless provides\nsatisfactory results and can perform at a similar level as classical kernel\nmethods with quadratic scaling during training. In addition, our (simulated)\nalgorithm is able to handle high-dimensional datasets such as MNIST with 784\nfeatures without any dimensionality reduction.\n","authors":["Kilian Tscharke","Sebastian Issel","Pascal Debus"],"pdf_url":"https://arxiv.org/pdf/2405.00304v2.pdf","comment":"Accepted to IEEE International Conference on Quantum Computing and\n  Engineering (QCE) 2024"},{"id":"http://arxiv.org/abs/2407.10969v3","updated":"2024-07-24T14:57:48Z","published":"2024-07-15T17:59:29Z","title":"Q-Sparse: All Large Language Models can be Fully Sparsely-Activated","summary":"  We introduce, Q-Sparse, a simple yet effective approach to training\nsparsely-activated large language models (LLMs). Q-Sparse enables full sparsity\nof activations in LLMs which can bring significant efficiency gains in\ninference. This is achieved by applying top-K sparsification to the activations\nand the straight-through-estimator to the training. We also introduce Block\nQ-Sparse for batch training and inference. The key results from this work are,\n(1) Q-Sparse can achieve results comparable to those of baseline LLMs while\nbeing much more efficient at inference time; (2) We present an\ninference-optimal scaling law for sparsely-activated LLMs; (3) Q-Sparse is\neffective in different settings, including training-from-scratch,\ncontinue-training of off-the-shelf LLMs, and finetuning; (4) Q-Sparse works for\nboth full-precision and 1-bit LLMs (e.g., BitNet b1.58). Particularly, the\nsynergy of BitNet b1.58 and Q-Sparse (can be equipped with MoE) provides the\ncornerstone and a clear path to revolutionize the efficiency, including cost\nand energy consumption, of future LLMs.\n","authors":["Hongyu Wang","Shuming Ma","Ruiping Wang","Furu Wei"],"pdf_url":"https://arxiv.org/pdf/2407.10969v3.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2407.17333v1","updated":"2024-07-24T14:55:37Z","published":"2024-07-24T14:55:37Z","title":"Global and Local Confidence Based Fraud Detection Graph Neural Network","summary":"  This paper presents the Global and Local Confidence Graph Neural Network\n(GLC-GNN), an innovative approach to graph-based anomaly detection that\naddresses the challenges of heterophily and camouflage in fraudulent\nactivities. By introducing a prototype to encapsulate the global features of a\ngraph and calculating a Global Confidence (GC) value for each node, GLC-GNN\neffectively distinguishes between benign and fraudulent nodes. The model\nleverages GC to generate attention values for message aggregation, enhancing\nits ability to capture both homophily and heterophily. Through extensive\nexperiments on four open datasets, GLC-GNN demonstrates superior performance\nover state-of-the-art models in accuracy and convergence speed, while\nmaintaining a compact model size and expedited training process. The\nintegration of global and local confidence measures in GLC-GNN offers a robust\nsolution for detecting anomalies in graphs, with significant implications for\nfraud detection across diverse domains.\n","authors":["Jiaxun Liu","Yue Tian","Guanjun Liu"],"pdf_url":"https://arxiv.org/pdf/2407.17333v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17329v1","updated":"2024-07-24T14:53:01Z","published":"2024-07-24T14:53:01Z","title":"Low dimensional representation of multi-patient flow cytometry datasets\n  using optimal transport for minimal residual disease detection in leukemia","summary":"  Representing and quantifying Minimal Residual Disease (MRD) in Acute Myeloid\nLeukemia (AML), a type of cancer that affects the blood and bone marrow, is\nessential in the prognosis and follow-up of AML patients. As traditional\ncytological analysis cannot detect leukemia cells below 5\\%, the analysis of\nflow cytometry dataset is expected to provide more reliable results. In this\npaper, we explore statistical learning methods based on optimal transport (OT)\nto achieve a relevant low-dimensional representation of multi-patient flow\ncytometry measurements (FCM) datasets considered as high-dimensional\nprobability distributions. Using the framework of OT, we justify the use of the\nK-means algorithm for dimensionality reduction of multiple large-scale point\nclouds through mean measure quantization by merging all the data into a single\npoint cloud. After this quantization step, the visualization of the intra and\ninter-patients FCM variability is carried out by embedding low-dimensional\nquantized probability measures into a linear space using either Wasserstein\nPrincipal Component Analysis (PCA) through linearized OT or log-ratio PCA of\ncompositional data. Using a publicly available FCM dataset and a FCM dataset\nfrom Bordeaux University Hospital, we demonstrate the benefits of our approach\nover the popular kernel mean embedding technique for statistical learning from\nmultiple high-dimensional probability distributions. We also highlight the\nusefulness of our methodology for low-dimensional projection and clustering\npatient measurements according to their level of MRD in AML from FCM. In\nparticular, our OT-based approach allows a relevant and informative\ntwo-dimensional representation of the results of the FlowSom algorithm, a\nstate-of-the-art method for the detection of MRD in AML using multi-patient\nFCM.\n","authors":["Erell Gachon","Jérémie Bigot","Elsa Cazelles","Aguirre Mimoun","Jean-Philippe Vial"],"pdf_url":"https://arxiv.org/pdf/2407.17329v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17303v1","updated":"2024-07-24T14:17:16Z","published":"2024-07-24T14:17:16Z","title":"MoveLight: Enhancing Traffic Signal Control through Movement-Centric\n  Deep Reinforcement Learning","summary":"  This paper introduces MoveLight, a novel traffic signal control system that\nenhances urban traffic management through movement-centric deep reinforcement\nlearning. By leveraging detailed real-time data and advanced machine learning\ntechniques, MoveLight overcomes the limitations of traditional traffic signal\ncontrol methods. It employs a lane-level control approach using the FRAP\nalgorithm to achieve dynamic and adaptive traffic signal control, optimizing\ntraffic flow, reducing congestion, and improving overall efficiency. Our\nresearch demonstrates the scalability and effectiveness of MoveLight across\nsingle intersections, arterial roads, and network levels. Experimental results\nusing real-world datasets from Cologne and Hangzhou show significant\nimprovements in metrics such as queue length, delay, and throughput compared to\nexisting methods. This study highlights the transformative potential of deep\nreinforcement learning in intelligent traffic signal control, setting a new\nstandard for sustainable and efficient urban transportation systems.\n","authors":["Junqi Shao","Chenhao Zheng","Yuxuan Chen","Yucheng Huang","Rui Zhang"],"pdf_url":"https://arxiv.org/pdf/2407.17303v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.01860v2","updated":"2024-07-24T14:10:13Z","published":"2023-10-03T07:49:17Z","title":"High-Probability Convergence for Composite and Distributed Stochastic\n  Minimization and Variational Inequalities with Heavy-Tailed Noise","summary":"  High-probability analysis of stochastic first-order optimization methods\nunder mild assumptions on the noise has been gaining a lot of attention in\nrecent years. Typically, gradient clipping is one of the key algorithmic\ningredients to derive good high-probability guarantees when the noise is\nheavy-tailed. However, if implemented na\\\"ively, clipping can spoil the\nconvergence of the popular methods for composite and distributed optimization\n(Prox-SGD/Parallel SGD) even in the absence of any noise. Due to this reason,\nmany works on high-probability analysis consider only unconstrained\nnon-distributed problems, and the existing results for composite/distributed\nproblems do not include some important special cases (like strongly convex\nproblems) and are not optimal. To address this issue, we propose new stochastic\nmethods for composite and distributed optimization based on the clipping of\nstochastic gradient differences and prove tight high-probability convergence\nresults (including nearly optimal ones) for the new methods. Using similar\nideas, we also develop new methods for composite and distributed variational\ninequalities and analyze the high-probability convergence of these methods.\n","authors":["Eduard Gorbunov","Abdurakhmon Sadiev","Marina Danilova","Samuel Horváth","Gauthier Gidel","Pavel Dvurechensky","Alexander Gasnikov","Peter Richtárik"],"pdf_url":"https://arxiv.org/pdf/2310.01860v2.pdf","comment":"ICML 2024; changes in version 2: minor corrections (typos were fixed\n  and the structure was modified)"},{"id":"http://arxiv.org/abs/2407.17296v1","updated":"2024-07-24T14:05:44Z","published":"2024-07-24T14:05:44Z","title":"Enhanced SMC$^2$: Leveraging Gradient Information from Differentiable\n  Particle Filters Within Langevin Proposals","summary":"  Sequential Monte Carlo Squared (SMC$^2$) is a Bayesian method which can infer\nthe states and parameters of non-linear, non-Gaussian state-space models. The\nstandard random-walk proposal in SMC$^2$ faces challenges, particularly with\nhigh-dimensional parameter spaces. This study outlines a novel approach by\nharnessing first-order gradients derived from a Common Random Numbers -\nParticle Filter (CRN-PF) using PyTorch. The resulting gradients can be\nleveraged within a Langevin proposal without accept/reject. Including Langevin\ndynamics within the proposal can result in a higher effective sample size and\nmore accurate parameter estimates when compared with the random-walk. The\nresulting algorithm is parallelized on distributed memory using Message Passing\nInterface (MPI) and runs in $\\mathcal{O}(\\log_2N)$ time complexity. Utilizing\n64 computational cores we obtain a 51x speed-up when compared to a single core.\nA GitHub link is given which provides access to the code.\n","authors":["Conor Rosato","Joshua Murphy","Alessandro Varsi","Paul Horridge","Simon Maskell"],"pdf_url":"https://arxiv.org/pdf/2407.17296v1.pdf","comment":"8 pages, 3 images. Accepted to 2024 IEEE International Conference on\n  Multisensor Fusion and Integration (MFI 2024). https://mfi2024.org/. arXiv\n  admin note: text overlap with arXiv:2311.12973"},{"id":"http://arxiv.org/abs/2407.17284v1","updated":"2024-07-24T13:50:21Z","published":"2024-07-24T13:50:21Z","title":"A Novel Two-Step Fine-Tuning Pipeline for Cold-Start Active Learning in\n  Text Classification Tasks","summary":"  This is the first work to investigate the effectiveness of BERT-based\ncontextual embeddings in active learning (AL) tasks on cold-start scenarios,\nwhere traditional fine-tuning is infeasible due to the absence of labeled data.\nOur primary contribution is the proposal of a more robust fine-tuning pipeline\n- DoTCAL - that diminishes the reliance on labeled data in AL using two steps:\n(1) fully leveraging unlabeled data through domain adaptation of the embeddings\nvia masked language modeling and (2) further adjusting model weights using\nlabeled data selected by AL. Our evaluation contrasts BERT-based embeddings\nwith other prevalent text representation paradigms, including Bag of Words\n(BoW), Latent Semantic Indexing (LSI), and FastText, at two critical stages of\nthe AL process: instance selection and classification. Experiments conducted on\neight ATC benchmarks with varying AL budgets (number of labeled instances) and\nnumber of instances (about 5,000 to 300,000) demonstrate DoTCAL's superior\neffectiveness, achieving up to a 33% improvement in Macro-F1 while reducing\nlabeling efforts by half compared to the traditional one-step method. We also\nfound that in several tasks, BoW and LSI (due to information aggregation)\nproduce results superior (up to 59% ) to BERT, especially in low-budget\nscenarios and hard-to-classify tasks, which is quite surprising.\n","authors":["Fabiano Belém","Washington Cunha","Celso França","Claudio Andrade","Leonardo Rocha","Marcos André Gonçalves"],"pdf_url":"https://arxiv.org/pdf/2407.17284v1.pdf","comment":"11 pages, 4 figures, 2 Tables, and 1 algorithm"},{"id":"http://arxiv.org/abs/2401.09769v3","updated":"2024-07-24T13:49:13Z","published":"2024-01-18T07:36:38Z","title":"Learning from Graphs with Heterophily: Progress and Future","summary":"  Graphs are structured data that models complex relations between real-world\nentities. Heterophilous graphs, where linked nodes are prone to be with\ndifferent labels or dissimilar features, have recently attracted significant\nattention and found many applications. Meanwhile, increasing efforts have been\nmade to advance learning from heterophilous graphs. Although there exist\nsurveys on the relevant topic, they focus on heterophilous GNNs, which are only\nsub-topics of heterophilous graph learning. In this survey, we comprehensively\noverview existing works on learning from graphs with heterophily.First, we\ncollect over 180 publications and introduce the development of this field.\nThen, we systematically categorize existing methods based on a hierarchical\ntaxonomy including learning strategies, model architectures and practical\napplications. Finally, we discuss the primary challenges of existing studies\nand highlight promising avenues for future research.More publication details\nand corresponding open-source codes can be accessed and will be continuously\nupdated at our\nrepositories:https://github.com/gongchenghua/Papers-Graphs-with-Heterophily.\n","authors":["Chenghua Gong","Yao Cheng","Xiang Li","Caihua Shan","Siqiang Luo"],"pdf_url":"https://arxiv.org/pdf/2401.09769v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17280v1","updated":"2024-07-24T13:46:50Z","published":"2024-07-24T13:46:50Z","title":"Enhanced Feature Learning via Regularisation: Integrating Neural\n  Networks and Kernel Methods","summary":"  We propose a new method for feature learning and function estimation in\nsupervised learning via regularised empirical risk minimisation. Our approach\nconsiders functions as expectations of Sobolev functions over all possible\none-dimensional projections of the data. This framework is similar to kernel\nridge regression, where the kernel is $\\mathbb{E}_w ( k^{(B)}(w^\\top x,w^\\top\nx^\\prime))$, with $k^{(B)}(a,b) := \\min(|a|, |b|)1_{ab>0}$ the Brownian kernel,\nand the distribution of the projections $w$ is learnt. This can also be viewed\nas an infinite-width one-hidden layer neural network, optimising the first\nlayer's weights through gradient descent and explicitly adjusting the\nnon-linearity and weights of the second layer. We introduce an efficient\ncomputation method for the estimator, called Brownian Kernel Neural Network\n(BKerNN), using particles to approximate the expectation. The optimisation is\nprincipled due to the positive homogeneity of the Brownian kernel. Using\nRademacher complexity, we show that BKerNN's expected risk converges to the\nminimal risk with explicit high-probability rates of $O( \\min((d/n)^{1/2},\nn^{-1/6}))$ (up to logarithmic factors). Numerical experiments confirm our\noptimisation intuitions, and BKerNN outperforms kernel ridge regression, and\nfavourably compares to a one-hidden layer neural network with ReLU activations\nin various settings and real data sets.\n","authors":["Bertille Follain","Francis Bach"],"pdf_url":"https://arxiv.org/pdf/2407.17280v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.14324v2","updated":"2024-07-24T13:34:14Z","published":"2023-11-24T07:53:48Z","title":"Large Language Models as Topological Structure Enhancers for\n  Text-Attributed Graphs","summary":"  The latest advancements in large language models (LLMs) have revolutionized\nthe field of natural language processing (NLP). Inspired by the success of LLMs\nin NLP tasks, some recent work has begun investigating the potential of\napplying LLMs in graph learning tasks. However, most of the existing work\nfocuses on utilizing LLMs as powerful node feature augmenters, leaving\nemploying LLMs to enhance graph topological structures an understudied problem.\nIn this work, we explore how to leverage the information retrieval and text\ngeneration capabilities of LLMs to refine/enhance the topological structure of\ntext-attributed graphs (TAGs) under the node classification setting. First, we\npropose using LLMs to help remove unreliable edges and add reliable ones in the\nTAG. Specifically, we first let the LLM output the semantic similarity between\nnode attributes through delicate prompt designs, and then perform edge deletion\nand edge addition based on the similarity. Second, we propose using\npseudo-labels generated by the LLM to improve graph topology, that is, we\nintroduce the pseudo-label propagation as a regularization to guide the graph\nneural network (GNN) in learning proper edge weights. Finally, we incorporate\nthe two aforementioned LLM-based methods for graph topological refinement into\nthe process of GNN training, and perform extensive experiments on four\nreal-world datasets. The experimental results demonstrate the effectiveness of\nLLM-based graph topology refinement (achieving a 0.15%--2.47% performance gain\non public benchmarks).\n","authors":["Shengyin Sun","Yuxiang Ren","Chen Ma","Xuecang Zhang"],"pdf_url":"https://arxiv.org/pdf/2311.14324v2.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2209.15609v3","updated":"2024-07-24T13:31:07Z","published":"2022-09-30T17:34:48Z","title":"$Φ$-DVAE: Physics-Informed Dynamical Variational Autoencoders for\n  Unstructured Data Assimilation","summary":"  Incorporating unstructured data into physical models is a challenging problem\nthat is emerging in data assimilation. Traditional approaches focus on\nwell-defined observation operators whose functional forms are typically assumed\nto be known. This prevents these methods from achieving a consistent model-data\nsynthesis in configurations where the mapping from data-space to model-space is\nunknown. To address these shortcomings, in this paper we develop a\nphysics-informed dynamical variational autoencoder ($\\Phi$-DVAE) to embed\ndiverse data streams into time-evolving physical systems described by\ndifferential equations. Our approach combines a standard, possibly nonlinear,\nfilter for the latent state-space model and a VAE, to assimilate the\nunstructured data into the latent dynamical system. Unstructured data, in our\nexample systems, comes in the form of video data and velocity field\nmeasurements, however the methodology is suitably generic to allow for\narbitrary unknown observation operators. A variational Bayesian framework is\nused for the joint estimation of the encoding, latent states, and unknown\nsystem parameters. To demonstrate the method, we provide case studies with the\nLorenz-63 ordinary differential equation, and the advection and Korteweg-de\nVries partial differential equations. Our results, with synthetic data, show\nthat $\\Phi$-DVAE provides a data efficient dynamics encoding methodology which\nis competitive with standard approaches. Unknown parameters are recovered with\nuncertainty quantification, and unseen data are accurately predicted.\n","authors":["Alex Glyn-Davies","Connor Duffin","Ö. Deniz Akyildiz","Mark Girolami"],"pdf_url":"https://arxiv.org/pdf/2209.15609v3.pdf","comment":"29 pages, 9 figures, updated version"},{"id":"http://arxiv.org/abs/2301.07609v5","updated":"2024-07-24T13:23:17Z","published":"2023-01-18T15:40:19Z","title":"Physics-informed Information Field Theory for Modeling Physical Systems\n  with Uncertainty Quantification","summary":"  Data-driven approaches coupled with physical knowledge are powerful\ntechniques to model systems. The goal of such models is to efficiently solve\nfor the underlying field by combining measurements with known physical laws. As\nmany systems contain unknown elements, such as missing parameters, noisy data,\nor incomplete physical laws, this is widely approached as an uncertainty\nquantification problem. The common techniques to handle all the variables\ntypically depend on the numerical scheme used to approximate the posterior, and\nit is desirable to have a method which is independent of any such\ndiscretization. Information field theory (IFT) provides the tools necessary to\nperform statistics over fields that are not necessarily Gaussian. We extend IFT\nto physics-informed IFT (PIFT) by encoding the functional priors with\ninformation about the physical laws which describe the field. The posteriors\nderived from this PIFT remain independent of any numerical scheme and can\ncapture multiple modes, allowing for the solution of problems which are\nill-posed. We demonstrate our approach through an analytical example involving\nthe Klein-Gordon equation. We then develop a variant of stochastic gradient\nLangevin dynamics to draw samples from the joint posterior over the field and\nmodel parameters. We apply our method to numerical examples with various\ndegrees of model-form error and to inverse problems involving nonlinear\ndifferential equations. As an addendum, the method is equipped with a metric\nwhich allows the posterior to automatically quantify model-form uncertainty.\nBecause of this, our numerical experiments show that the method remains robust\nto even an incorrect representation of the physics given sufficient data. We\nnumerically demonstrate that the method correctly identifies when the physics\ncannot be trusted, in which case it automatically treats learning the field as\na regression problem.\n","authors":["Alex Alberts","Ilias Bilionis"],"pdf_url":"https://arxiv.org/pdf/2301.07609v5.pdf","comment":"32 pages, 8 figures. Published in Journal of Computational Physics"},{"id":"http://arxiv.org/abs/2306.00833v2","updated":"2024-07-24T13:13:20Z","published":"2023-06-01T15:55:46Z","title":"When Does Bottom-up Beat Top-down in Hierarchical Community Detection?","summary":"  Hierarchical clustering of networks consists in finding a tree of\ncommunities, such that lower levels of the hierarchy reveal finer-grained\ncommunity structures. There are two main classes of algorithms tackling this\nproblem. Divisive ($\\textit{top-down}$) algorithms recursively partition the\nnodes into two communities, until a stopping rule indicates that no further\nsplit is needed. In contrast, agglomerative ($\\textit{bottom-up}$) algorithms\nfirst identify the smallest community structure and then repeatedly merge the\ncommunities using a $\\textit{linkage}$ method. In this article, we establish\ntheoretical guarantees for the recovery of the hierarchical tree and community\nstructure of a Hierarchical Stochastic Block Model by a bottom-up algorithm. We\nalso establish that this bottom-up algorithm attains the information-theoretic\nthreshold for exact recovery at intermediate levels of the hierarchy. Notably,\nthese recovery conditions are less restrictive compared to those existing for\ntop-down algorithms. This shows that bottom-up algorithms extend the feasible\nregion for achieving exact recovery at intermediate levels. Numerical\nexperiments on both synthetic and real data sets confirm the superiority of\nbottom-up algorithms over top-down algorithms. We also observe that top-down\nalgorithms can produce dendrograms with inversions. These findings contribute\nto a better understanding of hierarchical clustering techniques and their\napplications in network analysis.\n","authors":["Maximilien Dreveton","Daichi Kuroda","Matthias Grossglauser","Patrick Thiran"],"pdf_url":"https://arxiv.org/pdf/2306.00833v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17246v1","updated":"2024-07-24T13:05:17Z","published":"2024-07-24T13:05:17Z","title":"Channel-Aware Low-Rank Adaptation in Time Series Forecasting","summary":"  The balance between model capacity and generalization has been a key focus of\nrecent discussions in long-term time series forecasting. Two representative\nchannel strategies are closely associated with model expressivity and\nrobustness, including channel independence (CI) and channel dependence (CD).\nThe former adopts individual channel treatment and has been shown to be more\nrobust to distribution shifts, but lacks sufficient capacity to model\nmeaningful channel interactions. The latter is more expressive for representing\ncomplex cross-channel dependencies, but is prone to overfitting. To balance the\ntwo strategies, we present a channel-aware low-rank adaptation method to\ncondition CD models on identity-aware individual components. As a plug-in\nsolution, it is adaptable for a wide range of backbone architectures. Extensive\nexperiments show that it can consistently and significantly improve the\nperformance of both CI and CD models with demonstrated efficiency and\nflexibility. The code is available at https://github.com/tongnie/C-LoRA.\n","authors":["Tong Nie","Yuewen Mei","Guoyang Qin","Jian Sun","Wei Ma"],"pdf_url":"https://arxiv.org/pdf/2407.17246v1.pdf","comment":"Accepted by CIKM 2024, short research paper track"},{"id":"http://arxiv.org/abs/2401.17505v4","updated":"2024-07-24T12:57:56Z","published":"2024-01-30T23:46:35Z","title":"Arrows of Time for Large Language Models","summary":"  We study the probabilistic modeling performed by Autoregressive Large\nLanguage Models (LLMs) through the angle of time directionality, addressing a\nquestion first raised in (Shannon, 1951). For large enough models, we\nempirically find a time asymmetry in their ability to learn natural language: a\ndifference in the average log-perplexity when trying to predict the next token\nversus when trying to predict the previous one. This difference is at the same\ntime subtle and very consistent across various modalities (language, model\nsize, training time, ...). Theoretically, this is surprising: from an\ninformation-theoretic point of view, there should be no such difference. We\nprovide a theoretical framework to explain how such an asymmetry can appear\nfrom sparsity and computational complexity considerations, and outline a number\nof perspectives opened by our results.\n","authors":["Vassilis Papadopoulos","Jérémie Wenger","Clément Hongler"],"pdf_url":"https://arxiv.org/pdf/2401.17505v4.pdf","comment":"Corrected typos in Table 2. Added links. 12 figures, 20 pages"},{"id":"http://arxiv.org/abs/2405.15771v2","updated":"2024-07-24T12:56:41Z","published":"2024-03-13T17:47:39Z","title":"Adaptive Splitting of Reusable Temporal Monitors for Rare Traffic\n  Violations","summary":"  Autonomous Vehicles (AVs) are often tested in simulation to estimate the\nprobability they will violate safety specifications. Two common issues arise\nwhen using existing techniques to produce this estimation: If violations occur\nrarely, simple Monte-Carlo sampling techniques can fail to produce efficient\nestimates; if simulation horizons are too long, importance sampling techniques\n(which learn proposal distributions from past simulations) can fail to\nconverge. This paper addresses both issues by interleaving rare-event sampling\ntechniques with online specification monitoring algorithms. We use adaptive\nmulti-level splitting to decompose simulations into partial trajectories, then\ncalculate the distance of those partial trajectories to failure by leveraging\nrobustness metrics from Signal Temporal Logic (STL). By caching those partial\nrobustness metric values, we can efficiently re-use computations across\nmultiple sampling stages. Our experiments on an interstate lane-change scenario\nshow our method is viable for testing simulated AV-pipelines, efficiently\nestimating failure probabilities for STL specifications based on real traffic\nrules. We produce better estimates than Monte-Carlo and importance sampling in\nfewer simulations.\n","authors":["Craig Innes","Subramanian Ramamoorthy"],"pdf_url":"https://arxiv.org/pdf/2405.15771v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17238v1","updated":"2024-07-24T12:53:26Z","published":"2024-07-24T12:53:26Z","title":"Pretrained Visual Representations in Reinforcement Learning","summary":"  Visual reinforcement learning (RL) has made significant progress in recent\nyears, but the choice of visual feature extractor remains a crucial design\ndecision. This paper compares the performance of RL algorithms that train a\nconvolutional neural network (CNN) from scratch with those that utilize\npre-trained visual representations (PVRs). We evaluate the Dormant Ratio\nMinimization (DRM) algorithm, a state-of-the-art visual RL method, against\nthree PVRs: ResNet18, DINOv2, and Visual Cortex (VC). We use the Metaworld\nPush-v2 and Drawer-Open-v2 tasks for our comparison. Our results show that the\nchoice of training from scratch compared to using PVRs for maximising\nperformance is task-dependent, but PVRs offer advantages in terms of reduced\nreplay buffer size and faster training times. We also identify a strong\ncorrelation between the dormant ratio and model performance, highlighting the\nimportance of exploration in visual RL. Our study provides insights into the\ntrade-offs between training from scratch and using PVRs, informing the design\nof future visual RL algorithms.\n","authors":["Emlyn Williams","Athanasios Polydoros"],"pdf_url":"https://arxiv.org/pdf/2407.17238v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17236v1","updated":"2024-07-24T12:45:02Z","published":"2024-07-24T12:45:02Z","title":"Statistical Batch-Based Bearing Fault Detection","summary":"  In the domain of rotating machinery, bearings are vulnerable to different\nmechanical faults, including ball, inner, and outer race faults. Various\ntechniques can be used in condition-based monitoring, from classical signal\nanalysis to deep learning methods. Based on the complex working conditions of\nrotary machines, multivariate statistical process control charts such as\nHotelling's $T^2$ and Squared Prediction Error are useful for providing early\nwarnings. However, these methods are rarely applied to condition monitoring of\nrotating machinery due to the univariate nature of the datasets. In the present\npaper, we propose a multivariate statistical process control-based fault\ndetection method that utilizes multivariate data composed of Fourier transform\nfeatures extracted for fixed-time batches. Our approach makes use of the\nmultidimensional nature of Fourier transform characteristics, which record more\ndetailed information about the machine's status, in an effort to enhance early\ndefect detection and diagnosis. Experiments with varying vibration measurement\nlocations (Fan End, Drive End), fault types (ball, inner, and outer race\nfaults), and motor loads (0-3 horsepower) are used to validate the suggested\napproach. The outcomes illustrate our method's effectiveness in fault detection\nand point to possible broader uses in industrial maintenance.\n","authors":["Victoria Jorrya","Zina-Sabrina Duma","Tuomas Sihvonen","Satu-Pia Reinikainen","Lassi Roininen"],"pdf_url":"https://arxiv.org/pdf/2407.17236v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.02263v3","updated":"2024-07-24T12:36:41Z","published":"2024-07-02T13:40:29Z","title":"FreeCG: Free the Design Space of Clebsch-Gordan Transform for Machine\n  Learning Force Fields","summary":"  The Clebsch-Gordan Transform (CG transform) effectively encodes many-body\ninteractions. Many studies have proven its accuracy in depicting atomic\nenvironments, although this comes with high computational needs. The\ncomputational burden of this challenge is hard to reduce due to the need for\npermutation equivariance, which limits the design space of the CG transform\nlayer. We show that, implementing the CG transform layer on\npermutation-invariant inputs allows complete freedom in the design of this\nlayer without affecting symmetry. Developing further on this premise, our idea\nis to create a CG transform layer that operates on permutation-invariant\nabstract edges generated from real edge information. We bring in group CG\ntransform with sparse path, abstract edges shuffling, and attention enhancer to\nform a powerful and efficient CG transform layer. Our method, known as FreeCG,\nachieves State-of-The-Art (SoTA) results in force prediction for MD17, rMD17,\nMD22, and property prediction in QM9 datasets with notable enhancement. The\nextensibility to other models is also examined. Molecular dynamics simulations\nare carried out on MD17 and other periodic systems, including water and LiPS,\nshowcasing the capacity for real-world applications of FreeCG. It introduces a\nnovel paradigm for carrying out efficient and expressive CG transform in future\ngeometric neural network designs.\n","authors":["Shihao Shao","Haoran Geng","Zun Wang","Qinghua Cui"],"pdf_url":"https://arxiv.org/pdf/2407.02263v3.pdf","comment":"29 pages, 8 tables, 10 figures"},{"id":"http://arxiv.org/abs/2407.17228v1","updated":"2024-07-24T12:32:08Z","published":"2024-07-24T12:32:08Z","title":"A Hybrid Federated Kernel Regularized Least Squares Algorithm","summary":"  Federated learning is becoming an increasingly viable and accepted strategy\nfor building machine learning models in critical privacy-preserving scenarios\nsuch as clinical settings. Often, the data involved is not limited to clinical\ndata but also includes additional omics features (e.g. proteomics).\nConsequently, data is distributed not only across hospitals but also across\nomics centers, which are labs capable of generating such additional features\nfrom biosamples. This scenario leads to a hybrid setting where data is\nscattered both in terms of samples and features. In this hybrid setting, we\npresent an efficient reformulation of the Kernel Regularized Least Squares\nalgorithm, introduce two variants and validate them using well-established\ndatasets. Lastly, we discuss security measures to defend against possible\nattacks.\n","authors":["Celeste Damiani","Yulia Rodina","Sergio Decherchi"],"pdf_url":"https://arxiv.org/pdf/2407.17228v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17226v1","updated":"2024-07-24T12:26:21Z","published":"2024-07-24T12:26:21Z","title":"Sublinear Regret for An Actor-Critic Algorithm in Continuous-Time\n  Linear-Quadratic Reinforcement Learning","summary":"  We study reinforcement learning (RL) for a class of continuous-time\nlinear-quadratic (LQ) control problems for diffusions where volatility of the\nstate processes depends on both state and control variables. We apply a\nmodel-free approach that relies neither on knowledge of model parameters nor on\ntheir estimations, and devise an actor-critic algorithm to learn the optimal\npolicy parameter directly. Our main contributions include the introduction of a\nnovel exploration schedule and a regret analysis of the proposed algorithm. We\nprovide the convergence rate of the policy parameter to the optimal one, and\nprove that the algorithm achieves a regret bound of $O(N^{\\frac{3}{4}})$ up to\na logarithmic factor. We conduct a simulation study to validate the theoretical\nresults and demonstrate the effectiveness and reliability of the proposed\nalgorithm. We also perform numerical comparisons between our method and those\nof the recent model-based stochastic LQ RL studies adapted to the state- and\ncontrol-dependent volatility setting, demonstrating a better performance of the\nformer in terms of regret bounds.\n","authors":["Yilie Huang","Yanwei Jia","Xun Yu Zhou"],"pdf_url":"https://arxiv.org/pdf/2407.17226v1.pdf","comment":"42 pages, 4 figures"},{"id":"http://arxiv.org/abs/2310.08582v2","updated":"2024-07-24T12:25:17Z","published":"2023-10-12T17:59:50Z","title":"Tree-Planner: Efficient Close-loop Task Planning with Large Language\n  Models","summary":"  This paper studies close-loop task planning, which refers to the process of\ngenerating a sequence of skills (a plan) to accomplish a specific goal while\nadapting the plan based on real-time observations. Recently, prompting Large\nLanguage Models (LLMs) to generate actions iteratively has become a prevalent\nparadigm due to its superior performance and user-friendliness. However, this\nparadigm is plagued by two inefficiencies: high token consumption and redundant\nerror correction, both of which hinder its scalability for large-scale testing\nand applications. To address these issues, we propose Tree-Planner, which\nreframes task planning with LLMs into three distinct phases: plan sampling,\naction tree construction, and grounded deciding. Tree-Planner starts by using\nan LLM to sample a set of potential plans before execution, followed by the\naggregation of them to form an action tree. Finally, the LLM performs a\ntop-down decision-making process on the tree, taking into account real-time\nenvironmental information. Experiments show that Tree-Planner achieves\nstate-of-the-art performance while maintaining high efficiency. By decomposing\nLLM queries into a single plan-sampling call and multiple grounded-deciding\ncalls, a considerable part of the prompt are less likely to be repeatedly\nconsumed. As a result, token consumption is reduced by 92.2% compared to the\npreviously best-performing model. Additionally, by enabling backtracking on the\naction tree as needed, the correction process becomes more flexible, leading to\na 40.5% decrease in error corrections.\n","authors":["Mengkang Hu","Yao Mu","Xinmiao Yu","Mingyu Ding","Shiguang Wu","Wenqi Shao","Qiguang Chen","Bin Wang","Yu Qiao","Ping Luo"],"pdf_url":"https://arxiv.org/pdf/2310.08582v2.pdf","comment":"Published in ICLR 2024"},{"id":"http://arxiv.org/abs/2402.02333v2","updated":"2024-07-24T12:23:41Z","published":"2024-02-04T04:00:33Z","title":"Copyright Protection in Generative AI: A Technical Perspective","summary":"  Generative AI has witnessed rapid advancement in recent years, expanding\ntheir capabilities to create synthesized content such as text, images, audio,\nand code. The high fidelity and authenticity of contents generated by these\nDeep Generative Models (DGMs) have sparked significant copyright concerns.\nThere have been various legal debates on how to effectively safeguard\ncopyrights in DGMs. This work delves into this issue by providing a\ncomprehensive overview of copyright protection from a technical perspective. We\nexamine from two distinct viewpoints: the copyrights pertaining to the source\ndata held by the data owners and those of the generative models maintained by\nthe model builders. For data copyright, we delve into methods data owners can\nprotect their content and DGMs can be utilized without infringing upon these\nrights. For model copyright, our discussion extends to strategies for\npreventing model theft and identifying outputs generated by specific models.\nFinally, we highlight the limitations of existing techniques and identify areas\nthat remain unexplored. Furthermore, we discuss prospective directions for the\nfuture of copyright protection, underscoring its importance for the sustainable\nand ethical development of Generative AI.\n","authors":["Jie Ren","Han Xu","Pengfei He","Yingqian Cui","Shenglai Zeng","Jiankun Zhang","Hongzhi Wen","Jiayuan Ding","Pei Huang","Lingjuan Lyu","Hui Liu","Yi Chang","Jiliang Tang"],"pdf_url":"https://arxiv.org/pdf/2402.02333v2.pdf","comment":"26 pages"},{"id":"http://arxiv.org/abs/2407.17216v1","updated":"2024-07-24T12:15:59Z","published":"2024-07-24T12:15:59Z","title":"An Adaptive Second-order Method for a Class of Nonconvex Nonsmooth\n  Composite Optimization","summary":"  This paper explores a specific type of nonconvex sparsity-promoting\nregularization problems, namely those involving $\\ell_p$-norm regularization,\nin conjunction with a twice continuously differentiable loss function. We\npropose a novel second-order algorithm designed to effectively address this\nclass of challenging nonconvex and nonsmooth problems, showcasing several\ninnovative features: (i) The use of an alternating strategy to solve a\nreweighted $\\ell_1$ regularized subproblem and the subspace approximate Newton\nstep. (ii) The reweighted $\\ell_1$ regularized subproblem relies on a convex\napproximation to the nonconvex regularization term, enabling a closed-form\nsolution characterized by the soft-thresholding operator. This feature allows\nour method to be applied to various nonconvex regularization problems. (iii)\nOur algorithm ensures that the iterates maintain their sign values and that\nnonzero components are kept away from 0 for a sufficient number of iterations,\neventually transitioning to a perturbed Newton method. (iv) We provide\ntheoretical guarantees of global convergence, local superlinear convergence in\nthe presence of the Kurdyka-\\L ojasiewicz (KL) property, and local quadratic\nconvergence when employing the exact Newton step in our algorithm. We also\nshowcase the effectiveness of our approach through experiments on a diverse set\nof model prediction problems.\n","authors":["Hao Wang","Xiangyu Yang","Yichen Zhu"],"pdf_url":"https://arxiv.org/pdf/2407.17216v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17214v1","updated":"2024-07-24T12:14:19Z","published":"2024-07-24T12:14:19Z","title":"Application of Machine Learning and Convex Limiting to Subgrid Flux\n  Modeling in the Shallow-Water Equations","summary":"  We propose a combination of machine learning and flux limiting for\nproperty-preserving subgrid scale modeling in the context of flux-limited\nfinite volume methods for the one-dimensional shallow-water equations. The\nnumerical fluxes of a conservative target scheme are fitted to the coarse-mesh\naverages of a monotone fine-grid discretization using a neural network to\nparametrize the subgrid scale components. To ensure positivity preservation and\nthe validity of local maximum principles, we use a flux limiter that constrains\nthe intermediate states of an equivalent fluctuation form to stay in a convex\nadmissible set. The results of our numerical studies confirm that the proposed\ncombination of machine learning with monolithic convex limiting produces\nmeaningful closures even in scenarios for which the network was not trained.\n","authors":["Ilya Timofeyev","Alexey Schwarzmann","Dmitri Kuzmin"],"pdf_url":"https://arxiv.org/pdf/2407.17214v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17213v1","updated":"2024-07-24T12:11:09Z","published":"2024-07-24T12:11:09Z","title":"Spectrum-Informed Multistage Neural Networks: Multiscale Function\n  Approximators of Machine Precision","summary":"  Deep learning frameworks have become powerful tools for approaching\nscientific problems such as turbulent flow, which has wide-ranging\napplications. In practice, however, existing scientific machine learning\napproaches have difficulty fitting complex, multi-scale dynamical systems to\nvery high precision, as required in scientific contexts. We propose using the\nnovel multistage neural network approach with a spectrum-informed\ninitialization to learn the residue from the previous stage, utilizing the\nspectral biases associated with neural networks to capture high frequency\nfeatures in the residue, and successfully tackle the spectral bias of neural\nnetworks. This approach allows the neural network to fit target functions to\ndouble floating-point machine precision $O(10^{-16})$.\n","authors":["Jakin Ng","Yongji Wang","Ching-Yao Lai"],"pdf_url":"https://arxiv.org/pdf/2407.17213v1.pdf","comment":"8 pages, 3 figures, ICML 2024 workshop (AI for Science: Scaling in AI\n  for Scientific Discovery)"},{"id":"http://arxiv.org/abs/2407.17209v1","updated":"2024-07-24T12:09:07Z","published":"2024-07-24T12:09:07Z","title":"Nonverbal Immediacy Analysis in Education: A Multimodal Computational\n  Model","summary":"  This paper introduces a novel computational approach for analyzing nonverbal\nsocial behavior in educational settings. Integrating multimodal behavioral\ncues, including facial expressions, gesture intensity, and spatial dynamics,\nthe model assesses the nonverbal immediacy (NVI) of teachers from RGB classroom\nvideos. A dataset of 400 30-second video segments from German classrooms was\nconstructed for model training and validation. The gesture intensity regressor\nachieved a correlation of 0.84, the perceived distance regressor 0.55, and the\nNVI model 0.44 with median human ratings. The model demonstrates the potential\nto provide a valuable support in nonverbal behavior assessment, approximating\nthe accuracy of individual human raters. Validated against both questionnaire\ndata and trained observer ratings, our models show moderate to strong\ncorrelations with relevant educational outcomes, indicating their efficacy in\nreflecting effective teaching behaviors. This research advances the objective\nassessment of nonverbal communication behaviors, opening new pathways for\neducational research.\n","authors":["Uroš Petković","Jonas Frenkel","Olaf Hellwich","Rebecca Lazarides"],"pdf_url":"https://arxiv.org/pdf/2407.17209v1.pdf","comment":"12 pages, 3 figures. Camera-ready version for the SAB 2024: 17th\n  International Conference on the Simulation of Adaptive Behavior"},{"id":"http://arxiv.org/abs/2407.17206v1","updated":"2024-07-24T12:06:09Z","published":"2024-07-24T12:06:09Z","title":"Take a Step and Reconsider: Sequence Decoding for Self-Improved Neural\n  Combinatorial Optimization","summary":"  The constructive approach within Neural Combinatorial Optimization (NCO)\ntreats a combinatorial optimization problem as a finite Markov decision\nprocess, where solutions are built incrementally through a sequence of\ndecisions guided by a neural policy network. To train the policy, recent\nresearch is shifting toward a 'self-improved' learning methodology that\naddresses the limitations of reinforcement learning and supervised approaches.\nHere, the policy is iteratively trained in a supervised manner, with solutions\nderived from the current policy serving as pseudo-labels. The way these\nsolutions are obtained from the policy determines the quality of the\npseudo-labels. In this paper, we present a simple and problem-independent\nsequence decoding method for self-improved learning based on sampling sequences\nwithout replacement. We incrementally follow the best solution found and repeat\nthe sampling process from intermediate partial solutions. By modifying the\npolicy to ignore previously sampled sequences, we force it to consider only\nunseen alternatives, thereby increasing solution diversity. Experimental\nresults for the Traveling Salesman and Capacitated Vehicle Routing Problem\ndemonstrate its strong performance. Furthermore, our method outperforms\nprevious NCO approaches on the Job Shop Scheduling Problem.\n","authors":["Jonathan Pirnay","Dominik G. Grimm"],"pdf_url":"https://arxiv.org/pdf/2407.17206v1.pdf","comment":"Accepted at ECAI-2024"},{"id":"http://arxiv.org/abs/2407.17200v1","updated":"2024-07-24T12:00:30Z","published":"2024-07-24T12:00:30Z","title":"Generalization Bounds of Surrogate Policies for Combinatorial\n  Optimization Problems","summary":"  A recent stream of structured learning approaches has improved the practical\nstate of the art for a range of combinatorial optimization problems with\ncomplex objectives encountered in operations research. Such approaches train\npolicies that chain a statistical model with a surrogate combinatorial\noptimization oracle to map any instance of the problem to a feasible solution.\nThe key idea is to exploit the statistical distribution over instances instead\nof dealing with instances separately. However learning such policies by risk\nminimization is challenging because the empirical risk is piecewise constant in\nthe parameters, and few theoretical guarantees have been provided so far. In\nthis article, we investigate methods that smooth the risk by perturbing the\npolicy, which eases optimization and improves generalization. Our main\ncontribution is a generalization bound that controls the perturbation bias, the\nstatistical learning error, and the optimization error. Our analysis relies on\nthe introduction of a uniform weak property, which captures and quantifies the\ninterplay of the statistical model and the surrogate combinatorial optimization\noracle. This property holds under mild assumptions on the statistical model,\nthe surrogate optimization, and the instance data distribution. We illustrate\nthe result on a range of applications such as stochastic vehicle scheduling. In\nparticular, such policies are relevant for contextual stochastic optimization\nand our results cover this case.\n","authors":["Pierre-Cyril Aubin-Frankowski","Yohann De Castro","Axel Parmentier","Alessandro Rudi"],"pdf_url":"https://arxiv.org/pdf/2407.17200v1.pdf","comment":"10 pages main document, 3 pages supplement"},{"id":"http://arxiv.org/abs/2407.17195v1","updated":"2024-07-24T11:55:18Z","published":"2024-07-24T11:55:18Z","title":"Surrogate-guided optimization in quantum networks","summary":"  We propose an optimization algorithm to improve the design and performance of\nquantum communication networks. When physical architectures become too complex\nfor analytical methods, numerical simulation becomes essential to study quantum\nnetwork behavior. Although highly informative, these simulations involve\ncomplex numerical functions without known analytical forms, making traditional\noptimization techniques that assume continuity, differentiability, or convexity\ninapplicable. Additionally, quantum network simulations are computationally\ndemanding, rendering global approaches like Simulated Annealing or genetic\nalgorithms,\n  which require extensive function evaluations, impractical. We introduce a\nmore efficient optimization workflow using machine learning models, which serve\nas surrogates for a given objective function. We demonstrate the effectiveness\nof our approach by applying it to three well-known optimization problems in\nquantum networking: quantum memory allocation for multiple network nodes,\ntuning an experimental parameter in all physical links of a quantum\nentanglement switch, and finding efficient protocol settings within a large\nasymmetric quantum network. The solutions found by our algorithm consistently\noutperform those obtained with our baseline approaches -- Simulated Annealing\nand Bayesian optimization -- in the allotted time limit by up to 18\\% and 20\\%,\nrespectively. Our framework thus allows for more comprehensive quantum network\nstudies, integrating surrogate-assisted optimization with existing quantum\nnetwork simulators.\n","authors":["Luise Prielinger","Álvaro G. Iñesta","Gayane Vardoyan"],"pdf_url":"https://arxiv.org/pdf/2407.17195v1.pdf","comment":"20 pages (including supplementary notes), 12 figures"},{"id":"http://arxiv.org/abs/2406.02765v3","updated":"2024-07-24T11:35:26Z","published":"2024-06-04T20:33:29Z","title":"Discovering Dynamic Symbolic Policies with Genetic Programming","summary":"  Artificial intelligence techniques are increasingly being applied to solve\ncontrol problems, but often rely on black-box methods without transparent\noutput generation. To improve the interpretability and transparency in control\nsystems, models can be defined as white-box symbolic policies described by\nmathematical expressions. While current approaches to learn symbolic policies\nfocus on static policies that directly map observations to control signals,\nthese may fail in partially observable and volatile environments. We instead\nconsider dynamic symbolic policies with memory, optimised with genetic\nprogramming. The resulting policies are robust, and consist of easy to\ninterpret coupled differential equations. Our results show that dynamic\nsymbolic policies compare with black-box policies on a variety of control\ntasks. Furthermore, the benefit of the memory in dynamic policies is\ndemonstrated on experiments where static policies fall short. Overall, we\npresent a method for evolving high-performing symbolic policies that offer\ninterpretability and transparency, which lacks in black-box models.\n","authors":["Sigur de Vries","Sander Keemink","Marcel van Gerven"],"pdf_url":"https://arxiv.org/pdf/2406.02765v3.pdf","comment":"19 pages including references and appendix, 5 figures, 1 algorithm, 5\n  tables"},{"id":"http://arxiv.org/abs/2407.17182v1","updated":"2024-07-24T11:34:24Z","published":"2024-07-24T11:34:24Z","title":"Solving the Electrical Impedance Tomography Problem with a DeepONet Type\n  Neural Network: Theory and Application","summary":"  In this work, we consider the non-invasive medical imaging modality of\nElectrical Impedance Tomography, where the problem is to recover the\nconductivity in a medium from a set of data that arises out of a\ncurrent-to-voltage map (Neumann-to-Dirichlet operator) defined on the boundary\nof the medium. We formulate this inverse problem as an operator-learning\nproblem where the goal is to learn the implicitly defined operator-to-function\nmap between the space of Neumann-to-Dirichlet operators to the space of\nadmissible conductivities. Subsequently, we use an operator-learning\narchitecture, popularly called DeepONets, to learn this operator-to-function\nmap. Thus far, most of the operator learning architectures have been\nimplemented to learn operators between function spaces. In this work, we\ngeneralize the earlier works and use a DeepONet to actually {learn an\noperator-to-function} map. We provide a Universal Approximation Theorem type\nresult which guarantees that this implicitly defined operator-to-function map\nbetween the space of Neumann-to-Dirichlet operator to the space of conductivity\nfunction can be approximated to an arbitrary degree using such a DeepONet.\nFurthermore, we provide a computational implementation of our proposed approach\nand compare it against a standard baseline. We show that the proposed approach\nachieves good reconstructions and outperforms the baseline method in our\nexperiments.\n","authors":["Anuj Abhishek","Thilo Strauss"],"pdf_url":"https://arxiv.org/pdf/2407.17182v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17174v1","updated":"2024-07-24T11:24:25Z","published":"2024-07-24T11:24:25Z","title":"NarrationDep: Narratives on Social Media For Automatic Depression\n  Detection","summary":"  Social media posts provide valuable insight into the narrative of users and\ntheir intentions, including providing an opportunity to automatically model\nwhether a social media user is depressed or not. The challenge lies in\nfaithfully modelling user narratives from their online social media posts,\nwhich could potentially be useful in several different applications. We have\ndeveloped a novel and effective model called \\texttt{NarrationDep}, which\nfocuses on detecting narratives associated with depression. By analyzing a\nuser's tweets, \\texttt{NarrationDep} accurately identifies crucial narratives.\n\\texttt{NarrationDep} is a deep learning framework that jointly models\nindividual user tweet representations and clusters of users' tweets. As a\nresult, \\texttt{NarrationDep} is characterized by a novel two-layer deep\nlearning model: the first layer models using social media text posts, and the\nsecond layer learns semantic representations of tweets associated with a\ncluster. To faithfully model these cluster representations, the second layer\nincorporates a novel component that hierarchically learns from users' posts.\nThe results demonstrate that our framework outperforms other comparative models\nincluding recently developed models on a variety of datasets.\n","authors":["Hamad Zogan","Imran Razzak","Shoaib Jameel","Guandong Xu"],"pdf_url":"https://arxiv.org/pdf/2407.17174v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.15260v2","updated":"2024-07-24T11:21:47Z","published":"2022-03-29T06:15:54Z","title":"Efficient Convex Optimization Requires Superlinear Memory","summary":"  We show that any memory-constrained, first-order algorithm which minimizes\n$d$-dimensional, $1$-Lipschitz convex functions over the unit ball to\n$1/\\mathrm{poly}(d)$ accuracy using at most $d^{1.25 - \\delta}$ bits of memory\nmust make at least $\\tilde{\\Omega}(d^{1 + (4/3)\\delta})$ first-order queries\n(for any constant $\\delta \\in [0, 1/4]$). Consequently, the performance of such\nmemory-constrained algorithms are a polynomial factor worse than the optimal\n$\\tilde{O}(d)$ query bound for this problem obtained by cutting plane methods\nthat use $\\tilde{O}(d^2)$ memory. This resolves a COLT 2019 open problem of\nWoodworth and Srebro.\n","authors":["Annie Marsden","Vatsal Sharan","Aaron Sidford","Gregory Valiant"],"pdf_url":"https://arxiv.org/pdf/2203.15260v2.pdf","comment":"33 pages, 1 figure"},{"id":"http://arxiv.org/abs/2407.16417v2","updated":"2024-07-24T11:19:22Z","published":"2024-07-23T12:00:44Z","title":"On the Utility of Speech and Audio Foundation Models for Marmoset Call\n  Analysis","summary":"  Marmoset monkeys encode vital information in their calls and serve as a\nsurrogate model for neuro-biologists to understand the evolutionary origins of\nhuman vocal communication. Traditionally analyzed with signal processing-based\nfeatures, recent approaches have utilized self-supervised models pre-trained on\nhuman speech for feature extraction, capitalizing on their ability to learn a\nsignal's intrinsic structure independently of its acoustic domain. However, the\nutility of such foundation models remains unclear for marmoset call analysis in\nterms of multi-class classification, bandwidth, and pre-training domain. This\nstudy assesses feature representations derived from speech and general audio\ndomains, across pre-training bandwidths of 4, 8, and 16 kHz for marmoset\ncall-type and caller classification tasks. Results show that models with higher\nbandwidth improve performance, and pre-training on speech or general audio\nyields comparable results, improving over a spectral baseline.\n","authors":["Eklavya Sarkar","Mathew Magimai. -Doss"],"pdf_url":"https://arxiv.org/pdf/2407.16417v2.pdf","comment":"Accepted at Interspeech 2024 satellite event (VIHAR 2024)"},{"id":"http://arxiv.org/abs/2407.17164v1","updated":"2024-07-24T11:12:01Z","published":"2024-07-24T11:12:01Z","title":"Robust Deep Hawkes Process under Label Noise of Both Event and\n  Occurrence","summary":"  Integrating deep neural networks with the Hawkes process has significantly\nimproved predictive capabilities in finance, health informatics, and\ninformation technology. Nevertheless, these models often face challenges in\nreal-world settings, particularly due to substantial label noise. This issue is\nof significant concern in the medical field, where label noise can arise from\ndelayed updates in electronic medical records or misdiagnoses, leading to\nincreased prediction risks. Our research indicates that deep Hawkes process\nmodels exhibit reduced robustness when dealing with label noise, particularly\nwhen it affects both event types and timing. To address these challenges, we\nfirst investigate the influence of label noise in approximated intensity\nfunctions and present a novel framework, the Robust Deep Hawkes Process (RDHP),\nto overcome the impact of label noise on the intensity function of Hawkes\nmodels, considering both the events and their occurrences. We tested RDHP using\nmultiple open-source benchmarks with synthetic noise and conducted a case study\non obstructive sleep apnea-hypopnea syndrome (OSAHS) in a real-world setting\nwith inherent label noise. The results demonstrate that RDHP can effectively\nperform classification and regression tasks, even in the presence of noise\nrelated to events and their timing. To the best of our knowledge, this is the\nfirst study to successfully address both event and time label noise in deep\nHawkes process models, offering a promising solution for medical applications,\nspecifically in diagnosing OSAHS.\n","authors":["Xiaoyu Tan","Bin Li","Xihe Qiu","Jingjing Huang","Yinghui Xu","Wei Chu"],"pdf_url":"https://arxiv.org/pdf/2407.17164v1.pdf","comment":"ECAI2024"},{"id":"http://arxiv.org/abs/2407.17165v1","updated":"2024-07-24T11:12:01Z","published":"2024-07-24T11:12:01Z","title":"Explainable Artificial Intelligence Techniques for Irregular Temporal\n  Classification of Multidrug Resistance Acquisition in Intensive Care Unit\n  Patients","summary":"  Antimicrobial Resistance represents a significant challenge in the Intensive\nCare Unit (ICU), where patients are at heightened risk of Multidrug-Resistant\n(MDR) infections-pathogens resistant to multiple antimicrobial agents. This\nstudy introduces a novel methodology that integrates Gated Recurrent Units\n(GRUs) with advanced intrinsic and post-hoc interpretability techniques for\ndetecting the onset of MDR in patients across time. Within interpretability\nmethods, we propose Explainable Artificial Intelligence (XAI) approaches to\nhandle irregular Multivariate Time Series (MTS), introducing Irregular Time\nShapley Additive Explanations (IT-SHAP), a modification of Shapley Additive\nExplanations designed for irregular MTS with Recurrent Neural Networks focused\non temporal outputs. Our methodology aims to identify specific risk factors\nassociated with MDR in ICU patients. GRU with Hadamard's attention demonstrated\nhigh initial specificity and increasing sensitivity over time, correlating with\nincreased nosocomial infection risks during prolonged ICU stays. XAI analysis,\nenhanced by Hadamard attention and IT-SHAP, identified critical factors such as\nprevious non-resistant cultures, specific antibiotic usage patterns, and\nhospital environment dynamics. These insights suggest that early detection of\nat-risk patients can inform interventions such as preventive isolation and\ncustomized treatments, significantly improving clinical outcomes. The proposed\nGRU model for temporal classification achieved an average Receiver Operating\nCharacteristic Area Under the Curve of 78.27 +- 1.26 over time, indicating\nstrong predictive performance. In summary, this study highlights the clinical\nutility of our methodology, which combines predictive accuracy with\ninterpretability, thereby facilitating more effective healthcare interventions\nby professionals.\n","authors":["Óscar Escudero-Arnanz","Cristina Soguero-Ruiz","Joaquín Álvarez-Rodríguez","Antonio G. Marques"],"pdf_url":"https://arxiv.org/pdf/2407.17165v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17163v1","updated":"2024-07-24T11:07:20Z","published":"2024-07-24T11:07:20Z","title":"dlordinal: a Python package for deep ordinal classification","summary":"  dlordinal is a new Python library that unifies many recent deep ordinal\nclassification methodologies available in the literature. Developed using\nPyTorch as underlying framework, it implements the top performing\nstate-of-the-art deep learning techniques for ordinal classification problems.\nOrdinal approaches are designed to leverage the ordering information present in\nthe target variable. Specifically, it includes loss functions, various output\nlayers, dropout techniques, soft labelling methodologies, and other\nclassification strategies, all of which are appropriately designed to\nincorporate the ordinal information. Furthermore, as the performance metrics to\nassess novel proposals in ordinal classification depend on the distance between\ntarget and predicted classes in the ordinal scale, suitable ordinal evaluation\nmetrics are also included. dlordinal is distributed under the BSD-3-Clause\nlicense and is available at https://github.com/ayrna/dlordinal.\n","authors":["Francisco Bérchez-Moreno","Víctor M. Vargas","Rafael Ayllón-Gavilán","David Guijo-Rubio","César Hervás-Martínez","Juan C. Fernández","Pedro A. Gutiérrez"],"pdf_url":"https://arxiv.org/pdf/2407.17163v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17161v1","updated":"2024-07-24T11:05:05Z","published":"2024-07-24T11:05:05Z","title":"Quantum Supervised Learning","summary":"  Recent advancements in quantum computing have positioned it as a prospective\nsolution for tackling intricate computational challenges, with supervised\nlearning emerging as a promising domain for its application. Despite this\npotential, the field of quantum machine learning is still in its early stages,\nand there persists a level of skepticism regarding a possible near-term quantum\nadvantage. This paper aims to provide a classical perspective on current\nquantum algorithms for supervised learning, effectively bridging traditional\nmachine learning principles with advancements in quantum machine learning.\nSpecifically, this study charts a research trajectory that diverges from the\npredominant focus of quantum machine learning literature, originating from the\nprerequisites of classical methodologies and elucidating the potential impact\nof quantum approaches. Through this exploration, our objective is to deepen the\nunderstanding of the convergence between classical and quantum methods, thereby\nlaying the groundwork for future advancements in both domains and fostering the\ninvolvement of classical practitioners in the field of quantum machine\nlearning.\n","authors":["Antonio Macaluso"],"pdf_url":"https://arxiv.org/pdf/2407.17161v1.pdf","comment":"16 pages, 3 figures, 1 table"},{"id":"http://arxiv.org/abs/2407.16680v2","updated":"2024-07-24T10:58:48Z","published":"2024-07-23T17:45:16Z","title":"A Simulation Benchmark for Autonomous Racing with Large-Scale Human Data","summary":"  Despite the availability of international prize-money competitions, scaled\nvehicles, and simulation environments, research on autonomous racing and the\ncontrol of sports cars operating close to the limit of handling has been\nlimited by the high costs of vehicle acquisition and management, as well as the\nlimited physics accuracy of open-source simulators. In this paper, we propose a\nracing simulation platform based on the simulator Assetto Corsa to test,\nvalidate, and benchmark autonomous driving algorithms, including reinforcement\nlearning (RL) and classical Model Predictive Control (MPC), in realistic and\nchallenging scenarios. Our contributions include the development of this\nsimulation platform, several state-of-the-art algorithms tailored to the racing\nenvironment, and a comprehensive dataset collected from human drivers.\nAdditionally, we evaluate algorithms in the offline RL setting. All the\nnecessary code (including environment and benchmarks), working examples,\ndatasets, and videos are publicly released and can be found at:\nhttps://assetto-corsa-gym.github.io\n","authors":["Adrian Remonda","Nicklas Hansen","Ayoub Raji","Nicola Musiu","Marko Bertogna","Eduardo Veas","Xiaolong Wang"],"pdf_url":"https://arxiv.org/pdf/2407.16680v2.pdf","comment":"Project page and code can be found at:\n  \\url{https://assetto-corsa-gym.github.io/}"},{"id":"http://arxiv.org/abs/2407.17156v1","updated":"2024-07-24T10:54:23Z","published":"2024-07-24T10:54:23Z","title":"Path Following and Stabilisation of a Bicycle Model using a\n  Reinforcement Learning Approach","summary":"  Over the years, complex control approaches have been developed to control the\nmotion of a bicycle. Reinforcement Learning (RL), a branch of machine learning,\npromises easy deployment of so-called agents. Deployed agents are increasingly\nconsidered as an alternative to controllers for mechanical systems. The present\nwork introduces an RL approach to do path following with a virtual bicycle\nmodel while simultaneously stabilising it laterally. The bicycle, modelled as\nthe Whipple benchmark model and using multibody system dynamics, has no\nstabilisation aids. The agent succeeds in both path following and stabilisation\nof the bicycle model exclusively by outputting steering angles, which are\nconverted into steering torques via a PD controller. Curriculum learning is\napplied as a state-of-the-art training strategy. Different settings for the\nimplemented RL framework are investigated and compared to each other. The\nperformance of the deployed agents is evaluated using different types of paths\nand measurements. The ability of the deployed agents to do path following and\nstabilisation of the bicycle model travelling between 2m/s and 7m/s along\ncomplex paths including full circles, slalom manoeuvres, and lane changes is\ndemonstrated. Explanatory methods for machine learning are used to analyse the\nfunctionality of a deployed agent and link the introduced RL approach with\nresearch in the field of bicycle dynamics.\n","authors":["Sebastian Weyrer","Peter Manzl","A. L. Schwab","Johannes Gerstmayr"],"pdf_url":"https://arxiv.org/pdf/2407.17156v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2205.09615v5","updated":"2024-07-24T10:49:23Z","published":"2022-05-19T15:13:00Z","title":"EXACT: How to Train Your Accuracy","summary":"  Classification tasks are usually evaluated in terms of accuracy. However,\naccuracy is discontinuous and cannot be directly optimized using gradient\nascent. Popular methods minimize cross-entropy, hinge loss, or other surrogate\nlosses, which can lead to suboptimal results. In this paper, we propose a new\noptimization framework by introducing stochasticity to a model's output and\noptimizing expected accuracy, i.e. accuracy of the stochastic model. Extensive\nexperiments on linear models and deep image classification show that the\nproposed optimization method is a powerful alternative to widely used\nclassification losses.\n","authors":["Ivan Karpukhin","Stanislav Dereka","Sergey Kolesnikov"],"pdf_url":"https://arxiv.org/pdf/2205.09615v5.pdf","comment":"Pattern Recognition Letters (2024)"},{"id":"http://arxiv.org/abs/2309.08546v3","updated":"2024-07-24T10:16:59Z","published":"2023-09-15T17:10:51Z","title":"Towards Robust Continual Learning with Bayesian Adaptive Moment\n  Regularization","summary":"  The pursuit of long-term autonomy mandates that machine learning models must\ncontinuously adapt to their changing environments and learn to solve new tasks.\nContinual learning seeks to overcome the challenge of catastrophic forgetting,\nwhere learning to solve new tasks causes a model to forget previously learnt\ninformation. Prior-based continual learning methods are appealing as they are\ncomputationally efficient and do not require auxiliary models or data storage.\nHowever, prior-based approaches typically fail on important benchmarks and are\nthus limited in their potential applications compared to their memory-based\ncounterparts. We introduce Bayesian adaptive moment regularization (BAdam), a\nnovel prior-based method that better constrains parameter growth, reducing\ncatastrophic forgetting. Our method boasts a range of desirable properties such\nas being lightweight and task label-free, converging quickly, and offering\ncalibrated uncertainty that is important for safe real-world deployment.\nResults show that BAdam achieves state-of-the-art performance for prior-based\nmethods on challenging single-headed class-incremental experiments such as\nSplit MNIST and Split FashionMNIST, and does so without relying on task labels\nor discrete task boundaries.\n","authors":["Jack Foster","Alexandra Brintrup"],"pdf_url":"https://arxiv.org/pdf/2309.08546v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2008.07361v2","updated":"2024-07-24T09:56:06Z","published":"2020-08-14T11:00:13Z","title":"Logistic regression models for patient-level prediction based on massive\n  observational data: Do we need all data?","summary":"  Objective: Provide guidance on sample size considerations for developing\npredictive models by empirically establishing the adequate sample size, which\nbalances the competing objectives of improving model performance and reducing\nmodel complexity as well as computational requirements.\n  Materials and Methods: We empirically assess the effect of sample size on\nprediction performance and model complexity by generating learning curves for\n81 prediction problems (23 outcomes predicted in a depression cohort, 58\noutcomes predicted in a hypertension cohort) in three large observational\nhealth databases, requiring training of 17,248 prediction models. The adequate\nsample size was defined as the sample size for which the performance of a model\nequalled the maximum model performance minus a small threshold value.\n  Results: The adequate sample size achieves a median reduction of the number\nof observations of 9.5%, 37.3%, 58.5%, and 78.5% for the thresholds of 0.001,\n0.005, 0.01, and 0.02, respectively. The median reduction of the number of\npredictors in the models was 8.6%, 32.2%, 48.2%, and 68.3% for the thresholds\nof 0.001, 0.005, 0.01, and 0.02, respectively.\n  Discussion: Based on our results a conservative, yet significant, reduction\nin sample size and model complexity can be estimated for future prediction\nwork. Though, if a researcher is willing to generate a learning curve a much\nlarger reduction of the model complexity may be possible as suggested by a\nlarge outcome-dependent variability.\n  Conclusion: Our results suggest that in most cases only a fraction of the\navailable data was sufficient to produce a model close to the performance of\none developed on the full data set, but with a substantially reduced model\ncomplexity.\n","authors":["Luis H. John","Jan A. Kors","Jenna M. Reps","Patrick B. Ryan","Peter R. Rijnbeek"],"pdf_url":"https://arxiv.org/pdf/2008.07361v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17125v1","updated":"2024-07-24T09:48:48Z","published":"2024-07-24T09:48:48Z","title":"Behavioral Testing: Can Large Language Models Implicitly Resolve\n  Ambiguous Entities?","summary":"  One of the major aspects contributing to the striking performance of large\nlanguage models (LLMs) is the vast amount of factual knowledge accumulated\nduring pre-training. Yet, many LLMs suffer from self-inconsistency, which\nraises doubts about their trustworthiness and reliability. In this paper, we\nfocus on entity type ambiguity and analyze current state-of-the-art LLMs for\ntheir proficiency and consistency in applying their factual knowledge when\nprompted for entities under ambiguity. To do so, we propose an evaluation\nprotocol that disentangles knowing from applying knowledge, and test\nstate-of-the-art LLMs on 49 entities. Our experiments reveal that LLMs perform\npoorly with ambiguous prompts, achieving only 80% accuracy. Our results further\ndemonstrate systematic discrepancies in LLM behavior and their failure to\nconsistently apply information, indicating that the models can exhibit\nknowledge without being able to utilize it, significant biases for preferred\nreadings, as well as self inconsistencies. Our study highlights the importance\nof handling entity ambiguity in future for more trustworthy LLMs\n","authors":["Anastasiia Sedova","Robert Litschko","Diego Frassinelli","Benjamin Roth","Barbara Plank"],"pdf_url":"https://arxiv.org/pdf/2407.17125v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.06580v2","updated":"2024-07-24T09:38:49Z","published":"2024-02-09T17:55:01Z","title":"SAE: Single Architecture Ensemble Neural Networks","summary":"  Ensembles of separate neural networks (NNs) have shown superior accuracy and\nconfidence calibration over single NN across tasks. To improve the hardware\nefficiency of ensembles of separate NNs, recent methods create ensembles within\na single network via adding early exits or considering multi input multi output\napproaches. However, it is unclear which of these methods is the most effective\nfor a given task, needing a manual and separate search through each method. Our\nnovel Single Architecture Ensemble (SAE) framework enables an automatic and\njoint search through the early exit and multi input multi output configurations\nand their previously unobserved in-between combinations. SAE consists of two\nparts: a scalable search space that generalises the previous methods and their\nin-between configurations, and an optimisation objective that allows learning\nthe optimal configuration for a given task. Our image classification and\nregression experiments show that with SAE we can automatically find diverse\nconfigurations that fit the task, achieving competitive accuracy or confidence\ncalibration to baselines while reducing the compute operations or parameter\ncount by up to $1.5{\\sim}3.7\\times$.\n","authors":["Martin Ferianc","Hongxiang Fan","Miguel Rodrigues"],"pdf_url":"https://arxiv.org/pdf/2402.06580v2.pdf","comment":"Accepted at BMVC'24"},{"id":"http://arxiv.org/abs/2407.17120v1","updated":"2024-07-24T09:30:04Z","published":"2024-07-24T09:30:04Z","title":"Parameter-Efficient Fine-Tuning for Continual Learning: A Neural Tangent\n  Kernel Perspective","summary":"  Parameter-efficient fine-tuning for continual learning (PEFT-CL) has shown\npromise in adapting pre-trained models to sequential tasks while mitigating\ncatastrophic forgetting problem. However, understanding the mechanisms that\ndictate continual performance in this paradigm remains elusive. To tackle this\ncomplexity, we undertake a rigorous analysis of PEFT-CL dynamics to derive\nrelevant metrics for continual scenarios using Neural Tangent Kernel (NTK)\ntheory. With the aid of NTK as a mathematical analysis tool, we recast the\nchallenge of test-time forgetting into the quantifiable generalization gaps\nduring training, identifying three key factors that influence these gaps and\nthe performance of PEFT-CL: training sample size, task-level feature\northogonality, and regularization. To address these challenges, we introduce\nNTK-CL, a novel framework that eliminates task-specific parameter storage while\nadaptively generating task-relevant features. Aligning with theoretical\nguidance, NTK-CL triples the feature representation of each sample,\ntheoretically and empirically reducing the magnitude of both task-interplay and\ntask-specific generalization gaps. Grounded in NTK analysis, our approach\nimposes an adaptive exponential moving average mechanism and constraints on\ntask-level feature orthogonality, maintaining intra-task NTK forms while\nattenuating inter-task NTK forms. Ultimately, by fine-tuning optimizable\nparameters with appropriate regularization, NTK-CL achieves state-of-the-art\nperformance on established PEFT-CL benchmarks. This work provides a theoretical\nfoundation for understanding and improving PEFT-CL models, offering insights\ninto the interplay between feature representation, task orthogonality, and\ngeneralization, contributing to the development of more efficient continual\nlearning systems.\n","authors":["Jingren Liu","Zhong Ji","YunLong Yu","Jiale Cao","Yanwei Pang","Jungong Han","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2407.17120v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.17147v3","updated":"2024-07-24T09:28:11Z","published":"2024-04-26T04:34:45Z","title":"On the Federated Learning Framework for Cooperative Perception","summary":"  Cooperative perception is essential to enhance the efficiency and safety of\nfuture transportation systems, requiring extensive data sharing among vehicles\non the road, which raises significant privacy concerns. Federated learning\noffers a promising solution by enabling data privacy-preserving collaborative\nenhancements in perception, decision-making, and planning among connected and\nautonomous vehicles (CAVs). However, federated learning is impeded by\nsignificant challenges arising from data heterogeneity across diverse clients,\npotentially diminishing model accuracy and prolonging convergence periods. This\nstudy introduces a specialized federated learning framework for CP, termed the\nfederated dynamic weighted aggregation (FedDWA) algorithm, facilitated by\ndynamic adjusting loss (DALoss) function. This framework employs dynamic client\nweighting to direct model convergence and integrates a novel loss function that\nutilizes Kullback-Leibler divergence (KLD) to counteract the detrimental\neffects of non-independently and identically distributed (Non-IID) and\nunbalanced data. Utilizing the BEV transformer as the primary model, our\nrigorous testing on the OpenV2V dataset, augmented with FedBEVT data,\ndemonstrates significant improvements in the average intersection over union\n(IoU). These results highlight the substantial potential of our federated\nlearning framework to address data heterogeneity challenges in CP, thereby\nenhancing the accuracy of environmental perception models and facilitating more\nrobust and efficient collaborative learning solutions in the transportation\nsector.\n","authors":["Zhenrong Zhang","Jianan Liu","Xi Zhou","Tao Huang","Qing-Long Han","Jingxin Liu","Hongbin Liu"],"pdf_url":"https://arxiv.org/pdf/2404.17147v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17117v1","updated":"2024-07-24T09:25:54Z","published":"2024-07-24T09:25:54Z","title":"EverAdapt: Continuous Adaptation for Dynamic Machine Fault Diagnosis\n  Environments","summary":"  Unsupervised Domain Adaptation (UDA) has emerged as a key solution in\ndata-driven fault diagnosis, addressing domain shift where models underperform\nin changing environments. However, under the realm of continually changing\nenvironments, UDA tends to underperform on previously seen domains when\nadapting to new ones - a problem known as catastrophic forgetting. To address\nthis limitation, we introduce the EverAdapt framework, specifically designed\nfor continuous model adaptation in dynamic environments. Central to EverAdapt\nis a novel Continual Batch Normalization (CBN), which leverages source domain\nstatistics as a reference point to standardize feature representations across\ndomains. EverAdapt not only retains statistical information from previous\ndomains but also adapts effectively to new scenarios. Complementing CBN, we\ndesign a class-conditional domain alignment module for effective integration of\ntarget domains, and a Sample-efficient Replay strategy to reinforce memory\nretention. Experiments on real-world datasets demonstrate EverAdapt superiority\nin maintaining robust fault diagnosis in dynamic environments. Our code is\navailable: https://github.com/mohamedr002/EverAdapt\n","authors":[" Edward","Mohamed Ragab","Yuecong Xu","Min Wu","Yuecong Xu","Zhenghua Chen","Abdulla Alseiari","Xiaoli Li"],"pdf_url":"https://arxiv.org/pdf/2407.17117v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17112v1","updated":"2024-07-24T09:23:22Z","published":"2024-07-24T09:23:22Z","title":"Neural Dueling Bandits","summary":"  Contextual dueling bandit is used to model the bandit problems, where a\nlearner's goal is to find the best arm for a given context using observed noisy\npreference feedback over the selected arms for the past contexts. However,\nexisting algorithms assume the reward function is linear, which can be complex\nand non-linear in many real-life applications like online recommendations or\nranking web search results. To overcome this challenge, we use a neural network\nto estimate the reward function using preference feedback for the previously\nselected arms. We propose upper confidence bound- and Thompson sampling-based\nalgorithms with sub-linear regret guarantees that efficiently select arms in\neach round. We then extend our theoretical results to contextual bandit\nproblems with binary feedback, which is in itself a non-trivial contribution.\nExperimental results on the problem instances derived from synthetic datasets\ncorroborate our theoretical results.\n","authors":["Arun Verma","Zhongxiang Dai","Xiaoqiang Lin","Patrick Jaillet","Bryan Kian Hsiang Low"],"pdf_url":"https://arxiv.org/pdf/2407.17112v1.pdf","comment":"Accepted at ICML 2024 Workshop on Foundations of Reinforcement\n  Learning and Control"},{"id":"http://arxiv.org/abs/2201.08712v3","updated":"2024-07-24T09:17:56Z","published":"2022-01-21T14:16:56Z","title":"Improved Random Features for Dot Product Kernels","summary":"  Dot product kernels, such as polynomial and exponential (softmax) kernels,\nare among the most widely used kernels in machine learning, as they enable\nmodeling the interactions between input features, which is crucial in\napplications like computer vision, natural language processing, and recommender\nsystems. We make several novel contributions for improving the efficiency of\nrandom feature approximations for dot product kernels, to make these kernels\nmore useful in large scale learning. First, we present a generalization of\nexisting random feature approximations for polynomial kernels, such as\nRademacher and Gaussian sketches and TensorSRHT, using complex-valued random\nfeatures. We show empirically that the use of complex features can\nsignificantly reduce the variances of these approximations. Second, we provide\na theoretical analysis for understanding the factors affecting the efficiency\nof various random feature approximations, by deriving closed-form expressions\nfor their variances. These variance formulas elucidate conditions under which\ncertain approximations (e.g., TensorSRHT) achieve lower variances than others\n(e.g., Rademacher sketches), and conditions under which the use of complex\nfeatures leads to lower variances than real features. Third, by using these\nvariance formulas, which can be evaluated in practice, we develop a data-driven\noptimization approach to improve random feature approximations for general dot\nproduct kernels, which is also applicable to the Gaussian kernel. We describe\nthe improvements brought by these contributions with extensive experiments on a\nvariety of tasks and datasets.\n","authors":["Jonas Wacker","Motonobu Kanagawa","Maurizio Filippone"],"pdf_url":"https://arxiv.org/pdf/2201.08712v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17097v1","updated":"2024-07-24T08:49:18Z","published":"2024-07-24T08:49:18Z","title":"Towards Robust Knowledge Tracing Models via k-Sparse Attention","summary":"  Knowledge tracing (KT) is the problem of predicting students' future\nperformance based on their historical interaction sequences. With the advanced\ncapability of capturing contextual long-term dependency, attention mechanism\nbecomes one of the essential components in many deep learning based KT (DLKT)\nmodels. In spite of the impressive performance achieved by these attentional\nDLKT models, many of them are often vulnerable to run the risk of overfitting,\nespecially on small-scale educational datasets. Therefore, in this paper, we\npropose \\textsc{sparseKT}, a simple yet effective framework to improve the\nrobustness and generalization of the attention based DLKT approaches.\nSpecifically, we incorporate a k-selection module to only pick items with the\nhighest attention scores. We propose two sparsification heuristics : (1)\nsoft-thresholding sparse attention and (2) top-$K$ sparse attention. We show\nthat our \\textsc{sparseKT} is able to help attentional KT models get rid of\nirrelevant student interactions and have comparable predictive performance when\ncompared to 11 state-of-the-art KT models on three publicly available\nreal-world educational datasets. To encourage reproducible research, we make\nour data and code publicly available at\n\\url{https://github.com/pykt-team/pykt-toolkit}\\footnote{We merged our model to\nthe \\textsc{pyKT} benchmark at \\url{https://pykt.org/}.}.\n","authors":["Shuyan Huang","Zitao Liu","Xiangyu Zhao","Weiqi Luo","Jian Weng"],"pdf_url":"https://arxiv.org/pdf/2407.17097v1.pdf","comment":"Accepted at SIGIR'2023 (revised version with additional results)"},{"id":"http://arxiv.org/abs/2402.03365v2","updated":"2024-07-24T08:48:38Z","published":"2024-01-31T11:03:58Z","title":"Heterophily-Aware Fair Recommendation using Graph Convolutional Networks","summary":"  In recent years, graph neural networks (GNNs) have become a popular tool to\nimprove the accuracy and performance of recommender systems. Modern recommender\nsystems are not only designed to serve end users, but also to benefit other\nparticipants, such as items and items providers. These participants may have\ndifferent or conflicting goals and interests, which raise the need for fairness\nand popularity bias considerations. GNN-based recommendation methods also face\nthe challenges of unfairness and popularity bias and their normalization and\naggregation processes suffer from these challenges. In this paper, we propose a\nfair GNN-based recommender system, called HetroFair, to improve items' side\nfairness. HetroFair uses two separate components to generate fairness-aware\nembeddings: i) fairnessaware attention which incorporates dot product in the\nnormalization process of GNNs, to decrease the effect of nodes' degrees, and\nii) heterophily feature weighting to assign distinct weights to different\nfeatures during the aggregation process. In order to evaluate the effectiveness\nof HetroFair, we conduct extensive experiments over six real-world datasets.\nOur experimental results reveal that HetroFair not only alleviates the\nunfairness and popularity bias on items' side, but also achieves superior\naccuracy on users' side. Our implementation is publicly available at\nhttps://github.com/NematGH/HetroFair.\n","authors":["Nemat Gholinejad","Mostafa Haghir Chehreghani"],"pdf_url":"https://arxiv.org/pdf/2402.03365v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17087v1","updated":"2024-07-24T08:34:08Z","published":"2024-07-24T08:34:08Z","title":"Assessing Non-Nested Configurations of Multifidelity Machine Learning\n  for Quantum-Chemical Properties","summary":"  Multifidelity machine learning (MFML) for quantum chemical (QC) properties\nhas seen strong development in the recent years. The method has been shown to\nreduce the cost of generating training data for high-accuracy low-cost ML\nmodels. In such a set-up, the ML models are trained on molecular geometries and\nsome property of interest computed at various computational chemistry\naccuracies, or fidelities. These are then combined in training the MFML models.\nIn some multifidelity models, the training data is required to be nested, that\nis the same molecular geometries are included to calculate the property across\nall the fidelities. In these multifidelity models, the requirement of a nested\nconfiguration restricts the kind of sampling that can be performed while\nselection training samples at different fidelities.\n  This work assesses the use of non-nested training data for two of these\nmultifidelity methods, namely MFML and optimized MFML (o-MFML). The assessment\nis carried out for the prediction of ground state energies and first vertical\nexcitation energies of a diverse collection of molecules of the CheMFi dataset.\nResults indicate that the MFML method still requires a nested structure of\ntraining data across the fidelities. However, the o-MFML method shows promising\nresults for non-nested multifidelity training data with model errors comparable\nto the nested configurations.\n","authors":["Vivin Vinod","Peter Zaspel"],"pdf_url":"https://arxiv.org/pdf/2407.17087v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.03227v2","updated":"2024-07-24T08:31:21Z","published":"2023-09-04T02:30:19Z","title":"Learning a Patent-Informed Biomedical Knowledge Graph Reveals\n  Technological Potential of Drug Repositioning Candidates","summary":"  Drug repositioning-a promising strategy for discovering new therapeutic uses\nfor existing drugs-has been increasingly explored in the computational science\nliterature using biomedical databases. However, the technological potential of\ndrug repositioning candidates has often been overlooked. This study presents a\nnovel protocol to comprehensively analyse various sources such as\npharmaceutical patents and biomedical databases, and identify drug\nrepositioning candidates with both technological potential and scientific\nevidence. To this end, first, we constructed a scientific biomedical knowledge\ngraph (s-BKG) comprising relationships between drugs, diseases, and genes\nderived from biomedical databases. Our protocol involves identifying drugs that\nexhibit limited association with the target disease but are closely located in\nthe s-BKG, as potential drug candidates. We constructed a patent-informed\nbiomedical knowledge graph (p-BKG) by adding pharmaceutical patent information.\nFinally, we developed a graph embedding protocol to ascertain the structure of\nthe p-BKG, thereby calculating the relevance scores of those candidates with\ntarget disease-related patents to evaluate their technological potential. Our\ncase study on Alzheimer's disease demonstrates its efficacy and feasibility,\nwhile the quantitative outcomes and systematic methods are expected to bridge\nthe gap between computational discoveries and successful market applications in\ndrug repositioning research.\n","authors":["Yongseung Jegal","Jaewoong Choi","Jiho Lee","Ki-Su Park","Seyoung Lee","Janghyeok Yoon"],"pdf_url":"https://arxiv.org/pdf/2309.03227v2.pdf","comment":"We are sorry to withdraw this paper. We found some critical errors in\n  the introduction and results sections. Specifically, we found that the first\n  author have wrongly inserted citations on background works and he made\n  mistakes in the graph embedding methods and relevant results are wrongly\n  calculated. In this regard, we tried to revise this paper and withdraw the\n  current version. Thank you"},{"id":"http://arxiv.org/abs/2407.17085v1","updated":"2024-07-24T08:22:49Z","published":"2024-07-24T08:22:49Z","title":"OVR: A Dataset for Open Vocabulary Temporal Repetition Counting in\n  Videos","summary":"  We introduce a dataset of annotations of temporal repetitions in videos. The\ndataset, OVR (pronounced as over), contains annotations for over 72K videos,\nwith each annotation specifying the number of repetitions, the start and end\ntime of the repetitions, and also a free-form description of what is repeating.\nThe annotations are provided for videos sourced from Kinetics and Ego4D, and\nconsequently cover both Exo and Ego viewing conditions, with a huge variety of\nactions and activities. Moreover, OVR is almost an order of magnitude larger\nthan previous datasets for video repetition. We also propose a baseline\ntransformer-based counting model, OVRCounter, that can localise and count\nrepetitions in videos that are up to 320 frames long. The model is trained and\nevaluated on the OVR dataset, and its performance assessed with and without\nusing text to specify the target class to count. The performance is also\ncompared to a prior repetition counting model. The dataset is available for\ndownload at: https://sites.google.com/view/openvocabreps/\n","authors":["Debidatta Dwibedi","Yusuf Aytar","Jonathan Tompson","Andrew Zisserman"],"pdf_url":"https://arxiv.org/pdf/2407.17085v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10834v2","updated":"2024-07-24T08:14:50Z","published":"2024-07-15T15:45:07Z","title":"MetaLLM: A High-performant and Cost-efficient Dynamic Framework for\n  Wrapping LLMs","summary":"  The rapid progress in machine learning (ML) has brought forth many large\nlanguage models (LLMs) that excel in various tasks and areas. These LLMs come\nwith different abilities and costs in terms of computation or pricing. Since\nthe demand for each query can vary, e.g., because of the queried domain or its\ncomplexity, defaulting to one LLM in an application is not usually the best\nchoice, whether it is the biggest, priciest, or even the one with the best\naverage test performance. Consequently, picking the right LLM that is both\naccurate and cost-effective for an application remains a challenge. In this\npaper, we introduce MetaLLM, a framework that dynamically and intelligently\nroutes each query to the optimal LLM (among several available LLMs) for\nclassification tasks, achieving significantly improved accuracy and\ncost-effectiveness. By framing the selection problem as a multi-armed bandit,\nMetaLLM balances prediction accuracy and cost efficiency under uncertainty. Our\nexperiments, conducted on popular LLM platforms such as OpenAI's GPT models,\nAmazon's Titan, Anthropic's Claude, and Meta's LLaMa, showcase MetaLLM's\nefficacy in real-world scenarios, laying the groundwork for future extensions\nbeyond classification tasks.\n","authors":["Quang H. Nguyen","Duy C. Hoang","Juliette Decugis","Saurav Manchanda","Nitesh V. Chawla","Khoa D. Doan"],"pdf_url":"https://arxiv.org/pdf/2407.10834v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.08330v2","updated":"2024-07-24T08:13:26Z","published":"2024-01-16T12:49:10Z","title":"Boosting Gradient Ascent for Continuous DR-submodular Maximization","summary":"  Projected Gradient Ascent (PGA) is the most commonly used optimization scheme\nin machine learning and operations research areas. Nevertheless, numerous\nstudies and examples have shown that the PGA methods may fail to achieve the\ntight approximation ratio for continuous DR-submodular maximization problems.\nTo address this challenge, we present a boosting technique in this paper, which\ncan efficiently improve the approximation guarantee of the standard PGA to\n\\emph{optimal} with only small modifications on the objective function. The\nfundamental idea of our boosting technique is to exploit non-oblivious search\nto derive a novel auxiliary function $F$, whose stationary points are excellent\napproximations to the global maximum of the original DR-submodular objective\n$f$. Specifically, when $f$ is monotone and $\\gamma$-weakly DR-submodular, we\npropose an auxiliary function $F$ whose stationary points can provide a better\n$(1-e^{-\\gamma})$-approximation than the\n$(\\gamma^2/(1+\\gamma^2))$-approximation guaranteed by the stationary points of\n$f$ itself. Similarly, for the non-monotone case, we devise another auxiliary\nfunction $F$ whose stationary points can achieve an optimal\n$\\frac{1-\\min_{\\boldsymbol{x}\\in\\mathcal{C}}\\|\\boldsymbol{x}\\|_{\\infty}}{4}$-approximation\nguarantee where $\\mathcal{C}$ is a convex constraint set. In contrast, the\nstationary points of the original non-monotone DR-submodular function can be\narbitrarily bad~\\citep{chen2023continuous}. Furthermore, we demonstrate the\nscalability of our boosting technique on four problems. In all of these four\nproblems, our resulting variants of boosting PGA algorithm beat the previous\nstandard PGA in several aspects such as approximation ratio and efficiency.\nFinally, we corroborate our theoretical findings with numerical experiments,\nwhich demonstrate the effectiveness of our boosting PGA methods.\n","authors":["Qixin Zhang","Zongqi Wan","Zengde Deng","Zaiyi Chen","Xiaoming Sun","Jialin Zhang","Yu Yang"],"pdf_url":"https://arxiv.org/pdf/2401.08330v2.pdf","comment":"74 pages, 6 figures and 9 tables. An extended version of Stochastic\n  Continuous Submodular Maximization: Boosting via Non-oblivious Function (ICML\n  2022)"},{"id":"http://arxiv.org/abs/2401.06821v4","updated":"2024-07-24T08:12:11Z","published":"2024-01-11T21:04:28Z","title":"Surrogate Neural Networks Local Stability for Aircraft Predictive\n  Maintenance","summary":"  Surrogate Neural Networks are nowadays routinely used in industry as\nsubstitutes for computationally demanding engineering simulations (e.g., in\nstructural analysis). They allow to generate faster predictions and thus\nanalyses in industrial applications e.g., during a product design, testing or\nmonitoring phases. Due to their performance and time-efficiency, these\nsurrogate models are now being developed for use in safety-critical\napplications. Neural network verification and in particular the assessment of\ntheir robustness (e.g., to perturbations) is the next critical step to allow\ntheir inclusion in real-life applications and certification. We assess the\napplicability and scalability of empirical and formal methods in the context of\naircraft predictive maintenance for surrogate neural networks designed to\npredict the stress sustained by an aircraft part from external loads. The case\nstudy covers a high-dimensional input and output space and the verification\nprocess thus accommodates multi-objective constraints. We explore the\ncomplementarity of verification methods in assessing the local stability\nproperty of such surrogate models to input noise. We showcase the effectiveness\nof sequentially combining methods in one verification 'pipeline' and\ndemonstrate the subsequent gain in runtime required to assess the targeted\nproperty.\n","authors":["Mélanie Ducoffe","Guillaume Povéda","Audrey Galametz","Ryma Boumazouza","Marion-Cécile Martin","Julien Baris","Derk Daverschot","Eugene O'Higgins"],"pdf_url":"https://arxiv.org/pdf/2401.06821v4.pdf","comment":"Peer-reviewed and accepted at the 29th International Conference on\n  Formal Methods for Industrial Critical Systems (FMICS 2024) - 15 pages"},{"id":"http://arxiv.org/abs/2407.17073v1","updated":"2024-07-24T08:02:41Z","published":"2024-07-24T08:02:41Z","title":"Contrastive Learning Is Not Optimal for Quasiperiodic Time Series","summary":"  Despite recent advancements in Self-Supervised Learning (SSL) for time series\nanalysis, a noticeable gap persists between the anticipated achievements and\nactual performance. While these methods have demonstrated formidable\ngeneralization capabilities with minimal labels in various domains, their\neffectiveness in distinguishing between different classes based on a limited\nnumber of annotated records is notably lacking. Our hypothesis attributes this\nbottleneck to the prevalent use of Contrastive Learning, a shared training\nobjective in previous state-of-the-art (SOTA) methods. By mandating\ndistinctiveness between representations for negative pairs drawn from separate\nrecords, this approach compels the model to encode unique record-based patterns\nbut simultaneously neglects changes occurring across the entire record. To\novercome this challenge, we introduce Distilled Embedding for Almost-Periodic\nTime Series (DEAPS) in this paper, offering a non-contrastive method tailored\nfor quasiperiodic time series, such as electrocardiogram (ECG) data. By\navoiding the use of negative pairs, we not only mitigate the model's blindness\nto temporal changes but also enable the integration of a \"Gradual Loss (Lgra)\"\nfunction. This function guides the model to effectively capture dynamic\npatterns evolving throughout the record. The outcomes are promising, as DEAPS\ndemonstrates a notable improvement of +10% over existing SOTA methods when just\na few annotated records are presented to fit a Machine Learning (ML) model\nbased on the learned representation.\n","authors":["Adrian Atienza","Jakob Bardram","Sadasivan Puthusserypady"],"pdf_url":"https://arxiv.org/pdf/2407.17073v1.pdf","comment":"Accepted to IJCAI 2024"},{"id":"http://arxiv.org/abs/2407.17072v1","updated":"2024-07-24T07:59:18Z","published":"2024-07-24T07:59:18Z","title":"An Efficient Procedure for Computing Bayesian Network Structure Learning","summary":"  We propose a globally optimal Bayesian network structure discovery algorithm\nbased on a progressively leveled scoring approach. Bayesian network structure\ndiscovery is a fundamental yet NP-hard problem in the field of probabilistic\ngraphical models, and as the number of variables increases, memory usage grows\nexponentially. The simple and effective method proposed by Silander and\nMyllym\\\"aki has been widely applied in this field, as it incrementally\ncalculates local scores to achieve global optimality. However, existing methods\nthat utilize disk storage, while capable of handling networks with a larger\nnumber of variables, introduce issues such as latency, fragmentation, and\nadditional overhead associated with disk I/O operations. To avoid these\nproblems, we explore how to further enhance computational efficiency and reduce\npeak memory usage using only memory. We introduce an efficient hierarchical\ncomputation method that requires only a single traversal of all local\nstructures, retaining only the data and information necessary for the current\ncomputation, thereby improving efficiency and significantly reducing memory\nrequirements. Experimental results indicate that our method, when using only\nmemory, not only reduces peak memory usage but also improves computational\nefficiency compared to existing methods, demonstrating good scalability for\nhandling larger networks and exhibiting stable experimental results.\nUltimately, we successfully achieved the processing of a Bayesian network with\n28 variables using only memory.\n","authors":["Hongming Huang","Joe Suzuki"],"pdf_url":"https://arxiv.org/pdf/2407.17072v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17070v1","updated":"2024-07-24T07:55:49Z","published":"2024-07-24T07:55:49Z","title":"Curriculum Negative Mining For Temporal Networks","summary":"  Temporal networks are effective in capturing the evolving interactions of\nnetworks over time, such as social networks and e-commerce networks. In recent\nyears, researchers have primarily concentrated on developing specific model\narchitectures for Temporal Graph Neural Networks (TGNNs) in order to improve\nthe representation quality of temporal nodes and edges. However, limited\nattention has been given to the quality of negative samples during the training\nof TGNNs. When compared with static networks, temporal networks present two\nspecific challenges for negative sampling: positive sparsity and positive\nshift. Positive sparsity refers to the presence of a single positive sample\namidst numerous negative samples at each timestamp, while positive shift\nrelates to the variations in positive samples across different timestamps. To\nrobustly address these challenges in training TGNNs, we introduce Curriculum\nNegative Mining (CurNM), a model-aware curriculum learning framework that\nadaptively adjusts the difficulty of negative samples. Within this framework,\nwe first establish a dynamically updated negative pool that balances random,\nhistorical, and hard negatives to address the challenges posed by positive\nsparsity. Secondly, we implement a temporal-aware negative selection module\nthat focuses on learning from the disentangled factors of recently active\nedges, thus accurately capturing shifting preferences. Extensive experiments on\n12 datasets and 3 TGNNs demonstrate that our method outperforms baseline\nmethods by a significant margin. Additionally, thorough ablation studies and\nparameter sensitivity experiments verify the usefulness and robustness of our\napproach. Our code is available at https://github.com/zziyue83/CurNM.\n","authors":["Ziyue Chen","Tongya Zheng","Mingli Song"],"pdf_url":"https://arxiv.org/pdf/2407.17070v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15665v2","updated":"2024-07-24T07:51:20Z","published":"2024-07-22T14:28:46Z","title":"A spatiotemporal deep learning framework for prediction of crack\n  dynamics in heterogeneous solids: efficient mapping of concrete\n  microstructures to its fracture properties","summary":"  A spatiotemporal deep learning framework is proposed that is capable of 2D\nfull-field prediction of fracture in concrete mesostructures. This framework\nnot only predicts fractures but also captures the entire history of the\nfracture process, from the crack initiation in the interfacial transition zone\nto the subsequent propagation of the cracks in the mortar matrix. In addition,\na convolutional neural network is developed which can predict the averaged\nstress-strain curve of the mesostructures. The UNet modeling framework, which\ncomprises an encoder-decoder section with skip connections, is used as the deep\nlearning surrogate model. Training and test data are generated from\nhigh-fidelity fracture simulations of randomly generated concrete\nmesostructures. These mesostructures include geometric variabilities such as\ndifferent aggregate particle geometrical features, spatial distribution, and\nthe total volume fraction of aggregates. The fracture simulations are carried\nout in Abaqus, utilizing the cohesive phase-field fracture modeling technique\nas the fracture modeling approach. In this work, to reduce the number of\ntraining datasets, the spatial distribution of three sets of material\nproperties for three-phase concrete mesostructures, along with the spatial\nphase-field damage index, are fed to the UNet to predict the corresponding\nstress and spatial damage index at the subsequent step. It is shown that after\nthe training process using this methodology, the UNet model is capable of\naccurately predicting damage on the unseen test dataset by using 470 datasets.\nMoreover, another novel aspect of this work is the conversion of irregular\nfinite element data into regular grids using a developed pipeline. This\napproach allows for the implementation of less complex UNet architecture and\nfacilitates the integration of phase-field fracture equations into surrogate\nmodels for future developments.\n","authors":["Rasoul Najafi Koopas","Shahed Rezaei","Natalie Rauter","Richard Ostwald","Rolf Lammering"],"pdf_url":"https://arxiv.org/pdf/2407.15665v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.08261v3","updated":"2024-07-24T07:26:59Z","published":"2023-02-16T12:38:01Z","title":"Knowledge-augmented Graph Machine Learning for Drug Discovery: A Survey","summary":"  The integration of Artificial Intelligence (AI) into the field of drug\ndiscovery has been a growing area of interdisciplinary scientific research.\nHowever, conventional AI models are heavily limited in handling complex\nbiomedical structures (such as 2D or 3D protein and molecule structures) and\nproviding interpretations for outputs, which hinders their practical\napplication. As of late, Graph Machine Learning (GML) has gained considerable\nattention for its exceptional ability to model graph-structured biomedical data\nand investigate their properties and functional relationships. Despite\nextensive efforts, GML methods still suffer from several deficiencies, such as\nthe limited ability to handle supervision sparsity and provide interpretability\nin learning and inference processes, and their ineffectiveness in utilising\nrelevant domain knowledge. In response, recent studies have proposed\nintegrating external biomedical knowledge into the GML pipeline to realise more\nprecise and interpretable drug discovery with limited training instances.\nHowever, a systematic definition for this burgeoning research direction is yet\nto be established. This survey presents a comprehensive overview of\nlong-standing drug discovery principles, provides the foundational concepts and\ncutting-edge techniques for graph-structured data and knowledge databases, and\nformally summarises Knowledge-augmented Graph Machine Learning (KaGML) for drug\ndiscovery. we propose a thorough review of related KaGML works, collected\nfollowing a carefully designed search methodology, and organise them into four\ncategories following a novel-defined taxonomy. To facilitate research in this\npromptly emerging field, we also share collected practical resources that are\nvaluable for intelligent drug discovery and provide an in-depth discussion of\nthe potential avenues for future advancements.\n","authors":["Zhiqiang Zhong","Anastasia Barkova","Davide Mottin"],"pdf_url":"https://arxiv.org/pdf/2302.08261v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.01804v2","updated":"2024-07-24T07:19:00Z","published":"2024-07-01T21:06:34Z","title":"DCoM: Active Learning for All Learners","summary":"  Deep Active Learning (AL) techniques can be effective in reducing annotation\ncosts for training deep models. However, their effectiveness in low- and\nhigh-budget scenarios seems to require different strategies, and achieving\noptimal results across varying budget scenarios remains a challenge. In this\nstudy, we introduce Dynamic Coverage & Margin mix (DCoM), a novel active\nlearning approach designed to bridge this gap. Unlike existing strategies, DCoM\ndynamically adjusts its strategy, considering the competence of the current\nmodel. Through theoretical analysis and empirical evaluations on diverse\ndatasets, including challenging computer vision tasks, we demonstrate DCoM's\nability to overcome the cold start problem and consistently improve results\nacross different budgetary constraints. Thus DCoM achieves state-of-the-art\nperformance in both low- and high-budget regimes.\n","authors":["Inbal Mishal","Daphna Weinshall"],"pdf_url":"https://arxiv.org/pdf/2407.01804v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.20091v3","updated":"2024-07-24T07:06:43Z","published":"2024-05-30T14:27:40Z","title":"VAAD: Visual Attention Analysis Dashboard applied to e-Learning","summary":"  In this paper, we present an approach in the Multimodal Learning Analytics\nfield. Within this approach, we have developed a tool to visualize and analyze\neye movement data collected during learning sessions in online courses. The\ntool is named VAAD, an acronym for Visual Attention Analysis Dashboard. These\neye movement data have been gathered using an eye-tracker and subsequently\nprocessed and visualized for interpretation. The purpose of the tool is to\nconduct a descriptive analysis of the data by facilitating its visualization,\nenabling the identification of differences and learning patterns among various\nlearner populations. Additionally, it integrates a predictive module capable of\nanticipating learner activities during a learning session. Consequently, VAAD\nholds the potential to offer valuable insights into online learning behaviors\nfrom both descriptive and predictive perspectives.\n","authors":["Miriam Navarro","Álvaro Becerra","Roberto Daza","Ruth Cobos","Aythami Morales","Julian Fierrez"],"pdf_url":"https://arxiv.org/pdf/2405.20091v3.pdf","comment":"Accepted in CEDI 2024 (VII Congreso Espa\\~nol de Inform\\'atica), A\n  Coru\\~na, Spain"},{"id":"http://arxiv.org/abs/2407.17040v1","updated":"2024-07-24T07:02:16Z","published":"2024-07-24T07:02:16Z","title":"Time Series Missing Imputation with Multivariate Radial Basis Function\n  Neural Network","summary":"  Researchers have been persistently working to address the issue of missing\nvalues in time series data. Numerous models have been proposed, striving to\nestimate the distribution of the data. The Radial Basis Functions Neural\nNetwork (RBFNN) has recently exhibited exceptional performance in estimating\ndata distribution. In this paper, we propose a time series imputation model\nbased on RBFNN. Our imputation model learns local information from timestamps\nto create a continuous function. Additionally, we incorporate time gaps to\nfacilitate learning information considering the missing terms of missing\nvalues. We name this model the Missing Imputation Multivariate RBFNN\n(MIM-RBFNN). However, MIM-RBFNN relies on a local information-based learning\napproach, which presents difficulties in utilizing temporal information.\nTherefore, we propose an extension called the Missing Value Imputation\nRecurrent Neural Network with Continuous Function (MIRNN-CF) using the\ncontinuous function generated by MIM-RBFNN. We evaluate the performance using\ntwo real-world datasets with non-random missing and random missing patterns,\nand conduct an ablation study comparing MIM-RBFNN and MIRNN-CF.\n","authors":["Chanyoung Jung","Yun Jang"],"pdf_url":"https://arxiv.org/pdf/2407.17040v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17033v1","updated":"2024-07-24T06:39:58Z","published":"2024-07-24T06:39:58Z","title":"Sparse Inducing Points in Deep Gaussian Processes: Enhancing Modeling\n  with Denoising Diffusion Variational Inference","summary":"  Deep Gaussian processes (DGPs) provide a robust paradigm for Bayesian deep\nlearning. In DGPs, a set of sparse integration locations called inducing points\nare selected to approximate the posterior distribution of the model. This is\ndone to reduce computational complexity and improve model efficiency. However,\ninferring the posterior distribution of inducing points is not straightforward.\nTraditional variational inference approaches to posterior approximation often\nlead to significant bias. To address this issue, we propose an alternative\nmethod called Denoising Diffusion Variational Inference (DDVI) that uses a\ndenoising diffusion stochastic differential equation (SDE) to generate\nposterior samples of inducing variables. We rely on score matching methods for\ndenoising diffusion model to approximate score functions with a neural network.\nFurthermore, by combining classical mathematical theory of SDEs with the\nminimization of KL divergence between the approximate and true processes, we\npropose a novel explicit variational lower bound for the marginal likelihood\nfunction of DGP. Through experiments on various datasets and comparisons with\nbaseline methods, we empirically demonstrate the effectiveness of DDVI for\nposterior inference of inducing points for DGP models.\n","authors":["Jian Xu","Delu Zeng","John Paisley"],"pdf_url":"https://arxiv.org/pdf/2407.17033v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17032v1","updated":"2024-07-24T06:35:05Z","published":"2024-07-24T06:35:05Z","title":"Gymnasium: A Standard Interface for Reinforcement Learning Environments","summary":"  Gymnasium is an open-source library providing an API for reinforcement\nlearning environments. Its main contribution is a central abstraction for wide\ninteroperability between benchmark environments and training algorithms.\nGymnasium comes with various built-in environments and utilities to simplify\nresearchers' work along with being supported by most training libraries. This\npaper outlines the main design decisions for Gymnasium, its key features, and\nthe differences to alternative APIs.\n","authors":["Mark Towers","Ariel Kwiatkowski","Jordan Terry","John U. Balis","Gianluca De Cola","Tristan Deleu","Manuel Goulão","Andreas Kallinteris","Markus Krimmel","Arjun KG","Rodrigo Perez-Vicente","Andrea Pierré","Sander Schulhoff","Jun Jet Tai","Hannah Tan","Omar G. Younis"],"pdf_url":"https://arxiv.org/pdf/2407.17032v1.pdf","comment":"6 pages, 1 figure, preprint"},{"id":"http://arxiv.org/abs/2010.01874v3","updated":"2024-07-24T06:25:27Z","published":"2020-10-05T09:22:31Z","title":"Diversity-Preserving K-Armed Bandits, Revisited","summary":"  We consider the bandit-based framework for diversity-preserving\nrecommendations introduced by Celis et al. (2019), who approached it in the\ncase of a polytope mainly by a reduction to the setting of linear bandits. We\ndesign a UCB algorithm using the specific structure of the setting and show\nthat it enjoys a bounded distribution-dependent regret in the natural cases\nwhen the optimal mixed actions put some probability mass on all actions (i.e.,\nwhen diversity is desirable). The regret lower bounds provided show that\notherwise, at least when the model is mean-unbounded, a $\\ln T$ regret is\nsuffered. We also discuss an example beyond the special case of polytopes.\n","authors":["Hédi Hadiji","Sébastien Gerchinovitz","Jean-Michel Loubes","Gilles Stoltz"],"pdf_url":"https://arxiv.org/pdf/2010.01874v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17029v1","updated":"2024-07-24T06:16:37Z","published":"2024-07-24T06:16:37Z","title":"Accurate and Efficient Fine-Tuning of Quantized Large Language Models\n  Through Optimal Balance","summary":"  Large Language Models (LLMs) have demonstrated impressive performance across\nvarious domains. However, the enormous number of model parameters makes\nfine-tuning challenging, significantly limiting their application and\ndeployment. Existing solutions combine parameter quantization with Low-Rank\nAdaptation (LoRA), greatly reducing memory usage but resulting in noticeable\nperformance degradation. In this paper, we identify an imbalance in fine-tuning\nquantized pre-trained models: overly complex adapter inputs and outputs versus\nlow effective trainability of the adaptation. We propose Quantized LLMs with\nBalanced-rank Adaptation (Q-BaRA), which simplifies the adapter inputs and\noutputs while increasing the adapter's rank to achieve a more suitable balance\nfor fine-tuning quantized LLMs. Additionally, for scenarios where fine-tuned\nLLMs need to be deployed as low-precision inference models, we introduce\nQuantization-Aware Fine-tuning with Higher Rank Adaptation (QA-HiRA), which\nsimplifies the adapter inputs and outputs to align with the pre-trained model's\nblock-wise quantization while employing a single matrix to achieve a higher\nrank. Both Q-BaRA and QA-HiRA are easily implemented and offer the following\noptimizations: (i) Q-BaRA consistently achieves the highest accuracy compared\nto baselines and other variants, requiring the same number of trainable\nparameters and computational effort; (ii) QA-HiRA naturally merges adapter\nparameters into the block-wise quantized model after fine-tuning, achieving the\nhighest accuracy compared to other methods. We apply our Q-BaRA and QA-HiRA to\nthe LLaMA and LLaMA2 model families and validate their effectiveness across\ndifferent fine-tuning datasets and downstream scenarios.\n  Code will be made available at\n\\href{https://github.com/xiaocaigou/qbaraqahira}{https://github.com/xiaocaigou/qbaraqahira}\n","authors":["Ao Shen","Qiang Wang","Zhiquan Lai","Xionglve Li","Dongsheng Li"],"pdf_url":"https://arxiv.org/pdf/2407.17029v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.09084v4","updated":"2024-07-24T06:07:28Z","published":"2023-08-17T16:23:52Z","title":"MovePose: A High-performance Human Pose Estimation Algorithm on Mobile\n  and Edge Devices","summary":"  We present MovePose, an optimized lightweight convolutional neural network\ndesigned specifically for real-time body pose estimation on CPU-based mobile\ndevices. The current solutions do not provide satisfactory accuracy and speed\nfor human posture estimation, and MovePose addresses this gap. It aims to\nmaintain real-time performance while improving the accuracy of human posture\nestimation for mobile devices. Our MovePose algorithm has attained an Mean\nAverage Precision (mAP) score of 68.0 on the COCO \\cite{cocodata} validation\ndataset. The MovePose algorithm displayed efficiency with a performance of 69+\nframes per second (fps) when run on an Intel i9-10920x CPU. Additionally, it\nshowcased an increased performance of 452+ fps on an NVIDIA RTX3090 GPU. On an\nAndroid phone equipped with a Snapdragon 8 + 4G processor, the fps reached\nabove 11. To enhance accuracy, we incorporated three techniques: deconvolution,\nlarge kernel convolution, and coordinate classification methods. Compared to\nbasic upsampling, deconvolution is trainable, improves model capacity, and\nenhances the receptive field. Large kernel convolution strengthens these\nproperties at a decreased computational cost. In summary, MovePose provides\nhigh accuracy and real-time performance, marking it a potential tool for a\nvariety of applications, including those focused on mobile-side human posture\nestimation. The code and models for this algorithm will be made publicly\naccessible.\n","authors":["Dongyang Yu","Haoyue Zhang","Ruisheng Zhao","Guoqi Chen","Wangpeng An","Yanhong Yang"],"pdf_url":"https://arxiv.org/pdf/2308.09084v4.pdf","comment":"This paper has been accepted by ICANN 2024 and is an oral\n  presentation"},{"id":"http://arxiv.org/abs/2406.11233v2","updated":"2024-07-24T05:22:48Z","published":"2024-06-17T06:00:24Z","title":"Probing the Decision Boundaries of In-context Learning in Large Language\n  Models","summary":"  In-context learning is a key paradigm in large language models (LLMs) that\nenables them to generalize to new tasks and domains by simply prompting these\nmodels with a few exemplars without explicit parameter updates. Many attempts\nhave been made to understand in-context learning in LLMs as a function of model\nscale, pretraining data, and other factors. In this work, we propose a new\nmechanism to probe and understand in-context learning from the lens of decision\nboundaries for in-context binary classification. Decision boundaries are\nstraightforward to visualize and provide important information about the\nqualitative behavior of the inductive biases of standard classifiers. To our\nsurprise, we find that the decision boundaries learned by current LLMs in\nsimple binary classification tasks are often irregular and non-smooth,\nregardless of linear separability in the underlying task. This paper\ninvestigates the factors influencing these decision boundaries and explores\nmethods to enhance their generalizability. We assess various approaches,\nincluding training-free and fine-tuning methods for LLMs, the impact of model\narchitecture, and the effectiveness of active prompting techniques for\nsmoothing decision boundaries in a data-efficient manner. Our findings provide\na deeper understanding of in-context learning dynamics and offer practical\nimprovements for enhancing robustness and generalizability of in-context\nlearning.\n","authors":["Siyan Zhao","Tung Nguyen","Aditya Grover"],"pdf_url":"https://arxiv.org/pdf/2406.11233v2.pdf","comment":"18 pages, code at https://github.com/siyan-zhao/ICL_decision_boundary"},{"id":"http://arxiv.org/abs/2407.15899v2","updated":"2024-07-24T05:05:53Z","published":"2024-07-22T10:20:34Z","title":"Spatial-Temporal Cross-View Contrastive Pre-training for Check-in\n  Sequence Representation Learning","summary":"  The rapid growth of location-based services (LBS) has yielded massive amounts\nof data on human mobility. Effectively extracting meaningful representations\nfor user-generated check-in sequences is pivotal for facilitating various\ndownstream services. However, the user-generated check-in data are\nsimultaneously influenced by the surrounding objective circumstances and the\nuser's subjective intention. Specifically, the temporal uncertainty and spatial\ndiversity exhibited in check-in data make it difficult to capture the\nmacroscopic spatial-temporal patterns of users and to understand the semantics\nof user mobility activities. Furthermore, the distinct characteristics of the\ntemporal and spatial information in check-in sequences call for an effective\nfusion method to incorporate these two types of information. In this paper, we\npropose a novel Spatial-Temporal Cross-view Contrastive Representation (STCCR)\nframework for check-in sequence representation learning. Specifically, STCCR\naddresses the above challenges by employing self-supervision from \"spatial\ntopic\" and \"temporal intention\" views, facilitating effective fusion of spatial\nand temporal information at the semantic level. Besides, STCCR leverages\ncontrastive clustering to uncover users' shared spatial topics from diverse\nmobility activities, while employing angular momentum contrast to mitigate the\nimpact of temporal uncertainty and noise. We extensively evaluate STCCR on\nthree real-world datasets and demonstrate its superior performance across three\ndownstream tasks.\n","authors":["Letian Gong","Huaiyu Wan","Shengnan Guo","Xiucheng Li","Yan Lin","Erwen Zheng","Tianyi Wang","Zeyu Zhou","Youfang Lin"],"pdf_url":"https://arxiv.org/pdf/2407.15899v2.pdf","comment":"This paper has been accepted as a regular paper at IEEE TKDE"},{"id":"http://arxiv.org/abs/2405.07987v4","updated":"2024-07-24T05:01:21Z","published":"2024-05-13T17:58:30Z","title":"The Platonic Representation Hypothesis","summary":"  We argue that representations in AI models, particularly deep networks, are\nconverging. First, we survey many examples of convergence in the literature:\nover time and across multiple domains, the ways by which different neural\nnetworks represent data are becoming more aligned. Next, we demonstrate\nconvergence across data modalities: as vision models and language models get\nlarger, they measure distance between datapoints in a more and more alike way.\nWe hypothesize that this convergence is driving toward a shared statistical\nmodel of reality, akin to Plato's concept of an ideal reality. We term such a\nrepresentation the platonic representation and discuss several possible\nselective pressures toward it. Finally, we discuss the implications of these\ntrends, their limitations, and counterexamples to our analysis.\n","authors":["Minyoung Huh","Brian Cheung","Tongzhou Wang","Phillip Isola"],"pdf_url":"https://arxiv.org/pdf/2405.07987v4.pdf","comment":"Equal contributions. Project: https://phillipi.github.io/prh/ Code:\n  https://github.com/minyoungg/platonic-rep"},{"id":"http://arxiv.org/abs/2407.16999v1","updated":"2024-07-24T04:47:36Z","published":"2024-07-24T04:47:36Z","title":"SepsisLab: Early Sepsis Prediction with Uncertainty Quantification and\n  Active Sensing","summary":"  Sepsis is the leading cause of in-hospital mortality in the USA. Early sepsis\nonset prediction and diagnosis could significantly improve the survival of\nsepsis patients. Existing predictive models are usually trained on high-quality\ndata with few missing information, while missing values widely exist in\nreal-world clinical scenarios (especially in the first hours of admissions to\nthe hospital), which causes a significant decrease in accuracy and an increase\nin uncertainty for the predictive models. The common method to handle missing\nvalues is imputation, which replaces the unavailable variables with estimates\nfrom the observed data. The uncertainty of imputation results can be propagated\nto the sepsis prediction outputs, which have not been studied in existing works\non either sepsis prediction or uncertainty quantification. In this study, we\nfirst define such propagated uncertainty as the variance of prediction output\nand then introduce uncertainty propagation methods to quantify the propagated\nuncertainty. Moreover, for the potential high-risk patients with low confidence\ndue to limited observations, we propose a robust active sensing algorithm to\nincrease confidence by actively recommending clinicians to observe the most\ninformative variables. We validate the proposed models in both publicly\navailable data (i.e., MIMIC-III and AmsterdamUMCdb) and proprietary data in The\nOhio State University Wexner Medical Center (OSUWMC). The experimental results\nshow that the propagated uncertainty is dominant at the beginning of admissions\nto hospitals and the proposed algorithm outperforms state-of-the-art active\nsensing methods. Finally, we implement a SepsisLab system for early sepsis\nprediction and active sensing based on our pre-trained models. Clinicians and\npotential sepsis patients can benefit from the system in early prediction and\ndiagnosis of sepsis.\n","authors":["Changchang Yin","Pin-Yu Chen","Bingsheng Yao","Dakuo Wang","Jeffrey Caterino","Ping Zhang"],"pdf_url":"https://arxiv.org/pdf/2407.16999v1.pdf","comment":"To be published in KDD 2024"},{"id":"http://arxiv.org/abs/2407.14504v2","updated":"2024-07-24T04:33:55Z","published":"2024-07-19T17:58:00Z","title":"Nonlinear Schrödinger Network","summary":"  Deep neural networks (DNNs) have achieved exceptional performance across\nvarious fields by learning complex nonlinear mappings from large-scale\ndatasets. However, they encounter challenges such as high computational costs\nand limited interpretability. To address these issues, hybrid approaches that\nintegrate physics with AI are gaining interest. This paper introduces a novel\nphysics-based AI model called the \"Nonlinear Schr\\\"odinger Network\", which\ntreats the Nonlinear Schr\\\"odinger Equation (NLSE) as a general-purpose\ntrainable model for learning complex patterns including nonlinear mappings and\nmemory effects from data. Existing physics-informed machine learning methods\nuse neural networks to approximate the solutions of partial differential\nequations (PDEs). In contrast, our approach directly treats the PDE as a\ntrainable model to obtain general nonlinear mappings that would otherwise\nrequire neural networks. As a type of physics-AI symbiosis, it offers a more\ninterpretable and parameter-efficient alternative to traditional black-box\nneural networks, achieving comparable or better accuracy in some time series\nclassification tasks while significantly reducing the number of required\nparameters. Notably, the trained Nonlinear Schr\\\"odinger Network is\ninterpretable, with all parameters having physical meanings as properties of a\nvirtual physical system that transforms the data to a more separable space.\nThis interpretability allows for insight into the underlying dynamics of the\ndata transformation process. Applications to time series forecasting have also\nbeen explored. While our current implementation utilizes the NLSE, the proposed\nmethod of using physics equations as trainable models to learn nonlinear\nmappings from data is not limited to the NLSE and may be extended to other\nmaster equations of physics.\n","authors":["Yiming Zhou","Callen MacPhee","Tingyi Zhou","Bahram Jalali"],"pdf_url":"https://arxiv.org/pdf/2407.14504v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16994v1","updated":"2024-07-24T04:27:55Z","published":"2024-07-24T04:27:55Z","title":"A Voter-Based Stochastic Rejection-Method Framework for Asymptotically\n  Safe Language Model Outputs","summary":"  This paper proposes a new method for preventing unsafe or otherwise low\nquality large language model (LLM) outputs, by leveraging the stochasticity of\nLLMs. We propose a system whereby LLM checkers vote on the acceptability of a\ngenerated output, regenerating it if a threshold of disapproval is reached,\nuntil sufficient checkers approve. We further propose estimators for cost and\nfailure rate, and based on those estimators and experimental data tailored to\nthe application, we propose an algorithm that achieves a desired failure rate\nat the least possible cost. We demonstrate that, under these models, failure\nrate decreases exponentially as a function of cost when voter count and\nthreshold are chosen according to the algorithm, and that the models reasonably\nestimate the actual performance of such a system in action, even with limited\ndata.\n","authors":["Jake R. Watts","Joel Sokol"],"pdf_url":"https://arxiv.org/pdf/2407.16994v1.pdf","comment":"7 pages, 2 figures"},{"id":"http://arxiv.org/abs/2303.04778v2","updated":"2024-07-24T04:10:16Z","published":"2023-03-08T18:20:56Z","title":"Fourier-MIONet: Fourier-enhanced multiple-input neural operators for\n  multiphase modeling of geological carbon sequestration","summary":"  Geologic carbon sequestration (GCS) is a safety-critical technology that aims\nto reduce the amount of carbon dioxide in the atmosphere, which also places\nhigh demands on reliability. Multiphase flow in porous media is essential to\nunderstand CO$_2$ migration and pressure fields in the subsurface associated\nwith GCS. However, numerical simulation for such problems in 4D is\ncomputationally challenging and expensive, due to the multiphysics and\nmultiscale nature of the highly nonlinear governing partial differential\nequations (PDEs). It prevents us from considering multiple subsurface scenarios\nand conducting real-time optimization. Here, we develop a Fourier-enhanced\nmultiple-input neural operator (Fourier-MIONet) to learn the solution operator\nof the problem of multiphase flow in porous media. Fourier-MIONet utilizes the\nrecently developed framework of the multiple-input deep neural operators\n(MIONet) and incorporates the Fourier neural operator (FNO) in the network\narchitecture. Once Fourier-MIONet is trained, it can predict the evolution of\nsaturation and pressure of the multiphase flow under various reservoir\nconditions, such as permeability and porosity heterogeneity, anisotropy,\ninjection configurations, and multiphase flow properties. Compared to the\nenhanced FNO (U-FNO), the proposed Fourier-MIONet has 90% fewer unknown\nparameters, and it can be trained in significantly less time (about 3.5 times\nfaster) with much lower CPU memory ($<$ 15%) and GPU memory ($<$ 35%)\nrequirements, to achieve similar prediction accuracy. In addition to the lower\ncomputational cost, Fourier-MIONet can be trained with only 6 snapshots of time\nto predict the PDE solutions for 30 years. The excellent generalizability of\nFourier-MIONet is enabled by its adherence to the physical principle that the\nsolution to a PDE is continuous over time.\n","authors":["Zhongyi Jiang","Min Zhu","Lu Lu"],"pdf_url":"https://arxiv.org/pdf/2303.04778v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16985v1","updated":"2024-07-24T04:04:56Z","published":"2024-07-24T04:04:56Z","title":"Sparse Tensor PCA via Tensor Decomposition for Unsupervised Feature\n  Selection","summary":"  Recently, introducing Tensor Decomposition (TD) methods into unsupervised\nfeature selection (UFS) has been a rising research point. A tensor structure is\nbeneficial for mining the relations between different modes and helps relieve\nthe computation burden. However, while existing methods exploit TD to minimize\nthe reconstruction error of a data tensor, they don't fully utilize the\ninterpretable and discriminative information in the factor matrices. Moreover,\nmost methods require domain knowledge to perform feature selection. To solve\nthe above problems, we develop two Sparse Tensor Principal Component Analysis\n(STPCA) models that utilize the projection directions in the factor matrices to\nperform UFS. The first model extends Tucker Decomposition to a multiview sparse\nregression form and is transformed into several alternatively solved convex\nsubproblems. The second model formulates a sparse version of the family of\nTensor Singular Value Decomposition (T-SVDs) and is transformed into individual\nconvex subproblems. For both models, we prove the optimal solution of each\nsubproblem falls onto the Hermitian Positive Semidefinite Cone (HPSD).\nAccordingly, we design two fast algorithms based on HPSD projection and prove\ntheir convergence. According to the experimental results on two original\nsynthetic datasets (Orbit and Array Signal) and five real-world datasets, the\ntwo proposed methods are suitable for handling different data tensor scenarios\nand outperform the state-of-the-art UFS methods.\n","authors":["Junjing Zheng","Xinyu Zhang","Weidong Jiang"],"pdf_url":"https://arxiv.org/pdf/2407.16985v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16984v1","updated":"2024-07-24T04:01:09Z","published":"2024-07-24T04:01:09Z","title":"scGHSOM: Hierarchical clustering and visualization of single-cell and\n  CRISPR data using growing hierarchical SOM","summary":"  High-dimensional single-cell data poses significant challenges in identifying\nunderlying biological patterns due to the complexity and heterogeneity of\ncellular states. We propose a comprehensive gene-cell dependency visualization\nvia unsupervised clustering, Growing Hierarchical Self-Organizing Map (GHSOM),\nspecifically designed for analyzing high-dimensional single-cell data like\nsingle-cell sequencing and CRISPR screens. GHSOM is applied to cluster samples\nin a hierarchical structure such that the self-growth structure of clusters\nsatisfies the required variations between and within. We propose a novel\nSignificant Attributes Identification Algorithm to identify features that\ndistinguish clusters. This algorithm pinpoints attributes with minimal\nvariation within a cluster but substantial variation between clusters. These\nkey attributes can then be used for targeted data retrieval and downstream\nanalysis. Furthermore, we present two innovative visualization tools: Cluster\nFeature Map and Cluster Distribution Map. The Cluster Feature Map highlights\nthe distribution of specific features across the hierarchical structure of\nGHSOM clusters. This allows for rapid visual assessment of cluster uniqueness\nbased on chosen features. The Cluster Distribution Map depicts leaf clusters as\ncircles on the GHSOM grid, with circle size reflecting cluster data size and\ncolor customizable to visualize features like cell type or other attributes. We\napply our analysis to three single-cell datasets and one CRISPR dataset\n(cell-gene database) and evaluate clustering methods with internal and external\nCH and ARI scores. GHSOM performs well, being the best performer in internal\nevaluation (CH=4.2). In external evaluation, GHSOM has the third-best\nperformance of all methods.\n","authors":["Shang-Jung Wen","Jia-Ming Chang","Fang Yu"],"pdf_url":"https://arxiv.org/pdf/2407.16984v1.pdf","comment":"Abstract presentation at BIOKDD@ACM KDD 2024"},{"id":"http://arxiv.org/abs/2407.16975v1","updated":"2024-07-24T03:43:55Z","published":"2024-07-24T03:43:55Z","title":"On the Parameter Identifiability of Partially Observed Linear Causal\n  Models","summary":"  Linear causal models are important tools for modeling causal dependencies and\nyet in practice, only a subset of the variables can be observed. In this paper,\nwe examine the parameter identifiability of these models by investigating\nwhether the edge coefficients can be recovered given the causal structure and\npartially observed data. Our setting is more general than that of prior\nresearch - we allow all variables, including both observed and latent ones, to\nbe flexibly related, and we consider the coefficients of all edges, whereas\nmost existing works focus only on the edges between observed variables.\nTheoretically, we identify three types of indeterminacy for the parameters in\npartially observed linear causal models. We then provide graphical conditions\nthat are sufficient for all parameters to be identifiable and show that some of\nthem are provably necessary. Methodologically, we propose a novel\nlikelihood-based parameter estimation method that addresses the variance\nindeterminacy of latent variables in a specific way and can asymptotically\nrecover the underlying parameters up to trivial indeterminacy. Empirical\nstudies on both synthetic and real-world datasets validate our identifiability\ntheory and the effectiveness of the proposed method in the finite-sample\nregime.\n","authors":["Xinshuai Dong","Ignavier Ng","Biwei Huang","Yuewen Sun","Songyao Jin","Roberto Legaspi","Peter Spirtes","Kun Zhang"],"pdf_url":"https://arxiv.org/pdf/2407.16975v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.06433v4","updated":"2024-07-24T03:43:32Z","published":"2023-02-13T15:12:15Z","title":"Label-efficient Time Series Representation Learning: A Review","summary":"  Label-efficient time series representation learning, which aims to learn\neffective representations with limited labeled data, is crucial for deploying\ndeep learning models in real-world applications. To address the scarcity of\nlabeled time series data, various strategies, e.g., transfer learning,\nself-supervised learning, and semi-supervised learning, have been developed. In\nthis survey, we introduce a novel taxonomy for the first time, categorizing\nexisting approaches as in-domain or cross-domain, based on their reliance on\nexternal data sources or not. Furthermore, we present a review of the recent\nadvances in each strategy, conclude the limitations of current methodologies,\nand suggest future research directions that promise further improvements in the\nfield.\n","authors":["Emadeldeen Eldele","Mohamed Ragab","Zhenghua Chen","Min Wu","Chee-Keong Kwoh","Xiaoli Li"],"pdf_url":"https://arxiv.org/pdf/2302.06433v4.pdf","comment":"Accepted in the IEEE Transactions on Artificial Intelligence (TAI)\n  https://ieeexplore.ieee.org/document/10601520"},{"id":"http://arxiv.org/abs/2407.08108v2","updated":"2024-07-24T03:37:17Z","published":"2024-07-11T00:54:56Z","title":"CADC: Encoding User-Item Interactions for Compressing Recommendation\n  Model Training Data","summary":"  Deep learning recommendation models (DLRMs) are at the heart of the current\ne-commerce industry. However, the amount of training data used to train these\nlarge models is growing exponentially, leading to substantial training hurdles.\nThe training dataset contains two primary types of information: content-based\ninformation (features of users and items) and collaborative information\n(interactions between users and items). One approach to reduce the training\ndataset is to remove user-item interactions. But that significantly diminishes\ncollaborative information, which is crucial for maintaining accuracy due to its\ninclusion of interaction histories. This loss profoundly impacts DLRM\nperformance.\n  This paper makes an important observation that if one can capture the\nuser-item interaction history to enrich the user and item embeddings, then the\ninteraction history can be compressed without losing model accuracy. Thus, this\nwork, Collaborative Aware Data Compression (CADC), takes a two-step approach to\ntraining dataset compression. In the first step, we use matrix factorization of\nthe user-item interaction matrix to create a novel embedding representation for\nboth the users and items. Once the user and item embeddings are enriched by the\ninteraction history information the approach then applies uniform random\nsampling of the training dataset to drastically reduce the training dataset\nsize while minimizing model accuracy drop. The source code of CADC is available\nat\n\\href{https://anonymous.4open.science/r/DSS-RM-8C1D/README.md}{https://anonymous.4open.science/r/DSS-RM-8C1D/README.md}.\n","authors":["Hossein Entezari Zarch","Abdulla Alshabanah","Chaoyi Jiang","Murali Annavaram"],"pdf_url":"https://arxiv.org/pdf/2407.08108v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.16452v2","updated":"2024-07-24T03:32:09Z","published":"2023-09-28T13:59:50Z","title":"On the Trade-offs between Adversarial Robustness and Actionable\n  Explanations","summary":"  As machine learning models are increasingly being employed in various\nhigh-stakes settings, it becomes important to ensure that predictions of these\nmodels are not only adversarially robust, but also readily explainable to\nrelevant stakeholders. However, it is unclear if these two notions can be\nsimultaneously achieved or if there exist trade-offs between them. In this\nwork, we make one of the first attempts at studying the impact of adversarially\nrobust models on actionable explanations which provide end users with a means\nfor recourse. We theoretically and empirically analyze the cost (ease of\nimplementation) and validity (probability of obtaining a positive model\nprediction) of recourses output by state-of-the-art algorithms when the\nunderlying models are adversarially robust vs. non-robust. More specifically,\nwe derive theoretical bounds on the differences between the cost and the\nvalidity of the recourses generated by state-of-the-art algorithms for\nadversarially robust vs. non-robust linear and non-linear models. Our empirical\nresults with multiple real-world datasets validate our theoretical results and\nshow the impact of varying degrees of model robustness on the cost and validity\nof the resulting recourses. Our analyses demonstrate that adversarially robust\nmodels significantly increase the cost and reduce the validity of the resulting\nrecourses, thus shedding light on the inherent trade-offs between adversarial\nrobustness and actionable explanations.\n","authors":["Satyapriya Krishna","Chirag Agarwal","Himabindu Lakkaraju"],"pdf_url":"https://arxiv.org/pdf/2309.16452v2.pdf","comment":"Accepted in the 7th AAAI Conference on AI, Ethics, and Society, 2024"},{"id":"http://arxiv.org/abs/2407.16970v1","updated":"2024-07-24T03:32:05Z","published":"2024-07-24T03:32:05Z","title":"Towards Aligning Language Models with Textual Feedback","summary":"  We present ALT (ALignment with Textual feedback), an approach that aligns\nlanguage models with user preferences expressed in text. We argue that text\noffers greater expressiveness, enabling users to provide richer feedback than\nsimple comparative preferences and this richer feedback can lead to more\nefficient and effective alignment. ALT aligns the model by conditioning its\ngeneration on the textual feedback. Our method relies solely on language\nmodeling techniques and requires minimal hyper-parameter tuning, though it\nstill presents the main benefits of RL-based alignment algorithms and can\neffectively learn from textual feedback. We explore the efficacy and efficiency\nof textual feedback across different tasks such as toxicity reduction,\nsummarization, and dialog response generation. We find that ALT outperforms PPO\nfor the task of toxicity reduction while being able to match its performance on\nsummarization with only 20% of the samples. We also explore how ALT can be used\nwith feedback provided by an existing LLM where we explore an LLM providing\nconstrained and unconstrained textual feedback. We also outline future\ndirections to align models with natural language feedback.\n","authors":["Saüc Abadal Lloret","Shehzaad Dhuliawala","Keerthiram Murugesan","Mrinmaya Sachan"],"pdf_url":"https://arxiv.org/pdf/2407.16970v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16968v1","updated":"2024-07-24T03:26:26Z","published":"2024-07-24T03:26:26Z","title":"Stochastic Variance-Reduced Iterative Hard Thresholding in Graph\n  Sparsity Optimization","summary":"  Stochastic optimization algorithms are widely used for large-scale data\nanalysis due to their low per-iteration costs, but they often suffer from slow\nasymptotic convergence caused by inherent variance. Variance-reduced techniques\nhave been therefore used to address this issue in structured sparse models\nutilizing sparsity-inducing norms or $\\ell_0$-norms. However, these techniques\nare not directly applicable to complex (non-convex) graph sparsity models,\nwhich are essential in applications like disease outbreak monitoring and social\nnetwork analysis. In this paper, we introduce two stochastic variance-reduced\ngradient-based methods to solve graph sparsity optimization: GraphSVRG-IHT and\nGraphSCSG-IHT. We provide a general framework for theoretical analysis,\ndemonstrating that our methods enjoy a linear convergence speed. Extensive\nexperiments validate\n","authors":["Derek Fox","Samuel Hernandez","Qianqian Tong"],"pdf_url":"https://arxiv.org/pdf/2407.16968v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16964v1","updated":"2024-07-24T03:02:57Z","published":"2024-07-24T03:02:57Z","title":"When AI Defeats Password Deception! A Deep Learning Framework to\n  Distinguish Passwords and Honeywords","summary":"  \"Honeywords\" have emerged as a promising defense mechanism for detecting data\nbreaches and foiling offline dictionary attacks (ODA) by deceiving attackers\nwith false passwords. In this paper, we propose PassFilter, a novel deep\nlearning (DL) based attack framework, fundamental in its ability to identify\npasswords from a set of sweetwords associated with a user account, effectively\nchallenging a variety of honeywords generation techniques (HGTs). The DL model\nin PassFilter is trained with a set of previously collected or adversarially\ngenerated passwords and honeywords, and carefully orchestrated to predict\nwhether a sweetword is the password or a honeyword. Our model can compromise\nthe security of state-of-the-art, heuristics-based, and representation\nlearning-based HGTs proposed by Dionysiou et al. Specifically, our analysis\nwith nine publicly available password datasets shows that PassFilter\nsignificantly outperforms the baseline random guessing success rate of 5%,\nachieving 6.10% to 52.78% on the 1st guessing attempt, considering 20\nsweetwords per account. This success rate rapidly increases with additional\nlogin attempts before account lock-outs, often allowed on many real-world\nonline services to maintain reasonable usability. For example, it ranges from\n41.78% to 96.80% for five attempts, and from 72.87% to 99.00% for ten attempts,\ncompared to 25% and 50% random guessing, respectively. We also examined\nPassFilter against general-purpose language models used for honeyword\ngeneration, like those proposed by Yu et al. These honeywords also proved\nvulnerable to our attack, with success rates of 14.19% for 1st guessing\nattempt, increasing to 30.23%, 41.70%, and 63.10% after 3rd, 5th, and 10th\nguessing attempts, respectively. Our findings demonstrate the effectiveness of\nDL model deployed in PassFilter in breaching state-of-the-art HGTs and\ncompromising password security based on ODA.\n","authors":["Jimmy Dani","Brandon McCulloh","Nitesh Saxena"],"pdf_url":"https://arxiv.org/pdf/2407.16964v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.11271v2","updated":"2024-07-24T02:59:40Z","published":"2024-06-17T07:21:36Z","title":"MINT-1T: Scaling Open-Source Multimodal Data by 10x: A Multimodal\n  Dataset with One Trillion Tokens","summary":"  Multimodal interleaved datasets featuring free-form interleaved sequences of\nimages and text are crucial for training frontier large multimodal models\n(LMMs). Despite the rapid progression of open-source LMMs, there remains a\npronounced scarcity of large-scale, diverse open-source multimodal interleaved\ndatasets. In response, we introduce MINT-1T, the most extensive and diverse\nopen-source Multimodal INTerleaved dataset to date. MINT-1T comprises one\ntrillion text tokens and 3.4 billion images, a 10x scale-up from existing\nopen-source datasets. Additionally, we include previously untapped sources such\nas PDFs and ArXiv papers. As scaling multimodal interleaved datasets requires\nsubstantial engineering effort, sharing the data curation process and releasing\nthe dataset greatly benefits the community. Our experiments show that LMMs\ntrained on MINT-1T rival the performance of models trained on the previous\nleading dataset, OBELICS. Our data and code will be released at\nhttps://github.com/mlfoundations/MINT-1T.\n","authors":["Anas Awadalla","Le Xue","Oscar Lo","Manli Shu","Hannah Lee","Etash Kumar Guha","Matt Jordan","Sheng Shen","Mohamed Awadalla","Silvio Savarese","Caiming Xiong","Ran Xu","Yejin Choi","Ludwig Schmidt"],"pdf_url":"https://arxiv.org/pdf/2406.11271v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16959v1","updated":"2024-07-24T02:56:22Z","published":"2024-07-24T02:56:22Z","title":"Dynamic Graph Transformer with Correlated Spatial-Temporal Positional\n  Encoding","summary":"  Learning effective representations for Continuous-Time Dynamic Graphs (CTDGs)\nhas garnered significant research interest, largely due to its powerful\ncapabilities in modeling complex interactions between nodes. A fundamental and\ncrucial requirement for representation learning in CTDGs is the appropriate\nestimation and preservation of proximity. However, due to the sparse and\nevolving characteristics of CTDGs, the spatial-temporal properties inherent in\nhigh-order proximity remain largely unexplored. Despite its importance, this\nproperty presents significant challenges due to the computationally intensive\nnature of personalized interaction intensity estimation and the dynamic\nattributes of CTDGs. To this end, we propose a novel Correlated\nSpatial-Temporal Positional encoding that incorporates a parameter-free\npersonalized interaction intensity estimation under the weak assumption of the\nPoisson Point Process. Building on this, we introduce the Dynamic Graph\nTransformer with \\Correlated Spatial-Temporal Positional Encoding (CorDGT),\nwhich efficiently retains the evolving spatial-temporal high-order proximity\nfor effective node representation learning in CTDGs. Extensive experiments on\nseven small and two large-scale datasets demonstrate the superior performance\nand scalability of the proposed CorDGT.\n","authors":["Zhe Wang","Sheng Zhou","Jiawei Chen","Zhen Zhang","Binbin Hu","Yan Feng","Chun Chen","Can Wang"],"pdf_url":"https://arxiv.org/pdf/2407.16959v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16958v1","updated":"2024-07-24T02:52:02Z","published":"2024-07-24T02:52:02Z","title":"Cheems: Wonderful Matrices More Efficient and More Effective\n  Architecture","summary":"  Recent studies have shown that, relative position encoding performs well in\nselective state space model scanning algorithms, and the architecture that\nbalances SSM and Attention enhances the efficiency and effectiveness of the\nalgorithm, while the sparse activation of the mixture of experts reduces the\ntraining cost. I studied the effectiveness of using different position\nencodings in structured state space dual algorithms, and the more effective\nSSD-Attn internal and external function mixing method, and designed a more\nefficient cross domain mixture of experts. I found that the same matrix is very\nwonderful in different algorithms, which allows us to establish a new hybrid\nsparse architecture: Cheems. Compared with other hybrid architectures, it is\nmore efficient and more effective in language modeling tasks.\n","authors":["Jingze Shi","Lu He","Yuhan Wang","Tianyu He","Bingheng Wu","Mingkun Hou"],"pdf_url":"https://arxiv.org/pdf/2407.16958v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16951v1","updated":"2024-07-24T02:37:42Z","published":"2024-07-24T02:37:42Z","title":"Towards Transfer Unlearning: Empirical Evidence of Cross-Domain Bias\n  Mitigation","summary":"  Large language models (LLMs) often inherit biases from vast amounts of\ntraining corpora. Traditional debiasing methods, while effective to some\nextent, do not completely eliminate memorized biases and toxicity in LLMs. In\nthis paper, we study an unlearning-based approach to debiasing in LLMs by\nperforming gradient ascent on hate speech against minority groups, i.e.,\nminimizing the likelihood of biased or toxic content. Specifically, we propose\na mask language modeling unlearning technique, which unlearns the harmful part\nof the text. This method enables LLMs to selectively forget and disassociate\nfrom biased and harmful content. Experimental results demonstrate the\neffectiveness of our approach in diminishing bias while maintaining the\nlanguage modeling abilities. Surprisingly, the results also unveil an\nunexpected potential for cross-domain transfer unlearning: debiasing in one\nbias form (e.g. gender) may contribute to mitigating others (e.g. race and\nreligion).\n","authors":["Huimin Lu","Masaru Isonuma","Junichiro Mori","Ichiro Sakata"],"pdf_url":"https://arxiv.org/pdf/2407.16951v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16944v1","updated":"2024-07-24T02:23:18Z","published":"2024-07-24T02:23:18Z","title":"An Adaptive Gradient Regularization Method","summary":"  Optimizer plays an important role in neural network training with high\nefficiency and performance. Weight update based on its gradient is the central\npart of the optimizer. It has been shown that normalization and standardization\noperation on weight and gradient can accelerate the training process and\nimprove performance such as Weight Standardization (WS), weight normalization\n(WN) and gradient normalization (GN); there is also gradient centralization\n(GC). In this work, we introduce a new optimization technique based on the\ngradient magnitude in a gradient vector named adaptive gradient regularization\n(AGR), which normalizes the gradient vector in all dimensions as a coefficient\nvector and subtracts the product of the gradient and its coefficient vector by\nthe vanilla gradient. It can be viewed as an adaptive gradient clipping method.\nWe show that the AGR can improve the loss function Lipschitzness with a more\nstable training process and better generalization performance. AGR is very\nsimple to be embedded into vanilla optimizers such as Adan and AdamW with only\nthree lines of code. Our experiments are conducted in image generation, image\nclassification and language representation, which shows that our AGR improves\nthe training result.\n","authors":["Huixiu Jiang","Yu Bao","Rutong Si"],"pdf_url":"https://arxiv.org/pdf/2407.16944v1.pdf","comment":"11 pages, 11 figures"},{"id":"http://arxiv.org/abs/2407.16940v1","updated":"2024-07-24T02:20:29Z","published":"2024-07-24T02:20:29Z","title":"GV-Rep: A Large-Scale Dataset for Genetic Variant Representation\n  Learning","summary":"  Genetic variants (GVs) are defined as differences in the DNA sequences among\nindividuals and play a crucial role in diagnosing and treating genetic\ndiseases. The rapid decrease in next generation sequencing cost has led to an\nexponential increase in patient-level GV data. This growth poses a challenge\nfor clinicians who must efficiently prioritize patient-specific GVs and\nintegrate them with existing genomic databases to inform patient management. To\naddressing the interpretation of GVs, genomic foundation models (GFMs) have\nemerged. However, these models lack standardized performance assessments,\nleading to considerable variability in model evaluations. This poses the\nquestion: How effectively do deep learning methods classify unknown GVs and\nalign them with clinically-verified GVs? We argue that representation learning,\nwhich transforms raw data into meaningful feature spaces, is an effective\napproach for addressing both indexing and classification challenges. We\nintroduce a large-scale Genetic Variant dataset, named GV-Rep, featuring\nvariable-length contexts and detailed annotations, designed for deep learning\nmodels to learn GV representations across various traits, diseases, tissue\ntypes, and experimental contexts. Our contributions are three-fold: (i)\nConstruction of a comprehensive dataset with 7 million records, each labeled\nwith characteristics of the corresponding variants, alongside additional data\nfrom 17,548 gene knockout tests across 1,107 cell types, 1,808 variant\ncombinations, and 156 unique clinically verified GVs from real-world patients.\n(ii) Analysis of the structure and properties of the dataset. (iii)\nExperimentation of the dataset with pre-trained GFMs. The results show a\nsignificant gap between GFMs current capabilities and accurate GV\nrepresentation. We hope this dataset will help advance genomic deep learning to\nbridge this gap.\n","authors":["Zehui Li","Vallijah Subasri","Guy-Bart Stan","Yiren Zhao","Bo Wang"],"pdf_url":"https://arxiv.org/pdf/2407.16940v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2407.16938v1","updated":"2024-07-24T02:16:52Z","published":"2024-07-24T02:16:52Z","title":"Synthetic Trajectory Generation Through Convolutional Neural Networks","summary":"  Location trajectories provide valuable insights for applications from urban\nplanning to pandemic control. However, mobility data can also reveal sensitive\ninformation about individuals, such as political opinions, religious beliefs,\nor sexual orientations. Existing privacy-preserving approaches for publishing\nthis data face a significant utility-privacy trade-off. Releasing synthetic\ntrajectory data generated through deep learning offers a promising solution.\nDue to the trajectories' sequential nature, most existing models are based on\nrecurrent neural networks (RNNs). However, research in generative adversarial\nnetworks (GANs) largely employs convolutional neural networks (CNNs) for image\ngeneration. This discrepancy raises the question of whether advances in\ncomputer vision can be applied to trajectory generation. In this work, we\nintroduce a Reversible Trajectory-to-CNN Transformation (RTCT) that adapts\ntrajectories into a format suitable for CNN-based models. We integrated this\ntransformation with the well-known DCGAN in a proof-of-concept (PoC) and\nevaluated its performance against an RNN-based trajectory GAN using four\nmetrics across two datasets. The PoC was superior in capturing spatial\ndistributions compared to the RNN model but had difficulty replicating\nsequential and temporal properties. Although the PoC's utility is not\nsufficient for practical applications, the results demonstrate the\ntransformation's potential to facilitate the use of CNNs for trajectory\ngeneration, opening up avenues for future research. To support continued\nresearch, all source code has been made available under an open-source license.\n","authors":["Jesse Merhi","Erik Buchholz","Salil S. Kanhere"],"pdf_url":"https://arxiv.org/pdf/2407.16938v1.pdf","comment":"To appear in the proceedings of the 21st Annual International\n  Conference on Privacy, Security & Trust (PST 2024)"},{"id":"http://arxiv.org/abs/2407.16936v1","updated":"2024-07-24T02:15:48Z","published":"2024-07-24T02:15:48Z","title":"Provable Benefit of Annealed Langevin Monte Carlo for Non-log-concave\n  Sampling","summary":"  We address the outstanding problem of sampling from an unnormalized density\nthat may be non-log-concave and multimodal. To enhance the performance of\nsimple Markov chain Monte Carlo (MCMC) methods, techniques of annealing type\nhave been widely used. However, quantitative theoretical guarantees of these\ntechniques are under-explored. This study takes a first step toward providing a\nnon-asymptotic analysis of annealed MCMC. Specifically, we establish, for the\nfirst time, an oracle complexity of $\\widetilde{O}\\left(\\frac{d\\beta^2{\\cal\nA}^2}{\\varepsilon^6}\\right)$ for simple annealed Langevin Monte Carlo algorithm\nto achieve $\\varepsilon^2$ accuracy in Kullback-Leibler divergence to the\ntarget distribution $\\pi\\propto{\\rm e}^{-V}$ on $\\mathbb{R}^d$ with\n$\\beta$-smooth potential $V$. Here, ${\\cal A}$ represents the action of a curve\nof probability measures interpolating the target distribution $\\pi$ and a\nreadily sampleable distribution.\n","authors":["Wei Guo","Molei Tao","Yongxin Chen"],"pdf_url":"https://arxiv.org/pdf/2407.16936v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.10081v3","updated":"2024-07-24T02:08:25Z","published":"2023-06-16T07:07:58Z","title":"Optimizer's Information Criterion: Dissecting and Correcting Bias in\n  Data-Driven Optimization","summary":"  In data-driven optimization, the sample performance of the obtained decision\ntypically incurs an optimistic bias against the true performance, a phenomenon\ncommonly known as the Optimizer's Curse and intimately related to overfitting\nin machine learning. Common techniques to correct this bias, such as\ncross-validation, require repeatedly solving additional optimization problems\nand are therefore computationally expensive. We develop a general bias\ncorrection approach, building on what we call Optimizer's Information Criterion\n(OIC), that directly approximates the first-order bias and does not require\nsolving any additional optimization problems. Our OIC generalizes the\ncelebrated Akaike Information Criterion to evaluate the objective performance\nin data-driven optimization, which crucially involves not only model fitting\nbut also its interplay with the downstream optimization. As such it can be used\nfor decision selection instead of only model selection. We apply our approach\nto a range of data-driven optimization formulations comprising empirical and\nparametric models, their regularized counterparts, and furthermore contextual\noptimization. Finally, we provide numerical validation on the superior\nperformance of our approach under synthetic and real-world datasets.\n","authors":["Garud Iyengar","Henry Lam","Tianyu Wang"],"pdf_url":"https://arxiv.org/pdf/2306.10081v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.03718v2","updated":"2024-07-24T02:03:47Z","published":"2024-07-04T08:08:12Z","title":"Multi-Convformer: Extending Conformer with Multiple Convolution Kernels","summary":"  Convolutions have become essential in state-of-the-art end-to-end Automatic\nSpeech Recognition~(ASR) systems due to their efficient modelling of local\ncontext. Notably, its use in Conformers has led to superior performance\ncompared to vanilla Transformer-based ASR systems. While components other than\nthe convolution module in the Conformer have been reexamined, altering the\nconvolution module itself has been far less explored. Towards this, we\nintroduce Multi-Convformer that uses multiple convolution kernels within the\nconvolution module of the Conformer in conjunction with gating. This helps in\nimproved modeling of local dependencies at varying granularities. Our model\nrivals existing Conformer variants such as CgMLP and E-Branchformer in\nperformance, while being more parameter efficient. We empirically compare our\napproach with Conformer and its variants across four different datasets and\nthree different modelling paradigms and show up to 8% relative word error\nrate~(WER) improvements.\n","authors":["Darshan Prabhu","Yifan Peng","Preethi Jyothi","Shinji Watanabe"],"pdf_url":"https://arxiv.org/pdf/2407.03718v2.pdf","comment":"Accepted to INTERSPEECH 2024"},{"id":"http://arxiv.org/abs/2407.16935v1","updated":"2024-07-24T02:03:28Z","published":"2024-07-24T02:03:28Z","title":"Federated Automatic Latent Variable Selection in Multi-output Gaussian\n  Processes","summary":"  This paper explores a federated learning approach that automatically selects\nthe number of latent processes in multi-output Gaussian processes (MGPs). The\nMGP has seen great success as a transfer learning tool when data is generated\nfrom multiple sources/units/entities. A common approach in MGPs to transfer\nknowledge across units involves gathering all data from each unit to a central\nserver and extracting common independent latent processes to express each unit\nas a linear combination of the shared latent patterns. However, this approach\nposes key challenges in (i) determining the adequate number of latent processes\nand (ii) relying on centralized learning which leads to potential privacy risks\nand significant computational burdens on the central server. To address these\nissues, we propose a hierarchical model that places spike-and-slab priors on\nthe coefficients of each latent process. These priors help automatically select\nonly needed latent processes by shrinking the coefficients of unnecessary ones\nto zero. To estimate the model while avoiding the drawbacks of centralized\nlearning, we propose a variational inference-based approach, that formulates\nmodel inference as an optimization problem compatible with federated settings.\nWe then design a federated learning algorithm that allows units to jointly\nselect and infer the common latent processes without sharing their data. We\nalso discuss an efficient learning approach for a new unit within our proposed\nfederated framework. Simulation and case studies on Li-ion battery degradation\nand air temperature data demonstrate the advantageous features of our proposed\napproach.\n","authors":["Jingyi Gao","Seokhyun Chung"],"pdf_url":"https://arxiv.org/pdf/2407.16935v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16933v1","updated":"2024-07-24T01:54:30Z","published":"2024-07-24T01:54:30Z","title":"Deep Koopman-based Control of Quality Variation in Multistage\n  Manufacturing Systems","summary":"  This paper presents a modeling-control synthesis to address the quality\ncontrol challenges in multistage manufacturing systems (MMSs). A new\nfeedforward control scheme is developed to minimize the quality variations\ncaused by process disturbances in MMSs. Notably, the control framework\nleverages a stochastic deep Koopman (SDK) model to capture the quality\npropagation mechanism in the MMSs, highlighted by its ability to transform the\nnonlinear propagation dynamics into a linear one. Two roll-to-roll case studies\nare presented to validate the proposed method and demonstrate its\neffectiveness. The overall method is suitable for nonlinear MMSs and does not\nrequire extensive expert knowledge.\n","authors":["Zhiyi Chen","Harshal Maske","Devesh Upadhyay","Huanyi Shui","Xun Huan","Jun Ni"],"pdf_url":"https://arxiv.org/pdf/2407.16933v1.pdf","comment":"The paper was in the proceeding of 2024 American Control Conference.\n  This submitted version addresses a minor correction to one equation (Eq. 14),\n  while the results and conclusions remain the same"},{"id":"http://arxiv.org/abs/2407.04418v2","updated":"2024-07-24T01:32:05Z","published":"2024-07-05T11:09:05Z","title":"Enabling On-Device LLMs Personalization with Smartphone Sensing","summary":"  This demo presents a novel end-to-end framework that combines on-device large\nlanguage models (LLMs) with smartphone sensing technologies to achieve\ncontext-aware and personalized services. The framework addresses critical\nlimitations of current personalization solutions via cloud LLMs, such as\nprivacy concerns, latency and cost, and limited personal information. To\nachieve this, we innovatively proposed deploying LLMs on smartphones with\nmultimodal sensor data through context-aware sensing and customized prompt\nengineering, ensuring privacy and enhancing personalization performance. A case\nstudy involving a university student demonstrated the capability of the\nframework to provide tailored recommendations. In addition, we show that the\nframework achieves the best trade-off in privacy, performance, latency, cost,\nbattery and energy consumption between on-device and cloud LLMs. To the best of\nour knowledge, this is the first framework to provide on-device LLMs\npersonalization with smartphone sensing. Future work will incorporate more\ndiverse sensor data and involve extensive user studies to enhance\npersonalization. Our proposed framework has the potential to substantially\nimprove user experiences across domains including healthcare, productivity, and\nentertainment.\n","authors":["Shiquan Zhang","Ying Ma","Le Fang","Hong Jia","Simon D'Alfonso","Vassilis Kostakos"],"pdf_url":"https://arxiv.org/pdf/2407.04418v2.pdf","comment":"5 pages, 3 figures, conference demo paper"},{"id":"http://arxiv.org/abs/2407.16927v1","updated":"2024-07-24T01:28:04Z","published":"2024-07-24T01:28:04Z","title":"DeepCell: A Ubiquitous Accurate Provider-side Cellular-based\n  Localization","summary":"  Although outdoor localization is already available to the general public and\nbusinesses through the wide spread use of the GPS, it is not supported by\nlow-end phones, requires a direct line of sight to satellites and can drain\nphone battery quickly. The current fingerprinting solutions can provide\nhigh-accuracy localization but are based on the client side. This limits their\nubiquitous deployment and accuracy. In this paper, we introduce DeepCell: a\nprovider-side fingerprinting localization system that can provide high accuracy\nlocalization for any cell phone. To build its fingerprint, DeepCell leverages\nthe unlabeled cellular measurements recorded by the cellular provider while\nopportunistically synchronizing with selected client devices to get location\nlabels. The fingerprint is then used to train a deep neural network model that\nis harnessed for localization. To achieve this goal, DeepCell need to address a\nnumber of challenges including using unlabeled data from the provider side,\nhandling noise and sparsity, scaling the data to large areas, and finally\nproviding enough data that is required for training deep models without\noverhead. Evaluation of DeepCell in a typical realistic environment shows that\nit can achieve a consistent median accuracy of 29m. This accuracy outperforms\nthe state-of-the-art client-based cellular-based systems by more than 75.4%. In\naddition, the same accuracy is extended to low-end phones.\n","authors":["Ahmed Shokry","Moustafa Youssef"],"pdf_url":"https://arxiv.org/pdf/2407.16927v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2106.13632"},{"id":"http://arxiv.org/abs/2407.16923v1","updated":"2024-07-24T01:15:53Z","published":"2024-07-24T01:15:53Z","title":"Handling Device Heterogeneity for Deep Learning-based Localization","summary":"  Deep learning-based fingerprinting is one of the current promising\ntechnologies for outdoor localization in cellular networks. However, deploying\nsuch localization systems for heterogeneous phones affects their accuracy as\nthe cellular received signal strength (RSS) readings vary for different types\nof phones. In this paper, we introduce a number of techniques for addressing\nthe phones heterogeneity problem in the deep-learning based localization\nsystems. The basic idea is either to approximate a function that maps the\ncellular RSS measurements between different devices or to transfer the\nknowledge across them.\n  Evaluation of the proposed techniques using different Android phones on four\nindependent testbeds shows that our techniques can improve the localization\naccuracy by more than 220% for the four testbeds as compared to the\nstate-of-the-art systems. This highlights the promise of the proposed device\nheterogeneity handling techniques for enabling a wide deployment of deep\nlearning-based localization systems over different devices.\n","authors":["Ahmed Shokry","Moustafa Youssef"],"pdf_url":"https://arxiv.org/pdf/2407.16923v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14710v2","updated":"2024-07-24T01:15:40Z","published":"2024-07-20T00:11:59Z","title":"Universally Harmonizing Differential Privacy Mechanisms for Federated\n  Learning: Boosting Accuracy and Convergence","summary":"  Differentially private federated learning (DP-FL) is a promising technique\nfor collaborative model training while ensuring provable privacy for clients.\nHowever, optimizing the tradeoff between privacy and accuracy remains a\ncritical challenge. To our best knowledge, we propose the first DP-FL framework\n(namely UDP-FL), which universally harmonizes any randomization mechanism\n(e.g., an optimal one) with the Gaussian Moments Accountant (viz. DP-SGD) to\nsignificantly boost accuracy and convergence. Specifically, UDP-FL demonstrates\nenhanced model performance by mitigating the reliance on Gaussian noise. The\nkey mediator variable in this transformation is the R\\'enyi Differential\nPrivacy notion, which is carefully used to harmonize privacy budgets. We also\npropose an innovative method to theoretically analyze the convergence for DP-FL\n(including our UDP-FL ) based on mode connectivity analysis. Moreover, we\nevaluate our UDP-FL through extensive experiments benchmarked against\nstate-of-the-art (SOTA) methods, demonstrating superior performance on both\nprivacy guarantees and model performance. Notably, UDP-FL exhibits substantial\nresilience against different inference attacks, indicating a significant\nadvance in safeguarding sensitive data in federated learning environments.\n","authors":["Shuya Feng","Meisam Mohammady","Hanbin Hong","Shenao Yan","Ashish Kundu","Binghui Wang","Yuan Hong"],"pdf_url":"https://arxiv.org/pdf/2407.14710v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.09017v3","updated":"2024-07-24T01:15:20Z","published":"2024-07-12T06:10:01Z","title":"AI-Driven Guided Response for Security Operation Centers with Microsoft\n  Copilot for Security","summary":"  Security operation centers contend with a constant stream of security\nincidents, ranging from straightforward to highly complex. To address this, we\ndeveloped Copilot Guided Response (CGR), an industry-scale ML architecture that\nguides security analysts across three key tasks -- (1) investigation, providing\nessential historical context by identifying similar incidents; (2) triaging to\nascertain the nature of the incident -- whether it is a true positive, false\npositive, or benign positive; and (3) remediation, recommending tailored\ncontainment actions. CGR is integrated into the Microsoft Defender XDR product\nand deployed worldwide, generating millions of recommendations across thousands\nof customers. Our extensive evaluation, incorporating internal evaluation,\ncollaboration with security experts, and customer feedback, demonstrates that\nCGR delivers high-quality recommendations across all three tasks. We provide a\ncomprehensive overview of the CGR architecture, setting a precedent as the\nfirst cybersecurity company to openly discuss these capabilities in such depth.\nAdditionally, we GUIDE, the largest public collection of real-world security\nincidents, spanning 13M evidences across 1M annotated incidents. By enabling\nresearchers and practitioners to conduct research on real-world data, GUIDE\nadvances the state of cybersecurity and supports the development of\nnext-generation machine learning systems.\n","authors":["Scott Freitas","Jovan Kalajdjieski","Amir Gharib","Robert McCann"],"pdf_url":"https://arxiv.org/pdf/2407.09017v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.11933v2","updated":"2024-07-24T01:13:35Z","published":"2024-02-19T08:19:26Z","title":"SLADE: Detecting Dynamic Anomalies in Edge Streams without Labels via\n  Self-Supervised Learning","summary":"  To detect anomalies in real-world graphs, such as social, email, and\nfinancial networks, various approaches have been developed. While they\ntypically assume static input graphs, most real-world graphs grow over time,\nnaturally represented as edge streams. In this context, we aim to achieve three\ngoals: (a) instantly detecting anomalies as they occur, (b) adapting to\ndynamically changing states, and (c) handling the scarcity of dynamic anomaly\nlabels. In this paper, we propose SLADE (Self-supervised Learning for Anomaly\nDetection in Edge Streams) for rapid detection of dynamic anomalies in edge\nstreams, without relying on labels. SLADE detects the shifts of nodes into\nabnormal states by observing deviations in their interaction patterns over\ntime. To this end, it trains a deep neural network to perform two\nself-supervised tasks: (a) minimizing drift in node representations and (b)\ngenerating long-term interaction patterns from short-term ones. Failure in\nthese tasks for a node signals its deviation from the norm. Notably, the neural\nnetwork and tasks are carefully designed so that all required operations can be\nperformed in constant time (w.r.t. the graph size) in response to each new edge\nin the input stream. In dynamic anomaly detection across four real-world\ndatasets, SLADE outperforms nine competing methods, even those leveraging label\nsupervision.\n","authors":["Jongha Lee","Sunwoo Kim","Kijung Shin"],"pdf_url":"https://arxiv.org/pdf/2402.11933v2.pdf","comment":"15 pages, 6 figures, To Appear in KDD 2024"},{"id":"http://arxiv.org/abs/2404.01039v2","updated":"2024-07-24T01:10:49Z","published":"2024-04-01T10:50:34Z","title":"A Survey on Hypergraph Neural Networks: An In-Depth and Step-By-Step\n  Guide","summary":"  Higher-order interactions (HOIs) are ubiquitous in real-world complex systems\nand applications. Investigation of deep learning for HOIs, thus, has become a\nvaluable agenda for the data mining and machine learning communities. As\nnetworks of HOIs are expressed mathematically as hypergraphs, hypergraph neural\nnetworks (HNNs) have emerged as a powerful tool for representation learning on\nhypergraphs. Given the emerging trend, we present the first survey dedicated to\nHNNs, with an in-depth and step-by-step guide. Broadly, the present survey\noverviews HNN architectures, training strategies, and applications. First, we\nbreak existing HNNs down into four design components: (i) input features, (ii)\ninput structures, (iii) message-passing schemes, and (iv) training strategies.\nSecond, we examine how HNNs address and learn HOIs with each of their\ncomponents. Third, we overview the recent applications of HNNs in\nrecommendation, bioinformatics and medical science, time series analysis, and\ncomputer vision. Lastly, we conclude with a discussion on limitations and\nfuture directions.\n","authors":["Sunwoo Kim","Soo Yong Lee","Yue Gao","Alessia Antelmi","Mirko Polato","Kijung Shin"],"pdf_url":"https://arxiv.org/pdf/2404.01039v2.pdf","comment":"To appear in KDD 2024 (survey paper)"},{"id":"http://arxiv.org/abs/2112.08222v5","updated":"2024-07-24T01:06:36Z","published":"2021-12-15T15:57:33Z","title":"Guaranteed Trajectory Tracking under Learned Dynamics with Contraction\n  Metrics and Disturbance Estimation","summary":"  This paper presents an approach to trajectory-centric learning control based\non contraction metrics and disturbance estimation for nonlinear systems subject\nto matched uncertainties. The approach uses deep neural networks to learn\nuncertain dynamics while still providing guarantees of transient tracking\nperformance throughout the learning phase. Within the proposed approach, a\ndisturbance estimation law is adopted to estimate the pointwise value of the\nuncertainty, with pre-computable estimation error bounds (EEBs). The learned\ndynamics, the estimated disturbances, and the EEBs are then incorporated in a\nrobust Riemann energy condition to compute the control law that guarantees\nexponential convergence of actual trajectories to desired ones throughout the\nlearning phase, even when the learned model is poor. On the other hand, with\nimproved accuracy, the learned model can help improve the robustness of the\ntracking controller, e.g., against input delays, and can be incorporated to\nplan better trajectories with improved performance, e.g., lower energy\nconsumption and shorter travel time.The proposed framework is validated on a\nplanar quadrotor example.\n","authors":["Pan Zhao","Ziyao Guo","Yikun Cheng","Aditya Gahlawat","Hyungsoo Kang","Naira Hovakimyan"],"pdf_url":"https://arxiv.org/pdf/2112.08222v5.pdf","comment":"18 pages, 8 figures"},{"id":"http://arxiv.org/abs/2407.16917v1","updated":"2024-07-24T00:44:52Z","published":"2024-07-24T00:44:52Z","title":"TelescopeML -- I. An End-to-End Python Package for Interpreting\n  Telescope Datasets through Training Machine Learning Models, Generating\n  Statistical Reports, and Visualizing Results","summary":"  We are on the verge of a revolutionary era in space exploration, thanks to\nadvancements in telescopes such as the James Webb Space Telescope\n(\\textit{JWST}). High-resolution, high signal-to-noise spectra from exoplanet\nand brown dwarf atmospheres have been collected over the past few decades,\nrequiring the development of accurate and reliable pipelines and tools for\ntheir analysis. Accurately and swiftly determining the spectroscopic parameters\nfrom the observational spectra of these objects is crucial for understanding\ntheir atmospheric composition and guiding future follow-up observations.\n\\texttt{TelescopeML} is a Python package developed to perform three main tasks:\n1. Process the synthetic astronomical datasets for training a CNN model and\nprepare the observational dataset for later use for prediction; 2. Train a CNN\nmodel by implementing the optimal hyperparameters; and 3. Deploy the trained\nCNN models on the actual observational data to derive the output spectroscopic\nparameters.\n","authors":[" Ehsan"," Gharib-Nezhad","Natasha E. Batalha","Hamed Valizadegan","Miguel J. S. Martinho","Mahdi Habibi","Gopal Nookula"],"pdf_url":"https://arxiv.org/pdf/2407.16917v1.pdf","comment":"Please find the accepted paper with complete reference list at\n  https://joss.theoj.org/papers/10.21105/joss.06346"},{"id":"http://arxiv.org/abs/2403.01046v4","updated":"2024-07-24T00:32:35Z","published":"2024-03-02T00:33:45Z","title":"A Library of Mirrors: Deep Neural Nets in Low Dimensions are Convex\n  Lasso Models with Reflection Features","summary":"  We prove that training neural networks on 1-D data is equivalent to solving\nconvex Lasso problems with discrete, explicitly defined dictionary matrices. We\nconsider neural networks with piecewise linear activations and depths ranging\nfrom 2 to an arbitrary but finite number of layers. We first show that\ntwo-layer networks with piecewise linear activations are equivalent to Lasso\nmodels using a discrete dictionary of ramp functions, with breakpoints\ncorresponding to the training data points. In certain general architectures\nwith absolute value or ReLU activations, a third layer surprisingly creates\nfeatures that reflect the training data about themselves. Additional layers\nprogressively generate reflections of these reflections. The Lasso\nrepresentation provides valuable insights into the analysis of globally optimal\nnetworks, elucidating their solution landscapes and enabling closed-form\nsolutions in certain special cases. Numerical results show that reflections\nalso occur when optimizing standard deep networks using standard non-convex\noptimizers. Additionally, we demonstrate our theory with autoregressive time\nseries models.\n","authors":["Emi Zeger","Yifei Wang","Aaron Mishkin","Tolga Ergen","Emmanuel Candès","Mert Pilanci"],"pdf_url":"https://arxiv.org/pdf/2403.01046v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16912v1","updated":"2024-07-24T00:13:00Z","published":"2024-07-24T00:13:00Z","title":"Cross-Domain Policy Transfer by Representation Alignment via\n  Multi-Domain Behavioral Cloning","summary":"  Transferring learned skills across diverse situations remains a fundamental\nchallenge for autonomous agents, particularly when agents are not allowed to\ninteract with an exact target setup. While prior approaches have predominantly\nfocused on learning domain translation, they often struggle with handling\nsignificant domain gaps or out-of-distribution tasks. In this paper, we present\na simple approach for cross-domain policy transfer that learns a shared latent\nrepresentation across domains and a common abstract policy on top of it. Our\napproach leverages multi-domain behavioral cloning on unaligned trajectories of\nproxy tasks and employs maximum mean discrepancy (MMD) as a regularization term\nto encourage cross-domain alignment. The MMD regularization better preserves\nstructures of latent state distributions than commonly used\ndomain-discriminative distribution matching, leading to higher transfer\nperformance. Moreover, our approach involves training only one multi-domain\npolicy, which makes extension easier than existing methods. Empirical\nevaluations demonstrate the efficacy of our method across various domain\nshifts, especially in scenarios where exact domain translation is challenging,\nsuch as cross-morphology or cross-viewpoint settings. Our ablation studies\nfurther reveal that multi-domain behavioral cloning implicitly contributes to\nrepresentation alignment alongside domain-adversarial regularization.\n","authors":["Hayato Watahiki","Ryo Iwase","Ryosuke Unno","Yoshimasa Tsuruoka"],"pdf_url":"https://arxiv.org/pdf/2407.16912v1.pdf","comment":"CoLLAs 2024 (Oral). Code:\n  https://github.com/hwatahiki/portable-latent-policy"}],"Multimedia":[{"id":"http://arxiv.org/abs/2407.17274v1","updated":"2024-07-24T13:39:51Z","published":"2024-07-24T13:39:51Z","title":"Revolutionizing Text-to-Image Retrieval as Autoregressive Token-to-Voken\n  Generation","summary":"  Text-to-image retrieval is a fundamental task in multimedia processing,\naiming to retrieve semantically relevant cross-modal content. Traditional\nstudies have typically approached this task as a discriminative problem,\nmatching the text and image via the cross-attention mechanism (one-tower\nframework) or in a common embedding space (two-tower framework). Recently,\ngenerative cross-modal retrieval has emerged as a new research line, which\nassigns images with unique string identifiers and generates the target\nidentifier as the retrieval target. Despite its great potential, existing\ngenerative approaches are limited due to the following issues: insufficient\nvisual information in identifiers, misalignment with high-level semantics, and\nlearning gap towards the retrieval target. To address the above issues, we\npropose an autoregressive voken generation method, named AVG. AVG tokenizes\nimages into vokens, i.e., visual tokens, and innovatively formulates the\ntext-to-image retrieval task as a token-to-voken generation problem. AVG\ndiscretizes an image into a sequence of vokens as the identifier of the image,\nwhile maintaining the alignment with both the visual information and high-level\nsemantics of the image. Additionally, to bridge the learning gap between\ngenerative training and the retrieval target, we incorporate discriminative\ntraining to modify the learning direction during token-to-voken training.\nExtensive experiments demonstrate that AVG achieves superior results in both\neffectiveness and efficiency.\n","authors":["Yongqi Li","Hongru Cai","Wenjie Wang","Leigang Qu","Yinwei Wei","Wenjie Li","Liqiang Nie","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2407.17274v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2407.17205v1","updated":"2024-07-24T12:04:25Z","published":"2024-07-24T12:04:25Z","title":"The Sketchfab 3D Creative Commons Collection (S3D3C)","summary":"  The technology to capture, create, and use three-dimensional (3D) models has\nbecome increasingly accessible in recent years.\n  With increasing numbers of use cases for 3D models and collections of rapidly\nincreasing size, better methods to analyze the content of 3D models are\nrequired.\n  While previously proposed 3D model collections for research purposes exist,\nthese often contain only untextured geometry and are typically designed for a\nspecific application, which limits their use in quantitative evaluations of\nmodern 3D model analysis methods.\n  In this paper, we introduce the Sketchfab 3D Creative Commons Collection\n(S3D3C), a new 3D model research collection consisting of 40,802 creative\ncommons licensed models downloaded from the 3D model platform Sketchfab.\n  By including popular freely available models with a wide variety of technical\nproperties, such as textures, materials, and animations, we enable its use in\nthe evaluation of state-of-the-art geometry-based and view-based 3D model\nanalysis and retrieval techniques.\n","authors":["Florian Spiess","Raphael Waltenspül","Heiko Schuldt"],"pdf_url":"https://arxiv.org/pdf/2407.17205v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.01976v2","updated":"2024-07-24T11:45:48Z","published":"2024-07-02T06:29:05Z","title":"A Bounding Box is Worth One Token: Interleaving Layout and Text in a\n  Large Language Model for Document Understanding","summary":"  Recently, many studies have demonstrated that exclusively incorporating\nOCR-derived text and spatial layouts with large language models (LLMs) can be\nhighly effective for document understanding tasks. However, existing methods\nthat integrate spatial layouts with text have limitations, such as producing\noverly long text sequences or failing to fully leverage the autoregressive\ntraits of LLMs. In this work, we introduce Interleaving Layout and Text in a\nLarge Language Model (LayTextLLM)} for document understanding. In particular,\nLayTextLLM projects each bounding box to a single embedding and interleaves it\nwith text, efficiently avoiding long sequence issues while leveraging\nautoregressive traits of LLMs. LayTextLLM not only streamlines the interaction\nof layout and textual data but also shows enhanced performance in Key\nInformation Extraction (KIE) and Visual Question Answering (VQA). Comprehensive\nbenchmark evaluations reveal significant improvements, with a 27.2% increase on\nKIE tasks and 12.0% on VQA tasks compared to previous state-of-the-art document\nunderstanding MLLMs, as well as a 15.1% improvement over other SOTA OCR-based\nLLMs on KIE tasks.\n","authors":["Jinghui Lu","Haiyang Yu","Yanjie Wang","Yongjie Ye","Jingqun Tang","Ziwei Yang","Binghong Wu","Qi Liu","Hao Feng","Han Wang","Hao Liu","Can Huang"],"pdf_url":"https://arxiv.org/pdf/2407.01976v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17028v1","updated":"2024-07-24T06:15:28Z","published":"2024-07-24T06:15:28Z","title":"Enhancing Environmental Monitoring through Multispectral Imaging: The\n  WasteMS Dataset for Semantic Segmentation of Lakeside Waste","summary":"  Environmental monitoring of lakeside green areas is crucial for environmental\nprotection. Compared to manual inspections, computer vision technologies offer\na more efficient solution when deployed on-site. Multispectral imaging provides\ndiverse information about objects under different spectrums, aiding in the\ndifferentiation between waste and lakeside lawn environments. This study\nintroduces WasteMS, the first multispectral dataset established for the\nsemantic segmentation of lakeside waste. WasteMS includes a diverse range of\nwaste types in lawn environments, captured under various lighting conditions.\nWe implemented a rigorous annotation process to label waste in images.\nRepresentative semantic segmentation frameworks were used to evaluate\nsegmentation accuracy using WasteMS. Challenges encountered when using WasteMS\nfor segmenting waste on lakeside lawns were discussed. The WasteMS dataset is\navailable at https://github.com/zhuqinfeng1999/WasteMS.\n","authors":["Qinfeng Zhu","Ningxin Weng","Lei Fan","Yuanzhi Cai"],"pdf_url":"https://arxiv.org/pdf/2407.17028v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16248v2","updated":"2024-07-24T05:56:55Z","published":"2024-07-23T07:36:54Z","title":"Spatiotemporal Graph Guided Multi-modal Network for Livestreaming\n  Product Retrieval","summary":"  With the rapid expansion of e-commerce, more consumers have become accustomed\nto making purchases via livestreaming. Accurately identifying the products\nbeing sold by salespeople, i.e., livestreaming product retrieval (LPR), poses a\nfundamental and daunting challenge. The LPR task encompasses three primary\ndilemmas in real-world scenarios: 1) the recognition of intended products from\ndistractor products present in the background; 2) the video-image heterogeneity\nthat the appearance of products showcased in live streams often deviates\nsubstantially from standardized product images in stores; 3) there are numerous\nconfusing products with subtle visual nuances in the shop. To tackle these\nchallenges, we propose the Spatiotemporal Graphing Multi-modal Network (SGMN).\nFirst, we employ a text-guided attention mechanism that leverages the spoken\ncontent of salespeople to guide the model to focus toward intended products,\nemphasizing their salience over cluttered background products. Second, a\nlong-range spatiotemporal graph network is further designed to achieve both\ninstance-level interaction and frame-level matching, solving the misalignment\ncaused by video-image heterogeneity. Third, we propose a multi-modal hard\nexample mining, assisting the model in distinguishing highly similar products\nwith fine-grained features across the video-image-text domain. Through\nextensive quantitative and qualitative experiments, we demonstrate the superior\nperformance of our proposed SGMN model, surpassing the state-of-the-art methods\nby a substantial margin. The code is available at\nhttps://github.com/Huxiaowan/SGMN.\n","authors":["Xiaowan Hu","Yiyi Chen","Yan Li","Minquan Wang","Haoqian Wang","Quan Chen","Han Li","Peng Jiang"],"pdf_url":"https://arxiv.org/pdf/2407.16248v2.pdf","comment":"9 pages, 12 figures"},{"id":"http://arxiv.org/abs/2407.16977v1","updated":"2024-07-24T03:45:35Z","published":"2024-07-24T03:45:35Z","title":"Selective Vision-Language Subspace Projection for Few-shot CLIP","summary":"  Vision-language models such as CLIP are capable of mapping the different\nmodality data into a unified feature space, enabling zero/few-shot inference by\nmeasuring the similarity of given images and texts. However, most existing\nmethods overlook modality gaps in CLIP's encoded features, which is shown as\nthe text and image features lie far apart from each other, resulting in limited\nclassification performance. To tackle this issue, we introduce a method called\nSelective Vision-Language Subspace Projection (SSP), which incorporates local\nimage features and utilizes them as a bridge to enhance the alignment between\nimage-text pairs. Specifically, our SSP framework comprises two parallel\nmodules: a vision projector and a language projector. Both projectors utilize\nlocal image features to span the respective subspaces for image and texts,\nthereby projecting the image and text features into their respective subspaces\nto achieve alignment. Moreover, our approach entails only training-free matrix\ncalculations and can be seamlessly integrated into advanced CLIP-based few-shot\nlearning frameworks. Extensive experiments on 11 datasets have demonstrated\nSSP's superior text-image alignment capabilities, outperforming the\nstate-of-the-art alignment methods. The code is available at\nhttps://github.com/zhuhsingyuu/SSP\n","authors":["Xingyu Zhu","Beier Zhu","Yi Tan","Shuo Wang","Yanbin Hao","Hanwang Zhang"],"pdf_url":"https://arxiv.org/pdf/2407.16977v1.pdf","comment":"Accepted to ACM MultiMedia 2024"},{"id":"http://arxiv.org/abs/2407.16552v2","updated":"2024-07-24T01:09:36Z","published":"2024-07-23T15:05:55Z","title":"MicroEmo: Time-Sensitive Multimodal Emotion Recognition with\n  Micro-Expression Dynamics in Video Dialogues","summary":"  Multimodal Large Language Models (MLLMs) have demonstrated remarkable\nmultimodal emotion recognition capabilities, integrating multimodal cues from\nvisual, acoustic, and linguistic contexts in the video to recognize human\nemotional states. However, existing methods ignore capturing local facial\nfeatures of temporal dynamics of micro-expressions and do not leverage the\ncontextual dependencies of the utterance-aware temporal segments in the video,\nthereby limiting their expected effectiveness to a certain extent. In this\nwork, we propose MicroEmo, a time-sensitive MLLM aimed at directing attention\nto the local facial micro-expression dynamics and the contextual dependencies\nof utterance-aware video clips. Our model incorporates two key architectural\ncontributions: (1) a global-local attention visual encoder that integrates\nglobal frame-level timestamp-bound image features with local facial features of\ntemporal dynamics of micro-expressions; (2) an utterance-aware video Q-Former\nthat captures multi-scale and contextual dependencies by generating visual\ntoken sequences for each utterance segment and for the entire video then\ncombining them. Preliminary qualitative experiments demonstrate that in a new\nExplainable Multimodal Emotion Recognition (EMER) task that exploits\nmulti-modal and multi-faceted clues to predict emotions in an open-vocabulary\n(OV) manner, MicroEmo demonstrates its effectiveness compared with the latest\nmethods.\n","authors":["Liyun Zhang"],"pdf_url":"https://arxiv.org/pdf/2407.16552v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17536v1","updated":"2024-07-24T07:32:26Z","published":"2024-07-24T07:32:26Z","title":"Improved symbolic drum style classification with grammar-based\n  hierarchical representations","summary":"  Deep learning models have become a critical tool for analysis and\nclassification of musical data. These models operate either on the audio\nsignal, e.g. waveform or spectrogram, or on a symbolic representation, such as\nMIDI. In the latter, musical information is often reduced to basic features,\ni.e. durations, pitches and velocities. Most existing works then rely on\ngeneric tokenization strategies from classical natural language processing, or\nmatrix representations, e.g. piano roll. In this work, we evaluate how enriched\nrepresentations of symbolic data can impact deep models, i.e. Transformers and\nRNN, for music style classification. In particular, we examine representations\nthat explicitly incorporate musical information implicitly present in MIDI-like\nencodings, such as rhythmic organization, and show that they outperform generic\ntokenization strategies. We introduce a new tree-based representation of MIDI\ndata built upon a context-free musical grammar. We show that this grammar\nrepresentation accurately encodes high-level rhythmic information and\noutperforms existing encodings on the GrooveMIDI Dataset for drumming style\nclassification, while being more compact and parameter-efficient.\n","authors":["Léo Géré","Philippe Rigaux","Nicolas Audebert"],"pdf_url":"https://arxiv.org/pdf/2407.17536v1.pdf","comment":"International Society for Music Information Retrieval Conference\n  2024, Nov 2024, San Francisco, United States"}]},"2024-07-25T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2407.18248v1","updated":"2024-07-25T17:59:16Z","published":"2024-07-25T17:59:16Z","title":"Self-Training with Direct Preference Optimization Improves\n  Chain-of-Thought Reasoning","summary":"  Effective training of language models (LMs) for mathematical reasoning tasks\ndemands high-quality supervised fine-tuning data. Besides obtaining annotations\nfrom human experts, a common alternative is sampling from larger and more\npowerful LMs. However, this knowledge distillation approach can be costly and\nunstable, particularly when relying on closed-source, proprietary LMs like\nGPT-4, whose behaviors are often unpredictable. In this work, we demonstrate\nthat the reasoning abilities of small-scale LMs can be enhanced through\nself-training, a process where models learn from their own outputs. We also\nshow that the conventional self-training can be further augmented by a\npreference learning algorithm called Direct Preference Optimization (DPO). By\nintegrating DPO into self-training, we leverage preference data to guide LMs\ntowards more accurate and diverse chain-of-thought reasoning. We evaluate our\nmethod across various mathematical reasoning tasks using different base models.\nOur experiments show that this approach not only improves LMs' reasoning\nperformance but also offers a more cost-effective and scalable solution\ncompared to relying on large proprietary LMs.\n","authors":["Tianduo Wang","Shichen Li","Wei Lu"],"pdf_url":"https://arxiv.org/pdf/2407.18248v1.pdf","comment":"ACL 2024. Code and data are available at\n  https://github.com/TianduoWang/DPO-ST"},{"id":"http://arxiv.org/abs/2407.18242v1","updated":"2024-07-25T17:57:12Z","published":"2024-07-25T17:57:12Z","title":"LoRA-Pro: Are Low-Rank Adapters Properly Optimized?","summary":"  Low-Rank Adaptation, also known as LoRA, has emerged as a prominent method\nfor parameter-efficient fine-tuning foundation models by re-parameterizing the\noriginal matrix into the product of two low-rank matrices. Despite its\nefficiency, LoRA often yields inferior performance compared to full\nfine-tuning. In this paper, we propose LoRA-Pro to bridge this performance gap.\nFirstly, we delve into the optimization processes in LoRA and full fine-tuning.\nWe reveal that while LoRA employs low-rank approximation, it neglects to\napproximate the optimization process of full fine-tuning. To address this, we\nintroduce a novel concept called the \"equivalent gradient.\" This virtual\ngradient makes the optimization process on the re-parameterized matrix\nequivalent to LoRA, which can be used to quantify the differences between LoRA\nand full fine-tuning. The equivalent gradient is derived from the gradients of\nmatrices $A$ and $B$. To narrow the performance gap, our approach minimizes the\ndifferences between the equivalent gradient and the gradient obtained from full\nfine-tuning during the optimization process. By solving this objective, we\nderive optimal closed-form solutions for updating matrices $A$ and $B$. Our\nmethod constrains the optimization process, shrinking the performance gap\nbetween LoRA and full fine-tuning. Extensive experiments on natural language\nprocessing tasks validate the effectiveness of our method.\n","authors":["Zhengbo Wang","Jian Liang"],"pdf_url":"https://arxiv.org/pdf/2407.18242v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10444v2","updated":"2024-07-25T17:51:50Z","published":"2024-03-15T16:28:22Z","title":"Block Verification Accelerates Speculative Decoding","summary":"  Speculative decoding is an effective method for lossless acceleration of\nlarge language models during inference. It uses a fast model to draft a block\nof tokens which are then verified in parallel by the target model, and provides\na guarantee that the output is distributed identically to a sample from the\ntarget model. In prior works, draft verification is performed independently\ntoken-by-token. Surprisingly, we show that this approach is not optimal. We\npropose Block Verification, a simple draft verification algorithm that verifies\nthe entire block jointly and provides additional wall-clock speedup. We prove\nthat the proposed mechanism is optimal in the expected number of tokens\nproduced each iteration and specifically is never worse than the standard\ntoken-level verification. Empirically, block verification provides modest but\nconsistent wall-clock speedups over the standard token verification algorithm\nof 5%-8% in a range of tasks and datasets. Given that block verification does\nnot increase code complexity, maintains the strong lossless guarantee of the\nstandard speculative decoding verification algorithm, cannot deteriorate\nperformance, and, in fact, consistently improves it, it can be used as a good\ndefault in speculative decoding implementations.\n","authors":["Ziteng Sun","Uri Mendlovic","Yaniv Leviathan","Asaf Aharoni","Ahmad Beirami","Jae Hun Ro","Ananda Theertha Suresh"],"pdf_url":"https://arxiv.org/pdf/2403.10444v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.18219v1","updated":"2024-07-25T17:35:59Z","published":"2024-07-25T17:35:59Z","title":"Recursive Introspection: Teaching Language Model Agents How to\n  Self-Improve","summary":"  A central piece in enabling intelligent agentic behavior in foundation models\nis to make them capable of introspecting upon their behavior, reasoning, and\ncorrecting their mistakes as more computation or interaction is available. Even\nthe strongest proprietary large language models (LLMs) do not quite exhibit the\nability of continually improving their responses sequentially, even in\nscenarios where they are explicitly told that they are making a mistake. In\nthis paper, we develop RISE: Recursive IntroSpEction, an approach for\nfine-tuning LLMs to introduce this capability, despite prior work hypothesizing\nthat this capability may not be possible to attain. Our approach prescribes an\niterative fine-tuning procedure, which attempts to teach the model how to alter\nits response after having executed previously unsuccessful attempts to solve a\nhard test-time problem, with optionally additional environment feedback. RISE\nposes fine-tuning for a single-turn prompt as solving a multi-turn Markov\ndecision process (MDP), where the initial state is the prompt. Inspired by\nprinciples in online imitation learning and reinforcement learning, we propose\nstrategies for multi-turn data collection and training so as to imbue an LLM\nwith the capability to recursively detect and correct its previous mistakes in\nsubsequent iterations. Our experiments show that RISE enables Llama2, Llama3,\nand Mistral models to improve themselves with more turns on math reasoning\ntasks, outperforming several single-turn strategies given an equal amount of\ninference-time computation. We also find that RISE scales well, often attaining\nlarger benefits with more capable models. Our analysis shows that RISE makes\nmeaningful improvements to responses to arrive at the correct solution for\nchallenging prompts, without disrupting one-turn abilities as a result of\nexpressing more complex distributions.\n","authors":["Yuxiao Qu","Tianjun Zhang","Naman Garg","Aviral Kumar"],"pdf_url":"https://arxiv.org/pdf/2407.18219v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.18213v1","updated":"2024-07-25T17:26:41Z","published":"2024-07-25T17:26:41Z","title":"Exploring Scaling Trends in LLM Robustness","summary":"  Language model capabilities predictably improve from scaling a model's size\nand training data. Motivated by this, increasingly large language models have\nbeen trained, yielding an array of impressive capabilities. Yet these models\nare vulnerable to adversarial prompts, such as \"jailbreaks\" that hijack models\nto perform undesired behaviors, posing a significant risk of misuse. Prior work\nindicates that computer vision models become more robust with model and data\nscaling, raising the question: does language model robustness also improve with\nscale? We study this question empirically, finding that larger models respond\nsubstantially better to adversarial training, but there is little to no benefit\nfrom model scale in the absence of explicit defenses.\n","authors":["Nikolhaus Howe","Michał Zajac","Ian McKenzie","Oskar Hollinsworth","Tom Tseng","Pierre-Luc Bacon","Adam Gleave"],"pdf_url":"https://arxiv.org/pdf/2407.18213v1.pdf","comment":"31 pages"},{"id":"http://arxiv.org/abs/2406.05981v3","updated":"2024-07-25T17:20:48Z","published":"2024-06-10T02:47:55Z","title":"ShiftAddLLM: Accelerating Pretrained LLMs via Post-Training\n  Multiplication-Less Reparameterization","summary":"  Large language models (LLMs) have shown impressive performance on language\ntasks but face challenges when deployed on resource-constrained devices due to\ntheir extensive parameters and reliance on dense multiplications, resulting in\nhigh memory demands and latency bottlenecks. Shift-and-add reparameterization\noffers a promising solution by replacing costly multiplications with\nhardware-friendly primitives in both the attention and multi-layer perceptron\n(MLP) layers of an LLM. However, current reparameterization techniques require\ntraining from scratch or full parameter fine-tuning to restore accuracy, which\nis resource-intensive for LLMs. To address this, we propose accelerating\npretrained LLMs through post-training shift-and-add reparameterization,\ncreating efficient multiplication-free models, dubbed ShiftAddLLM.\nSpecifically, we quantize each weight matrix into binary matrices paired with\ngroup-wise scaling factors. The associated multiplications are reparameterized\ninto (1) shifts between activations and scaling factors and (2) queries and\nadds according to the binary matrices. To reduce accuracy loss, we present a\nmulti-objective optimization method to minimize both weight and output\nactivation reparameterization errors. Additionally, based on varying\nsensitivity across layers to reparameterization, we develop an automated bit\nallocation strategy to further reduce memory usage and latency. Experiments on\nfive LLM families and eight tasks consistently validate the effectiveness of\nShiftAddLLM, achieving average perplexity improvements of 5.6 and 22.7 points\nat comparable or lower latency compared to the most competitive quantized LLMs\nat 3 and 2 bits, respectively, and more than 80% memory and energy reductions\nover the original LLMs. Codes and models are available at\nhttps://github.com/GATECH-EIC/ShiftAddLLM.\n","authors":["Haoran You","Yipin Guo","Yichao Fu","Wei Zhou","Huihong Shi","Xiaofan Zhang","Souvik Kundu","Amir Yazdanbakhsh","Yingyan Celine Lin"],"pdf_url":"https://arxiv.org/pdf/2406.05981v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07368v2","updated":"2024-07-25T17:18:01Z","published":"2024-06-11T15:34:43Z","title":"When Linear Attention Meets Autoregressive Decoding: Towards More\n  Effective and Efficient Linearized Large Language Models","summary":"  Autoregressive Large Language Models (LLMs) have achieved impressive\nperformance in language tasks but face two significant bottlenecks: (1)\nquadratic complexity in the attention module as the number of tokens increases,\nand (2) limited efficiency due to the sequential processing nature of\nautoregressive LLMs during generation. While linear attention and speculative\ndecoding offer potential solutions, their applicability and synergistic\npotential for enhancing autoregressive LLMs remain uncertain. We conduct the\nfirst comprehensive study on the efficacy of existing linear attention methods\nfor autoregressive LLMs, integrating them with speculative decoding. We\nintroduce an augmentation technique for linear attention that ensures\ncompatibility with speculative decoding, enabling more efficient training and\nserving of LLMs. Extensive experiments and ablation studies involving seven\nexisting linear attention models and five encoder/decoder-based LLMs\nconsistently validate the effectiveness of our augmented linearized LLMs.\nNotably, our approach achieves up to a 6.67 reduction in perplexity on the\nLLaMA model and up to a 2$\\times$ speedup during generation compared to prior\nlinear attention methods. Codes and models are available at\nhttps://github.com/GATECH-EIC/Linearized-LLM.\n","authors":["Haoran You","Yichao Fu","Zheng Wang","Amir Yazdanbakhsh","Yingyan Celine Lin"],"pdf_url":"https://arxiv.org/pdf/2406.07368v2.pdf","comment":"Accepted by ICML 2024; 17 pages; 10 figures; 16 tables"},{"id":"http://arxiv.org/abs/2403.14236v4","updated":"2024-07-25T16:52:15Z","published":"2024-03-21T08:54:24Z","title":"A Unified Framework for Model Editing","summary":"  ROME and MEMIT are largely believed to be two different model editing\nalgorithms, with the major difference between them being the ability to perform\nbatched edits. In this paper, we unify these two algorithms under a single\nconceptual umbrella, optimizing for the same goal, which we call the\npreservation-memorization objective. ROME uses an equality constraint to\noptimize this objective to perform one edit at a time, whereas MEMIT employs a\nmore flexible least-square constraint that allows for batched edits. We\ngeneralize ROME and enable batched editing with equality constraint in the form\nof EMMET - an Equality-constrained Mass Model Editing algorithm for\nTransformers, a new batched memory-editing algorithm. EMMET can perform\nbatched-edits up to a batch-size of 10,000, with very similar performance to\nMEMIT across multiple dimensions. With the introduction of EMMET, we truly\nunify ROME and MEMIT and show that both algorithms are equivalent in terms of\ntheir optimization objective, their abilities (singular and batched editing),\ntheir model editing performance and their limitations.\n","authors":["Akshat Gupta","Dev Sajnani","Gopala Anumanchipalli"],"pdf_url":"https://arxiv.org/pdf/2403.14236v4.pdf","comment":"Under review. To appear as poster at KnowledgeableLM Workshop\n  co-located with ACL 2024"},{"id":"http://arxiv.org/abs/2407.12835v2","updated":"2024-07-25T16:50:58Z","published":"2024-07-03T18:42:55Z","title":"Regurgitative Training: The Value of Real Data in Training Large\n  Language Models","summary":"  What happens if we train a new Large Language Model (LLM) using data that are\nat least partially generated by other LLMs? The explosive success of LLMs means\nthat a substantial amount of content online will be generated by LLMs rather\nthan humans, which will inevitably enter the training datasets of\nnext-generation LLMs. We evaluate the implications of such \"regurgitative\ntraining\" on LLM performance. Through fine-tuning GPT-3.5 with data generated\neither by itself or by other LLMs in a machine translation task, we find strong\nevidence that regurgitative training clearly handicaps the performance of LLMs.\nThe same performance loss of regurgitative training is observed on transformer\nmodels that we train from scratch. We find suggestive evidence that the\nperformance disadvantage of regurgitative training can be attributed to at\nleast two mechanisms: (1) higher error rates and (2) lower lexical diversity in\nLLM-generated data as compared to real data. Based on these mechanisms, we\npropose and evaluate three different strategies to mitigate the performance\nloss of regurgitative training. First, we devise data-driven metrics to gauge\nthe quality of each LLM-generated data instance, and then carry out an ordered\ntraining process where high-quality data are added before low-quality ones.\nSecond, we combine data generated by multiple different LLMs (as an attempt to\nincrease lexical diversity). Third, we train an AI detection classifier to\ndifferentiate between LLM- and human-generated data, and include LLM-generated\ndata in the order of resemblance to human-generated data. All three strategies\ncan improve the performance of regurgitative training to some extent but are\nnot always able to fully close the gap from training with real data. Our\nresults highlight the value of real, human-generated data in training LLMs,\nwhich cannot be easily substituted by synthetic, LLM-generated data.\n","authors":["Jinghui Zhang","Dandan Qiao","Mochen Yang","Qiang Wei"],"pdf_url":"https://arxiv.org/pdf/2407.12835v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16470v2","updated":"2024-07-25T16:31:39Z","published":"2024-07-23T13:40:54Z","title":"Machine Translation Hallucination Detection for Low and High Resource\n  Languages using Large Language Models","summary":"  Recent advancements in massively multilingual machine translation systems\nhave significantly enhanced translation accuracy; however, even the best\nperforming systems still generate hallucinations, severely impacting user\ntrust. Detecting hallucinations in Machine Translation (MT) remains a critical\nchallenge, particularly since existing methods excel with High-Resource\nLanguages (HRLs) but exhibit substantial limitations when applied to\nLow-Resource Languages (LRLs). This paper evaluates hallucination detection\napproaches using Large Language Models (LLMs) and semantic similarity within\nmassively multilingual embeddings. Our study spans 16 language directions,\ncovering HRLs, LRLs, with diverse scripts. We find that the choice of model is\nessential for performance. On average, for HRLs, Llama3-70B outperforms the\nprevious state of the art by as much as 0.16 MCC (Matthews Correlation\nCoefficient). However, for LRLs we observe that Claude Sonnet outperforms other\nLLMs on average by 0.03 MCC. The key takeaway from our study is that LLMs can\nachieve performance comparable or even better than previously proposed models,\ndespite not being explicitly trained for any machine translation task. However,\ntheir advantage is less significant for LRLs.\n","authors":["Kenza Benkirane","Laura Gongas","Shahar Pelles","Naomi Fuchs","Joshua Darmon","Pontus Stenetorp","David Ifeoluwa Adelani","Eduardo Sánchez"],"pdf_url":"https://arxiv.org/pdf/2407.16470v2.pdf","comment":"Authors Kenza Benkirane and Laura Gongas contributed equally to this\n  work"},{"id":"http://arxiv.org/abs/2404.19708v2","updated":"2024-07-25T16:16:46Z","published":"2024-04-30T17:00:32Z","title":"Harmonic LLMs are Trustworthy","summary":"  We introduce an intuitive method to test the robustness (stability and\nexplainability) of any black-box LLM in real-time via its local deviation from\nharmoniticity, denoted as $\\gamma$. To the best of our knowledge this is the\nfirst completely model-agnostic and unsupervised method of measuring the\nrobustness of any given response from an LLM, based upon the model itself\nconforming to a purely mathematical standard. To show general application and\nimmediacy of results, we measure $\\gamma$ in 10 popular LLMs (ChatGPT,\nClaude-2.1, Claude3.0, GPT-4, GPT-4o, Smaug-72B, Mixtral-8x7B, Llama2-7B,\nMistral-7B and MPT-7B) across thousands of queries in three objective domains:\nWebQA, ProgrammingQA, and TruthfulQA. Across all models and domains tested,\nhuman annotation confirms that $\\gamma \\to 0$ indicates trustworthiness, and\nconversely searching higher values of $\\gamma$ easily exposes examples of\nhallucination, a fact that enables efficient adversarial prompt generation\nthrough stochastic gradient ascent in $\\gamma$. The low-$\\gamma$ leaders among\nthe models in the respective domains are GPT-4o, GPT-4, and Smaug-72B,\nproviding evidence that mid-size open-source models can win out against large\ncommercial models.\n","authors":["Nicholas S. Kersting","Mohammad Rahman","Suchismitha Vedala","Yang Wang"],"pdf_url":"https://arxiv.org/pdf/2404.19708v2.pdf","comment":"15 pages, 2 figures, 16 tables; added Claude-3.0, GPT-4o, Mistral-7B,\n  Mixtral-8x7B, and more annotation for other models"},{"id":"http://arxiv.org/abs/2407.18147v1","updated":"2024-07-25T15:58:19Z","published":"2024-07-25T15:58:19Z","title":"The FIGNEWS Shared Task on News Media Narratives","summary":"  We present an overview of the FIGNEWS shared task, organized as part of the\nArabicNLP 2024 conference co-located with ACL 2024. The shared task addresses\nbias and propaganda annotation in multilingual news posts. We focus on the\nearly days of the Israel War on Gaza as a case study. The task aims to foster\ncollaboration in developing annotation guidelines for subjective tasks by\ncreating frameworks for analyzing diverse narratives highlighting potential\nbias and propaganda. In a spirit of fostering and encouraging diversity, we\naddress the problem from a multilingual perspective, namely within five\nlanguages: English, French, Arabic, Hebrew, and Hindi. A total of 17 teams\nparticipated in two annotation subtasks: bias (16 teams) and propaganda (6\nteams). The teams competed in four evaluation tracks: guidelines development,\nannotation quality, annotation quantity, and consistency. Collectively, the\nteams produced 129,800 data points. Key findings and implications for the field\nare discussed.\n","authors":["Wajdi Zaghouani","Mustafa Jarrar","Nizar Habash","Houda Bouamor","Imed Zitouni","Mona Diab","Samhaa R. El-Beltagy","Muhammed AbuOdeh"],"pdf_url":"https://arxiv.org/pdf/2407.18147v1.pdf","comment":"18 pages, 10 tables, 1 figure, accepted to ArabicNLP 2024 co-located\n  with ACL 2024"},{"id":"http://arxiv.org/abs/2407.18129v1","updated":"2024-07-25T15:36:48Z","published":"2024-07-25T15:36:48Z","title":"Dallah: A Dialect-Aware Multimodal Large Language Model for Arabic","summary":"  Recent advancements have significantly enhanced the capabilities of\nMultimodal Large Language Models (MLLMs) in generating and understanding\nimage-to-text content. Despite these successes, progress is predominantly\nlimited to English due to the scarcity of high quality multimodal resources in\nother languages. This limitation impedes the development of competitive models\nin languages such as Arabic. To alleviate this situation, we introduce an\nefficient Arabic multimodal assistant, dubbed Dallah, that utilizes an advanced\nlanguage model based on LLaMA-2 to facilitate multimodal interactions. Dallah\ndemonstrates state-of-the-art performance in Arabic MLLMs. Through fine-tuning\nsix Arabic dialects, Dallah showcases its capability to handle complex\ndialectal interactions incorporating both textual and visual elements. The\nmodel excels in two benchmark tests: one evaluating its performance on Modern\nStandard Arabic (MSA) and another specifically designed to assess dialectal\nresponses. Beyond its robust performance in multimodal interaction tasks,\nDallah has the potential to pave the way for further development of\ndialect-aware Arabic MLLMs.\n","authors":["Fakhraddin Alwajih","Gagan Bhatia","Muhammad Abdul-Mageed"],"pdf_url":"https://arxiv.org/pdf/2407.18129v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.18119v1","updated":"2024-07-25T15:27:08Z","published":"2024-07-25T15:27:08Z","title":"Tracking linguistic information in transformer-based sentence embeddings\n  through targeted sparsification","summary":"  Analyses of transformer-based models have shown that they encode a variety of\nlinguistic information from their textual input. While these analyses have shed\na light on the relation between linguistic information on one side, and\ninternal architecture and parameters on the other, a question remains\nunanswered: how is this linguistic information reflected in sentence\nembeddings? Using datasets consisting of sentences with known structure, we\ntest to what degree information about chunks (in particular noun, verb or\nprepositional phrases), such as grammatical number, or semantic role, can be\nlocalized in sentence embeddings. Our results show that such information is not\ndistributed over the entire sentence embedding, but rather it is encoded in\nspecific regions. Understanding how the information from an input text is\ncompressed into sentence embeddings helps understand current transformer models\nand help build future explainable neural models.\n","authors":["Vivi Nastase","Paola Merlo"],"pdf_url":"https://arxiv.org/pdf/2407.18119v1.pdf","comment":"12 pages, 9 figures, 1 table, published in RepL4NLP 2024"},{"id":"http://arxiv.org/abs/2407.18078v1","updated":"2024-07-25T14:36:18Z","published":"2024-07-25T14:36:18Z","title":"PEFT-U: Parameter-Efficient Fine-Tuning for User Personalization","summary":"  The recent emergence of Large Language Models (LLMs) has heralded a new era\nof human-AI interaction. These sophisticated models, exemplified by Chat-GPT\nand its successors, have exhibited remarkable capabilities in language\nunderstanding. However, as these LLMs have undergone exponential growth, a\ncrucial dimension that remains understudied is the personalization of these\nmodels. Large foundation models such as GPT-3 etc. focus on creating a\nuniversal model that serves a broad range of tasks and users. This approach\nemphasizes the model's generalization capabilities, treating users as a\ncollective rather than as distinct individuals. While practical for many common\napplications, this one-size-fits-all approach often fails to address the rich\ntapestry of human diversity and individual needs. To explore this issue we\nintroduce the PEFT-U Benchmark: a new dataset for building and evaluating NLP\nmodels for user personalization. \\datasetname{} consists of a series of\nuser-centered tasks containing diverse and individualized expressions where the\npreferences of users can potentially differ for the same input. Using PEFT-U,\nwe explore the challenge of efficiently personalizing LLMs to accommodate\nuser-specific preferences in the context of diverse user-centered tasks.\n","authors":["Christopher Clarke","Yuzhao Heng","Lingjia Tang","Jason Mars"],"pdf_url":"https://arxiv.org/pdf/2407.18078v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.18061v1","updated":"2024-07-25T14:16:08Z","published":"2024-07-25T14:16:08Z","title":"Difficulty Estimation and Simplification of French Text Using LLMs","summary":"  We leverage generative large language models for language learning\napplications, focusing on estimating the difficulty of foreign language texts\nand simplifying them to lower difficulty levels. We frame both tasks as\nprediction problems and develop a difficulty classification model using labeled\nexamples, transfer learning, and large language models, demonstrating superior\naccuracy compared to previous approaches. For simplification, we evaluate the\ntrade-off between simplification quality and meaning preservation, comparing\nzero-shot and fine-tuned performances of large language models. We show that\nmeaningful text simplifications can be obtained with limited fine-tuning. Our\nexperiments are conducted on French texts, but our methods are\nlanguage-agnostic and directly applicable to other foreign languages.\n","authors":["Henri Jamet","Yash Raj Shrestha","Michalis Vlachos"],"pdf_url":"https://arxiv.org/pdf/2407.18061v1.pdf","comment":"10 pages, 4 figures"},{"id":"http://arxiv.org/abs/2407.18058v1","updated":"2024-07-25T14:15:05Z","published":"2024-07-25T14:15:05Z","title":"I can listen but cannot read: An evaluation of two-tower multimodal\n  systems for instrument recognition","summary":"  Music two-tower multimodal systems integrate audio and text modalities into a\njoint audio-text space, enabling direct comparison between songs and their\ncorresponding labels. These systems enable new approaches for classification\nand retrieval, leveraging both modalities. Despite the promising results they\nhave shown for zero-shot classification and retrieval tasks, closer inspection\nof the embeddings is needed. This paper evaluates the inherent zero-shot\nproperties of joint audio-text spaces for the case-study of instrument\nrecognition. We present an evaluation and analysis of two-tower systems for\nzero-shot instrument recognition and a detailed analysis of the properties of\nthe pre-joint and joint embeddings spaces. Our findings suggest that audio\nencoders alone demonstrate good quality, while challenges remain within the\ntext encoder or joint space projection. Specifically, two-tower systems exhibit\nsensitivity towards specific words, favoring generic prompts over musically\ninformed ones. Despite the large size of textual encoders, they do not yet\nleverage additional textual context or infer instruments accurately from their\ndescriptions. Lastly, a novel approach for quantifying the semantic\nmeaningfulness of the textual space leveraging an instrument ontology is\nproposed. This method reveals deficiencies in the systems' understanding of\ninstruments and provides evidence of the need for fine-tuning text encoders on\nmusical data.\n","authors":["Yannis Vasilakis","Rachel Bittner","Johan Pauwels"],"pdf_url":"https://arxiv.org/pdf/2407.18058v1.pdf","comment":"Accepted to ISMIR 2024"},{"id":"http://arxiv.org/abs/2407.18035v1","updated":"2024-07-25T13:29:37Z","published":"2024-07-25T13:29:37Z","title":"RestoreAgent: Autonomous Image Restoration Agent via Multimodal Large\n  Language Models","summary":"  Natural images captured by mobile devices often suffer from multiple types of\ndegradation, such as noise, blur, and low light. Traditional image restoration\nmethods require manual selection of specific tasks, algorithms, and execution\nsequences, which is time-consuming and may yield suboptimal results. All-in-one\nmodels, though capable of handling multiple tasks, typically support only a\nlimited range and often produce overly smooth, low-fidelity outcomes due to\ntheir broad data distribution fitting. To address these challenges, we first\ndefine a new pipeline for restoring images with multiple degradations, and then\nintroduce RestoreAgent, an intelligent image restoration system leveraging\nmultimodal large language models. RestoreAgent autonomously assesses the type\nand extent of degradation in input images and performs restoration through (1)\ndetermining the appropriate restoration tasks, (2) optimizing the task\nsequence, (3) selecting the most suitable models, and (4) executing the\nrestoration. Experimental results demonstrate the superior performance of\nRestoreAgent in handling complex degradation, surpassing human experts.\nFurthermore, the system modular design facilitates the fast integration of new\ntasks and models, enhancing its flexibility and scalability for various\napplications.\n","authors":["Haoyu Chen","Wenbo Li","Jinjin Gu","Jingjing Ren","Sixiang Chen","Tian Ye","Renjing Pei","Kaiwen Zhou","Fenglong Song","Lei Zhu"],"pdf_url":"https://arxiv.org/pdf/2407.18035v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.06543v2","updated":"2024-07-25T13:27:08Z","published":"2022-12-13T12:56:55Z","title":"Improving Stance Detection by Leveraging Measurement Knowledge from\n  Social Sciences: A Case Study of Dutch Political Tweets and Traditional\n  Gender Role Division","summary":"  Stance detection (SD) concerns automatically determining the viewpoint (i.e.,\nin favour of, against, or neutral) of a text's author towards a target. SD has\nbeen applied to many research topics, among which the detection of stances\nbehind political tweets is an important one. In this paper, we apply SD to a\ndataset of tweets from official party accounts in the Netherlands between 2017\nand 2021, with a focus on stances towards traditional gender role division, a\ndividing issue between (some) Dutch political parties. To implement and improve\nSD of traditional gender role division, we propose to leverage an established\nsurvey instrument from social sciences, which has been validated for the\npurpose of measuring attitudes towards traditional gender role division. Based\non our experiments, we show that using such a validated survey instrument helps\nto improve SD performance.\n","authors":["Qixiang Fang","Anastasia Giachanou","Ayoub Bagheri"],"pdf_url":"https://arxiv.org/pdf/2212.06543v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.01799v2","updated":"2024-07-25T13:12:47Z","published":"2024-04-02T09:58:57Z","title":"PATCH! Psychometrics-AssisTed benCHmarking of Large Language Models: A\n  Case Study of Proficiency in 8th Grade Mathematics","summary":"  Many existing benchmarks of large (multimodal) language models (LLMs) focus\non measuring LLMs' academic proficiency, often with also an interest in\ncomparing model performance with human test takers. While these benchmarks have\nproven key to the development of LLMs, they suffer from several limitations,\nincluding questionable measurement quality (e.g., Do they measure what they are\nsupposed to in a reliable way?), lack of quality assessment on the item level\n(e.g., Are some items more important or difficult than others?) and unclear\nhuman population reference (e.g., To whom can the model be compared?). In\nresponse to these challenges, we propose leveraging knowledge from\npsychometrics - a field dedicated to the measurement of latent variables like\nacademic proficiency - into LLM benchmarking. We make three primary\ncontributions. First, we introduce PATCH: a novel framework for\n{P}sychometrics-{A}ssis{T}ed ben{CH}marking of LLMs. PATCH addresses the\naforementioned limitations, presenting a new direction for LLM benchmark\nresearch. Second, we implement PATCH by measuring GPT-4 and Gemini-Pro-Vision's\nproficiency in 8th grade mathematics against 56 human populations. We show that\nadopting a psychometrics-based approach yields evaluation outcomes that diverge\nfrom those based on existing benchmarking practices. Third, we release 4\nhigh-quality datasets to support measuring and comparing LLM proficiency in\ngrade school mathematics and science against human populations.\n","authors":["Qixiang Fang","Daniel L. Oberski","Dong Nguyen"],"pdf_url":"https://arxiv.org/pdf/2404.01799v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19146v2","updated":"2024-07-25T13:09:18Z","published":"2024-06-27T13:02:43Z","title":"Resolving Discrepancies in Compute-Optimal Scaling of Language Models","summary":"  Kaplan et al. and Hoffmann et al. developed influential scaling laws for the\noptimal model size as a function of the compute budget, but these laws yield\nsubstantially different predictions. We explain the discrepancy by reproducing\nthe Kaplan scaling law on two datasets (OpenWebText2 and RefinedWeb) and\nidentifying three factors causing the difference: last layer computational\ncost, warmup duration, and scale-dependent optimizer tuning. With these factors\ncorrected, we obtain excellent agreement with the Hoffmann et al. (i.e.,\n\"Chinchilla\") scaling law. Counter to a hypothesis of Hoffmann et al., we find\nthat careful learning rate decay is not essential for the validity of their\nscaling law. As a secondary result, we derive scaling laws for the optimal\nlearning rate and batch size, finding that tuning the AdamW $\\beta_2$ parameter\nis essential at lower batch sizes.\n","authors":["Tomer Porian","Mitchell Wortsman","Jenia Jitsev","Ludwig Schmidt","Yair Carmon"],"pdf_url":"https://arxiv.org/pdf/2406.19146v2.pdf","comment":"Fixing bug in small models with tuned LR"},{"id":"http://arxiv.org/abs/2407.18008v1","updated":"2024-07-25T13:04:25Z","published":"2024-07-25T13:04:25Z","title":"GermanPartiesQA: Benchmarking Commercial Large Language Models for\n  Political Bias and Sycophancy","summary":"  LLMs are changing the way humans create and interact with content,\npotentially affecting citizens' political opinions and voting decisions. As\nLLMs increasingly shape our digital information ecosystems, auditing to\nevaluate biases, sycophancy, or steerability has emerged as an active field of\nresearch. In this paper, we evaluate and compare the alignment of six LLMs by\nOpenAI, Anthropic, and Cohere with German party positions and evaluate\nsycophancy based on a prompt experiment. We contribute to evaluating political\nbias and sycophancy in multi-party systems across major commercial LLMs. First,\nwe develop the benchmark dataset GermanPartiesQA based on the Voting Advice\nApplication Wahl-o-Mat covering 10 state and 1 national elections between 2021\nand 2023. In our study, we find a left-green tendency across all examined LLMs.\nWe then conduct our prompt experiment for which we use the benchmark and\nsociodemographic data of leading German parliamentarians to evaluate changes in\nLLMs responses. To differentiate between sycophancy and steerabilty, we use 'I\nam [politician X], ...' and 'You are [politician X], ...' prompts. Against our\nexpectations, we do not observe notable differences between prompting 'I am'\nand 'You are'. While our findings underscore that LLM responses can be\nideologically steered with political personas, they suggest that observed\nchanges in LLM outputs could be better described as personalization to the\ngiven context rather than sycophancy.\n","authors":["Jan Batzner","Volker Stocker","Stefan Schmid","Gjergji Kasneci"],"pdf_url":"https://arxiv.org/pdf/2407.18008v1.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2407.18003v1","updated":"2024-07-25T12:56:22Z","published":"2024-07-25T12:56:22Z","title":"Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache\n  Consumption","summary":"  Large Language Models (LLMs), epitomized by ChatGPT' s release in late 2022,\nhave revolutionized various industries with their advanced language\ncomprehension. However, their efficiency is challenged by the Transformer\narchitecture' s struggle with handling long texts. KV-Cache has emerged as a\npivotal solution to this issue, converting the time complexity of token\ngeneration from quadratic to linear, albeit with increased GPU memory overhead\nproportional to conversation length. With the development of the LLM community\nand academia, various KV-Cache compression methods have been proposed. In this\nreview, we dissect the various properties of KV-Cache and elaborate on various\nmethods currently used to optimize the KV-Cache space usage of LLMs. These\nmethods span the pre-training phase, deployment phase, and inference phase, and\nwe summarize the commonalities and differences among these methods.\nAdditionally, we list some metrics for evaluating the long-text capabilities of\nlarge language models, from both efficiency and capability perspectives. Our\nreview thus sheds light on the evolving landscape of LLM optimization, offering\ninsights into future advancements in this dynamic field.\n","authors":["Shi Luohe","Zhang Hongyi","Yao Yao","Li Zuchao","Zhao Hai"],"pdf_url":"https://arxiv.org/pdf/2407.18003v1.pdf","comment":"to be published in CoLM 2024"},{"id":"http://arxiv.org/abs/2407.17997v1","updated":"2024-07-25T12:44:45Z","published":"2024-07-25T12:44:45Z","title":"On the Effect of Purely Synthetic Training Data for Different Automatic\n  Speech Recognition Architectures","summary":"  In this work we evaluate the utility of synthetic data for training automatic\nspeech recognition (ASR). We use the ASR training data to train a\ntext-to-speech (TTS) system similar to FastSpeech-2. With this TTS we reproduce\nthe original training data, training ASR systems solely on synthetic data. For\nASR, we use three different architectures, attention-based encoder-decoder,\nhybrid deep neural network hidden Markov model and a Gaussian mixture hidden\nMarkov model, showing the different sensitivity of the models to synthetic data\ngeneration. In order to extend previous work, we present a number of ablation\nstudies on the effectiveness of synthetic vs. real training data for ASR. In\nparticular we focus on how the gap between training on synthetic and real data\nchanges by varying the speaker embedding or by scaling the model size. For the\nlatter we show that the TTS models generalize well, even when training scores\nindicate overfitting.\n","authors":["Nick Rossenbach","Benedikt Hilmes","Ralf Schlüter"],"pdf_url":"https://arxiv.org/pdf/2407.17997v1.pdf","comment":"Accepted at the SynData4GenAI 2024 workshop"},{"id":"http://arxiv.org/abs/2407.17974v1","updated":"2024-07-25T12:09:41Z","published":"2024-07-25T12:09:41Z","title":"What does Kiki look like? Cross-modal associations between speech sounds\n  and visual shapes in vision-and-language models","summary":"  Humans have clear cross-modal preferences when matching certain novel words\nto visual shapes. Evidence suggests that these preferences play a prominent\nrole in our linguistic processing, language learning, and the origins of\nsignal-meaning mappings. With the rise of multimodal models in AI, such as\nvision- and-language (VLM) models, it becomes increasingly important to uncover\nthe kinds of visio-linguistic associations these models encode and whether they\nalign with human representations. Informed by experiments with humans, we probe\nand compare four VLMs for a well-known human cross-modal preference, the\nbouba-kiki effect. We do not find conclusive evidence for this effect but\nsuggest that results may depend on features of the models, such as architecture\ndesign, model size, and training details. Our findings inform discussions on\nthe origins of the bouba-kiki effect in human cognition and future developments\nof VLMs that align well with human cross-modal associations.\n","authors":["Tessa Verhoef","Kiana Shahrasbi","Tom Kouwenhoven"],"pdf_url":"https://arxiv.org/pdf/2407.17974v1.pdf","comment":"Appeared at the 13th edition of the Workshop on Cognitive Modeling\n  and Computational Linguistics (CMCL 2024)"},{"id":"http://arxiv.org/abs/2404.00725v2","updated":"2024-07-25T11:37:54Z","published":"2024-03-31T15:55:49Z","title":"The Larger the Better? Improved LLM Code-Generation via Budget\n  Reallocation","summary":"  It is a common belief that large language models (LLMs) are better than\nsmaller-sized ones. However, larger models also require significantly more time\nand compute during inference. This begs the question: what happens when both\nmodels operate under the same budget? (e.g., compute, run-time). To address\nthis question, we analyze code generation LLMs of various sizes and make\ncomparisons such as running a 70B model once vs. generating five outputs from a\n13B model. We consider a standard unit-test setup, which can be used to select\nthe correct output from the smaller model. Our findings reveal that the\nrepeated use of smaller models can yield consistent improvements, with gains of\nup to 15% across five tasks. On the other hand, in scenarios where unit-tests\nare unavailable, a ranking-based selection of candidates from the smaller model\nfalls short of the performance of a single output from larger ones. Our results\nhighlight the potential of using smaller models instead of larger ones, and the\nimportance of studying approaches for ranking LLM outputs.\n","authors":["Michael Hassid","Tal Remez","Jonas Gehring","Roy Schwartz","Yossi Adi"],"pdf_url":"https://arxiv.org/pdf/2404.00725v2.pdf","comment":"COLM 2024"},{"id":"http://arxiv.org/abs/2407.17960v1","updated":"2024-07-25T11:29:27Z","published":"2024-07-25T11:29:27Z","title":"The Curious Case of Representational Alignment: Unravelling\n  Visio-Linguistic Tasks in Emergent Communication","summary":"  Natural language has the universal properties of being compositional and\ngrounded in reality. The emergence of linguistic properties is often\ninvestigated through simulations of emergent communication in referential\ngames. However, these experiments have yielded mixed results compared to\nsimilar experiments addressing linguistic properties of human language. Here we\naddress representational alignment as a potential contributing factor to these\nresults. Specifically, we assess the representational alignment between agent\nimage representations and between agent representations and input images. Doing\nso, we confirm that the emergent language does not appear to encode human-like\nconceptual visual features, since agent image representations drift away from\ninputs whilst inter-agent alignment increases. We moreover identify a strong\nrelationship between inter-agent alignment and topographic similarity, a common\nmetric for compositionality, and address its consequences. To address these\nissues, we introduce an alignment penalty that prevents representational drift\nbut interestingly does not improve performance on a compositional\ndiscrimination task. Together, our findings emphasise the key role\nrepresentational alignment plays in simulations of language emergence.\n","authors":["Tom Kouwenhoven","Max Peeperkorn","Bram van Dijk","Tessa Verhoef"],"pdf_url":"https://arxiv.org/pdf/2407.17960v1.pdf","comment":"Appeared at the 13th edition of the Workshop on Cognitive Modeling\n  and Computational Linguistics (CMCL 2024)"},{"id":"http://arxiv.org/abs/2407.17940v1","updated":"2024-07-25T10:58:42Z","published":"2024-07-25T10:58:42Z","title":"Positive Text Reframing under Multi-strategy Optimization","summary":"  Differing from sentiment transfer, positive reframing seeks to substitute\nnegative perspectives with positive expressions while preserving the original\nmeaning. With the emergence of pre-trained language models (PLMs), it is\npossible to achieve acceptable results by fine-tuning PLMs. Nevertheless,\ngenerating fluent, diverse and task-constrained reframing text remains a\nsignificant challenge. To tackle this issue, a \\textbf{m}ulti-\\textbf{s}trategy\n\\textbf{o}ptimization \\textbf{f}ramework (MSOF) is proposed in this paper.\nStarting from the objective of positive reframing, we first design positive\nsentiment reward and content preservation reward to encourage the model to\ntransform the negative expressions of the original text while ensuring the\nintegrity and consistency of the semantics. Then, different decoding\noptimization approaches are introduced to improve the quality of text\ngeneration. Finally, based on the modeling formula of positive reframing, we\npropose a multi-dimensional re-ranking method that further selects candidate\nsentences from three dimensions: strategy consistency, text similarity and\nfluency. Extensive experiments on two Seq2Seq PLMs, BART and T5, demonstrate\nour framework achieves significant improvements on unconstrained and controlled\npositive reframing tasks.\n","authors":["Shutong Jia","Biwei Cao","Qingqing Gao","Jiuxin Cao","Bo Liu"],"pdf_url":"https://arxiv.org/pdf/2407.17940v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17914v1","updated":"2024-07-25T10:08:37Z","published":"2024-07-25T10:08:37Z","title":"Modelling Multimodal Integration in Human Concept Processing with\n  Vision-and-Language Models","summary":"  Representations from deep neural networks (DNNs) have proven remarkably\npredictive of neural activity involved in both visual and linguistic\nprocessing. Despite these successes, most studies to date concern unimodal\nDNNs, encoding either visual or textual input but not both. Yet, there is\ngrowing evidence that human meaning representations integrate linguistic and\nsensory-motor information. Here we investigate whether the integration of\nmultimodal information operated by current vision-and-language DNN models\n(VLMs) leads to representations that are more aligned with human brain activity\nthan those obtained by language-only and vision-only DNNs. We focus on fMRI\nresponses recorded while participants read concept words in the context of\neither a full sentence or an accompanying picture. Our results reveal that VLM\nrepresentations correlate more strongly than language- and vision-only DNNs\nwith activations in brain areas functionally related to language processing. A\ncomparison between different types of visuo-linguistic architectures shows that\nrecent generative VLMs tend to be less brain-aligned than previous\narchitectures with lower performance on downstream applications. Moreover,\nthrough an additional analysis comparing brain vs. behavioural alignment across\nmultiple VLMs, we show that -- with one remarkable exception -- representations\nthat strongly align with behavioural judgments do not correlate highly with\nbrain responses. This indicates that brain similarity does not go hand in hand\nwith behavioural similarity, and vice versa.\n","authors":["Anna Bavaresco","Marianne de Heer Kloots","Sandro Pezzelle","Raquel Fernández"],"pdf_url":"https://arxiv.org/pdf/2407.17914v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17900v1","updated":"2024-07-25T09:42:24Z","published":"2024-07-25T09:42:24Z","title":"The Power of Combining Data and Knowledge: GPT-4o is an Effective\n  Interpreter of Machine Learning Models in Predicting Lymph Node Metastasis of\n  Lung Cancer","summary":"  Lymph node metastasis (LNM) is a crucial factor in determining the initial\ntreatment for patients with lung cancer, yet accurate preoperative diagnosis of\nLNM remains challenging. Recently, large language models (LLMs) have garnered\nsignificant attention due to their remarkable text generation capabilities.\nLeveraging the extensive medical knowledge learned from vast corpora, LLMs can\nestimate probabilities for clinical problems, though their performance has\nhistorically been inferior to data-driven machine learning models. In this\npaper, we propose a novel ensemble method that combines the medical knowledge\nacquired by LLMs with the latent patterns identified by machine learning models\nto enhance LNM prediction performance. Initially, we developed machine learning\nmodels using patient data. We then designed a prompt template to integrate the\npatient data with the predicted probability from the machine learning model.\nSubsequently, we instructed GPT-4o, the most advanced LLM developed by OpenAI,\nto estimate the likelihood of LNM based on patient data and then adjust the\nestimate using the machine learning output. Finally, we collected three outputs\nfrom the GPT-4o using the same prompt and ensembled these results as the final\nprediction. Using the proposed method, our models achieved an AUC value of\n0.765 and an AP value of 0.415 for LNM prediction, significantly improving\npredictive performance compared to baseline machine learning models. The\nexperimental results indicate that GPT-4o can effectively leverage its medical\nknowledge and the probabilities predicted by machine learning models to achieve\nmore accurate LNM predictions. These findings demonstrate that LLMs can perform\nwell in clinical risk prediction tasks, offering a new paradigm for integrating\nmedical knowledge and patient data in clinical predictions.\n","authors":["Danqing Hu","Bing Liu","Xiaofeng Zhu","Nan Wu"],"pdf_url":"https://arxiv.org/pdf/2407.17900v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14888v2","updated":"2024-07-25T09:19:06Z","published":"2024-03-21T23:48:21Z","title":"AutoRE: Document-Level Relation Extraction with Large Language Models","summary":"  Large Language Models (LLMs) have demonstrated exceptional abilities in\ncomprehending and generating text, motivating numerous researchers to utilize\nthem for Information Extraction (IE) purposes, including Relation Extraction\n(RE). Nonetheless, most existing methods are predominantly designed for\nSentence-level Relation Extraction (SentRE) tasks, which typically encompass a\nrestricted set of relations and triplet facts within a single sentence.\nFurthermore, certain approaches resort to treating relations as candidate\nchoices integrated into prompt templates, leading to inefficient processing and\nsuboptimal performance when tackling Document-Level Relation Extraction (DocRE)\ntasks, which entail handling multiple relations and triplet facts distributed\nacross a given document, posing distinct challenges. To overcome these\nlimitations, we introduce AutoRE, an end-to-end DocRE model that adopts a novel\nRE extraction paradigm named RHF (Relation-Head-Facts). Unlike existing\napproaches, AutoRE does not rely on the assumption of known relation options,\nmaking it more reflective of real-world scenarios. Additionally, we have\ndeveloped an easily extensible RE framework using a Parameters Efficient Fine\nTuning (PEFT) algorithm (QLoRA). Our experiments on the RE-DocRED dataset\nshowcase AutoRE's best performance, achieving state-of-the-art results,\nsurpassing TAG by 10.03\\% and 9.03\\% respectively on the dev and test set. The\ncode is available\\url{https://github.com/THUDM/AutoRE} and the demonstration\nvideo is provided https://www.youtube.com/watch?v=IhKRsZUAxKk\n","authors":["Lilong Xue","Dan Zhang","Yuxiao Dong","Jie Tang"],"pdf_url":"https://arxiv.org/pdf/2403.14888v2.pdf","comment":"11 pages, 4 figures"},{"id":"http://arxiv.org/abs/2407.05750v2","updated":"2024-07-25T09:17:21Z","published":"2024-07-08T09:03:12Z","title":"Large Language Models Understand Layout","summary":"  Large language models (LLMs) demonstrate extraordinary abilities in a wide\nrange of natural language processing (NLP) tasks. In this paper, we show that,\nbeyond text understanding capability, LLMs are capable of processing text\nlayouts that are denoted by spatial markers. They are able to answer questions\nthat require explicit spatial perceiving and reasoning, while a drastic\nperformance drop is observed when the spatial markers from the original data\nare excluded. We perform a series of experiments with the GPT-3.5, Baichuan2,\nLlama2 and ChatGLM3 models on various types of layout-sensitive datasets for\nfurther analysis. The experimental results reveal that the layout understanding\nability of LLMs is mainly introduced by the coding data for pretraining, which\nis further enhanced at the instruction-tuning stage. In addition, layout\nunderstanding can be enhanced by integrating low-cost, auto-generated data\napproached by a novel text game. Finally, we show that layout understanding\nability is beneficial for building efficient visual question-answering (VQA)\nsystems.\n","authors":["Weiming Li","Manni Duan","Dong An","Yan Shao"],"pdf_url":"https://arxiv.org/pdf/2407.05750v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.02750v2","updated":"2024-07-25T09:16:05Z","published":"2024-02-05T06:06:47Z","title":"KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache","summary":"  Efficiently serving large language models (LLMs) requires batching of many\nrequests to reduce the cost per request. Yet, with larger batch sizes and\nlonger context lengths, the key-value (KV) cache, which stores attention keys\nand values to avoid re-computations, significantly increases memory demands and\nbecomes the new bottleneck in speed and memory usage. Additionally, the loading\nof the KV cache causes the computational core to be idle, which limits the\ninference speed. A straightforward and effective solution to reduce KV cache\nsize is quantization, which decreases the total bytes taken by KV cache.\nHowever, there is a lack of in-depth studies that explore the element\ndistribution of KV cache to understand the hardness and limitation of KV cache\nquantization. To fill the gap, we conducted a comprehensive study on the\nelement distribution in KV cache of popular LLMs. Our findings indicate that\nthe key cache should be quantized per-channel, i.e., group elements along the\nchannel dimension and quantize them together. In contrast, the value cache\nshould be quantized per-token. From this analysis, we developed a tuning-free\n2bit KV cache quantization algorithm named KIVI. With hardware-friendly\nimplementation, KIVI can enable Llama, Falcon, and Mistral models to maintain\nalmost the same quality while using $\\mathbf{2.6\\times}$ less peak memory\n(including model weight). This reduction in memory usage enables up to\n$\\mathbf{4\\times}$ larger batch size, bringing $\\mathbf{2.35\\times \\sim\n3.47\\times}$ throughput on real LLM inference workload. The source code is\navailable at https://github.com/jy-yuan/KIVI.\n","authors":["Zirui Liu","Jiayi Yuan","Hongye Jin","Shaochen Zhong","Zhaozhuo Xu","Vladimir Braverman","Beidi Chen","Xia Hu"],"pdf_url":"https://arxiv.org/pdf/2402.02750v2.pdf","comment":"ICML2024"},{"id":"http://arxiv.org/abs/2407.17876v1","updated":"2024-07-25T08:46:49Z","published":"2024-07-25T08:46:49Z","title":"A Large-Scale Sensitivity Analysis on Latent Embeddings and\n  Dimensionality Reductions for Text Spatializations","summary":"  The semantic similarity between documents of a text corpus can be visualized\nusing map-like metaphors based on two-dimensional scatterplot layouts. These\nlayouts result from a dimensionality reduction on the document-term matrix or a\nrepresentation within a latent embedding, including topic models. Thereby, the\nresulting layout depends on the input data and hyperparameters of the\ndimensionality reduction and is therefore affected by changes in them.\nFurthermore, the resulting layout is affected by changes in the input data and\nhyperparameters of the dimensionality reduction. However, such changes to the\nlayout require additional cognitive efforts from the user. In this work, we\npresent a sensitivity study that analyzes the stability of these layouts\nconcerning (1) changes in the text corpora, (2) changes in the hyperparameter,\nand (3) randomness in the initialization. Our approach has two stages: data\nmeasurement and data analysis. First, we derived layouts for the combination of\nthree text corpora and six text embeddings and a grid-search-inspired\nhyperparameter selection of the dimensionality reductions. Afterward, we\nquantified the similarity of the layouts through ten metrics, concerning local\nand global structures and class separation. Second, we analyzed the resulting\n42817 tabular data points in a descriptive statistical analysis. From this, we\nderived guidelines for informed decisions on the layout algorithm and highlight\nspecific hyperparameter settings. We provide our implementation as a Git\nrepository at\nhttps://github.com/hpicgs/Topic-Models-and-Dimensionality-Reduction-Sensitivity-Study\nand results as Zenodo archive at https://doi.org/10.5281/zenodo.12772898.\n","authors":["Daniel Atzberger","Tim Cech","Willy Scheibel","Jürgen Döllner","Michael Behrisch","Tobias Schreck"],"pdf_url":"https://arxiv.org/pdf/2407.17876v1.pdf","comment":"To be published at IEEE VIS 2024 conference"},{"id":"http://arxiv.org/abs/2407.17874v1","updated":"2024-07-25T08:44:04Z","published":"2024-07-25T08:44:04Z","title":"Improving Domain-Specific ASR with LLM-Generated Contextual Descriptions","summary":"  End-to-end automatic speech recognition (E2E ASR) systems have significantly\nimproved speech recognition through training on extensive datasets. Despite\nthese advancements, they still struggle to accurately recognize domain specific\nwords, such as proper nouns and technical terminologies. To address this\nproblem, we propose a method to utilize the state-of-the-art Whisper without\nmodifying its architecture, preserving its generalization performance while\nenabling it to leverage descriptions effectively. Moreover, we propose two\nadditional training techniques to improve the domain specific ASR: decoder\nfine-tuning, and context perturbation. We also propose a method to use a Large\nLanguage Model (LLM) to generate descriptions with simple metadata, when\ndescriptions are unavailable. Our experiments demonstrate that proposed methods\nnotably enhance domain-specific ASR accuracy on real-life datasets, with\nLLM-generated descriptions outperforming human-crafted ones in effectiveness.\n","authors":["Jiwon Suh","Injae Na","Woohwan Jung"],"pdf_url":"https://arxiv.org/pdf/2407.17874v1.pdf","comment":"Accepted to INTERSPEECH 2024"},{"id":"http://arxiv.org/abs/2407.17870v1","updated":"2024-07-25T08:42:53Z","published":"2024-07-25T08:42:53Z","title":"Is the Digital Forensics and Incident Response Pipeline Ready for\n  Text-Based Threats in LLM Era?","summary":"  In the era of generative AI, the widespread adoption of Neural Text\nGenerators (NTGs) presents new cybersecurity challenges, particularly within\nthe realms of Digital Forensics and Incident Response (DFIR). These challenges\nprimarily involve the detection and attribution of sources behind advanced\nattacks like spearphishing and disinformation campaigns. As NTGs evolve, the\ntask of distinguishing between human and NTG-authored texts becomes critically\ncomplex. This paper rigorously evaluates the DFIR pipeline tailored for\ntext-based security systems, specifically focusing on the challenges of\ndetecting and attributing authorship of NTG-authored texts. By introducing a\nnovel human-NTG co-authorship text attack, termed CS-ACT, our study uncovers\nsignificant vulnerabilities in traditional DFIR methodologies, highlighting\ndiscrepancies between ideal scenarios and real-world conditions. Utilizing 14\ndiverse datasets and 43 unique NTGs, up to the latest GPT-4, our research\nidentifies substantial vulnerabilities in the forensic profiling phase,\nparticularly in attributing authorship to NTGs. Our comprehensive evaluation\npoints to factors such as model sophistication and the lack of distinctive\nstyle within NTGs as significant contributors for these vulnerabilities. Our\nfindings underscore the necessity for more sophisticated and adaptable\nstrategies, such as incorporating adversarial learning, stylizing NTGs, and\nimplementing hierarchical attribution through the mapping of NTG lineages to\nenhance source attribution. This sets the stage for future research and the\ndevelopment of more resilient text-based security systems.\n","authors":["Avanti Bhandarkar","Ronald Wilson","Anushka Swarup","Mengdi Zhu","Damon Woodard"],"pdf_url":"https://arxiv.org/pdf/2407.17870v1.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible"},{"id":"http://arxiv.org/abs/2407.17866v1","updated":"2024-07-25T08:36:58Z","published":"2024-07-25T08:36:58Z","title":"Financial Statement Analysis with Large Language Models","summary":"  We investigate whether an LLM can successfully perform financial statement\nanalysis in a way similar to a professional human analyst. We provide\nstandardized and anonymous financial statements to GPT4 and instruct the model\nto analyze them to determine the direction of future earnings. Even without any\nnarrative or industry-specific information, the LLM outperforms financial\nanalysts in its ability to predict earnings changes. The LLM exhibits a\nrelative advantage over human analysts in situations when the analysts tend to\nstruggle. Furthermore, we find that the prediction accuracy of the LLM is on\npar with the performance of a narrowly trained state-of-the-art ML model. LLM\nprediction does not stem from its training memory. Instead, we find that the\nLLM generates useful narrative insights about a company's future performance.\nLastly, our trading strategies based on GPT's predictions yield a higher Sharpe\nratio and alphas than strategies based on other models. Taken together, our\nresults suggest that LLMs may take a central role in decision-making.\n","authors":["Alex Kim","Maximilian Muhn","Valeri Nikolaev"],"pdf_url":"https://arxiv.org/pdf/2407.17866v1.pdf","comment":"Previously posted on SSRN (May 21, 2024). See\n  http://papers.ssrn.com/sol3/papers.cfm?abstract_id=4835311"},{"id":"http://arxiv.org/abs/2407.17863v1","updated":"2024-07-25T08:33:23Z","published":"2024-07-25T08:33:23Z","title":"factgenie: A Framework for Span-based Evaluation of Generated Texts","summary":"  We present factgenie: a framework for annotating and visualizing word spans\nin textual model outputs. Annotations can capture various span-based phenomena\nsuch as semantic inaccuracies or irrelevant text. With factgenie, the\nannotations can be collected both from human crowdworkers and large language\nmodels. Our framework consists of a web interface for data visualization and\ngathering text annotations, powered by an easily extensible codebase.\n","authors":["Zdeněk Kasner","Ondřej Plátek","Patrícia Schmidtová","Simone Balloccu","Ondřej Dušek"],"pdf_url":"https://arxiv.org/pdf/2407.17863v1.pdf","comment":"Accepted to INLG 2024 (System Demonstrations)"},{"id":"http://arxiv.org/abs/2407.17862v1","updated":"2024-07-25T08:31:57Z","published":"2024-07-25T08:31:57Z","title":"Exploring Description-Augmented Dataless Intent Classification","summary":"  In this work, we introduce several schemes to leverage description-augmented\nembedding similarity for dataless intent classification using current\nstate-of-the-art (SOTA) text embedding models. We report results of our methods\non four commonly used intent classification datasets and compare against\nprevious works of a similar nature. Our work shows promising results for\ndataless classification scaling to a large number of unseen intents. We show\ncompetitive results and significant improvements (+6.12\\% Avg.) over strong\nzero-shot baselines, all without training on labelled or task-specific data.\nFurthermore, we provide qualitative error analysis of the shortfalls of this\nmethodology to help guide future research in this area.\n","authors":["Ruoyu Hu","Foaad Khosmood","Abbas Edalat"],"pdf_url":"https://arxiv.org/pdf/2407.17862v1.pdf","comment":"Accepted to the 6th NLP for Conversational AI Workshop at ACL\n  2024(NLP4ConvAI)"},{"id":"http://arxiv.org/abs/2407.17854v1","updated":"2024-07-25T08:15:43Z","published":"2024-07-25T08:15:43Z","title":"Shapley Value-based Contrastive Alignment for Multimodal Information\n  Extraction","summary":"  The rise of social media and the exponential growth of multimodal\ncommunication necessitates advanced techniques for Multimodal Information\nExtraction (MIE). However, existing methodologies primarily rely on direct\nImage-Text interactions, a paradigm that often faces significant challenges due\nto semantic and modality gaps between images and text. In this paper, we\nintroduce a new paradigm of Image-Context-Text interaction, where large\nmultimodal models (LMMs) are utilized to generate descriptive textual context\nto bridge these gaps. In line with this paradigm, we propose a novel Shapley\nValue-based Contrastive Alignment (Shap-CA) method, which aligns both\ncontext-text and context-image pairs. Shap-CA initially applies the Shapley\nvalue concept from cooperative game theory to assess the individual\ncontribution of each element in the set of contexts, texts and images towards\ntotal semantic and modality overlaps. Following this quantitative evaluation, a\ncontrastive learning strategy is employed to enhance the interactive\ncontribution within context-text/image pairs, while minimizing the influence\nacross these pairs. Furthermore, we design an adaptive fusion module for\nselective cross-modal fusion. Extensive experiments across four MIE datasets\ndemonstrate that our method significantly outperforms existing state-of-the-art\nmethods.\n","authors":["Wen Luo","Yu Xia","Shen Tianshu","Sujian Li"],"pdf_url":"https://arxiv.org/pdf/2407.17854v1.pdf","comment":"Accepted at ACM Multimedia 2024"},{"id":"http://arxiv.org/abs/2407.17852v1","updated":"2024-07-25T08:08:55Z","published":"2024-07-25T08:08:55Z","title":"Scaling A Simple Approach to Zero-Shot Speech Recognition","summary":"  Despite rapid progress in increasing the language coverage of automatic\nspeech recognition, the field is still far from covering all languages with a\nknown writing script. Recent work showed promising results with a zero-shot\napproach requiring only a small amount of text data, however, accuracy heavily\ndepends on the quality of the used phonemizer which is often weak for unseen\nlanguages. In this paper, we present MMS Zero-shot a conceptually simpler\napproach based on romanization and an acoustic model trained on data in 1,078\ndifferent languages or three orders of magnitude more than prior art. MMS\nZero-shot reduces the average character error rate by a relative 46% over 100\nunseen languages compared to the best previous work. Moreover, the error rate\nof our approach is only 2.5x higher compared to in-domain supervised baselines,\nwhile our approach uses no labeled data for the evaluation languages at all.\n","authors":["Jinming Zhao","Vineel Pratap","Michael Auli"],"pdf_url":"https://arxiv.org/pdf/2407.17852v1.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2402.13055v2","updated":"2024-07-25T08:07:39Z","published":"2024-02-20T14:43:39Z","title":"Identifying Semantic Induction Heads to Understand In-Context Learning","summary":"  Although large language models (LLMs) have demonstrated remarkable\nperformance, the lack of transparency in their inference logic raises concerns\nabout their trustworthiness. To gain a better understanding of LLMs, we conduct\na detailed analysis of the operations of attention heads and aim to better\nunderstand the in-context learning of LLMs. Specifically, we investigate\nwhether attention heads encode two types of relationships between tokens\npresent in natural languages: the syntactic dependency parsed from sentences\nand the relation within knowledge graphs. We find that certain attention heads\nexhibit a pattern where, when attending to head tokens, they recall tail tokens\nand increase the output logits of those tail tokens. More crucially, the\nformulation of such semantic induction heads has a close correlation with the\nemergence of the in-context learning ability of language models. The study of\nsemantic attention heads advances our understanding of the intricate operations\nof attention heads in transformers, and further provides new insights into the\nin-context learning of LLMs.\n","authors":["Jie Ren","Qipeng Guo","Hang Yan","Dongrui Liu","Quanshi Zhang","Xipeng Qiu","Dahua Lin"],"pdf_url":"https://arxiv.org/pdf/2402.13055v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17844v1","updated":"2024-07-25T07:58:19Z","published":"2024-07-25T07:58:19Z","title":"Innovative Speech-Based Deep Learning Approaches for Parkinson's Disease\n  Classification: A Systematic Review","summary":"  Parkinson's disease (PD), the second most prevalent neurodegenerative\ndisorder worldwide, frequently presents with early-stage speech impairments.\nRecent advancements in Artificial Intelligence (AI), particularly deep learning\n(DL), have significantly enhanced PD diagnosis through the analysis of speech\ndata. Nevertheless, the progress of research is restricted by the limited\navailability of publicly accessible speech-based PD datasets, primarily due to\nprivacy and ethical concerns. This review covers the latest DL-based AI\napproaches for speech-based PD classification, focusing on performance,\navailable resources and associated challenges of 33 scientific works published\nbetween 2020 and March 2024. These DL approaches are categorized into\nend-to-end (E2E) learning, transfer learning (TL) and deep acoustic features\n(DAF) extraction. Among E2E approaches, Convolutional Neural Networks (CNNs)\nare prevalent, though Transformers are increasingly popular. E2E approaches\nface challenges such as limited data and computational resources, especially\nwith Transformers. TL addresses these issues by providing more robust PD\ndiagnosis and better generalizability across languages. DAF extraction aims to\nimprove the explainability and interpretability of results by examining the\nspecific effects of deep features on both other DL approaches and more\ntraditional machine learning (ML) methods. However, it often underperforms\ncompared to E2E and TL approaches. This review also discusses unresolved issues\nrelated to bias, explainability and privacy, highlighting the need for future\nresearch.\n","authors":["Lisanne van Gelderen","Cristian Tejedor-García"],"pdf_url":"https://arxiv.org/pdf/2407.17844v1.pdf","comment":"Submitted in Applied Sciences - peer reviewed Open Access journal.\n  This research was funded by the NWO research programme AiNed Fellowship\n  Grants under the project Responsible AI for Voice Diagnostics (RAIVD) - grant\n  number NGF.1607.22.013"},{"id":"http://arxiv.org/abs/2407.17075v2","updated":"2024-07-25T07:50:46Z","published":"2024-07-24T08:04:00Z","title":"SAFETY-J: Evaluating Safety with Critique","summary":"  The deployment of Large Language Models (LLMs) in content generation raises\nsignificant safety concerns, particularly regarding the transparency and\ninterpretability of content evaluations. Current methods, primarily focused on\nbinary safety classifications, lack mechanisms for detailed critique, limiting\ntheir utility for model improvement and user trust. To address these\nlimitations, we introduce SAFETY-J, a bilingual generative safety evaluator for\nEnglish and Chinese with critique-based judgment. SAFETY-J utilizes a robust\ntraining dataset that includes diverse dialogues and augmented query-response\npairs to assess safety across various scenarios comprehensively. We establish\nan automated meta-evaluation benchmark that objectively assesses the quality of\ncritiques with minimal human intervention, facilitating scalable and continuous\nimprovement. Additionally, SAFETY-J employs an iterative preference learning\ntechnique to dynamically refine safety assessments based on meta-evaluations\nand critiques. Our evaluations demonstrate that SAFETY-J provides more nuanced\nand accurate safety evaluations, thereby enhancing both critique quality and\npredictive reliability in complex content scenarios. To facilitate further\nresearch and application, we open-source SAFETY-J's training protocols,\ndatasets, and code at \\url{https://github.com/GAIR-NLP/Safety-J}.\n","authors":["Yixiu Liu","Yuxiang Zheng","Shijie Xia","Yuan Guo","Jiajun Li","Yi Tu","Chaoling Song","Pengfei Liu"],"pdf_url":"https://arxiv.org/pdf/2407.17075v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17125v2","updated":"2024-07-25T07:39:44Z","published":"2024-07-24T09:48:48Z","title":"Behavioral Testing: Can Large Language Models Implicitly Resolve\n  Ambiguous Entities?","summary":"  One of the major aspects contributing to the striking performance of large\nlanguage models (LLMs) is the vast amount of factual knowledge accumulated\nduring pre-training. Yet, many LLMs suffer from self-inconsistency, which\nraises doubts about their trustworthiness and reliability. In this paper, we\nfocus on entity type ambiguity and analyze current state-of-the-art LLMs for\ntheir proficiency and consistency in applying their factual knowledge when\nprompted for entities under ambiguity. To do so, we propose an evaluation\nprotocol that disentangles knowing from applying knowledge, and test\nstate-of-the-art LLMs on 49 entities. Our experiments reveal that LLMs perform\npoorly with ambiguous prompts, achieving only 80% accuracy. Our results further\ndemonstrate systematic discrepancies in LLM behavior and their failure to\nconsistently apply information, indicating that the models can exhibit\nknowledge without being able to utilize it, significant biases for preferred\nreadings, as well as self inconsistencies. Our study highlights the importance\nof handling entity ambiguity in future for more trustworthy LLMs\n","authors":["Anastasiia Sedova","Robert Litschko","Diego Frassinelli","Benjamin Roth","Barbara Plank"],"pdf_url":"https://arxiv.org/pdf/2407.17125v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17827v1","updated":"2024-07-25T07:35:27Z","published":"2024-07-25T07:35:27Z","title":"Unified Lexical Representation for Interpretable Visual-Language\n  Alignment","summary":"  Visual-Language Alignment (VLA) has gained a lot of attention since CLIP's\ngroundbreaking work. Although CLIP performs well, the typical direct latent\nfeature alignment lacks clarity in its representation and similarity scores. On\nthe other hand, lexical representation, a vector whose element represents the\nsimilarity between the sample and a word from the vocabulary, is a natural\nsparse representation and interpretable, providing exact matches for individual\nwords. However, lexical representations is difficult to learn due to no\nground-truth supervision and false-discovery issues, and thus requires complex\ndesign to train effectively. In this paper, we introduce LexVLA, a more\ninterpretable VLA framework by learning a unified lexical representation for\nboth modalities without complex design. We use DINOv2 as our visual model for\nits local-inclined features and Llama 2, a generative language model, to\nleverage its in-context lexical prediction ability. To avoid the false\ndiscovery, we propose an overuse penalty to refrain the lexical representation\nfrom falsely frequently activating meaningless words. We demonstrate that these\ntwo pre-trained uni-modal models can be well-aligned by fine-tuning on modest\nmulti-modal dataset and avoid intricate training configurations. On cross-modal\nretrieval benchmarks, LexVLA, trained on the CC-12M multi-modal dataset,\noutperforms baselines fine-tuned on larger datasets (e.g., YFCC15M) and those\ntrained from scratch on even bigger datasets (e.g., 1.1B data, including\nCC-12M). We conduct extensive experiments to analyze LexVLA.\n","authors":["Yifan Li","Yikai Wang","Yanwei Fu","Dongyu Ru","Zheng Zhang","Tong He"],"pdf_url":"https://arxiv.org/pdf/2407.17827v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17817v1","updated":"2024-07-25T07:10:31Z","published":"2024-07-25T07:10:31Z","title":"Demystifying Verbatim Memorization in Large Language Models","summary":"  Large Language Models (LLMs) frequently memorize long sequences verbatim,\noften with serious legal and privacy implications. Much prior work has studied\nsuch verbatim memorization using observational data. To complement such work,\nwe develop a framework to study verbatim memorization in a controlled setting\nby continuing pre-training from Pythia checkpoints with injected sequences. We\nfind that (1) non-trivial amounts of repetition are necessary for verbatim\nmemorization to happen; (2) later (and presumably better) checkpoints are more\nlikely to verbatim memorize sequences, even for out-of-distribution sequences;\n(3) the generation of memorized sequences is triggered by distributed model\nstates that encode high-level features and makes important use of general\nlanguage modeling capabilities. Guided by these insights, we develop stress\ntests to evaluate unlearning methods and find they often fail to remove the\nverbatim memorized information, while also degrading the LM. Overall, these\nfindings challenge the hypothesis that verbatim memorization stems from\nspecific model weights or mechanisms. Rather, verbatim memorization is\nintertwined with the LM's general capabilities and thus will be very difficult\nto isolate and suppress without degrading model quality.\n","authors":["Jing Huang","Diyi Yang","Christopher Potts"],"pdf_url":"https://arxiv.org/pdf/2407.17817v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.16228v2","updated":"2024-07-25T07:05:30Z","published":"2023-09-28T08:09:33Z","title":"Brand Network Booster: A new system for improving brand connectivity","summary":"  This paper presents a new decision support system offered for an in-depth\nanalysis of semantic networks, which can provide insights for a better\nexploration of a brand's image and the improvement of its connectivity. In\nterms of network analysis, we show that this goal is achieved by solving an\nextended version of the Maximum Betweenness Improvement problem, which includes\nthe possibility of considering adversarial nodes, constrained budgets, and\nweighted networks - where connectivity improvement can be obtained by adding\nlinks or increasing the weight of existing connections. Our contribution\nincludes a new algorithmic framework and the integration of this framework into\na software system called Brand Network Booster (BNB), which supports brand\nconnectivity evaluation and improvement. We present this new system together\nwith three case studies, and we also discuss its performance. Our tool and\napproach are valuable to both network scholars and in facilitating strategic\ndecision-making processes for marketing and communication managers across\nvarious sectors, be it public or private.\n","authors":["J. Cancellieri","W. Didimo","A. Fronzetti Colladon","F. Montecchiani","R. Vestrelli"],"pdf_url":"https://arxiv.org/pdf/2309.16228v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.06851v4","updated":"2024-07-25T06:41:43Z","published":"2023-11-12T14:01:38Z","title":"Automatic Textual Normalization for Hate Speech Detection","summary":"  Social media data is a valuable resource for research, yet it contains a wide\nrange of non-standard words (NSW). These irregularities hinder the effective\noperation of NLP tools. Current state-of-the-art methods for the Vietnamese\nlanguage address this issue as a problem of lexical normalization, involving\nthe creation of manual rules or the implementation of multi-staged deep\nlearning frameworks, which necessitate extensive efforts to craft intricate\nrules. In contrast, our approach is straightforward, employing solely a\nsequence-to-sequence (Seq2Seq) model. In this research, we provide a dataset\nfor textual normalization, comprising 2,181 human-annotated comments with an\ninter-annotator agreement of 0.9014. By leveraging the Seq2Seq model for\ntextual normalization, our results reveal that the accuracy achieved falls\nslightly short of 70%. Nevertheless, textual normalization enhances the\naccuracy of the Hate Speech Detection (HSD) task by approximately 2%,\ndemonstrating its potential to improve the performance of complex NLP tasks.\nOur dataset is accessible for research purposes.\n","authors":["Anh Thi-Hoang Nguyen","Dung Ha Nguyen","Nguyet Thi Nguyen","Khanh Thanh-Duy Ho","Kiet Van Nguyen"],"pdf_url":"https://arxiv.org/pdf/2311.06851v4.pdf","comment":"2023 International Conference on Intelligent Systems Design and\n  Applications (ISDA2023)"},{"id":"http://arxiv.org/abs/2306.17103v4","updated":"2024-07-25T06:15:20Z","published":"2023-06-29T17:01:51Z","title":"LyricWhiz: Robust Multilingual Zero-shot Lyrics Transcription by\n  Whispering to ChatGPT","summary":"  We introduce LyricWhiz, a robust, multilingual, and zero-shot automatic\nlyrics transcription method achieving state-of-the-art performance on various\nlyrics transcription datasets, even in challenging genres such as rock and\nmetal. Our novel, training-free approach utilizes Whisper, a weakly supervised\nrobust speech recognition model, and GPT-4, today's most performant chat-based\nlarge language model. In the proposed method, Whisper functions as the \"ear\" by\ntranscribing the audio, while GPT-4 serves as the \"brain,\" acting as an\nannotator with a strong performance for contextualized output selection and\ncorrection. Our experiments show that LyricWhiz significantly reduces Word\nError Rate compared to existing methods in English and can effectively\ntranscribe lyrics across multiple languages. Furthermore, we use LyricWhiz to\ncreate the first publicly available, large-scale, multilingual lyrics\ntranscription dataset with a CC-BY-NC-SA copyright license, based on\nMTG-Jamendo, and offer a human-annotated subset for noise level estimation and\nevaluation. We anticipate that our proposed method and dataset will advance the\ndevelopment of multilingual lyrics transcription, a challenging and emerging\ntask.\n","authors":["Le Zhuo","Ruibin Yuan","Jiahao Pan","Yinghao Ma","Yizhi LI","Ge Zhang","Si Liu","Roger Dannenberg","Jie Fu","Chenghua Lin","Emmanouil Benetos","Wei Xue","Yike Guo"],"pdf_url":"https://arxiv.org/pdf/2306.17103v4.pdf","comment":"9 pages, 2 figures, 5 tables, accepted by ISMIR 2023"},{"id":"http://arxiv.org/abs/2407.17773v1","updated":"2024-07-25T05:02:39Z","published":"2024-07-25T05:02:39Z","title":"KiVA: Kid-inspired Visual Analogies for Testing Large Multimodal Models","summary":"  This paper investigates visual analogical reasoning in large multimodal\nmodels (LMMs) compared to human adults and children. A \"visual analogy\" is an\nabstract rule inferred from one image and applied to another. While benchmarks\nexist for testing visual reasoning in LMMs, they require advanced skills and\nomit basic visual analogies that even young children can make. Inspired by\ndevelopmental psychology, we propose a new benchmark of 1,400 visual\ntransformations of everyday objects to test LMMs on visual analogical reasoning\nand compare them to children and adults. We structure the evaluation into three\nstages: identifying what changed (e.g., color, number, etc.), how it changed\n(e.g., added one object), and applying the rule to new scenarios. Our findings\nshow that while models like GPT-4V, LLaVA-1.5, and MANTIS identify the \"what\"\neffectively, they struggle with quantifying the \"how\" and extrapolating this\nrule to new objects. In contrast, children and adults exhibit much stronger\nanalogical reasoning at all three stages. Additionally, the strongest tested\nmodel, GPT-4V, performs better in tasks involving simple visual attributes like\ncolor and size, correlating with quicker human adult response times.\nConversely, more complex tasks such as number, rotation, and reflection, which\nnecessitate extensive cognitive processing and understanding of the 3D physical\nworld, present more significant challenges. Altogether, these findings\nhighlight the limitations of training models on data that primarily consists of\n2D images and text.\n","authors":["Eunice Yiu","Maan Qraitem","Charlie Wong","Anisa Noor Majhi","Yutong Bai","Shiry Ginosar","Alison Gopnik","Kate Saenko"],"pdf_url":"https://arxiv.org/pdf/2407.17773v1.pdf","comment":"9 pages. For the KiVA benchmark, see https://github.com/ey242/KiVA"},{"id":"http://arxiv.org/abs/2407.17772v1","updated":"2024-07-25T05:02:27Z","published":"2024-07-25T05:02:27Z","title":"ERIT Lightweight Multimodal Dataset for Elderly Emotion Recognition and\n  Multimodal Fusion Evaluation","summary":"  ERIT is a novel multimodal dataset designed to facilitate research in a\nlightweight multimodal fusion. It contains text and image data collected from\nvideos of elderly individuals reacting to various situations, as well as seven\nemotion labels for each data sample. Because of the use of labeled images of\nelderly users reacting emotionally, it is also facilitating research on emotion\nrecognition in an underrepresented age group in machine learning visual emotion\nrecognition. The dataset is validated through comprehensive experiments\nindicating its importance in neural multimodal fusion research.\n","authors":["Rita Frieske","Bertrand E. Shi"],"pdf_url":"https://arxiv.org/pdf/2407.17772v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17771v1","updated":"2024-07-25T04:58:08Z","published":"2024-07-25T04:58:08Z","title":"Banyan: Improved Representation Learning with Explicit Structure","summary":"  We present Banyan, an improved model to learn semantic representations by\ninducing explicit structure over data. In contrast to prior approaches using\nstructure spanning single sentences, Banyan learns by resolving multiple\nconstituent structures into a shared one explicitly incorporating global\ncontext. Combined with an improved message-passing scheme inspired by Griffin,\nBanyan learns significantly better representations, avoids spurious false\nnegatives with contrastive learning, and drastically improves memory efficiency\nin such explicit-structured models. Using the Self-StrAE framework, we show\nthat Banyan (a) outperforms baselines using sentential structure across various\nsettings (b) matches or outperforms unstructured baselines like GloVe\n(+augmentations) and a RoBERTa medium (+simcse) pre-trained on 100M tokens,\ndespite having just a handful of (non-embedding) parameters, and (c) also\nlearns effective representations across several low resource (Asian and\nAfrican) languages as measured on SemRel tasks.\n","authors":["Mattia Opper","N. Siddharth"],"pdf_url":"https://arxiv.org/pdf/2407.17771v1.pdf","comment":"First Draft"},{"id":"http://arxiv.org/abs/2407.17770v1","updated":"2024-07-25T04:57:31Z","published":"2024-07-25T04:57:31Z","title":"BotEval: Facilitating Interactive Human Evaluation","summary":"  Following the rapid progress in natural language processing (NLP) models,\nlanguage models are applied to increasingly more complex interactive tasks such\nas negotiations and conversation moderations. Having human evaluators directly\ninteract with these NLP models is essential for adequately evaluating the\nperformance on such interactive tasks. We develop BotEval, an easily\ncustomizable, open-source, evaluation toolkit that focuses on enabling\nhuman-bot interactions as part of the evaluation process, as opposed to human\nevaluators making judgements for a static input. BotEval balances flexibility\nfor customization and user-friendliness by providing templates for common use\ncases that span various degrees of complexity and built-in compatibility with\npopular crowdsourcing platforms. We showcase the numerous useful features of\nBotEval through a study that evaluates the performance of various chatbots on\ntheir effectiveness for conversational moderation and discuss how BotEval\ndiffers from other annotation tools.\n","authors":["Hyundong Cho","Thamme Gowda","Yuyang Huang","Zixun Lu","Tianli Tong","Jonathan May"],"pdf_url":"https://arxiv.org/pdf/2407.17770v1.pdf","comment":"ACL 2024 SDT, 10 pages"},{"id":"http://arxiv.org/abs/2407.10499v2","updated":"2024-07-25T04:44:54Z","published":"2024-07-15T07:43:55Z","title":"CIBench: Evaluating Your LLMs with a Code Interpreter Plugin","summary":"  While LLM-Based agents, which use external tools to solve complex problems,\nhave made significant progress, benchmarking their ability is challenging,\nthereby hindering a clear understanding of their limitations. In this paper, we\npropose an interactive evaluation framework, named CIBench, to comprehensively\nassess LLMs' ability to utilize code interpreters for data science tasks. Our\nevaluation framework includes an evaluation dataset and two evaluation modes.\nThe evaluation dataset is constructed using an LLM-human cooperative approach\nand simulates an authentic workflow by leveraging consecutive and interactive\nIPython sessions. The two evaluation modes assess LLMs' ability with and\nwithout human assistance. We conduct extensive experiments to analyze the\nability of 24 LLMs on CIBench and provide valuable insights for future LLMs in\ncode interpreter utilization.\n","authors":["Songyang Zhang","Chuyu Zhang","Yingfan Hu","Haowen Shen","Kuikun Liu","Zerun Ma","Fengzhe Zhou","Wenwei Zhang","Xuming He","Dahua Lin","Kai Chen"],"pdf_url":"https://arxiv.org/pdf/2407.10499v2.pdf","comment":"Under review. The first three authors contribute equally, and\n  Songyang Zhang is the project leader"},{"id":"http://arxiv.org/abs/2308.16884v2","updated":"2024-07-25T04:30:15Z","published":"2023-08-31T17:43:08Z","title":"The Belebele Benchmark: a Parallel Reading Comprehension Dataset in 122\n  Language Variants","summary":"  We present Belebele, a multiple-choice machine reading comprehension (MRC)\ndataset spanning 122 language variants. Significantly expanding the language\ncoverage of natural language understanding (NLU) benchmarks, this dataset\nenables the evaluation of text models in high-, medium-, and low-resource\nlanguages. Each question is based on a short passage from the Flores-200\ndataset and has four multiple-choice answers. The questions were carefully\ncurated to discriminate between models with different levels of general\nlanguage comprehension. The English dataset on its own proves difficult enough\nto challenge state-of-the-art language models. Being fully parallel, this\ndataset enables direct comparison of model performance across all languages. We\nuse this dataset to evaluate the capabilities of multilingual masked language\nmodels (MLMs) and large language models (LLMs). We present extensive results\nand find that despite significant cross-lingual transfer in English-centric\nLLMs, much smaller MLMs pretrained on balanced multilingual data still\nunderstand far more languages. We also observe that larger vocabulary size and\nconscious vocabulary construction correlate with better performance on\nlow-resource languages. Overall, Belebele opens up new avenues for evaluating\nand analyzing the multilingual capabilities of NLP systems.\n","authors":["Lucas Bandarkar","Davis Liang","Benjamin Muller","Mikel Artetxe","Satya Narayan Shukla","Donald Husa","Naman Goyal","Abhinandan Krishnan","Luke Zettlemoyer","Madian Khabsa"],"pdf_url":"https://arxiv.org/pdf/2308.16884v2.pdf","comment":"ACL 2024"},{"id":"http://arxiv.org/abs/2407.17745v1","updated":"2024-07-25T03:40:09Z","published":"2024-07-25T03:40:09Z","title":"Beyond Entity Alignment: Towards Complete Knowledge Graph Alignment via\n  Entity-Relation Synergy","summary":"  Knowledge Graph Alignment (KGA) aims to integrate knowledge from multiple\nsources to address the limitations of individual Knowledge Graphs (KGs) in\nterms of coverage and depth. However, current KGA models fall short in\nachieving a ``complete'' knowledge graph alignment. Existing models primarily\nemphasize the linkage of cross-graph entities but overlook aligning relations\nacross KGs, thereby providing only a partial solution to KGA. The semantic\ncorrelations embedded in relations are largely overlooked, potentially\nrestricting a comprehensive understanding of cross-KG signals. In this paper,\nwe propose to conceptualize relation alignment as an independent task and\nconduct KGA by decomposing it into two distinct but highly correlated\nsub-tasks: entity alignment and relation alignment. To capture the mutually\nreinforcing correlations between these objectives, we propose a novel\nExpectation-Maximization-based model, EREM, which iteratively optimizes both\nsub-tasks. Experimental results on real-world datasets demonstrate that EREM\nconsistently outperforms state-of-the-art models in both entity alignment and\nrelation alignment tasks.\n","authors":["Xiaohan Fang","Chaozhuo Li","Yi Zhao","Qian Zang","Litian Zhang","Jiquan Peng","Xi Zhang","Jibing Gong"],"pdf_url":"https://arxiv.org/pdf/2407.17745v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.11686v3","updated":"2024-07-25T03:34:56Z","published":"2024-07-16T13:03:58Z","title":"CCoE: A Compact LLM with Collaboration of Experts","summary":"  In the domain of Large Language Model (LLM), LLMs demonstrate significant\ncapabilities in natural language understanding and generation. With the growing\nneeds of applying LLMs on various domains, it is a research question that how\nto efficiently train and build a model that has expertise in different domains\nbut with a low training cost. We propose CCoE architecture, a framework of\neasily coupling multiple strong domain experts together to fuse into a big LLM,\nprovides a collective way of utilizing the different domain expert LLMs.\nBesides, training a large collaborative of multiple expert LLMs requires a high\nrequirements on training sources. CCoE bypasses this problem through isolating\nother experts and train each expert separately. The design of CCoE assembles\nmultiple expert LLMs through the CoE (Collaboration of Experts) layer. Each CoE\nlayer could have one or more expert LLMs. Expert LLMs have different number of\nlayers and have been well-trained for different domain tasks. Each expert is\nfine-tuned to be able to achieve the comparable results with SOTA domain LLMs.\nWe start from 5 experts in the domain of Code, Math, Law, text-to-SQL and\nMedical. The results indicate that our CCoE framework can easily and\nefficiently boost nearly 10%-20% performance on original base model in\ndifferent domains but using less resources on training, as well as inference.\n","authors":["Shaomang Huang","Jianfeng Pan","Hanzhong Zheng"],"pdf_url":"https://arxiv.org/pdf/2407.11686v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.01686v2","updated":"2024-07-25T03:29:09Z","published":"2024-05-02T19:20:11Z","title":"Automatically Extracting Numerical Results from Randomized Controlled\n  Trials with Large Language Models","summary":"  Meta-analyses statistically aggregate the findings of different randomized\ncontrolled trials (RCTs) to assess treatment effectiveness. Because this yields\nrobust estimates of treatment effectiveness, results from meta-analyses are\nconsidered the strongest form of evidence. However, rigorous evidence syntheses\nare time-consuming and labor-intensive, requiring manual extraction of data\nfrom individual trials to be synthesized. Ideally, language technologies would\npermit fully automatic meta-analysis, on demand. This requires accurately\nextracting numerical results from individual trials, which has been beyond the\ncapabilities of natural language processing (NLP) models to date. In this work,\nwe evaluate whether modern large language models (LLMs) can reliably perform\nthis task. We annotate (and release) a modest but granular evaluation dataset\nof clinical trial reports with numerical findings attached to interventions,\ncomparators, and outcomes. Using this dataset, we evaluate the performance of\nseven LLMs applied zero-shot for the task of conditionally extracting numerical\nfindings from trial reports. We find that massive LLMs that can accommodate\nlengthy inputs are tantalizingly close to realizing fully automatic\nmeta-analysis, especially for dichotomous (binary) outcomes (e.g., mortality).\nHowever, LLMs -- including ones trained on biomedical texts -- perform poorly\nwhen the outcome measures are complex and tallying the results requires\ninference. This work charts a path toward fully automatic meta-analysis of RCTs\nvia LLMs, while also highlighting the limitations of existing models for this\naim.\n","authors":["Hye Sun Yun","David Pogrebitskiy","Iain J. Marshall","Byron C. Wallace"],"pdf_url":"https://arxiv.org/pdf/2405.01686v2.pdf","comment":"25 pages, 7 figures, 6 tables, MLHC 2024"},{"id":"http://arxiv.org/abs/2311.07052v3","updated":"2024-07-25T03:20:15Z","published":"2023-11-13T03:36:18Z","title":"Towards the Law of Capacity Gap in Distilling Language Models","summary":"  Language model (LM) distillation is a trending area that aims to distil the\nknowledge residing in a large teacher LM to a small student one. While various\nmethods have been proposed to maximize the effectiveness of the distillation,\nsignificant challenges persist, particularly when there is a substantial\ncapacity gap between the teacher and student LMs. This issue, often referred to\nas the \\textit{curse} of capacity gap, suggests that a larger teacher does not\nnecessarily result in a superior student compared to one distilled from a\nsmaller teacher. In other words, there is likely an optimal teacher yielding\nthe best student along the scaling course of the teacher. However, the curse of\ncapacity gap can not be tackled without notable compute overhead, as indicated\nin previous studies. In the context of large LMs (LLMs), previously viable\napproaches become much less meaningful, as it is an impossible triangle to\ndistill an expected student from an optimal teacher student with small compute\noverhead. Fortunately, the impossible triangle can fortunately be possible\nprovided an inducted \\textit{law} of capacity gap. In this paper, we take the\nspirits of scaling law and reveal that the optimal teacher scale almost\nconsistently follows a linear scaling with the student scale across different\nmodel architectures and data scales. The law later guides us to distil a 3B\nstudent LM (termed \\textsc{MiniMA}) from LLaMA2-7B. \\textsc{MiniMA} is\ndemonstrated to outperform a wide range of 3B competitors and could even\ncompete with several 7B models.\n","authors":["Chen Zhang","Dawei Song","Zheyu Ye","Yan Gao"],"pdf_url":"https://arxiv.org/pdf/2311.07052v3.pdf","comment":"32 pages, 10 figures, 15 tables, work in progress. Code and\n  checkpoints are available at https://github.com/GeneZC/MiniMA"},{"id":"http://arxiv.org/abs/2407.17734v1","updated":"2024-07-25T03:12:57Z","published":"2024-07-25T03:12:57Z","title":"Cost-effective Instruction Learning for Pathology Vision and Language\n  Analysis","summary":"  The advent of vision-language models fosters the interactive conversations\nbetween AI-enabled models and humans. Yet applying these models into clinics\nmust deal with daunting challenges around large-scale training data, financial,\nand computational resources. Here we propose a cost-effective instruction\nlearning framework for conversational pathology named as CLOVER. CLOVER only\ntrains a lightweight module and uses instruction tuning while freezing the\nparameters of the large language model. Instead of using costly GPT-4, we\npropose well-designed prompts on GPT-3.5 for building generation-based\ninstructions, emphasizing the utility of pathological knowledge derived from\nthe Internet source. To augment the use of instructions, we construct a\nhigh-quality set of template-based instructions in the context of digital\npathology. From two benchmark datasets, our findings reveal the strength of\nhybrid-form instructions in the visual question-answer in pathology. Extensive\nresults show the cost-effectiveness of CLOVER in answering both open-ended and\nclosed-ended questions, where CLOVER outperforms strong baselines that possess\n37 times more training parameters and use instruction data generated from\nGPT-4. Through the instruction tuning, CLOVER exhibits robustness of few-shot\nlearning in the external clinical dataset. These findings demonstrate that\ncost-effective modeling of CLOVER could accelerate the adoption of rapid\nconversational applications in the landscape of digital pathology.\n","authors":["Kaitao Chen","Mianxin Liu","Fang Yan","Lei Ma","Xiaoming Shi","Lilong Wang","Xiaosong Wang","Lifeng Zhu","Zhe Wang","Mu Zhou","Shaoting Zhang"],"pdf_url":"https://arxiv.org/pdf/2407.17734v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.09530v4","updated":"2024-07-25T03:08:18Z","published":"2023-09-18T07:17:52Z","title":"Adapting Large Language Models to Domains via Reading Comprehension","summary":"  We explore how continued pre-training on domain-specific corpora influences\nlarge language models, revealing that training on the raw corpora endows the\nmodel with domain knowledge, but drastically hurts its prompting ability for\nquestion answering. Taken inspiration from human learning via reading\ncomprehension--practice after reading improves the ability to answer questions\nbased on the learned knowledge--we propose a simple method for transforming raw\ncorpora into reading comprehension texts. Each raw text is enriched with a\nseries of tasks related to its content. Our method, highly scalable and\napplicable to any pre-training corpora, consistently enhances performance\nacross various tasks in three different domains: biomedicine, finance, and law.\nNotably, our 7B language model achieves competitive performance with\ndomain-specific models of much larger scales, such as BloombergGPT-50B.\nFurthermore, we demonstrate that domain-specific reading comprehension texts\ncan improve the model's performance even on general benchmarks, showing the\npotential to develop a general model across even more domains. Our model, code,\nand data are available at https://github.com/microsoft/LMOps.\n","authors":["Daixuan Cheng","Shaohan Huang","Furu Wei"],"pdf_url":"https://arxiv.org/pdf/2309.09530v4.pdf","comment":"ICLR 2024 Conference"},{"id":"http://arxiv.org/abs/2407.17730v1","updated":"2024-07-25T03:01:47Z","published":"2024-07-25T03:01:47Z","title":"Are Large Language Models Possible to Conduct Cognitive Behavioral\n  Therapy?","summary":"  In contemporary society, the issue of psychological health has become\nincreasingly prominent, characterized by the diversification, complexity, and\nuniversality of mental disorders. Cognitive Behavioral Therapy (CBT), currently\nthe most influential and clinically effective psychological treatment method\nwith no side effects, has limited coverage and poor quality in most countries.\nIn recent years, researches on the recognition and intervention of emotional\ndisorders using large language models (LLMs) have been validated, providing new\npossibilities for psychological assistance therapy. However, are LLMs truly\npossible to conduct cognitive behavioral therapy? Many concerns have been\nraised by mental health experts regarding the use of LLMs for therapy. Seeking\nto answer this question, we collected real CBT corpus from online video\nwebsites, designed and conducted a targeted automatic evaluation framework\ninvolving the evaluation of emotion tendency of generated text, structured\ndialogue pattern and proactive inquiry ability. For emotion tendency, we\ncalculate the emotion tendency score of the CBT dialogue text generated by each\nmodel. For structured dialogue pattern, we use a diverse range of automatic\nevaluation metrics to compare speaking style, the ability to maintain\nconsistency of topic and the use of technology in CBT between different models\n. As for inquiring to guide the patient, we utilize PQA (Proactive Questioning\nAbility) metric. We also evaluated the CBT ability of the LLM after integrating\na CBT knowledge base to explore the help of introducing additional knowledge to\nenhance the model's CBT counseling ability. Four LLM variants with excellent\nperformance on natural language processing are evaluated, and the experimental\nresult shows the great potential of LLMs in psychological counseling realm,\nespecially after combining with other technological means.\n","authors":["Hao Shen","Zihan Li","Minqiang Yang","Minghui Ni","Yongfeng Tao","Zhengyang Yu","Weihao Zheng","Chen Xu","Bin Hu"],"pdf_url":"https://arxiv.org/pdf/2407.17730v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.07386v2","updated":"2024-07-25T02:46:50Z","published":"2024-02-12T03:05:54Z","title":"Chain-of-Layer: Iteratively Prompting Large Language Models for Taxonomy\n  Induction from Limited Examples","summary":"  Automatic taxonomy induction is crucial for web search, recommendation\nsystems, and question answering. Manual curation of taxonomies is expensive in\nterms of human effort, making automatic taxonomy construction highly desirable.\nIn this work, we introduce Chain-of-Layer which is an in-context learning\nframework designed to induct taxonomies from a given set of entities.\nChain-of-Layer breaks down the task into selecting relevant candidate entities\nin each layer and gradually building the taxonomy from top to bottom. To\nminimize errors, we introduce the Ensemble-based Ranking Filter to reduce the\nhallucinated content generated at each iteration. Through extensive\nexperiments, we demonstrate that Chain-of-Layer achieves state-of-the-art\nperformance on four real-world benchmarks.\n","authors":["Qingkai Zeng","Yuyang Bai","Zhaoxuan Tan","Shangbin Feng","Zhenwen Liang","Zhihan Zhang","Meng Jiang"],"pdf_url":"https://arxiv.org/pdf/2402.07386v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17716v1","updated":"2024-07-25T02:30:40Z","published":"2024-07-25T02:30:40Z","title":"Describe Where You Are: Improving Noise-Robustness for Speech Emotion\n  Recognition with Text Description of the Environment","summary":"  Speech emotion recognition (SER) systems often struggle in real-world\nenvironments, where ambient noise severely degrades their performance. This\npaper explores a novel approach that exploits prior knowledge of testing\nenvironments to maximize SER performance under noisy conditions. To address\nthis task, we propose a text-guided, environment-aware training where an SER\nmodel is trained with contaminated speech samples and their paired noise\ndescription. We use a pre-trained text encoder to extract the text-based\nenvironment embedding and then fuse it to a transformer-based SER model during\ntraining and inference. We demonstrate the effectiveness of our approach\nthrough our experiment with the MSP-Podcast corpus and real-world additive\nnoise samples collected from the Freesound repository. Our experiment indicates\nthat the text-based environment descriptions processed by a large language\nmodel (LLM) produce representations that improve the noise-robustness of the\nSER system. In addition, our proposed approach with an LLM yields better\nperformance than our environment-agnostic baselines, especially in low\nsignal-to-noise ratio (SNR) conditions. When testing at -5dB SNR level, our\nproposed method shows better performance than our best baseline model by 31.8 %\n(arousal), 23.5% (dominance), and 9.5% (valence).\n","authors":["Seong-Gyun Leem","Daniel Fulford","Jukka-Pekka Onnela","David Gard","Carlos Busso"],"pdf_url":"https://arxiv.org/pdf/2407.17716v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.01599v2","updated":"2024-07-25T02:25:11Z","published":"2024-06-26T02:20:23Z","title":"JailbreakZoo: Survey, Landscapes, and Horizons in Jailbreaking Large\n  Language and Vision-Language Models","summary":"  The rapid evolution of artificial intelligence (AI) through developments in\nLarge Language Models (LLMs) and Vision-Language Models (VLMs) has brought\nsignificant advancements across various technological domains. While these\nmodels enhance capabilities in natural language processing and visual\ninteractive tasks, their growing adoption raises critical concerns regarding\nsecurity and ethical alignment. This survey provides an extensive review of the\nemerging field of jailbreaking--deliberately circumventing the ethical and\noperational boundaries of LLMs and VLMs--and the consequent development of\ndefense mechanisms. Our study categorizes jailbreaks into seven distinct types\nand elaborates on defense strategies that address these vulnerabilities.\nThrough this comprehensive examination, we identify research gaps and propose\ndirections for future studies to enhance the security frameworks of LLMs and\nVLMs. Our findings underscore the necessity for a unified perspective that\nintegrates both jailbreak strategies and defensive solutions to foster a\nrobust, secure, and reliable environment for the next generation of language\nmodels. More details can be found on our website:\n\\url{https://chonghan-chen.com/llm-jailbreak-zoo-survey/}.\n","authors":["Haibo Jin","Leyang Hu","Xinuo Li","Peiyan Zhang","Chonghan Chen","Jun Zhuang","Haohan Wang"],"pdf_url":"https://arxiv.org/pdf/2407.01599v2.pdf","comment":"45 pages"},{"id":"http://arxiv.org/abs/2407.17695v1","updated":"2024-07-25T01:32:41Z","published":"2024-07-25T01:32:41Z","title":"Enhancing Agent Learning through World Dynamics Modeling","summary":"  While large language models (LLMs) have been increasingly deployed across\ntasks in language understanding and interactive decision-making, their\nimpressive performance is largely due to the comprehensive and in-depth domain\nknowledge embedded within them. However, the extent of this knowledge can vary\nacross different domains. Existing methods often assume that LLMs already\npossess such comprehensive and in-depth knowledge of their environment,\noverlooking potential gaps in their understanding of actual world dynamics. To\naddress this gap, we introduce Discover, Verify, and Evolve (DiVE), a framework\nthat discovers world dynamics from a small number of demonstrations, verifies\nthe correctness of these dynamics, and evolves new, advanced dynamics tailored\nto the current situation. Through extensive evaluations, we analyze the impact\nof each component on performance and compare the automatically generated\ndynamics from DiVE with human-annotated world dynamics. Our results demonstrate\nthat LLMs guided by DiVE can make better decisions, achieving rewards\ncomparable to human players in the Crafter environment.\n","authors":["Zhiyuan Sun","Haochen Shi","Marc-Alexandre Côté","Glen Berseth","Xingdi Yuan","Bang Liu"],"pdf_url":"https://arxiv.org/pdf/2407.17695v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17688v1","updated":"2024-07-25T01:11:38Z","published":"2024-07-25T01:11:38Z","title":"Examining the Influence of Political Bias on Large Language Model\n  Performance in Stance Classification","summary":"  Large Language Models (LLMs) have demonstrated remarkable capabilities in\nexecuting tasks based on natural language queries. However, these models,\ntrained on curated datasets, inherently embody biases ranging from racial to\nnational and gender biases. It remains uncertain whether these biases impact\nthe performance of LLMs for certain tasks. In this study, we investigate the\npolitical biases of LLMs within the stance classification task, specifically\nexamining whether these models exhibit a tendency to more accurately classify\npolitically-charged stances. Utilizing three datasets, seven LLMs, and four\ndistinct prompting schemes, we analyze the performance of LLMs on politically\noriented statements and targets. Our findings reveal a statistically\nsignificant difference in the performance of LLMs across various politically\noriented stance classification tasks. Furthermore, we observe that this\ndifference primarily manifests at the dataset level, with models and prompting\nschemes showing statistically similar performances across different stance\nclassification datasets. Lastly, we observe that when there is greater\nambiguity in the target the statement is directed towards, LLMs have poorer\nstance classification accuracy.\n","authors":["Lynnette Hui Xian Ng","Iain Cruickshank","Roy Ka-Wei Lee"],"pdf_url":"https://arxiv.org/pdf/2407.17688v1.pdf","comment":"Accepted at ICWSM 2025"},{"id":"http://arxiv.org/abs/2302.00509v2","updated":"2024-07-25T01:09:57Z","published":"2023-02-01T15:28:55Z","title":"Exploring Semantic Perturbations on Grover","summary":"  With news and information being as easy to access as they currently are, it\nis more important than ever to ensure that people are not mislead by what they\nread. Recently, the rise of neural fake news (AI-generated fake news) and its\ndemonstrated effectiveness at fooling humans has prompted the development of\nmodels to detect it. One such model is the Grover model, which can both detect\nneural fake news to prevent it, and generate it to demonstrate how a model\ncould be misused to fool human readers. In this work we explore the Grover\nmodel's fake news detection capabilities by performing targeted attacks through\nperturbations on input news articles. Through this we test Grover's resilience\nto these adversarial attacks and expose some potential vulnerabilities which\nshould be addressed in further iterations to ensure it can detect all types of\nfake news accurately.\n","authors":["Ziqing Ji","Pranav Kulkarni","Marko Neskovic","Kevin Nolan","Yan Xu"],"pdf_url":"https://arxiv.org/pdf/2302.00509v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17686v1","updated":"2024-07-25T01:07:09Z","published":"2024-07-25T01:07:09Z","title":"Transformers on Markov Data: Constant Depth Suffices","summary":"  Attention-based transformers have been remarkably successful at modeling\ngenerative processes across various domains and modalities. In this paper, we\nstudy the behavior of transformers on data drawn from \\kth Markov processes,\nwhere the conditional distribution of the next symbol in a sequence depends on\nthe previous $k$ symbols observed. We observe a surprising phenomenon\nempirically which contradicts previous findings: when trained for sufficiently\nlong, a transformer with a fixed depth and $1$ head per layer is able to\nachieve low test loss on sequences drawn from \\kth Markov sources, even as $k$\ngrows. Furthermore, this low test loss is achieved by the transformer's ability\nto represent and learn the in-context conditional empirical distribution. On\nthe theoretical side, our main result is that a transformer with a single head\nand three layers can represent the in-context conditional empirical\ndistribution for \\kth Markov sources, concurring with our empirical\nobservations. Along the way, we prove that \\textit{attention-only} transformers\nwith $O(\\log_2(k))$ layers can represent the in-context conditional empirical\ndistribution by composing induction heads to track the previous $k$ symbols in\nthe sequence. These results provide more insight into our current understanding\nof the mechanisms by which transformers learn to capture context, by\nunderstanding their behavior on Markov sources.\n","authors":["Nived Rajaraman","Marco Bondaschi","Kannan Ramchandran","Michael Gastpar","Ashok Vardhan Makkuva"],"pdf_url":"https://arxiv.org/pdf/2407.17686v1.pdf","comment":"29 pages, 10 figures"},{"id":"http://arxiv.org/abs/2407.17678v1","updated":"2024-07-25T00:27:07Z","published":"2024-07-25T00:27:07Z","title":"Efficient LLM Training and Serving with Heterogeneous Context Sharding\n  among Attention Heads","summary":"  Existing LLM training and inference frameworks struggle in boosting\nefficiency with sparsity while maintaining the integrity of context and model\narchitecture. Inspired by the sharding concept in database and the fact that\nattention parallelizes over heads on accelerators, we propose Sparsely-Sharded\n(S2) Attention, an attention algorithm that allocates heterogeneous context\npartitions for different attention heads to divide and conquer. S2-Attention\nenforces each attention head to only attend to a partition of contexts\nfollowing a strided sparsity pattern, while the full context is preserved as\nthe union of all the shards. As attention heads are processed in separate\nthread blocks, the context reduction for each head can thus produce end-to-end\nspeed-up and memory reduction. At inference, LLMs trained with S2-Attention can\nthen take the KV cache reduction as free meals with guaranteed model quality\npreserve. In experiments, we show S2-Attentioncan provide as much as (1) 25.3X\nwall-clock attention speed-up over FlashAttention-2, resulting in 6X reduction\nin end-to-end training time and 10X inference latency, (2) on-par model\ntraining quality compared to default attention, (3)perfect needle retrieval\naccuracy over 32K context window. On top of the algorithm, we build DKernel, an\nLLM training and inference kernel library that allows users to customize\nsparsity patterns for their own models. We open-sourced DKerneland make it\ncompatible with Megatron, Pytorch, and vLLM.\n","authors":["Xihui Lin","Yunan Zhang","Suyu Ge","Barun Patra","Vishrav Chaudhary","Xia Song"],"pdf_url":"https://arxiv.org/pdf/2407.17678v1.pdf","comment":"10 pages"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2407.18251v1","updated":"2024-07-25T17:59:48Z","published":"2024-07-25T17:59:48Z","title":"Sparse vs Contiguous Adversarial Pixel Perturbations in Multimodal\n  Models: An Empirical Analysis","summary":"  Assessing the robustness of multimodal models against adversarial examples is\nan important aspect for the safety of its users. We craft L0-norm perturbation\nattacks on the preprocessed input images. We launch them in a black-box setup\nagainst four multimodal models and two unimodal DNNs, considering both targeted\nand untargeted misclassification. Our attacks target less than 0.04% of\nperturbed image area and integrate different spatial positioning of perturbed\npixels: sparse positioning and pixels arranged in different contiguous shapes\n(row, column, diagonal, and patch). To the best of our knowledge, we are the\nfirst to assess the robustness of three state-of-the-art multimodal models\n(ALIGN, AltCLIP, GroupViT) against different sparse and contiguous pixel\ndistribution perturbations. The obtained results indicate that unimodal DNNs\nare more robust than multimodal models. Furthermore, models using CNN-based\nImage Encoder are more vulnerable than models with ViT - for untargeted\nattacks, we obtain a 99% success rate by perturbing less than 0.02% of the\nimage area.\n","authors":["Cristian-Alexandru Botocan","Raphael Meier","Ljiljana Dolamic"],"pdf_url":"https://arxiv.org/pdf/2407.18251v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.18249v1","updated":"2024-07-25T17:59:31Z","published":"2024-07-25T17:59:31Z","title":"Trajectory-aligned Space-time Tokens for Few-shot Action Recognition","summary":"  We propose a simple yet effective approach for few-shot action recognition,\nemphasizing the disentanglement of motion and appearance representations. By\nharnessing recent progress in tracking, specifically point trajectories and\nself-supervised representation learning, we build trajectory-aligned tokens\n(TATs) that capture motion and appearance information. This approach\nsignificantly reduces the data requirements while retaining essential\ninformation. To process these representations, we use a Masked Space-time\nTransformer that effectively learns to aggregate information to facilitate\nfew-shot action recognition. We demonstrate state-of-the-art results on\nfew-shot action recognition across multiple datasets. Our project page is\navailable at https://www.cs.umd.edu/~pulkit/tats\n","authors":["Pulkit Kumar","Namitha Padmanabhan","Luke Luo","Sai Saketh Rambhatla","Abhinav Shrivastava"],"pdf_url":"https://arxiv.org/pdf/2407.18249v1.pdf","comment":"ECCV 2024"},{"id":"http://arxiv.org/abs/2407.18247v1","updated":"2024-07-25T17:59:13Z","published":"2024-07-25T17:59:13Z","title":"RegionDrag: Fast Region-Based Image Editing with Diffusion Models","summary":"  Point-drag-based image editing methods, like DragDiffusion, have attracted\nsignificant attention. However, point-drag-based approaches suffer from\ncomputational overhead and misinterpretation of user intentions due to the\nsparsity of point-based editing instructions. In this paper, we propose a\nregion-based copy-and-paste dragging method, RegionDrag, to overcome these\nlimitations. RegionDrag allows users to express their editing instructions in\nthe form of handle and target regions, enabling more precise control and\nalleviating ambiguity. In addition, region-based operations complete editing in\none iteration and are much faster than point-drag-based methods. We also\nincorporate the attention-swapping technique for enhanced stability during\nediting. To validate our approach, we extend existing point-drag-based datasets\nwith region-based dragging instructions. Experimental results demonstrate that\nRegionDrag outperforms existing point-drag-based approaches in terms of speed,\naccuracy, and alignment with user intentions. Remarkably, RegionDrag completes\nthe edit on an image with a resolution of 512x512 in less than 2 seconds, which\nis more than 100x faster than DragDiffusion, while achieving better\nperformance. Project page: https://visual-ai.github.io/regiondrag.\n","authors":["Jingyi Lu","Xinghui Li","Kai Han"],"pdf_url":"https://arxiv.org/pdf/2407.18247v1.pdf","comment":"ECCV 2024, Project page: https://visual-ai.github.io/regiondrag"},{"id":"http://arxiv.org/abs/2407.18245v1","updated":"2024-07-25T17:58:17Z","published":"2024-07-25T17:58:17Z","title":"VGGHeads: A Large-Scale Synthetic Dataset for 3D Human Heads","summary":"  Human head detection, keypoint estimation, and 3D head model fitting are\nimportant tasks with many applications. However, traditional real-world\ndatasets often suffer from bias, privacy, and ethical concerns, and they have\nbeen recorded in laboratory environments, which makes it difficult for trained\nmodels to generalize. Here, we introduce VGGHeads -- a large scale synthetic\ndataset generated with diffusion models for human head detection and 3D mesh\nestimation. Our dataset comprises over 1 million high-resolution images, each\nannotated with detailed 3D head meshes, facial landmarks, and bounding boxes.\nUsing this dataset we introduce a new model architecture capable of\nsimultaneous heads detection and head meshes reconstruction from a single image\nin a single step. Through extensive experimental evaluations, we demonstrate\nthat models trained on our synthetic data achieve strong performance on real\nimages. Furthermore, the versatility of our dataset makes it applicable across\na broad spectrum of tasks, offering a general and comprehensive representation\nof human heads. Additionally, we provide detailed information about the\nsynthetic data generation pipeline, enabling it to be re-used for other tasks\nand domains.\n","authors":["Orest Kupyn","Eugene Khvedchenia","Christian Rupprecht"],"pdf_url":"https://arxiv.org/pdf/2407.18245v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.18244v1","updated":"2024-07-25T17:58:03Z","published":"2024-07-25T17:58:03Z","title":"RefMask3D: Language-Guided Transformer for 3D Referring Segmentation","summary":"  3D referring segmentation is an emerging and challenging vision-language task\nthat aims to segment the object described by a natural language expression in a\npoint cloud scene. The key challenge behind this task is vision-language\nfeature fusion and alignment. In this work, we propose RefMask3D to explore the\ncomprehensive multi-modal feature interaction and understanding. First, we\npropose a Geometry-Enhanced Group-Word Attention to integrate language with\ngeometrically coherent sub-clouds through cross-modal group-word attention,\nwhich effectively addresses the challenges posed by the sparse and irregular\nnature of point clouds. Then, we introduce a Linguistic Primitives Construction\nto produce semantic primitives representing distinct semantic attributes, which\ngreatly enhance the vision-language understanding at the decoding stage.\nFurthermore, we introduce an Object Cluster Module that analyzes the\ninterrelationships among linguistic primitives to consolidate their insights\nand pinpoint common characteristics, helping to capture holistic information\nand enhance the precision of target identification. The proposed RefMask3D\nachieves new state-of-the-art performance on 3D referring segmentation, 3D\nvisual grounding, and also 2D referring image segmentation. Especially,\nRefMask3D outperforms previous state-of-the-art method by a large margin of\n3.16% mIoU} on the challenging ScanRefer dataset. Code is available at\nhttps://github.com/heshuting555/RefMask3D.\n","authors":["Shuting He","Henghui Ding"],"pdf_url":"https://arxiv.org/pdf/2407.18244v1.pdf","comment":"ACM MM 2024, Code: https://github.com/heshuting555/RefMask3D"},{"id":"http://arxiv.org/abs/2407.18243v1","updated":"2024-07-25T17:57:48Z","published":"2024-07-25T17:57:48Z","title":"BIV-Priv-Seg: Locating Private Content in Images Taken by People With\n  Visual Impairments","summary":"  Individuals who are blind or have low vision (BLV) are at a heightened risk\nof sharing private information if they share photographs they have taken. To\nfacilitate developing technologies that can help preserve privacy, we introduce\nBIV-Priv-Seg, the first localization dataset originating from people with\nvisual impairments that shows private content. It contains 1,028 images with\nsegmentation annotations for 16 private object categories. We first\ncharacterize BIV-Priv-Seg and then evaluate modern models' performance for\nlocating private content in the dataset. We find modern models struggle most\nwith locating private objects that are not salient, small, and lack text as\nwell as recognizing when private content is absent from an image. We facilitate\nfuture extensions by sharing our new dataset with the evaluation server at\nhttps://vizwiz.org/tasks-and-datasets/object-localization.\n","authors":["Yu-Yun Tseng","Tanusree Sharma","Lotus Zhang","Abigale Stangl","Leah Findlater","Yang Wang","Danna Gurari Yu-Yun Tseng","Tanusree Sharma","Lotus Zhang","Abigale Stangl","Leah Findlater","Yang Wang","Danna Gurari"],"pdf_url":"https://arxiv.org/pdf/2407.18243v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.18240v1","updated":"2024-07-25T17:54:58Z","published":"2024-07-25T17:54:58Z","title":"CodedVO: Coded Visual Odometry","summary":"  Autonomous robots often rely on monocular cameras for odometry estimation and\nnavigation. However, the scale ambiguity problem presents a critical barrier to\neffective monocular visual odometry. In this paper, we present CodedVO, a novel\nmonocular visual odometry method that overcomes the scale ambiguity problem by\nemploying custom optics to physically encode metric depth information into\nimagery. By incorporating this information into our odometry pipeline, we\nachieve state-of-the-art performance in monocular visual odometry with a known\nscale. We evaluate our method in diverse indoor environments and demonstrate\nits robustness and adaptability. We achieve a 0.08m average trajectory error in\nodometry evaluation on the ICL-NUIM indoor odometry dataset.\n","authors":["Sachin Shah","Naitri Rajyaguru","Chahat Deep Singh","Christopher Metzler","Yiannis Aloimonos"],"pdf_url":"https://arxiv.org/pdf/2407.18240v1.pdf","comment":"7 pages, 4 figures, IEEE ROBOTICS AND AUTOMATION LETTERS"},{"id":"http://arxiv.org/abs/2407.18232v1","updated":"2024-07-25T17:50:32Z","published":"2024-07-25T17:50:32Z","title":"LION: Linear Group RNN for 3D Object Detection in Point Clouds","summary":"  The benefit of transformers in large-scale 3D point cloud perception tasks,\nsuch as 3D object detection, is limited by their quadratic computation cost\nwhen modeling long-range relationships. In contrast, linear RNNs have low\ncomputational complexity and are suitable for long-range modeling. Toward this\ngoal, we propose a simple and effective window-based framework built on LInear\ngrOup RNN (i.e., perform linear RNN for grouped features) for accurate 3D\nobject detection, called LION. The key property is to allow sufficient feature\ninteraction in a much larger group than transformer-based methods. However,\neffectively applying linear group RNN to 3D object detection in highly sparse\npoint clouds is not trivial due to its limitation in handling spatial modeling.\nTo tackle this problem, we simply introduce a 3D spatial feature descriptor and\nintegrate it into the linear group RNN operators to enhance their spatial\nfeatures rather than blindly increasing the number of scanning orders for voxel\nfeatures. To further address the challenge in highly sparse point clouds, we\npropose a 3D voxel generation strategy to densify foreground features thanks to\nlinear group RNN as a natural property of auto-regressive models. Extensive\nexperiments verify the effectiveness of the proposed components and the\ngeneralization of our LION on different linear group RNN operators including\nMamba, RWKV, and RetNet. Furthermore, it is worth mentioning that our\nLION-Mamba achieves state-of-the-art on Waymo, nuScenes, Argoverse V2, and ONCE\ndataset. Last but not least, our method supports kinds of advanced linear RNN\noperators (e.g., RetNet, RWKV, Mamba, xLSTM and TTT) on small but popular KITTI\ndataset for a quick experience with our linear RNN-based framework.\n","authors":["Zhe Liu","Jinghua Hou","Xinyu Wang","Xiaoqing Ye","Jingdong Wang","Hengshuang Zhao","Xiang Bai"],"pdf_url":"https://arxiv.org/pdf/2407.18232v1.pdf","comment":"Project page: https://happinesslz.github.io/projects/LION/"},{"id":"http://arxiv.org/abs/2407.13759v2","updated":"2024-07-25T17:47:20Z","published":"2024-07-18T17:56:30Z","title":"Streetscapes: Large-scale Consistent Street View Generation Using\n  Autoregressive Video Diffusion","summary":"  We present a method for generating Streetscapes-long sequences of views\nthrough an on-the-fly synthesized city-scale scene. Our generation is\nconditioned by language input (e.g., city name, weather), as well as an\nunderlying map/layout hosting the desired trajectory. Compared to recent models\nfor video generation or 3D view synthesis, our method can scale to much\nlonger-range camera trajectories, spanning several city blocks, while\nmaintaining visual quality and consistency. To achieve this goal, we build on\nrecent work on video diffusion, used within an autoregressive framework that\ncan easily scale to long sequences. In particular, we introduce a new temporal\nimputation method that prevents our autoregressive approach from drifting from\nthe distribution of realistic city imagery. We train our Streetscapes system on\na compelling source of data-posed imagery from Google Street View, along with\ncontextual map data-which allows users to generate city views conditioned on\nany desired city layout, with controllable camera poses. Please see more\nresults at our project page at https://boyangdeng.com/streetscapes.\n","authors":["Boyang Deng","Richard Tucker","Zhengqi Li","Leonidas Guibas","Noah Snavely","Gordon Wetzstein"],"pdf_url":"https://arxiv.org/pdf/2407.13759v2.pdf","comment":"*Equal Contributions; Fixed few duplicated references from 1st\n  upload; Project Page: https://boyangdeng.com/streetscapes"},{"id":"http://arxiv.org/abs/2211.10526v5","updated":"2024-07-25T17:29:22Z","published":"2022-11-18T22:49:04Z","title":"Castling-ViT: Compressing Self-Attention via Switching Towards\n  Linear-Angular Attention at Vision Transformer Inference","summary":"  Vision Transformers (ViTs) have shown impressive performance but still\nrequire a high computation cost as compared to convolutional neural networks\n(CNNs), one reason is that ViTs' attention measures global similarities and\nthus has a quadratic complexity with the number of input tokens. Existing\nefficient ViTs adopt local attention (e.g., Swin) or linear attention (e.g.,\nPerformer), which sacrifice ViTs' capabilities of capturing either global or\nlocal context. In this work, we ask an important research question: Can ViTs\nlearn both global and local context while being more efficient during\ninference? To this end, we propose a framework called Castling-ViT, which\ntrains ViTs using both linear-angular attention and masked softmax-based\nquadratic attention, but then switches to having only linear angular attention\nduring ViT inference. Our Castling-ViT leverages angular kernels to measure the\nsimilarities between queries and keys via spectral angles. And we further\nsimplify it with two techniques: (1) a novel linear-angular attention\nmechanism: we decompose the angular kernels into linear terms and high-order\nresiduals, and only keep the linear terms; and (2) we adopt two parameterized\nmodules to approximate high-order residuals: a depthwise convolution and an\nauxiliary masked softmax attention to help learn both global and local\ninformation, where the masks for softmax attention are regularized to gradually\nbecome zeros and thus incur no overhead during ViT inference. Extensive\nexperiments and ablation studies on three tasks consistently validate the\neffectiveness of the proposed Castling-ViT, e.g., achieving up to a 1.8% higher\naccuracy or 40% MACs reduction on ImageNet classification and 1.2 higher mAP on\nCOCO detection under comparable FLOPs, as compared to ViTs with vanilla\nsoftmax-based attentions.\n","authors":["Haoran You","Yunyang Xiong","Xiaoliang Dai","Bichen Wu","Peizhao Zhang","Haoqi Fan","Peter Vajda","Yingyan Celine Lin"],"pdf_url":"https://arxiv.org/pdf/2211.10526v5.pdf","comment":"CVPR 2023 Camera Ready"},{"id":"http://arxiv.org/abs/2407.18207v1","updated":"2024-07-25T17:17:10Z","published":"2024-07-25T17:17:10Z","title":"Geometry Fidelity for Spherical Images","summary":"  Spherical or omni-directional images offer an immersive visual format\nappealing to a wide range of computer vision applications. However, geometric\nproperties of spherical images pose a major challenge for models and metrics\ndesigned for ordinary 2D images. Here, we show that direct application of\nFr\\'echet Inception Distance (FID) is insufficient for quantifying geometric\nfidelity in spherical images. We introduce two quantitative metrics accounting\nfor geometric constraints, namely Omnidirectional FID (OmniFID) and\nDiscontinuity Score (DS). OmniFID is an extension of FID tailored to\nadditionally capture field-of-view requirements of the spherical format by\nleveraging cubemap projections. DS is a kernel-based seam alignment score of\ncontinuity across borders of 2D representations of spherical images. In\nexperiments, OmniFID and DS quantify geometry fidelity issues that are\nundetected by FID.\n","authors":["Anders Christensen","Nooshin Mojab","Khushman Patel","Karan Ahuja","Zeynep Akata","Ole Winther","Mar Gonzalez-Franco","Andrea Colaco"],"pdf_url":"https://arxiv.org/pdf/2407.18207v1.pdf","comment":"Accepted at ECCV 2024"},{"id":"http://arxiv.org/abs/2404.03613v4","updated":"2024-07-25T17:15:58Z","published":"2024-04-04T17:34:41Z","title":"Per-Gaussian Embedding-Based Deformation for Deformable 3D Gaussian\n  Splatting","summary":"  As 3D Gaussian Splatting (3DGS) provides fast and high-quality novel view\nsynthesis, it is a natural extension to deform a canonical 3DGS to multiple\nframes for representing a dynamic scene. However, previous works fail to\naccurately reconstruct complex dynamic scenes. We attribute the failure to the\ndesign of the deformation field, which is built as a coordinate-based function.\nThis approach is problematic because 3DGS is a mixture of multiple fields\ncentered at the Gaussians, not just a single coordinate-based framework. To\nresolve this problem, we define the deformation as a function of per-Gaussian\nembeddings and temporal embeddings. Moreover, we decompose deformations as\ncoarse and fine deformations to model slow and fast movements, respectively.\nAlso, we introduce a local smoothness regularization for per-Gaussian embedding\nto improve the details in dynamic regions. Project page:\nhttps://jeongminb.github.io/e-d3dgs/\n","authors":["Jeongmin Bae","Seoha Kim","Youngsik Yun","Hahyun Lee","Gun Bang","Youngjung Uh"],"pdf_url":"https://arxiv.org/pdf/2404.03613v4.pdf","comment":"ECCV 2024. Project page: https://jeongminb.github.io/e-d3dgs/"},{"id":"http://arxiv.org/abs/2407.18178v1","updated":"2024-07-25T16:37:07Z","published":"2024-07-25T16:37:07Z","title":"PianoMime: Learning a Generalist, Dexterous Piano Player from Internet\n  Demonstrations","summary":"  In this work, we introduce PianoMime, a framework for training a\npiano-playing agent using internet demonstrations. The internet is a promising\nsource of large-scale demonstrations for training our robot agents. In\nparticular, for the case of piano-playing, Youtube is full of videos of\nprofessional pianists playing a wide myriad of songs. In our work, we leverage\nthese demonstrations to learn a generalist piano-playing agent capable of\nplaying any arbitrary song. Our framework is divided into three parts: a data\npreparation phase to extract the informative features from the Youtube videos,\na policy learning phase to train song-specific expert policies from the\ndemonstrations and a policy distillation phase to distil the policies into a\nsingle generalist agent. We explore different policy designs to represent the\nagent and evaluate the influence of the amount of training data on the\ngeneralization capability of the agent to novel songs not available in the\ndataset. We show that we are able to learn a policy with up to 56\\% F1 score on\nunseen songs.\n","authors":["Cheng Qian","Julen Urain","Kevin Zakka","Jan Peters"],"pdf_url":"https://arxiv.org/pdf/2407.18178v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.18175v1","updated":"2024-07-25T16:35:46Z","published":"2024-07-25T16:35:46Z","title":"Quasar-ViT: Hardware-Oriented Quantization-Aware Architecture Search for\n  Vision Transformers","summary":"  Vision transformers (ViTs) have demonstrated their superior accuracy for\ncomputer vision tasks compared to convolutional neural networks (CNNs).\nHowever, ViT models are often computation-intensive for efficient deployment on\nresource-limited edge devices. This work proposes Quasar-ViT, a\nhardware-oriented quantization-aware architecture search framework for ViTs, to\ndesign efficient ViT models for hardware implementation while preserving the\naccuracy. First, Quasar-ViT trains a supernet using our row-wise flexible\nmixed-precision quantization scheme, mixed-precision weight entanglement, and\nsupernet layer scaling techniques. Then, it applies an efficient\nhardware-oriented search algorithm, integrated with hardware latency and\nresource modeling, to determine a series of optimal subnets from supernet under\ndifferent inference latency targets. Finally, we propose a series of\nmodel-adaptive designs on the FPGA platform to support the architecture search\nand mitigate the gap between the theoretical computation reduction and the\npractical inference speedup. Our searched models achieve 101.5, 159.6, and\n251.6 frames-per-second (FPS) inference speed on the AMD/Xilinx ZCU102 FPGA\nwith 80.4%, 78.6%, and 74.9% top-1 accuracy, respectively, for the ImageNet\ndataset, consistently outperforming prior works.\n","authors":["Zhengang Li","Alec Lu","Yanyue Xie","Zhenglun Kong","Mengshu Sun","Hao Tang","Zhong Jia Xue","Peiyan Dong","Caiwen Ding","Yanzhi Wang","Xue Lin","Zhenman Fang"],"pdf_url":"https://arxiv.org/pdf/2407.18175v1.pdf","comment":"Accepted by ICS 2024"},{"id":"http://arxiv.org/abs/2407.18145v1","updated":"2024-07-25T15:49:26Z","published":"2024-07-25T15:49:26Z","title":"Taxonomy-Aware Continual Semantic Segmentation in Hyperbolic Spaces for\n  Open-World Perception","summary":"  Semantic segmentation models are typically trained on a fixed set of classes,\nlimiting their applicability in open-world scenarios. Class-incremental\nsemantic segmentation aims to update models with emerging new classes while\npreventing catastrophic forgetting of previously learned ones. However,\nexisting methods impose strict rigidity on old classes, reducing their\neffectiveness in learning new incremental classes. In this work, we propose\nTaxonomy-Oriented Poincar\\'e-regularized Incremental-Class Segmentation\n(TOPICS) that learns feature embeddings in hyperbolic space following explicit\ntaxonomy-tree structures. This supervision provides plasticity for old classes,\nupdating ancestors based on new classes while integrating new classes at\nfitting positions. Additionally, we maintain implicit class relational\nconstraints on the geometric basis of the Poincar\\'e ball. This ensures that\nthe latent space can continuously adapt to new constraints while maintaining a\nrobust structure to combat catastrophic forgetting. We also establish eight\nrealistic incremental learning protocols for autonomous driving scenarios,\nwhere novel classes can originate from known classes or the background.\nExtensive evaluations of TOPICS on the Cityscapes and Mapillary Vistas 2.0\nbenchmarks demonstrate that it achieves state-of-the-art performance. We make\nthe code and trained models publicly available at\nhttp://topics.cs.uni-freiburg.de.\n","authors":["Julia Hindel","Daniele Cattaneo","Abhinav Valada"],"pdf_url":"https://arxiv.org/pdf/2407.18145v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08651v3","updated":"2024-07-25T15:46:50Z","published":"2024-03-13T16:06:07Z","title":"HAIFIT: Human-Centered AI for Fashion Image Translation","summary":"  In the realm of fashion design, sketches serve as the canvas for expressing\nan artist's distinctive drawing style and creative vision, capturing intricate\ndetails like stroke variations and texture nuances. The advent of\nsketch-to-image cross-modal translation technology has notably aided designers.\nHowever, existing methods often compromise these sketch details during image\ngeneration, resulting in images that deviate from the designer's intended\nconcept. This limitation hampers the ability to offer designers a precise\npreview of the final output. To overcome this challenge, we introduce HAIFIT, a\nnovel approach that transforms sketches into high-fidelity, lifelike clothing\nimages by integrating multi-scale features and capturing extensive feature map\ndependencies from diverse perspectives. Through extensive qualitative and\nquantitative evaluations conducted on our self-collected dataset, our method\ndemonstrates superior performance compared to existing methods in generating\nphotorealistic clothing images. Our method excels in preserving the distinctive\nstyle and intricate details essential for fashion design applications. In\naddition, our method also has obvious advantages in model training and\ninference speed, contributing to reducing designers' time costs and improving\ndesign efficiency.\n","authors":["Jianan Jiang","Xinglin Li","Weiren Yu","Di Wu"],"pdf_url":"https://arxiv.org/pdf/2403.08651v3.pdf","comment":"10 pages,7 figures"},{"id":"http://arxiv.org/abs/2404.03632v2","updated":"2024-07-25T15:45:58Z","published":"2024-04-04T17:53:33Z","title":"Reference-Based 3D-Aware Image Editing with Triplanes","summary":"  Generative Adversarial Networks (GANs) have emerged as powerful tools for\nhigh-quality image generation and real image editing by manipulating their\nlatent spaces. Recent advancements in GANs include 3D-aware models such as\nEG3D, which feature efficient triplane-based architectures capable of\nreconstructing 3D geometry from single images. However, limited attention has\nbeen given to providing an integrated framework for 3D-aware, high-quality,\nreference-based image editing. This study addresses this gap by exploring and\ndemonstrating the effectiveness of the triplane space for advanced\nreference-based edits. Our novel approach integrates encoding, automatic\nlocalization, spatial disentanglement of triplane features, and fusion learning\nto achieve the desired edits. Additionally, our framework demonstrates\nversatility and robustness across various domains, extending its effectiveness\nto animal face edits, partially stylized edits like cartoon faces, full-body\nclothing edits, and 360-degree head edits. Our method shows state-of-the-art\nperformance over relevant latent direction, text, and image-guided 2D and\n3D-aware diffusion and GAN methods, both qualitatively and quantitatively.\n","authors":["Bahri Batuhan Bilecen","Yigit Yalin","Ning Yu","Aysegul Dundar"],"pdf_url":"https://arxiv.org/pdf/2404.03632v2.pdf","comment":"20 pages, including supplementary material"},{"id":"http://arxiv.org/abs/2407.18137v1","updated":"2024-07-25T15:42:46Z","published":"2024-07-25T15:42:46Z","title":"XS-VID: An Extremely Small Video Object Detection Dataset","summary":"  Small Video Object Detection (SVOD) is a crucial subfield in modern computer\nvision, essential for early object discovery and detection. However, existing\nSVOD datasets are scarce and suffer from issues such as insufficiently small\nobjects, limited object categories, and lack of scene diversity, leading to\nunitary application scenarios for corresponding methods. To address this gap,\nwe develop the XS-VID dataset, which comprises aerial data from various periods\nand scenes, and annotates eight major object categories. To further evaluate\nexisting methods for detecting extremely small objects, XS-VID extensively\ncollects three types of objects with smaller pixel areas: extremely small\n(\\textit{es}, $0\\sim12^2$), relatively small (\\textit{rs}, $12^2\\sim20^2$), and\ngenerally small (\\textit{gs}, $20^2\\sim32^2$). XS-VID offers unprecedented\nbreadth and depth in covering and quantifying minuscule objects, significantly\nenriching the scene and object diversity in the dataset. Extensive validations\non XS-VID and the publicly available VisDrone2019VID dataset show that existing\nmethods struggle with small object detection and significantly underperform\ncompared to general object detectors. Leveraging the strengths of previous\nmethods and addressing their weaknesses, we propose YOLOFT, which enhances\nlocal feature associations and integrates temporal motion features,\nsignificantly improving the accuracy and stability of SVOD. Our datasets and\nbenchmarks are available at \\url{https://gjhhust.github.io/XS-VID/}.\n","authors":["Jiahao Guo","Ziyang Xu","Lianjun Wu","Fei Gao","Wenyu Liu","Xinggang Wang"],"pdf_url":"https://arxiv.org/pdf/2407.18137v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.18134v1","updated":"2024-07-25T15:38:16Z","published":"2024-07-25T15:38:16Z","title":"$\\mathbb{X}$-Sample Contrastive Loss: Improving Contrastive Learning\n  with Sample Similarity Graphs","summary":"  Learning good representations involves capturing the diverse ways in which\ndata samples relate. Contrastive loss - an objective matching related samples -\nunderlies methods from self-supervised to multimodal learning. Contrastive\nlosses, however, can be viewed more broadly as modifying a similarity graph to\nindicate how samples should relate in the embedding space. This view reveals a\nshortcoming in contrastive learning: the similarity graph is binary, as only\none sample is the related positive sample. Crucially, similarities\n\\textit{across} samples are ignored. Based on this observation, we revise the\nstandard contrastive loss to explicitly encode how a sample relates to others.\nWe experiment with this new objective, called $\\mathbb{X}$-Sample Contrastive,\nto train vision models based on similarities in class or text caption\ndescriptions. Our study spans three scales: ImageNet-1k with 1 million, CC3M\nwith 3 million, and CC12M with 12 million samples. The representations learned\nvia our objective outperform both contrastive self-supervised and\nvision-language models trained on the same data across a range of tasks. When\ntraining on CC12M, we outperform CLIP by $0.6\\%$ on both ImageNet and ImageNet\nReal. Our objective appears to work particularly well in lower-data regimes,\nwith gains over CLIP of $16.8\\%$ on ImageNet and $18.1\\%$ on ImageNet Real when\ntraining with CC3M. Finally, our objective seems to encourage the model to\nlearn representations that separate objects from their attributes and\nbackgrounds, with gains of $3.3$-$5.6$\\% over CLIP on ImageNet9. We hope the\nproposed solution takes a small step towards developing richer learning\nobjectives for understanding sample relations in foundation models.\n","authors":["Vlad Sobal","Mark Ibrahim","Randall Balestriero","Vivien Cabannes","Diane Bouchacourt","Pietro Astolfi","Kyunghyun Cho","Yann LeCun"],"pdf_url":"https://arxiv.org/pdf/2407.18134v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.18128v1","updated":"2024-07-25T15:35:44Z","published":"2024-07-25T15:35:44Z","title":"Estimating Earthquake Magnitude in Sentinel-1 Imagery via Ranking","summary":"  Earthquakes are commonly estimated using physical seismic stations, however,\ndue to the installation requirements and costs of these stations, global\ncoverage quickly becomes impractical. An efficient and lower-cost alternative\nis to develop machine learning models to globally monitor earth observation\ndata to pinpoint regions impacted by these natural disasters. However, due to\nthe small amount of historically recorded earthquakes, this becomes a low-data\nregime problem requiring algorithmic improvements to achieve peak performance\nwhen learning to regress earthquake magnitude. In this paper, we propose to\npose the estimation of earthquake magnitudes as a metric-learning problem,\ntraining models to not only estimate earthquake magnitude from Sentinel-1\nsatellite imagery but to additionally rank pairwise samples. Our experiments\nshow at max a 30%+ improvement in MAE over prior regression-only based methods,\nparticularly transformer-based architectures.\n","authors":["Daniele Rege Cambrin","Isaac Corley","Paolo Garza","Peyman Najafirad"],"pdf_url":"https://arxiv.org/pdf/2407.18128v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17449v2","updated":"2024-07-25T15:33:00Z","published":"2024-07-24T17:30:21Z","title":"Looking at Model Debiasing through the Lens of Anomaly Detection","summary":"  It is widely recognized that deep neural networks are sensitive to bias in\nthe data. This means that during training these models are likely to learn\nspurious correlations between data and labels, resulting in limited\ngeneralization abilities and low performance. In this context, model debiasing\napproaches can be devised aiming at reducing the model's dependency on such\nunwanted correlations, either leveraging the knowledge of bias information or\nnot. In this work, we focus on the latter and more realistic scenario, showing\nthe importance of accurately predicting the bias-conflicting and bias-aligned\nsamples to obtain compelling performance in bias mitigation. On this ground, we\npropose to conceive the problem of model bias from an out-of-distribution\nperspective, introducing a new bias identification method based on anomaly\ndetection. We claim that when data is mostly biased, bias-conflicting samples\ncan be regarded as outliers with respect to the bias-aligned distribution in\nthe feature space of a biased model, thus allowing for precisely detecting them\nwith an anomaly detection method. Coupling the proposed bias identification\napproach with bias-conflicting data upsampling and augmentation in a two-step\nstrategy, we reach state-of-the-art performance on synthetic and real benchmark\ndatasets. Ultimately, our proposed approach shows that the data bias issue does\nnot necessarily require complex debiasing methods, given that an accurate bias\nidentification procedure is defined.\n","authors":["Vito Paolo Pastore","Massimiliano Ciranni","Davide Marinelli","Francesca Odone","Vittorio Murino"],"pdf_url":"https://arxiv.org/pdf/2407.17449v2.pdf","comment":"15 pages, 7 figures"},{"id":"http://arxiv.org/abs/2407.18125v1","updated":"2024-07-25T15:32:59Z","published":"2024-07-25T15:32:59Z","title":"Self-supervised pre-training with diffusion model for few-shot landmark\n  detection in x-ray images","summary":"  In the last few years, deep neural networks have been extensively applied in\nthe medical domain for different tasks, ranging from image classification and\nsegmentation to landmark detection. However, the application of these\ntechnologies in the medical domain is often hindered by data scarcity, both in\nterms of available annotations and images. This study introduces a new\nself-supervised pre-training protocol based on diffusion models for landmark\ndetection in x-ray images. Our results show that the proposed self-supervised\nframework can provide accurate landmark detection with a minimal number of\navailable annotated training images (up to 50), outperforming ImageNet\nsupervised pre-training and state-of-the-art self-supervised pre-trainings for\nthree popular x-ray benchmark datasets. To our knowledge, this is the first\nexploration of diffusion models for self-supervised learning in landmark\ndetection, which may offer a valuable pre-training approach in few-shot\nregimes, for mitigating data scarcity.\n","authors":["Roberto Di Via","Francesca Odone","Vito Paolo Pastore"],"pdf_url":"https://arxiv.org/pdf/2407.18125v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13129v2","updated":"2024-07-25T15:32:39Z","published":"2024-03-19T19:58:54Z","title":"Better Call SAL: Towards Learning to Segment Anything in Lidar","summary":"  We propose the SAL (Segment Anything in Lidar) method consisting of a\ntext-promptable zero-shot model for segmenting and classifying any object in\nLidar, and a pseudo-labeling engine that facilitates model training without\nmanual supervision. While the established paradigm for Lidar Panoptic\nSegmentation (LPS) relies on manual supervision for a handful of object classes\ndefined a priori, we utilize 2D vision foundation models to generate 3D\nsupervision ``for free''. Our pseudo-labels consist of instance masks and\ncorresponding CLIP tokens, which we lift to Lidar using calibrated multi-modal\ndata. By training our model on these labels, we distill the 2D foundation\nmodels into our Lidar SAL model. Even without manual labels, our model achieves\n$91\\%$ in terms of class-agnostic segmentation and $54\\%$ in terms of zero-shot\nLidar Panoptic Segmentation of the fully supervised state-of-the-art.\nFurthermore, we outperform several baselines that do not distill but only lift\nimage features to 3D. More importantly, we demonstrate that SAL supports\narbitrary class prompts, can be easily extended to new datasets, and shows\nsignificant potential to improve with increasing amounts of self-labeled data.\nCode and models are available at this\n$\\href{https://github.com/nv-dvl/segment-anything-lidar}{URL}$.\n","authors":["Aljoša Ošep","Tim Meinhardt","Francesco Ferroni","Neehar Peri","Deva Ramanan","Laura Leal-Taixé"],"pdf_url":"https://arxiv.org/pdf/2403.13129v2.pdf","comment":"Accepted to ECCV 2024"},{"id":"http://arxiv.org/abs/2407.18121v1","updated":"2024-07-25T15:29:05Z","published":"2024-07-25T15:29:05Z","title":"Efficient Inference of Vision Instruction-Following Models with Elastic\n  Cache","summary":"  In the field of instruction-following large vision-language models (LVLMs),\nthe efficient deployment of these models faces challenges, notably due to the\nhigh memory demands of their key-value (KV) caches. Conventional cache\nmanagement strategies for LLMs focus on cache eviction, which often fails to\naddress the specific needs of multimodal instruction-following models.\nRecognizing this gap, in this paper, we introduce Elastic Cache, a novel\napproach that benefits from applying distinct acceleration methods for\ninstruction encoding and output generation stages. We investigate the metrics\nof importance in different stages and propose an importance-driven cache\nmerging strategy to prune redundancy caches. Instead of discarding less\nimportant caches, our strategy identifies important key/value vectors as anchor\npoints. Surrounding less important caches are then merged with these anchors,\nenhancing the preservation of contextual information in the KV caches while\nyielding an arbitrary acceleration ratio. For instruction encoding, we utilize\nthe frequency to evaluate the importance of caches. Regarding output\ngeneration, we prioritize tokens based on their distance with an offset, by\nwhich both the initial and most recent tokens are retained. Results on a range\nof LVLMs demonstrate that Elastic Cache not only boosts efficiency but also\nnotably outperforms existing pruning methods in language generation across\nvarious tasks. Code is available at https://github.com/liuzuyan/ElasticCache\n","authors":["Zuyan Liu","Benlin Liu","Jiahui Wang","Yuhao Dong","Guangyi Chen","Yongming Rao","Ranjay Krishna","Jiwen Lu"],"pdf_url":"https://arxiv.org/pdf/2407.18121v1.pdf","comment":"Accepted to ECCV 2024"},{"id":"http://arxiv.org/abs/2407.18112v1","updated":"2024-07-25T15:20:58Z","published":"2024-07-25T15:20:58Z","title":"Keypoint Promptable Re-Identification","summary":"  Occluded Person Re-Identification (ReID) is a metric learning task that\ninvolves matching occluded individuals based on their appearance. While many\nstudies have tackled occlusions caused by objects, multi-person occlusions\nremain less explored. In this work, we identify and address a critical\nchallenge overlooked by previous occluded ReID methods: the Multi-Person\nAmbiguity (MPA) arising when multiple individuals are visible in the same\nbounding box, making it impossible to determine the intended ReID target among\nthe candidates. Inspired by recent work on prompting in vision, we introduce\nKeypoint Promptable ReID (KPR), a novel formulation of the ReID problem that\nexplicitly complements the input bounding box with a set of semantic keypoints\nindicating the intended target. Since promptable re-identification is an\nunexplored paradigm, existing ReID datasets lack the pixel-level annotations\nnecessary for prompting. To bridge this gap and foster further research on this\ntopic, we introduce Occluded-PoseTrack ReID, a novel ReID dataset with\nkeypoints labels, that features strong inter-person occlusions. Furthermore, we\nrelease custom keypoint labels for four popular ReID benchmarks. Experiments on\nperson retrieval, but also on pose tracking, demonstrate that our method\nsystematically surpasses previous state-of-the-art approaches on various\noccluded scenarios. Our code, dataset and annotations are available at\nhttps://github.com/VlSomers/keypoint_promptable_reidentification.\n","authors":["Vladimir Somers","Christophe De Vleeschouwer","Alexandre Alahi"],"pdf_url":"https://arxiv.org/pdf/2407.18112v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.02878v2","updated":"2024-07-25T15:20:48Z","published":"2023-12-05T16:48:17Z","title":"Towards More Practical Group Activity Detection: A New Benchmark and\n  Model","summary":"  Group activity detection (GAD) is the task of identifying members of each\ngroup and classifying the activity of the group at the same time in a video.\nWhile GAD has been studied recently, there is still much room for improvement\nin both dataset and methodology due to their limited capability to address\npractical GAD scenarios. To resolve these issues, we first present a new\ndataset, dubbed Caf\\'e. Unlike existing datasets, Caf\\'e is constructed\nprimarily for GAD and presents more practical scenarios and metrics, as well as\nbeing large-scale and providing rich annotations. Along with the dataset, we\npropose a new GAD model that deals with an unknown number of groups and latent\ngroup members efficiently and effectively. We evaluated our model on three\ndatasets including Caf\\'e, where it outperformed previous work in terms of both\naccuracy and inference speed.\n","authors":["Dongkeun Kim","Youngkil Song","Minsu Cho","Suha Kwak"],"pdf_url":"https://arxiv.org/pdf/2312.02878v2.pdf","comment":"Accepted to ECCV 2024, Project page:\n  https://cvlab.postech.ac.kr/research/CAFE"},{"id":"http://arxiv.org/abs/2407.18105v1","updated":"2024-07-25T15:08:54Z","published":"2024-07-25T15:08:54Z","title":"Multi-Resolution Histopathology Patch Graphs for Ovarian Cancer\n  Subtyping","summary":"  Computer vision models are increasingly capable of classifying ovarian\nepithelial cancer subtypes, but they differ from pathologists by processing\nsmall tissue patches at a single resolution. Multi-resolution graph models\nleverage the spatial relationships of patches at multiple magnifications,\nlearning the context for each patch. In this study, we conduct the most\nthorough validation of a graph model for ovarian cancer subtyping to date.\nSeven models were tuned and trained using five-fold cross-validation on a set\nof 1864 whole slide images (WSIs) from 434 patients treated at Leeds Teaching\nHospitals NHS Trust. The cross-validation models were ensembled and evaluated\nusing a balanced hold-out test set of 100 WSIs from 30 patients, and an\nexternal validation set of 80 WSIs from 80 patients in the Transcanadian Study.\nThe best-performing model, a graph model using 10x+20x magnification data, gave\nbalanced accuracies of 73%, 88%, and 99% in cross-validation, hold-out testing,\nand external validation, respectively. However, this only exceeded the\nperformance of attention-based multiple instance learning in external\nvalidation, with a 93% balanced accuracy. Graph models benefitted greatly from\nusing the UNI foundation model rather than an ImageNet-pretrained ResNet50 for\nfeature extraction, with this having a much greater effect on performance than\nchanging the subsequent classification approach. The accuracy of the combined\nfoundation model and multi-resolution graph network offers a step towards the\nclinical applicability of these models, with a new highest-reported performance\nfor this task, though further validations are still required to ensure the\nrobustness and usability of the models.\n","authors":["Jack Breen","Katie Allen","Kieran Zucker","Nicolas M. Orsi","Nishant Ravikumar"],"pdf_url":"https://arxiv.org/pdf/2407.18105v1.pdf","comment":"Initially submitted version of a paper which has been accepted in the\n  GRAIL workshop at MICCAI 2024"},{"id":"http://arxiv.org/abs/2406.09272v3","updated":"2024-07-25T15:03:37Z","published":"2024-06-13T16:10:19Z","title":"Action2Sound: Ambient-Aware Generation of Action Sounds from Egocentric\n  Videos","summary":"  Generating realistic audio for human actions is important for many\napplications, such as creating sound effects for films or virtual reality\ngames. Existing approaches implicitly assume total correspondence between the\nvideo and audio during training, yet many sounds happen off-screen and have\nweak to no correspondence with the visuals -- resulting in uncontrolled ambient\nsounds or hallucinations at test time. We propose a novel ambient-aware audio\ngeneration model, AV-LDM. We devise a novel audio-conditioning mechanism to\nlearn to disentangle foreground action sounds from the ambient background\nsounds in in-the-wild training videos. Given a novel silent video, our model\nuses retrieval-augmented generation to create audio that matches the visual\ncontent both semantically and temporally. We train and evaluate our model on\ntwo in-the-wild egocentric video datasets, Ego4D and EPIC-KITCHENS, and we\nintroduce Ego4D-Sounds -- 1.2M curated clips with action-audio correspondence.\nOur model outperforms an array of existing methods, allows controllable\ngeneration of the ambient sound, and even shows promise for generalizing to\ncomputer graphics game clips. Overall, our approach is the first to focus\nvideo-to-audio generation faithfully on the observed visual content despite\ntraining from uncurated clips with natural background sounds.\n","authors":["Changan Chen","Puyuan Peng","Ami Baid","Zihui Xue","Wei-Ning Hsu","David Harwath","Kristen Grauman"],"pdf_url":"https://arxiv.org/pdf/2406.09272v3.pdf","comment":"Project page: https://vision.cs.utexas.edu/projects/action2sound.\n  ECCV 2024 camera-ready version"},{"id":"http://arxiv.org/abs/2407.18100v1","updated":"2024-07-25T15:03:36Z","published":"2024-07-25T15:03:36Z","title":"DINOv2 Rocks Geological Image Analysis: Classification, Segmentation,\n  and Interpretability","summary":"  This study investigates the interpretability, classification, and\nsegmentation of CT-scan images of rock samples, with a particular focus on the\napplication of DINOv2 within Geosciences. We compared various segmentation\ntechniques to evaluate their efficacy, efficiency, and adaptability in\ngeological image analysis. The methods assessed include the Otsu thresholding\nmethod, clustering techniques (K-means and fuzzy C-means), a supervised machine\nlearning approach (Random Forest), and deep learning methods (UNet and DINOv2).\nWe tested these methods using ten binary sandstone datasets and three\nmulti-class calcite datasets. To begin, we provide a thorough interpretability\nanalysis of DINOv2's features in the geoscientific context, discussing its\nsuitability and inherent ability to process CT-scanned rock data. In terms of\nclassification, the out-of-the-box DINOv2 demonstrates an impressive capability\nto perfectly classify rock images, even when the CT scans are out of its\noriginal training set. Regarding segmentation, thresholding and unsupervised\nmethods, while fast, perform poorly despite image preprocessing, whereas\nsupervised methods show better results. We underscore the computational demands\nof deep learning but highlight its minimal intervention, superior\ngeneralization, and performance without additional image preprocessing.\nAdditionally, we observe a lack of correlation between a network's depth or the\nnumber of parameters and its performance. Our results show that a LoRA\nfine-tuned DINOv2 excels in out-of-distribution segmentation and significantly\noutperforms other methods in multi-class segmentation. By systematically\ncomparing these methods, we identify the most efficient strategy for meticulous\nand laborious segmentation tasks. DINOv2 proves advantageous, achieving\nsegmentations that could be described as \"better than ground-truth\" against\nrelatively small training sets.\n","authors":["Florent Brondolo","Samuel Beaussant"],"pdf_url":"https://arxiv.org/pdf/2407.18100v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.18097v1","updated":"2024-07-25T15:02:24Z","published":"2024-07-25T15:02:24Z","title":"SSTD: Stripe-Like Space Target Detection using Single-Point Supervision","summary":"  Stripe-like space target detection (SSTD) plays a key role in enhancing space\nsituational awareness and assessing spacecraft behaviour. This domain faces\nthree challenges: the lack of publicly available datasets, interference from\nstray light and stars, and the variability of stripe-like targets, which\ncomplicates pixel-level annotation. In response, we introduces\n`AstroStripeSet', a pioneering dataset designed for SSTD, aiming to bridge the\ngap in academic resources and advance research in SSTD. Furthermore, we propose\na novel pseudo-label evolution teacher-student framework with single-point\nsupervision. This framework starts with generating initial pseudo-labels using\nthe zero-shot capabilities of the Segment Anything Model (SAM) in a\nsingle-point setting, and refines these labels iteratively. In our framework,\nthe fine-tuned StripeSAM serves as the teacher and the newly developed\nStripeNet as the student, consistently improving segmentation performance by\nimproving the quality of pseudo-labels. We also introduce `GeoDice', a new loss\nfunction customized for the linear characteristics of stripe-like targets.\nExtensive experiments show that the performance of our approach matches fully\nsupervised methods on all evaluation metrics, establishing a new\nstate-of-the-art (SOTA) benchmark. Our dataset and code will be made publicly\navailable.\n","authors":["Zijian Zhu","Ali Zia","Xuesong Li","Bingbing Dan","Yuebo Ma","Enhai Liu","Rujin Zhao"],"pdf_url":"https://arxiv.org/pdf/2407.18097v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.00300v2","updated":"2024-07-25T14:48:34Z","published":"2024-02-01T03:27:26Z","title":"Self-supervised learning of video representations from a child's\n  perspective","summary":"  Children learn powerful internal models of the world around them from a few\nyears of egocentric visual experience. Can such internal models be learned from\na child's visual experience with highly generic learning algorithms or do they\nrequire strong inductive biases? Recent advances in collecting large-scale,\nlongitudinal, developmentally realistic video datasets and generic\nself-supervised learning (SSL) algorithms are allowing us to begin to tackle\nthis nature vs. nurture question. However, existing work typically focuses on\nimage-based SSL algorithms and visual capabilities that can be learned from\nstatic images (e.g. object recognition), thus ignoring temporal aspects of the\nworld. To close this gap, here we train self-supervised video models on\nlongitudinal, egocentric headcam recordings collected from a child over a two\nyear period in their early development (6-31 months). The resulting models are\nhighly effective at facilitating the learning of action concepts from a small\nnumber of labeled examples; they have favorable data size scaling properties;\nand they display emergent video interpolation capabilities. Video models also\nlearn more robust object representations than image-based models trained with\nthe exact same data. These results suggest that important temporal aspects of a\nchild's internal model of the world may be learnable from their visual\nexperience using highly generic learning algorithms and without strong\ninductive biases.\n","authors":["A. Emin Orhan","Wentao Wang","Alex N. Wang","Mengye Ren","Brenden M. Lake"],"pdf_url":"https://arxiv.org/pdf/2402.00300v2.pdf","comment":"Published as a conference paper at CogSci 2024; code & models\n  available from https://github.com/eminorhan/video-models"},{"id":"http://arxiv.org/abs/2404.09556v2","updated":"2024-07-25T14:42:11Z","published":"2024-04-15T08:19:08Z","title":"nnU-Net Revisited: A Call for Rigorous Validation in 3D Medical Image\n  Segmentation","summary":"  The release of nnU-Net marked a paradigm shift in 3D medical image\nsegmentation, demonstrating that a properly configured U-Net architecture could\nstill achieve state-of-the-art results. Despite this, the pursuit of novel\narchitectures, and the respective claims of superior performance over the U-Net\nbaseline, continued. In this study, we demonstrate that many of these recent\nclaims fail to hold up when scrutinized for common validation shortcomings,\nsuch as the use of inadequate baselines, insufficient datasets, and neglected\ncomputational resources. By meticulously avoiding these pitfalls, we conduct a\nthorough and comprehensive benchmarking of current segmentation methods\nincluding CNN-based, Transformer-based, and Mamba-based approaches. In contrast\nto current beliefs, we find that the recipe for state-of-the-art performance is\n1) employing CNN-based U-Net models, including ResNet and ConvNeXt variants, 2)\nusing the nnU-Net framework, and 3) scaling models to modern hardware\nresources. These results indicate an ongoing innovation bias towards novel\narchitectures in the field and underscore the need for more stringent\nvalidation standards in the quest for scientific progress.\n","authors":["Fabian Isensee","Tassilo Wald","Constantin Ulrich","Michael Baumgartner","Saikat Roy","Klaus Maier-Hein","Paul F. Jaeger"],"pdf_url":"https://arxiv.org/pdf/2404.09556v2.pdf","comment":"Accepted at MICCAI 2024"},{"id":"http://arxiv.org/abs/2406.14549v2","updated":"2024-07-25T14:33:33Z","published":"2024-06-20T17:56:17Z","title":"Uncovering Latent Memories: Assessing Data Leakage and Memorization\n  Patterns in Frontier AI Models","summary":"  Frontier AI systems are making transformative impacts across society, but\nsuch benefits are not without costs: models trained on web-scale datasets\ncontaining personal and private data raise profound concerns about data privacy\nand security. Language models are trained on extensive corpora including\npotentially sensitive or proprietary information, and the risk of data leakage\n- where the model response reveals pieces of such information - remains\ninadequately understood. Prior work has investigated what factors drive\nmemorization and have identified that sequence complexity and the number of\nrepetitions drive memorization. Here, we focus on the evolution of memorization\nover training. We begin by reproducing findings that the probability of\nmemorizing a sequence scales logarithmically with the number of times it is\npresent in the data. We next show that sequences which are apparently not\nmemorized after the first encounter can be \"uncovered\" throughout the course of\ntraining even without subsequent encounters, a phenomenon we term \"latent\nmemorization\". The presence of latent memorization presents a challenge for\ndata privacy as memorized sequences may be hidden at the final checkpoint of\nthe model but remain easily recoverable. To this end, we develop a diagnostic\ntest relying on the cross entropy loss to uncover latent memorized sequences\nwith high accuracy.\n","authors":["Sunny Duan","Mikail Khona","Abhiram Iyer","Rylan Schaeffer","Ila R Fiete"],"pdf_url":"https://arxiv.org/pdf/2406.14549v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.10885v3","updated":"2024-07-25T14:30:22Z","published":"2024-02-16T18:43:02Z","title":"3D Diffuser Actor: Policy Diffusion with 3D Scene Representations","summary":"  Diffusion policies are conditional diffusion models that learn robot action\ndistributions conditioned on the robot and environment state. They have\nrecently shown to outperform both deterministic and alternative action\ndistribution learning formulations. 3D robot policies use 3D scene feature\nrepresentations aggregated from a single or multiple camera views using sensed\ndepth. They have shown to generalize better than their 2D counterparts across\ncamera viewpoints. We unify these two lines of work and present 3D Diffuser\nActor, a neural policy equipped with a novel 3D denoising transformer that\nfuses information from the 3D visual scene, a language instruction and\nproprioception to predict the noise in noised 3D robot pose trajectories. 3D\nDiffuser Actor sets a new state-of-the-art on RLBench with an absolute\nperformance gain of 18.1% over the current SOTA on a multi-view setup and an\nabsolute gain of 13.1% on a single-view setup. On the CALVIN benchmark, it\nimproves over the current SOTA by a 9% relative increase. It also learns to\ncontrol a robot manipulator in the real world from a handful of demonstrations.\nThrough thorough comparisons with the current SOTA policies and ablations of\nour model, we show 3D Diffuser Actor's design choices dramatically outperform\n2D representations, regression and classification objectives, absolute\nattentions, and holistic non-tokenized 3D scene embeddings.\n","authors":["Tsung-Wei Ke","Nikolaos Gkanatsios","Katerina Fragkiadaki"],"pdf_url":"https://arxiv.org/pdf/2402.10885v3.pdf","comment":"First two authors contributed equally"},{"id":"http://arxiv.org/abs/2407.18070v1","updated":"2024-07-25T14:25:17Z","published":"2024-07-25T14:25:17Z","title":"CSWin-UNet: Transformer UNet with Cross-Shaped Windows for Medical Image\n  Segmentation","summary":"  Deep learning, especially convolutional neural networks (CNNs) and\nTransformer architectures, have become the focus of extensive research in\nmedical image segmentation, achieving impressive results. However, CNNs come\nwith inductive biases that limit their effectiveness in more complex, varied\nsegmentation scenarios. Conversely, while Transformer-based methods excel at\ncapturing global and long-range semantic details, they suffer from high\ncomputational demands. In this study, we propose CSWin-UNet, a novel U-shaped\nsegmentation method that incorporates the CSWin self-attention mechanism into\nthe UNet to facilitate horizontal and vertical stripes self-attention. This\nmethod significantly enhances both computational efficiency and receptive field\ninteractions. Additionally, our innovative decoder utilizes a content-aware\nreassembly operator that strategically reassembles features, guided by\npredicted kernels, for precise image resolution restoration. Our extensive\nempirical evaluations on diverse datasets, including synapse multi-organ CT,\ncardiac MRI, and skin lesions, demonstrate that CSWin-UNet maintains low model\ncomplexity while delivering high segmentation accuracy.\n","authors":["Xiao Liu","Peng Gao","Tao Yu","Fei Wang","Ru-Yue Yuan"],"pdf_url":"https://arxiv.org/pdf/2407.18070v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.18067v1","updated":"2024-07-25T14:21:50Z","published":"2024-07-25T14:21:50Z","title":"HVM-1: Large-scale video models pretrained with nearly 5000 hours of\n  human-like video data","summary":"  We introduce Human-like Video Models (HVM-1), large-scale video models\npretrained with nearly 5000 hours of curated human-like video data (mostly\negocentric, temporally extended, continuous video recordings), using the\nspatiotemporal masked autoencoder (ST-MAE) algorithm. We release two 633M\nparameter models trained at spatial resolutions of 224x224 and 448x448 pixels.\nWe evaluate the performance of these models in downstream few-shot video and\nimage recognition tasks and compare them against a model pretrained with 1330\nhours of short action-oriented video clips from YouTube (Kinetics-700). HVM-1\nmodels perform competitively against the Kinetics-700 pretrained model in\ndownstream evaluations despite substantial qualitative differences between the\nspatiotemporal characteristics of the corresponding pretraining datasets. HVM-1\nmodels also learn more accurate and more robust object representations compared\nto models pretrained with the image-based MAE algorithm on the same data,\ndemonstrating the potential benefits of learning to predict temporal\nregularities in natural videos for learning better object representations.\n","authors":["A. Emin Orhan"],"pdf_url":"https://arxiv.org/pdf/2407.18067v1.pdf","comment":"10 pages, 5 figures, 1 table; code & models available from\n  https://github.com/eminorhan/hvm-1"},{"id":"http://arxiv.org/abs/2407.18054v1","updated":"2024-07-25T14:07:49Z","published":"2024-07-25T14:07:49Z","title":"LKCell: Efficient Cell Nuclei Instance Segmentation with Large\n  Convolution Kernels","summary":"  The segmentation of cell nuclei in tissue images stained with the blood dye\nhematoxylin and eosin (H$\\&$E) is essential for various clinical applications\nand analyses. Due to the complex characteristics of cellular morphology, a\nlarge receptive field is considered crucial for generating high-quality\nsegmentation. However, previous methods face challenges in achieving a balance\nbetween the receptive field and computational burden. To address this issue, we\npropose LKCell, a high-accuracy and efficient cell segmentation method. Its\ncore insight lies in unleashing the potential of large convolution kernels to\nachieve computationally efficient large receptive fields. Specifically, (1) We\ntransfer pre-trained large convolution kernel models to the medical domain for\nthe first time, demonstrating their effectiveness in cell segmentation. (2) We\nanalyze the redundancy of previous methods and design a new segmentation\ndecoder based on large convolution kernels. It achieves higher performance\nwhile significantly reducing the number of parameters. We evaluate our method\non the most challenging benchmark and achieve state-of-the-art results (0.5080\nmPQ) in cell nuclei instance segmentation with only 21.6% FLOPs compared with\nthe previous leading method. Our source code and models are available at\nhttps://github.com/hustvl/LKCell.\n","authors":["Ziwei Cui","Jingfeng Yao","Lunbin Zeng","Juan Yang","Wenyu Liu","Xinggang Wang"],"pdf_url":"https://arxiv.org/pdf/2407.18054v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.18046v1","updated":"2024-07-25T13:53:48Z","published":"2024-07-25T13:53:48Z","title":"GaussianSR: High Fidelity 2D Gaussian Splatting for Arbitrary-Scale\n  Image Super-Resolution","summary":"  Implicit neural representations (INRs) have significantly advanced the field\nof arbitrary-scale super-resolution (ASSR) of images. Most existing INR-based\nASSR networks first extract features from the given low-resolution image using\nan encoder, and then render the super-resolved result via a multi-layer\nperceptron decoder. Although these approaches have shown promising results,\ntheir performance is constrained by the limited representation ability of\ndiscrete latent codes in the encoded features. In this paper, we propose a\nnovel ASSR method named GaussianSR that overcomes this limitation through 2D\nGaussian Splatting (2DGS). Unlike traditional methods that treat pixels as\ndiscrete points, GaussianSR represents each pixel as a continuous Gaussian\nfield. The encoded features are simultaneously refined and upsampled by\nrendering the mutually stacked Gaussian fields. As a result, long-range\ndependencies are established to enhance representation ability. In addition, a\nclassifier is developed to dynamically assign Gaussian kernels to all pixels to\nfurther improve flexibility. All components of GaussianSR (i.e., encoder,\nclassifier, Gaussian kernels, and decoder) are jointly learned end-to-end.\nExperiments demonstrate that GaussianSR achieves superior ASSR performance with\nfewer parameters than existing methods while enjoying interpretable and\ncontent-aware feature aggregations.\n","authors":["Jintong Hu","Bin Xia","Bin Chen","Wenming Yang","Lei Zhang"],"pdf_url":"https://arxiv.org/pdf/2407.18046v1.pdf","comment":"13 pages, 12 figures"},{"id":"http://arxiv.org/abs/2407.18043v1","updated":"2024-07-25T13:44:49Z","published":"2024-07-25T13:44:49Z","title":"YOCO: You Only Calibrate Once for Accurate Extrinsic Parameter in\n  LiDAR-Camera Systems","summary":"  In a multi-sensor fusion system composed of cameras and LiDAR, precise\nextrinsic calibration contributes to the system's long-term stability and\naccurate perception of the environment. However, methods based on extracting\nand registering corresponding points still face challenges in terms of\nautomation and precision. This paper proposes a novel fully automatic extrinsic\ncalibration method for LiDAR-camera systems that circumvents the need for\ncorresponding point registration. In our approach, a novel algorithm to extract\nrequired LiDAR correspondence point is proposed. This method can effectively\nfilter out irrelevant points by computing the orientation of plane point clouds\nand extracting points by applying distance- and density-based thresholds. We\navoid the need for corresponding point registration by introducing extrinsic\nparameters between the LiDAR and camera into the projection of extracted points\nand constructing co-planar constraints. These parameters are then optimized to\nsolve for the extrinsic. We validated our method across multiple sets of\nLiDAR-camera systems. In synthetic experiments, our method demonstrates\nsuperior performance compared to current calibration techniques. Real-world\ndata experiments further confirm the precision and robustness of the proposed\nalgorithm, with average rotation and translation calibration errors between\nLiDAR and camera of less than 0.05 degree and 0.015m, respectively. This method\nenables automatic and accurate extrinsic calibration in a single one step,\nemphasizing the potential of calibration algorithms beyond using corresponding\npoint registration to enhance the automation and precision of LiDAR-camera\nsystem calibration.\n","authors":["Tianle Zeng","Dengke He","Feifan Yan","Meixi He"],"pdf_url":"https://arxiv.org/pdf/2407.18043v1.pdf","comment":"IEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT"},{"id":"http://arxiv.org/abs/2403.15377v2","updated":"2024-07-25T13:42:44Z","published":"2024-03-22T17:57:42Z","title":"InternVideo2: Scaling Foundation Models for Multimodal Video\n  Understanding","summary":"  We introduce InternVideo2, a new family of video foundation models (ViFM)\nthat achieve the state-of-the-art results in video recognition, video-text\ntasks, and video-centric dialogue. Our core design is a progressive training\napproach that unifies the masked video modeling, crossmodal contrastive\nlearning, and next token prediction, scaling up the video encoder size to 6B\nparameters. At the data level, we prioritize spatiotemporal consistency by\nsemantically segmenting videos and generating video-audio-speech captions. This\nimproves the alignment between video and text. Through extensive experiments,\nwe validate our designs and demonstrate superior performance on over 60 video\nand audio tasks. Notably, our model outperforms others on various video-related\ndialogue and long video understanding benchmarks, highlighting its ability to\nreason and comprehend longer contexts. Code and models are available at\nhttps://github.com/OpenGVLab/InternVideo/tree/main/InternVideo2/.\n","authors":["Yi Wang","Kunchang Li","Xinhao Li","Jiashuo Yu","Yinan He","Chenting Wang","Guo Chen","Baoqi Pei","Ziang Yan","Rongkun Zheng","Jilan Xu","Zun Wang","Yansong Shi","Tianxiang Jiang","Songze Li","Hongjie Zhang","Yifei Huang","Yu Qiao","Yali Wang","Limin Wang"],"pdf_url":"https://arxiv.org/pdf/2403.15377v2.pdf","comment":"a technical report about video understanding (accepted to ECCV2024)"},{"id":"http://arxiv.org/abs/2407.18038v1","updated":"2024-07-25T13:31:55Z","published":"2024-07-25T13:31:55Z","title":"TiCoSS: Tightening the Coupling between Semantic Segmentation and Stereo\n  Matching within A Joint Learning Framework","summary":"  Semantic segmentation and stereo matching, respectively analogous to the\nventral and dorsal streams in our human brain, are two key components of\nautonomous driving perception systems. Addressing these two tasks with separate\nnetworks is no longer the mainstream direction in developing computer vision\nalgorithms, particularly with the recent advances in large vision models and\nembodied artificial intelligence. The trend is shifting towards combining them\nwithin a joint learning framework, especially emphasizing feature sharing\nbetween the two tasks. The major contributions of this study lie in\ncomprehensively tightening the coupling between semantic segmentation and\nstereo matching. Specifically, this study introduces three novelties: (1) a\ntightly coupled, gated feature fusion strategy, (2) a hierarchical deep\nsupervision strategy, and (3) a coupling tightening loss function. The combined\nuse of these technical contributions results in TiCoSS, a state-of-the-art\njoint learning framework that simultaneously tackles semantic segmentation and\nstereo matching. Through extensive experiments on the KITTI and vKITTI2\ndatasets, along with qualitative and quantitative analyses, we validate the\neffectiveness of our developed strategies and loss function, and demonstrate\nits superior performance compared to prior arts, with a notable increase in\nmIoU by over 9%. Our source code will be publicly available at\nmias.group/TiCoSS upon publication.\n","authors":["Guanfeng Tang","Zhiyuan Wu","Rui Fan"],"pdf_url":"https://arxiv.org/pdf/2407.18038v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14242v2","updated":"2024-07-25T13:30:33Z","published":"2024-07-19T12:22:32Z","title":"Continual Panoptic Perception: Towards Multi-modal Incremental\n  Interpretation of Remote Sensing Images","summary":"  Continual learning (CL) breaks off the one-way training manner and enables a\nmodel to adapt to new data, semantics and tasks continuously. However, current\nCL methods mainly focus on single tasks. Besides, CL models are plagued by\ncatastrophic forgetting and semantic drift since the lack of old data, which\noften occurs in remote-sensing interpretation due to the intricate fine-grained\nsemantics. In this paper, we propose Continual Panoptic Perception (CPP), a\nunified continual learning model that leverages multi-task joint learning\ncovering pixel-level classification, instance-level segmentation and\nimage-level perception for universal interpretation in remote sensing images.\nConcretely, we propose a collaborative cross-modal encoder (CCE) to extract the\ninput image features, which supports pixel classification and caption\ngeneration synchronously. To inherit the knowledge from the old model without\nexemplar memory, we propose a task-interactive knowledge distillation (TKD)\nmethod, which leverages cross-modal optimization and task-asymmetric\npseudo-labeling (TPL) to alleviate catastrophic forgetting. Furthermore, we\nalso propose a joint optimization mechanism to achieve end-to-end multi-modal\npanoptic perception. Experimental results on the fine-grained panoptic\nperception dataset validate the effectiveness of the proposed model, and also\nprove that joint optimization can boost sub-task CL efficiency with over 13\\%\nrelative improvement on panoptic quality.\n","authors":["Bo Yuan","Danpei Zhao","Zhuoran Liu","Wentao Li","Tian Li"],"pdf_url":"https://arxiv.org/pdf/2407.14242v2.pdf","comment":"Accepted in ACMMM 2024"},{"id":"http://arxiv.org/abs/2407.18035v1","updated":"2024-07-25T13:29:37Z","published":"2024-07-25T13:29:37Z","title":"RestoreAgent: Autonomous Image Restoration Agent via Multimodal Large\n  Language Models","summary":"  Natural images captured by mobile devices often suffer from multiple types of\ndegradation, such as noise, blur, and low light. Traditional image restoration\nmethods require manual selection of specific tasks, algorithms, and execution\nsequences, which is time-consuming and may yield suboptimal results. All-in-one\nmodels, though capable of handling multiple tasks, typically support only a\nlimited range and often produce overly smooth, low-fidelity outcomes due to\ntheir broad data distribution fitting. To address these challenges, we first\ndefine a new pipeline for restoring images with multiple degradations, and then\nintroduce RestoreAgent, an intelligent image restoration system leveraging\nmultimodal large language models. RestoreAgent autonomously assesses the type\nand extent of degradation in input images and performs restoration through (1)\ndetermining the appropriate restoration tasks, (2) optimizing the task\nsequence, (3) selecting the most suitable models, and (4) executing the\nrestoration. Experimental results demonstrate the superior performance of\nRestoreAgent in handling complex degradation, surpassing human experts.\nFurthermore, the system modular design facilitates the fast integration of new\ntasks and models, enhancing its flexibility and scalability for various\napplications.\n","authors":["Haoyu Chen","Wenbo Li","Jinjin Gu","Jingjing Ren","Sixiang Chen","Tian Ye","Renjing Pei","Kaiwen Zhou","Fenglong Song","Lei Zhu"],"pdf_url":"https://arxiv.org/pdf/2407.18035v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.18034v1","updated":"2024-07-25T13:29:32Z","published":"2024-07-25T13:29:32Z","title":"AttentionHand: Text-driven Controllable Hand Image Generation for 3D\n  Hand Reconstruction in the Wild","summary":"  Recently, there has been a significant amount of research conducted on 3D\nhand reconstruction to use various forms of human-computer interaction.\nHowever, 3D hand reconstruction in the wild is challenging due to extreme lack\nof in-the-wild 3D hand datasets. Especially, when hands are in complex pose\nsuch as interacting hands, the problems like appearance similarity, self-handed\noccclusion and depth ambiguity make it more difficult. To overcome these\nissues, we propose AttentionHand, a novel method for text-driven controllable\nhand image generation. Since AttentionHand can generate various and numerous\nin-the-wild hand images well-aligned with 3D hand label, we can acquire a new\n3D hand dataset, and can relieve the domain gap between indoor and outdoor\nscenes. Our method needs easy-to-use four modalities (i.e, an RGB image, a hand\nmesh image from 3D label, a bounding box, and a text prompt). These modalities\nare embedded into the latent space by the encoding phase. Then, through the\ntext attention stage, hand-related tokens from the given text prompt are\nattended to highlight hand-related regions of the latent embedding. After the\nhighlighted embedding is fed to the visual attention stage, hand-related\nregions in the embedding are attended by conditioning global and local hand\nmesh images with the diffusion-based pipeline. In the decoding phase, the final\nfeature is decoded to new hand images, which are well-aligned with the given\nhand mesh image and text prompt. As a result, AttentionHand achieved\nstate-of-the-art among text-to-hand image generation models, and the\nperformance of 3D hand mesh reconstruction was improved by additionally\ntraining with hand images generated by AttentionHand.\n","authors":["Junho Park","Kyeongbo Kong","Suk-Ju Kang"],"pdf_url":"https://arxiv.org/pdf/2407.18034v1.pdf","comment":"Accepted by ECCV 2024"},{"id":"http://arxiv.org/abs/2407.09503v2","updated":"2024-07-25T13:29:27Z","published":"2024-06-14T09:39:53Z","title":"PARSE-Ego4D: Personal Action Recommendation Suggestions for Egocentric\n  Videos","summary":"  Intelligent assistance involves not only understanding but also action.\nExisting ego-centric video datasets contain rich annotations of the videos, but\nnot of actions that an intelligent assistant could perform in the moment. To\naddress this gap, we release PARSE-Ego4D, a new set of personal action\nrecommendation annotations for the Ego4D dataset. We take a multi-stage\napproach to generating and evaluating these annotations. First, we used a\nprompt-engineered large language model (LLM) to generate context-aware action\nsuggestions and identified over 18,000 action suggestions. While these\nsynthetic action suggestions are valuable, the inherent limitations of LLMs\nnecessitate human evaluation. To ensure high-quality and user-centered\nrecommendations, we conducted a large-scale human annotation study that\nprovides grounding in human preferences for all of PARSE-Ego4D. We analyze the\ninter-rater agreement and evaluate subjective preferences of participants.\nBased on our synthetic dataset and complete human annotations, we propose\nseveral new tasks for action suggestions based on ego-centric videos. We\nencourage novel solutions that improve latency and energy requirements. The\nannotations in PARSE-Ego4D will support researchers and developers who are\nworking on building action recommendation systems for augmented and virtual\nreality systems.\n","authors":["Steven Abreu","Tiffany D. Do","Karan Ahuja","Eric J. Gonzalez","Lee Payne","Daniel McDuff","Mar Gonzalez-Franco"],"pdf_url":"https://arxiv.org/pdf/2407.09503v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.18026v1","updated":"2024-07-25T13:23:57Z","published":"2024-07-25T13:23:57Z","title":"Segmentation-guided MRI reconstruction for meaningfully diverse\n  reconstructions","summary":"  Inverse problems, such as accelerated MRI reconstruction, are ill-posed and\nan infinite amount of possible and plausible solutions exist. This may not only\nlead to uncertainty in the reconstructed image but also in downstream tasks\nsuch as semantic segmentation. This uncertainty, however, is mostly not\nanalyzed in the literature, even though probabilistic reconstruction models are\ncommonly used. These models can be prone to ignore plausible but unlikely\nsolutions like rare pathologies. Building on MRI reconstruction approaches\nbased on diffusion models, we add guidance to the diffusion process during\ninference, generating two meaningfully diverse reconstructions corresponding to\nan upper and lower bound segmentation. The reconstruction uncertainty can then\nbe quantified by the difference between these bounds, which we coin the\n'uncertainty boundary'. We analyzed the behavior of the upper and lower bound\nsegmentations for a wide range of acceleration factors and found the\nuncertainty boundary to be both more reliable and more accurate compared to\nrepeated sampling. Code is available at https://github.com/NikolasMorshuis/SGR\n","authors":["Jan Nikolas Morshuis","Matthias Hein","Christian F. Baumgartner"],"pdf_url":"https://arxiv.org/pdf/2407.18026v1.pdf","comment":"Accepted at DGM4MICCAI 2024"},{"id":"http://arxiv.org/abs/2407.15396v2","updated":"2024-07-25T12:54:52Z","published":"2024-07-22T05:53:46Z","title":"Semantic Diversity-aware Prototype-based Learning for Unbiased Scene\n  Graph Generation","summary":"  The scene graph generation (SGG) task involves detecting objects within an\nimage and predicting predicates that represent the relationships between the\nobjects. However, in SGG benchmark datasets, each subject-object pair is\nannotated with a single predicate even though a single predicate may exhibit\ndiverse semantics (i.e., semantic diversity), existing SGG models are trained\nto predict the one and only predicate for each pair. This in turn results in\nthe SGG models to overlook the semantic diversity that may exist in a\npredicate, thus leading to biased predictions. In this paper, we propose a\nnovel model-agnostic Semantic Diversity-aware Prototype-based Learning (DPL)\nframework that enables unbiased predictions based on the understanding of the\nsemantic diversity of predicates. Specifically, DPL learns the regions in the\nsemantic space covered by each predicate to distinguish among the various\ndifferent semantics that a single predicate can represent. Extensive\nexperiments demonstrate that our proposed model-agnostic DPL framework brings\nsignificant performance improvement on existing SGG models, and also\neffectively understands the semantic diversity of predicates.\n","authors":["Jaehyeong Jeon","Kibum Kim","Kanghoon Yoon","Chanyoung Park"],"pdf_url":"https://arxiv.org/pdf/2407.15396v2.pdf","comment":"ECCV 2024"},{"id":"http://arxiv.org/abs/2407.18002v1","updated":"2024-07-25T12:53:21Z","published":"2024-07-25T12:53:21Z","title":"Network Inversion of Convolutional Neural Nets","summary":"  Neural networks have emerged as powerful tools across various applications,\nyet their decision-making process often remains opaque, leading to them being\nperceived as \"black boxes.\" This opacity raises concerns about their\ninterpretability and reliability, especially in safety-critical scenarios.\nNetwork inversion techniques offer a solution by allowing us to peek inside\nthese black boxes, revealing the features and patterns learned by the networks\nbehind their decision-making processes and thereby provide valuable insights\ninto how neural networks arrive at their conclusions, making them more\ninterpretable and trustworthy. This paper presents a simple yet effective\napproach to network inversion using a carefully conditioned generator that\nlearns the data distribution in the input space of the trained neural network,\nenabling the reconstruction of inputs that would most likely lead to the\ndesired outputs. To capture the diversity in the input space for a given\noutput, instead of simply revealing the conditioning labels to the generator,\nwe hideously encode the conditioning label information into vectors, further\nexemplified by heavy dropout in the generation process and minimisation of\ncosine similarity between the features corresponding to the generated images.\nThe paper concludes with immediate applications of Network Inversion including\nin interpretability, explainability and generation of adversarial samples.\n","authors":["Pirzada Suhail","Amit Sethi"],"pdf_url":"https://arxiv.org/pdf/2407.18002v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2108.08158v3","updated":"2024-07-25T12:52:52Z","published":"2021-08-18T14:04:52Z","title":"Practical X-ray Gastric Cancer Screening Using Refined Stochastic Data\n  Augmentation and Hard Boundary Box Training","summary":"  Endoscopy is widely used to diagnose gastric cancer and has a high diagnostic\nperformance, but because it must be performed by a physician, the number of\npeople who can be diagnosed is limited. Gastric X-ray, on the other hand, can\nbe performed by technicians and can screen a much larger number of patients\nthan endoscopy, but its correct diagnosis requires experience. We propose an\nunprecedented and practical gastric cancer diagnosis support system for gastric\nX-ray images, which will enable more people to be screened. The system is based\non a general deep learning-based object detection model and includes two novel\ntechnical proposals: refined probabilistic stomach image augmentation (R-sGAIA)\nand hard boundary box learning (HBBT). R-sGAIA is a probabilistic gastric fold\nregion enhancement method that provides more learning patterns for cancer\ndetection models. HBBT is an efficient training method for object detection\nmodels that allows the use of unannotated negative (i.e., healthy control)\nsamples that cannot be used for training in conventional detection models,\nthereby improving model performance. The sensitivity (SE) of the proposed\nsystem for gastric cancer (90.2%) is higher than that of the expert (85.5%),\nand two out of five candidates detected box are cancerous, achieving a high\nprecision while maintaining a high processing speed of 0.51 seconds/image. The\nproposed system showed 5.9 points higher on the F1 score compared to methods\nusing the same object detection model and state-of-the-art data augmentation.\nIn short, the system quickly and efficiently shows the radiologist where to\nlook, greatly reducing the radiologist's workload.\n","authors":["Hideaki Okamoto","Takakiyo Nomura","Kazuhito Nabeshima","Jun Hashimoto","Hitoshi Iyatomi"],"pdf_url":"https://arxiv.org/pdf/2108.08158v3.pdf","comment":"19 pages, 6 figures"},{"id":"http://arxiv.org/abs/2407.18000v1","updated":"2024-07-25T12:49:24Z","published":"2024-07-25T12:49:24Z","title":"Investigation to answer three key questions concerning plant pest\n  identification and development of a practical identification framework","summary":"  The development of practical and robust automated diagnostic systems for\nidentifying plant pests is crucial for efficient agricultural production. In\nthis paper, we first investigate three key research questions (RQs) that have\nnot been addressed thus far in the field of image-based plant pest\nidentification. Based on the knowledge gained, we then develop an accurate,\nrobust, and fast plant pest identification framework using 334K images\ncomprising 78 combinations of four plant portions (the leaf front, leaf back,\nfruit, and flower of cucumber, tomato, strawberry, and eggplant) and 20 pest\nspecies captured at 27 farms. The results reveal the following. (1) For an\nappropriate evaluation of the model, the test data should not include images of\nthe field from which the training images were collected, or other\nconsiderations to increase the diversity of the test set should be taken into\naccount. (2) Pre-extraction of ROIs, such as leaves and fruits, helps to\nimprove identification accuracy. (3) Integration of closely related species\nusing the same control methods and cross-crop training methods for the same\npests, are effective. Our two-stage plant pest identification framework,\nenabling ROI detection and convolutional neural network (CNN)-based\nidentification, achieved a highly practical performance of 91.0% and 88.5% in\nmean accuracy and macro F1 score, respectively, for 12,223 instances of test\ndata of 21 classes collected from unseen fields, where 25 classes of images\nfrom 318,971 samples were used for training; the average identification time\nwas 476 ms/image.\n","authors":["Ryosuke Wayama","Yuki Sasaki","Satoshi Kagiwada","Nobusuke Iwasaki","Hitoshi Iyatomi"],"pdf_url":"https://arxiv.org/pdf/2407.18000v1.pdf","comment":"40 pages, 10 figures"},{"id":"http://arxiv.org/abs/2407.17996v1","updated":"2024-07-25T12:43:41Z","published":"2024-07-25T12:43:41Z","title":"Joint RGB-Spectral Decomposition Model Guided Image Enhancement in\n  Mobile Photography","summary":"  The integration of miniaturized spectrometers into mobile devices offers new\navenues for image quality enhancement and facilitates novel downstream tasks.\nHowever, the broader application of spectral sensors in mobile photography is\nhindered by the inherent complexity of spectral images and the constraints of\nspectral imaging capabilities. To overcome these challenges, we propose a joint\nRGB-Spectral decomposition model guided enhancement framework, which consists\nof two steps: joint decomposition and prior-guided enhancement. Firstly, we\nleverage the complementarity between RGB and Low-resolution Multi-Spectral\nImages (Lr-MSI) to predict shading, reflectance, and material semantic priors.\nSubsequently, these priors are seamlessly integrated into the established\nHDRNet to promote dynamic range enhancement, color mapping, and grid expert\nlearning, respectively. Additionally, we construct a high-quality Mobile-Spec\ndataset to support our research, and our experiments validate the effectiveness\nof Lr-MSI in the tone enhancement task. This work aims to establish a solid\nfoundation for advancing spectral vision in mobile photography. The code is\navailable at \\url{https://github.com/CalayZhou/JDM-HDRNet}.\n","authors":["Kailai Zhou","Lijing Cai","Yibo Wang","Mengya Zhang","Bihan Wen","Qiu Shen","Xun Cao"],"pdf_url":"https://arxiv.org/pdf/2407.17996v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13675v3","updated":"2024-07-25T12:32:21Z","published":"2024-07-18T16:50:59Z","title":"MeshSegmenter: Zero-Shot Mesh Semantic Segmentation via Texture\n  Synthesis","summary":"  We present MeshSegmenter, a simple yet effective framework designed for\nzero-shot 3D semantic segmentation. This model successfully extends the\npowerful capabilities of 2D segmentation models to 3D meshes, delivering\naccurate 3D segmentation across diverse meshes and segment descriptions.\nSpecifically, our model leverages the Segment Anything Model (SAM) model to\nsegment the target regions from images rendered from the 3D shape. In light of\nthe importance of the texture for segmentation, we also leverage the pretrained\nstable diffusion model to generate images with textures from 3D shape, and\nleverage SAM to segment the target regions from images with textures. Textures\nsupplement the shape for segmentation and facilitate accurate 3D segmentation\neven in geometrically non-prominent areas, such as segmenting a car door within\na car mesh. To achieve the 3D segments, we render 2D images from different\nviews and conduct segmentation for both textured and untextured images. Lastly,\nwe develop a multi-view revoting scheme that integrates 2D segmentation results\nand confidence scores from various views onto the 3D mesh, ensuring the 3D\nconsistency of segmentation results and eliminating inaccuracies from specific\nperspectives. Through these innovations, MeshSegmenter offers stable and\nreliable 3D segmentation results both quantitatively and qualitatively,\nhighlighting its potential as a transformative tool in the field of 3D\nzero-shot segmentation. The code is available at\n\\url{https://github.com/zimingzhong/MeshSegmenter}.\n","authors":["Ziming Zhong","Yanxu Xu","Jing Li","Jiale Xu","Zhengxin Li","Chaohui Yu","Shenghua Gao"],"pdf_url":"https://arxiv.org/pdf/2407.13675v3.pdf","comment":"The paper was accepted by ECCV2024"},{"id":"http://arxiv.org/abs/2312.07423v2","updated":"2024-07-25T12:24:10Z","published":"2023-12-12T16:45:52Z","title":"Holoported Characters: Real-time Free-viewpoint Rendering of Humans from\n  Sparse RGB Cameras","summary":"  We present the first approach to render highly realistic free-viewpoint\nvideos of a human actor in general apparel, from sparse multi-view recording to\ndisplay, in real-time at an unprecedented 4K resolution. At inference, our\nmethod only requires four camera views of the moving actor and the respective\n3D skeletal pose. It handles actors in wide clothing, and reproduces even\nfine-scale dynamic detail, e.g. clothing wrinkles, face expressions, and hand\ngestures. At training time, our learning-based approach expects dense\nmulti-view video and a rigged static surface scan of the actor. Our method\ncomprises three main stages. Stage 1 is a skeleton-driven neural approach for\nhigh-quality capture of the detailed dynamic mesh geometry. Stage 2 is a novel\nsolution to create a view-dependent texture using four test-time camera views\nas input. Finally, stage 3 comprises a new image-based refinement network\nrendering the final 4K image given the output from the previous stages. Our\napproach establishes a new benchmark for real-time rendering resolution and\nquality using sparse input camera views, unlocking possibilities for immersive\ntelepresence.\n","authors":["Ashwath Shetty","Marc Habermann","Guoxing Sun","Diogo Luvizon","Vladislav Golyanik","Christian Theobalt"],"pdf_url":"https://arxiv.org/pdf/2312.07423v2.pdf","comment":"Project page: https://vcai.mpi-inf.mpg.de/projects/holochar/ 8 pages,\n  2 tables and 8 figures; presented at Computer Vision and Pattern Recognition\n  (CVPR) 2024"},{"id":"http://arxiv.org/abs/2406.09126v2","updated":"2024-07-25T11:50:52Z","published":"2024-06-13T13:59:47Z","title":"Auto-Vocabulary Segmentation for LiDAR Points","summary":"  Existing perception methods for autonomous driving fall short of recognizing\nunknown entities not covered in the training data. Open-vocabulary methods\noffer promising capabilities in detecting any object but are limited by\nuser-specified queries representing target classes. We propose AutoVoc3D, a\nframework for automatic object class recognition and open-ended segmentation.\nEvaluation on nuScenes showcases AutoVoc3D's ability to generate precise\nsemantic classes and accurate point-wise segmentation. Moreover, we introduce\nText-Point Semantic Similarity, a new metric to assess the semantic similarity\nbetween text and point cloud without eliminating novel classes.\n","authors":["Weijie Wei","Osman Ülger","Fatemeh Karimi Nejadasl","Theo Gevers","Martin R. Oswald"],"pdf_url":"https://arxiv.org/pdf/2406.09126v2.pdf","comment":"Accepted by CVPR 2024 OpenSun3D Workshop"},{"id":"http://arxiv.org/abs/2308.12112v4","updated":"2024-07-25T11:49:54Z","published":"2023-08-23T13:02:52Z","title":"Category Adaptation Meets Projected Distillation in Generalized\n  Continual Category Discovery","summary":"  Generalized Continual Category Discovery (GCCD) tackles learning from\nsequentially arriving, partially labeled datasets while uncovering new\ncategories. Traditional methods depend on feature distillation to prevent\nforgetting the old knowledge. However, this strategy restricts the model's\nability to adapt and effectively distinguish new categories. To address this,\nwe introduce a novel technique integrating a learnable projector with feature\ndistillation, thus enhancing model adaptability without sacrificing past\nknowledge. The resulting distribution shift of the previously learned\ncategories is mitigated with the auxiliary category adaptation network. We\ndemonstrate that while each component offers modest benefits individually,\ntheir combination - dubbed CAMP (Category Adaptation Meets Projected\ndistillation) - significantly improves the balance between learning new\ninformation and retaining old. CAMP exhibits superior performance across\nseveral GCCD and Class Incremental Learning scenarios. The code is available at\nhttps://github.com/grypesc/CAMP.\n","authors":["Grzegorz Rypeść","Daniel Marczak","Sebastian Cygert","Tomasz Trzciński","Bartłomiej Twardowski"],"pdf_url":"https://arxiv.org/pdf/2308.12112v4.pdf","comment":"Accepted for ECCV 2024"},{"id":"http://arxiv.org/abs/2407.17967v1","updated":"2024-07-25T11:39:20Z","published":"2024-07-25T11:39:20Z","title":"Lightweight Language-driven Grasp Detection using Conditional\n  Consistency Model","summary":"  Language-driven grasp detection is a fundamental yet challenging task in\nrobotics with various industrial applications. In this work, we present a new\napproach for language-driven grasp detection that leverages the concept of\nlightweight diffusion models to achieve fast inference time. By integrating\ndiffusion processes with grasping prompts in natural language, our method can\neffectively encode visual and textual information, enabling more accurate and\nversatile grasp positioning that aligns well with the text query. To overcome\nthe long inference time problem in diffusion models, we leverage the image and\ntext features as the condition in the consistency model to reduce the number of\ndenoising timesteps during inference. The intensive experimental results show\nthat our method outperforms other recent grasp detection methods and\nlightweight diffusion models by a clear margin. We further validate our method\nin real-world robotic experiments to demonstrate its fast inference time\ncapability.\n","authors":["Nghia Nguyen","Minh Nhat Vu","Baoru Huang","An Vuong","Ngan Le","Thieu Vo","Anh Nguyen"],"pdf_url":"https://arxiv.org/pdf/2407.17967v1.pdf","comment":"Accepted at IROS 2024"},{"id":"http://arxiv.org/abs/2309.10527v3","updated":"2024-07-25T11:26:49Z","published":"2023-09-19T11:13:01Z","title":"SPOT: Scalable 3D Pre-training via Occupancy Prediction for Learning\n  Transferable 3D Representations","summary":"  Annotating 3D LiDAR point clouds for perception tasks is fundamental for many\napplications e.g., autonomous driving, yet it still remains notoriously\nlabor-intensive. Pretraining-finetuning approach can alleviate the labeling\nburden by fine-tuning a pre-trained backbone across various downstream datasets\nas well as tasks. In this paper, we propose SPOT, namely Scalable Pre-training\nvia Occupancy prediction for learning Transferable 3D representations under\nsuch a label-efficient fine-tuning paradigm. SPOT achieves effectiveness on\nvarious public datasets with different downstream tasks, showcasing its general\nrepresentation power, cross-domain robustness and data scalability which are\nthree key factors for real-world application. Specifically, we both\ntheoretically and empirically show, for the first time, that general\nrepresentations learning can be achieved through the task of occupancy\nprediction. Then, to address the domain gap caused by different LiDAR sensors\nand annotation methods, we develop a beam re-sampling technique for point cloud\naugmentation combined with class-balancing strategy. Furthermore, scalable\npre-training is observed, that is, the downstream performance across all the\nexperiments gets better with more pre-training data. Additionally, such\npre-training strategy also remains compatible with unlabeled data. The hope is\nthat our findings will facilitate the understanding of LiDAR points and pave\nthe way for future advancements in LiDAR pre-training.\n","authors":["Xiangchao Yan","Runjian Chen","Bo Zhang","Hancheng Ye","Renqiu Xia","Jiakang Yuan","Hongbin Zhou","Xinyu Cai","Botian Shi","Wenqi Shao","Ping Luo","Yu Qiao","Tao Chen","Junchi Yan"],"pdf_url":"https://arxiv.org/pdf/2309.10527v3.pdf","comment":"15 pages, 8 figures, Code is available at\n  https://github.com/PJLab-ADG/3DTrans"},{"id":"http://arxiv.org/abs/2407.17956v1","updated":"2024-07-25T11:22:54Z","published":"2024-07-25T11:22:54Z","title":"SaccadeDet: A Novel Dual-Stage Architecture for Rapid and Accurate\n  Detection in Gigapixel Images","summary":"  The advancement of deep learning in object detection has predominantly\nfocused on megapixel images, leaving a critical gap in the efficient processing\nof gigapixel images. These super high-resolution images present unique\nchallenges due to their immense size and computational demands. To address\nthis, we introduce 'SaccadeDet', an innovative architecture for gigapixel-level\nobject detection, inspired by the human eye saccadic movement. The cornerstone\nof SaccadeDet is its ability to strategically select and process image regions,\ndramatically reducing computational load. This is achieved through a two-stage\nprocess: the 'saccade' stage, which identifies regions of probable interest,\nand the 'gaze' stage, which refines detection in these targeted areas. Our\napproach, evaluated on the PANDA dataset, not only achieves an 8x speed\nincrease over the state-of-the-art methods but also demonstrates significant\npotential in gigapixel-level pathology analysis through its application to\nWhole Slide Imaging.\n","authors":["Wenxi Li","Ruxin Zhang","Haozhe Lin","Yuchen Guo","Chao Ma","Xiaokang Yang"],"pdf_url":"https://arxiv.org/pdf/2407.17956v1.pdf","comment":"This paper is accepted to ECML-PKDD 2024"},{"id":"http://arxiv.org/abs/2407.17954v1","updated":"2024-07-25T11:19:55Z","published":"2024-07-25T11:19:55Z","title":"Scaling Training Data with Lossy Image Compression","summary":"  Empirically-determined scaling laws have been broadly successful in\npredicting the evolution of large machine learning models with training data\nand number of parameters. As a consequence, they have been useful for\noptimizing the allocation of limited resources, most notably compute time.\n  In certain applications, storage space is an important constraint, and data\nformat needs to be chosen carefully as a consequence. Computer vision is a\nprominent example: images are inherently analog, but are always stored in a\ndigital format using a finite number of bits. Given a dataset of digital\nimages, the number of bits $L$ to store each of them can be further reduced\nusing lossy data compression. This, however, can degrade the quality of the\nmodel trained on such images, since each example has lower resolution.\n  In order to capture this trade-off and optimize storage of training data, we\npropose a `storage scaling law' that describes the joint evolution of test\nerror with sample size and number of bits per image. We prove that this law\nholds within a stylized model for image compression, and verify it empirically\non two computer vision tasks, extracting the relevant parameters. We then show\nthat this law can be used to optimize the lossy compression level. At given\nstorage, models trained on optimally compressed images present a significantly\nsmaller test error with respect to models trained on the original data.\nFinally, we investigate the potential benefits of randomizing the compression\nlevel.\n","authors":["Katherine L. Mentzer","Andrea Montanari"],"pdf_url":"https://arxiv.org/pdf/2407.17954v1.pdf","comment":"21 pages, 27 figures"},{"id":"http://arxiv.org/abs/2407.17952v1","updated":"2024-07-25T11:16:37Z","published":"2024-07-25T11:16:37Z","title":"BetterDepth: Plug-and-Play Diffusion Refiner for Zero-Shot Monocular\n  Depth Estimation","summary":"  By training over large-scale datasets, zero-shot monocular depth estimation\n(MDE) methods show robust performance in the wild but often suffer from\ninsufficiently precise details. Although recent diffusion-based MDE approaches\nexhibit appealing detail extraction ability, they still struggle in\ngeometrically challenging scenes due to the difficulty of gaining robust\ngeometric priors from diverse datasets. To leverage the complementary merits of\nboth worlds, we propose BetterDepth to efficiently achieve geometrically\ncorrect affine-invariant MDE performance while capturing fine-grained details.\nSpecifically, BetterDepth is a conditional diffusion-based refiner that takes\nthe prediction from pre-trained MDE models as depth conditioning, in which the\nglobal depth context is well-captured, and iteratively refines details based on\nthe input image. For the training of such a refiner, we propose global\npre-alignment and local patch masking methods to ensure the faithfulness of\nBetterDepth to depth conditioning while learning to capture fine-grained scene\ndetails. By efficient training on small-scale synthetic datasets, BetterDepth\nachieves state-of-the-art zero-shot MDE performance on diverse public datasets\nand in-the-wild scenes. Moreover, BetterDepth can improve the performance of\nother MDE models in a plug-and-play manner without additional re-training.\n","authors":["Xiang Zhang","Bingxin Ke","Hayko Riemenschneider","Nando Metzger","Anton Obukhov","Markus Gross","Konrad Schindler","Christopher Schroers"],"pdf_url":"https://arxiv.org/pdf/2407.17952v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07720v3","updated":"2024-07-25T11:15:37Z","published":"2024-07-10T14:53:37Z","title":"SvANet: A Scale-variant Attention-based Network for Small Medical Object\n  Segmentation","summary":"  Early detection and accurate diagnosis can predict the risk of malignant\ndisease transformation, thereby increasing the probability of effective\ntreatment. A mild syndrome with small infected regions is an ominous warning\nand is foremost in the early diagnosis of diseases. Deep learning algorithms,\nsuch as convolutional neural networks (CNNs), have been used to segment natural\nor medical objects, showing promising results. However, analyzing medical\nobjects of small areas in images remains a challenge due to information losses\nand compression defects caused by convolution and pooling operations in CNNs.\nThese losses and defects become increasingly significant as the network\ndeepens, particularly for small medical objects. To address these challenges,\nwe propose a novel scale-variant attention-based network (SvANet) for accurate\nsmall-scale object segmentation in medical images. The SvANet consists of Monte\nCarlo attention, scale-variant attention, and vision transformer, which\nincorporates cross-scale features and alleviates compression artifacts for\nenhancing the discrimination of small medical objects. Quantitative\nexperimental results demonstrate the superior performance of SvANet, achieving\n96.12%, 96.11%, 89.79%, 84.15%, 80.25%, 73.05%, and 72.58% in mean Dice\ncoefficient for segmenting kidney tumors, skin lesions, hepatic tumors, polyps,\nsurgical excision cells, retinal vasculatures, and sperms, which occupy less\nthan 1% of the image areas in KiTS23, ISIC 2018, ATLAS, PolypGen, TissueNet,\nFIVES, and SpermHealth datasets, respectively.\n","authors":["Wei Dai","Rui Liu","Zixuan Wu","Tianyi Wu","Min Wang","Junxian Zhou","Yixuan Yuan","Jun Liu"],"pdf_url":"https://arxiv.org/pdf/2407.07720v3.pdf","comment":"14 pages, 9 figures, under review"},{"id":"http://arxiv.org/abs/2407.17950v1","updated":"2024-07-25T11:11:05Z","published":"2024-07-25T11:11:05Z","title":"Real Time American Sign Language Detection Using Yolo-v9","summary":"  This paper focuses on real-time American Sign Language Detection. YOLO is a\nconvolutional neural network (CNN) based model, which was first released in\n2015. In recent years, it gained popularity for its real-time detection\ncapabilities. Our study specifically targets YOLO-v9 model, released in 2024.\nAs the model is newly introduced, not much work has been done on it, especially\nnot in Sign Language Detection. Our paper provides deep insight on how YOLO- v9\nworks and better than previous model.\n","authors":["Amna Imran","Meghana Shashishekhara Hulikal","Hamza A. A. Gardi"],"pdf_url":"https://arxiv.org/pdf/2407.17950v1.pdf","comment":"11 pages, 13 figures, 1 table"},{"id":"http://arxiv.org/abs/2407.17938v1","updated":"2024-07-25T10:55:19Z","published":"2024-07-25T10:55:19Z","title":"Analyzing Brain Tumor Connectomics using Graphs and Persistent Homology","summary":"  Recent advances in molecular and genetic research have identified a diverse\nrange of brain tumor sub-types, shedding light on differences in their\nmolecular mechanisms, heterogeneity, and origins. The present study performs\nwhole-brain connectome analysis using diffusionweighted images. To achieve\nthis, both graph theory and persistent homology - a prominent approach in\ntopological data analysis are employed in order to quantify changes in the\nstructural connectivity of the wholebrain connectome in subjects with brain\ntumors. Probabilistic tractography is used to map the number of streamlines\nconnecting 84 distinct brain regions, as delineated by the Desikan-Killiany\natlas from FreeSurfer. These streamline mappings form the connectome matrix, on\nwhich persistent homology based analysis and graph theoretical analysis are\nexecuted to evaluate the discriminatory power between tumor sub-types that\ninclude meningioma and glioma. A detailed statistical analysis is conducted on\npersistent homology-derived topological features and graphical features to\nidentify the brain regions where differences between study groups are\nstatistically significant (p < 0.05). For classification purpose, graph-based\nlocal features are utilized, achieving a highest accuracy of 88%. In\nclassifying tumor sub-types, an accuracy of 80% is attained. The findings\nobtained from this study underscore the potential of persistent homology and\ngraph theoretical analysis of the whole-brain connectome in detecting\nalterations in structural connectivity patterns specific to different types of\nbrain tumors.\n","authors":["Debanjali Bhattacharya","Ninad Aithal","Manish Jayswal","Neelam Sinha"],"pdf_url":"https://arxiv.org/pdf/2407.17938v1.pdf","comment":"15 Pages, 7 Figures, 2 Tables, TGI3-MICCAI Workshop"},{"id":"http://arxiv.org/abs/2407.13842v2","updated":"2024-07-25T10:51:19Z","published":"2024-07-18T18:24:51Z","title":"Language-Driven 6-DoF Grasp Detection Using Negative Prompt Guidance","summary":"  6-DoF grasp detection has been a fundamental and challenging problem in\nrobotic vision. While previous works have focused on ensuring grasp stability,\nthey often do not consider human intention conveyed through natural language,\nhindering effective collaboration between robots and users in complex 3D\nenvironments. In this paper, we present a new approach for language-driven\n6-DoF grasp detection in cluttered point clouds. We first introduce\nGrasp-Anything-6D, a large-scale dataset for the language-driven 6-DoF grasp\ndetection task with 1M point cloud scenes and more than 200M\nlanguage-associated 3D grasp poses. We further introduce a novel diffusion\nmodel that incorporates a new negative prompt guidance learning strategy. The\nproposed negative prompt strategy directs the detection process toward the\ndesired object while steering away from unwanted ones given the language input.\nOur method enables an end-to-end framework where humans can command the robot\nto grasp desired objects in a cluttered scene using natural language. Intensive\nexperimental results show the effectiveness of our method in both benchmarking\nexperiments and real-world scenarios, surpassing other baselines. In addition,\nwe demonstrate the practicality of our approach in real-world robotic\napplications. Our project is available at\nhttps://airvlab.github.io/grasp-anything.\n","authors":["Toan Nguyen","Minh Nhat Vu","Baoru Huang","An Vuong","Quan Vuong","Ngan Le","Thieu Vo","Anh Nguyen"],"pdf_url":"https://arxiv.org/pdf/2407.13842v2.pdf","comment":"Accepted at ECCV 2024"},{"id":"http://arxiv.org/abs/2407.17933v1","updated":"2024-07-25T10:46:29Z","published":"2024-07-25T10:46:29Z","title":"Segmentation by registration-enabled SAM prompt engineering using five\n  reference images","summary":"  The recently proposed Segment Anything Model (SAM) is a general tool for\nimage segmentation, but it requires additional adaptation and careful\nfine-tuning for medical image segmentation, especially for small,\nirregularly-shaped, and boundary-ambiguous anatomical structures such as the\nknee cartilage that is of interest in this work. Repaired cartilage, after\ncertain surgical procedures, exhibits imaging patterns unseen to pre-training,\nposing further challenges for using models like SAM with or without\ngeneral-purpose fine-tuning. To address this, we propose a novel\nregistration-based prompt engineering framework for medical image segmentation\nusing SAM. This approach utilises established image registration algorithms to\nalign the new image (to-be-segmented) and a small number of reference images,\nwithout requiring segmentation labels. The spatial transformations generated by\nregistration align either the new image or pre-defined point-based prompts,\nbefore using them as input to SAM. This strategy, requiring as few as five\nreference images with defined point prompts, effectively prompts SAM for\ninference on new images, without needing any segmentation labels. Evaluation of\nMR images from patients who received cartilage stem cell therapy yielded Dice\nscores of 0.89, 0.87, 0.53, and 0.52 for segmenting femur, tibia, femoral- and\ntibial cartilages, respectively. This outperforms atlas-based label fusion and\nis comparable to supervised nnUNet, an upper-bound fair baseline in this\napplication, both of which require full segmentation labels for reference\nsamples. The codes are available at:\nhttps://github.com/chrissyinreallife/KneeSegmentWithSAM.git\n","authors":["Yaxi Chen","Aleksandra Ivanova","Shaheer U. Saeed","Rikin Hargunani","Jie Huang","Chaozong Liu","Yipeng Hu"],"pdf_url":"https://arxiv.org/pdf/2407.17933v1.pdf","comment":"Accepted to the 11th International Workshop on Biomedical Image\n  Registration (WBIR 2024)"},{"id":"http://arxiv.org/abs/2407.17929v1","updated":"2024-07-25T10:38:32Z","published":"2024-07-25T10:38:32Z","title":"Guided Latent Slot Diffusion for Object-Centric Learning","summary":"  Slot attention aims to decompose an input image into a set of meaningful\nobject files (slots). These latent object representations enable various\ndownstream tasks. Yet, these slots often bind to object parts, not objects\nthemselves, especially for real-world datasets. To address this, we introduce\nGuided Latent Slot Diffusion - GLASS, an object-centric model that uses\ngenerated captions as a guiding signal to better align slots with objects. Our\nkey insight is to learn the slot-attention module in the space of generated\nimages. This allows us to repurpose the pre-trained diffusion decoder model,\nwhich reconstructs the images from the slots, as a semantic mask generator\nbased on the generated captions. GLASS learns an object-level representation\nsuitable for multiple tasks simultaneously, e.g., segmentation, image\ngeneration, and property prediction, outperforming previous methods. For object\ndiscovery, GLASS achieves approx. a +35% and +10% relative improvement for mIoU\nover the previous state-of-the-art (SOTA) method on the VOC and COCO datasets,\nrespectively, and establishes a new SOTA FID score for conditional image\ngeneration amongst slot-attention-based methods. For the segmentation task,\nGLASS surpasses SOTA weakly-supervised and language-based segmentation models,\nwhich were specifically designed for the task.\n","authors":["Krishnakant Singh","Simone Schaub-Meyer","Stefan Roth"],"pdf_url":"https://arxiv.org/pdf/2407.17929v1.pdf","comment":"Project Page: https://guided-sa.github.io"},{"id":"http://arxiv.org/abs/2407.17927v1","updated":"2024-07-25T10:24:54Z","published":"2024-07-25T10:24:54Z","title":"Invariance of deep image quality metrics to affine transformations","summary":"  Deep architectures are the current state-of-the-art in predicting subjective\nimage quality. Usually, these models are evaluated according to their ability\nto correlate with human opinion in databases with a range of distortions that\nmay appear in digital media. However, these oversee affine transformations\nwhich may represent better the changes in the images actually happening in\nnatural conditions. Humans can be particularly invariant to these natural\ntransformations, as opposed to the digital ones. In this work, we evaluate\nstate-of-the-art deep image quality metrics by assessing their invariance to\naffine transformations, specifically: rotation, translation, scaling, and\nchanges in spectral illumination. We propose a methodology to assign\ninvisibility thresholds for any perceptual metric. This methodology involves\ntransforming the distance measured by an arbitrary metric to a common distance\nrepresentation based on available subjectively rated databases. We\npsychophysically measure an absolute detection threshold in that common\nrepresentation and express it in the physical units of each affine transform\nfor each metric. By doing so, we allow the analyzed metrics to be directly\ncomparable with actual human thresholds. We find that none of the\nstate-of-the-art metrics shows human-like results under this strong test based\non invisibility thresholds. This means that tuning the models exclusively to\npredict the visibility of generic distortions may disregard other properties of\nhuman vision as for instance invariances or invisibility thresholds.\n","authors":["Nuria Alabau-Bosque","Paula Daudén-Oliver","Jorge Vila-Tomás","Valero Laparra","Jesús Malo"],"pdf_url":"https://arxiv.org/pdf/2407.17927v1.pdf","comment":"12 pages 13 figures"},{"id":"http://arxiv.org/abs/2402.16598v4","updated":"2024-07-25T10:10:45Z","published":"2024-02-26T14:28:39Z","title":"PCR-99: A Practical Method for Point Cloud Registration with 99%\n  Outliers","summary":"  We propose a robust method for point cloud registration that can handle both\nunknown scales and extreme outlier ratios. Our method, dubbed PCR-99, uses a\ndeterministic 3-point sampling approach with two novel mechanisms that\nsignificantly boost the speed: (1) an improved ordering of the samples based on\npairwise scale consistency, prioritizing the point correspondences that are\nmore likely to be inliers, and (2) an efficient outlier rejection scheme based\non triplet scale consistency, prescreening bad samples and reducing the number\nof hypotheses to be tested. Our evaluation shows that, up to 98% outlier ratio,\nthe proposed method achieves comparable performance to the state of the art. At\n99% outlier ratio, however, it outperforms the state of the art for both\nknown-scale and unknown-scale problems. Especially for the latter, we observe a\nclear superiority in terms of robustness and speed.\n","authors":["Seong Hun Lee","Javier Civera","Patrick Vandewalle"],"pdf_url":"https://arxiv.org/pdf/2402.16598v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17911v1","updated":"2024-07-25T10:06:26Z","published":"2024-07-25T10:06:26Z","title":"ReCorD: Reasoning and Correcting Diffusion for HOI Generation","summary":"  Diffusion models revolutionize image generation by leveraging natural\nlanguage to guide the creation of multimedia content. Despite significant\nadvancements in such generative models, challenges persist in depicting\ndetailed human-object interactions, especially regarding pose and object\nplacement accuracy. We introduce a training-free method named Reasoning and\nCorrecting Diffusion (ReCorD) to address these challenges. Our model couples\nLatent Diffusion Models with Visual Language Models to refine the generation\nprocess, ensuring precise depictions of HOIs. We propose an interaction-aware\nreasoning module to improve the interpretation of the interaction, along with\nan interaction correcting module to refine the output image for more precise\nHOI generation delicately. Through a meticulous process of pose selection and\nobject positioning, ReCorD achieves superior fidelity in generated images while\nefficiently reducing computational requirements. We conduct comprehensive\nexperiments on three benchmarks to demonstrate the significant progress in\nsolving text-to-image generation tasks, showcasing ReCorD's ability to render\ncomplex interactions accurately by outperforming existing methods in HOI\nclassification score, as well as FID and Verb CLIP-Score. Project website is\navailable at https://alberthkyhky.github.io/ReCorD/ .\n","authors":["Jian-Yu Jiang-Lin","Kang-Yang Huang","Ling Lo","Yi-Ning Huang","Terence Lin","Jhih-Ciang Wu","Hong-Han Shuai","Wen-Huang Cheng"],"pdf_url":"https://arxiv.org/pdf/2407.17911v1.pdf","comment":"Accepted by ACM MM 2024. Project website:\n  https://alberthkyhky.github.io/ReCorD/"},{"id":"http://arxiv.org/abs/2403.11761v2","updated":"2024-07-25T10:02:18Z","published":"2024-03-18T13:14:46Z","title":"BEVCar: Camera-Radar Fusion for BEV Map and Object Segmentation","summary":"  Semantic scene segmentation from a bird's-eye-view (BEV) perspective plays a\ncrucial role in facilitating planning and decision-making for mobile robots.\nAlthough recent vision-only methods have demonstrated notable advancements in\nperformance, they often struggle under adverse illumination conditions such as\nrain or nighttime. While active sensors offer a solution to this challenge, the\nprohibitively high cost of LiDARs remains a limiting factor. Fusing camera data\nwith automotive radars poses a more inexpensive alternative but has received\nless attention in prior research. In this work, we aim to advance this\npromising avenue by introducing BEVCar, a novel approach for joint BEV object\nand map segmentation. The core novelty of our approach lies in first learning a\npoint-based encoding of raw radar data, which is then leveraged to efficiently\ninitialize the lifting of image features into the BEV space. We perform\nextensive experiments on the nuScenes dataset and demonstrate that BEVCar\noutperforms the current state of the art. Moreover, we show that incorporating\nradar information significantly enhances robustness in challenging\nenvironmental conditions and improves segmentation performance for distant\nobjects. To foster future research, we provide the weather split of the\nnuScenes dataset used in our experiments, along with our code and trained\nmodels at http://bevcar.cs.uni-freiburg.de.\n","authors":["Jonas Schramm","Niclas Vödisch","Kürsat Petek","B Ravi Kiran","Senthil Yogamani","Wolfram Burgard","Abhinav Valada"],"pdf_url":"https://arxiv.org/pdf/2403.11761v2.pdf","comment":"Accepted for the IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS), 2024"},{"id":"http://arxiv.org/abs/2407.17909v1","updated":"2024-07-25T10:00:21Z","published":"2024-07-25T10:00:21Z","title":"Separating Novel Features for Logical Anomaly Detection: A\n  Straightforward yet Effective Approach","summary":"  Vision-based inspection algorithms have significantly contributed to quality\ncontrol in industrial settings, particularly in addressing structural defects\nlike dent and contamination which are prevalent in mass production. Extensive\nresearch efforts have led to the development of related benchmarks such as\nMVTec AD (Bergmann et al., 2019). However, in industrial settings, there can be\ninstances of logical defects, where acceptable items are found in unsuitable\nlocations or product pairs do not match as expected. Recent methods tackling\nlogical defects effectively employ knowledge distillation to generate\ndifference maps. Knowledge distillation (KD) is used to learn normal data\ndistribution in unsupervised manner. Despite their effectiveness, these methods\noften overlook the potential false negatives. Excessive similarity between the\nteacher network and student network can hinder the generation of a suitable\ndifference map for logical anomaly detection. This technical report provides\ninsights on handling potential false negatives by utilizing a simple constraint\nin KD-based logical anomaly detection methods. We select EfficientAD as a\nstate-of-the-art baseline and apply a margin-based constraint to its\nunsupervised learning scheme. Applying this constraint, we can improve the\nAUROC for MVTec LOCO AD by 1.3 %.\n","authors":["Kangil Lee","Geonuk Kim"],"pdf_url":"https://arxiv.org/pdf/2407.17909v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17907v1","updated":"2024-07-25T09:53:12Z","published":"2024-07-25T09:53:12Z","title":"Amortized Posterior Sampling with Diffusion Prior Distillation","summary":"  We propose a variational inference approach to sample from the posterior\ndistribution for solving inverse problems. From a pre-trained diffusion model,\nour approach trains a conditional flow model to minimize the divergence between\nthe proposal variational distribution and the posterior distribution implicitly\ndefined through the diffusion model. Once trained, the flow model is capable of\nsampling from the posterior distribution with a single NFE, amortized with\nrespect to the measurement. The proposed method paves a new path for distilling\na diffusion prior for efficient posterior sampling. We show that our method is\napplicable to standard signals in Euclidean space, as well as signals on\nmanifold.\n","authors":["Abbas Mammadov","Hyungjin Chung","Jong Chul Ye"],"pdf_url":"https://arxiv.org/pdf/2407.17907v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17906v1","updated":"2024-07-25T09:51:14Z","published":"2024-07-25T09:51:14Z","title":"Hierarchical Object Detection and Recognition Framework for Practical\n  Plant Disease Diagnosis","summary":"  Recently, object detection methods (OD; e.g., YOLO-based models) have been\nwidely utilized in plant disease diagnosis. These methods demonstrate\nrobustness to distance variations and excel at detecting small lesions compared\nto classification methods (CL; e.g., CNN models). However, there are issues\nsuch as low diagnostic performance for hard-to-detect diseases and high\nlabeling costs. Additionally, since healthy cases cannot be explicitly trained,\nthere is a risk of false positives. We propose the Hierarchical object\ndetection and recognition framework (HODRF), a sophisticated and highly\nintegrated two-stage system that combines the strengths of both OD and CL for\nplant disease diagnosis. In the first stage, HODRF uses OD to identify regions\nof interest (ROIs) without specifying the disease. In the second stage, CL\ndiagnoses diseases surrounding the ROIs. HODRF offers several advantages: (1)\nSince OD detects only one type of ROI, HODRF can detect diseases with limited\ntraining images by leveraging its ability to identify other lesions. (2) While\nOD over-detects healthy cases, HODRF significantly reduces these errors by\nusing CL in the second stage. (3) CL's accuracy improves in HODRF as it\nidentifies diagnostic targets given as ROIs, making it less vulnerable to size\nchanges. (4) HODRF benefits from CL's lower annotation costs, allowing it to\nlearn from a larger number of images. We implemented HODRF using YOLOv7 for OD\nand EfficientNetV2 for CL and evaluated its performance on a large-scale\ndataset (4 crops, 20 diseased and healthy classes, 281K images). HODRF\noutperformed YOLOv7 alone by 5.8 to 21.5 points on healthy data and 0.6 to 7.5\npoints on macro F1 scores, and it improved macro F1 by 1.1 to 7.2 points over\nEfficientNetV2.\n","authors":["Kohei Iwano","Shogo Shibuya","Satoshi Kagiwada","Hitoshi Iyatomi"],"pdf_url":"https://arxiv.org/pdf/2407.17906v1.pdf","comment":"6 pages, 3 figures"},{"id":"http://arxiv.org/abs/2407.17905v1","updated":"2024-07-25T09:51:09Z","published":"2024-07-25T09:51:09Z","title":"StreamMOS: Streaming Moving Object Segmentation with Multi-View\n  Perception and Dual-Span Memory","summary":"  Moving object segmentation based on LiDAR is a crucial and challenging task\nfor autonomous driving and mobile robotics. Most approaches explore\nspatio-temporal information from LiDAR sequences to predict moving objects in\nthe current frame. However, they often focus on transferring temporal cues in a\nsingle inference and regard every prediction as independent of others. This may\ncause inconsistent segmentation results for the same object in different\nframes. To overcome this issue, we propose a streaming network with a memory\nmechanism, called StreamMOS, to build the association of features and\npredictions among multiple inferences. Specifically, we utilize a short-term\nmemory to convey historical features, which can be regarded as spatial prior of\nmoving objects and adopted to enhance current inference by temporal fusion.\nMeanwhile, we build a long-term memory to store previous predictions and\nexploit them to refine the present forecast at voxel and instance levels\nthrough voting. Besides, we present multi-view encoder with cascade projection\nand asymmetric convolution to extract motion feature of objects in different\nrepresentations. Extensive experiments validate that our algorithm gets\ncompetitive performance on SemanticKITTI and Sipailou Campus datasets. Code\nwill be released at https://github.com/NEU-REAL/StreamMOS.git.\n","authors":["Zhiheng Li","Yubo Cui","Jiexi Zhong","Zheng Fang"],"pdf_url":"https://arxiv.org/pdf/2407.17905v1.pdf","comment":"8 pages, 7 figures"},{"id":"http://arxiv.org/abs/2405.10870v2","updated":"2024-07-25T09:51:06Z","published":"2024-05-17T16:01:11Z","title":"Multicenter Privacy-Preserving Model Training for Deep Learning Brain\n  Metastases Autosegmentation","summary":"  Objectives: This work aims to explore the impact of multicenter data\nheterogeneity on deep learning brain metastases (BM) autosegmentation\nperformance, and assess the efficacy of an incremental transfer learning\ntechnique, namely learning without forgetting (LWF), to improve model\ngeneralizability without sharing raw data.\n  Materials and methods: A total of six BM datasets from University Hospital\nErlangen (UKER), University Hospital Zurich (USZ), Stanford, UCSF, NYU and\nBraTS Challenge 2023 on BM segmentation were used for this evaluation. First,\nthe multicenter performance of a convolutional neural network (DeepMedic) for\nBM autosegmentation was established for exclusive single-center training and\nfor training on pooled data, respectively. Subsequently bilateral collaboration\nwas evaluated, where a UKER pretrained model is shared to another center for\nfurther training using transfer learning (TL) either with or without LWF.\n  Results: For single-center training, average F1 scores of BM detection range\nfrom 0.625 (NYU) to 0.876 (UKER) on respective single-center test data. Mixed\nmulticenter training notably improves F1 scores at Stanford and NYU, with\nnegligible improvement at other centers. When the UKER pretrained model is\napplied to USZ, LWF achieves a higher average F1 score (0.839) than naive TL\n(0.570) and single-center training (0.688) on combined UKER and USZ test data.\nNaive TL improves sensitivity and contouring accuracy, but compromises\nprecision. Conversely, LWF demonstrates commendable sensitivity, precision and\ncontouring accuracy. When applied to Stanford, similar performance was\nobserved.\n  Conclusion: Data heterogeneity results in varying performance in BM\nautosegmentation, posing challenges to model generalizability. LWF is a\npromising approach to peer-to-peer privacy-preserving model training.\n","authors":["Yixing Huang","Zahra Khodabakhshi","Ahmed Gomaa","Manuel Schmidt","Rainer Fietkau","Matthias Guckenberger","Nicolaus Andratschke","Christoph Bert","Stephanie Tanadini-Lang","Florian Putz"],"pdf_url":"https://arxiv.org/pdf/2405.10870v2.pdf","comment":"Official published version in the Green Journal:\n  https://doi.org/10.1016/j.radonc.2024.110419"},{"id":"http://arxiv.org/abs/2407.17324v2","updated":"2024-07-25T09:50:03Z","published":"2024-07-24T14:48:40Z","title":"Enhanced Deep Learning Methodologies and MRI Selection Techniques for\n  Dementia Diagnosis in the Elderly Population","summary":"  Dementia, a debilitating neurological condition affecting millions worldwide,\npresents significant diagnostic challenges. In this work, we introduce a novel\nmethodology for the classification of demented and non-demented elderly\npatients using 3D brain Magnetic Resonance Imaging (MRI) scans. Our approach\nfeatures a unique technique for selectively processing MRI slices, focusing on\nthe most relevant brain regions and excluding less informative sections. This\nmethodology is complemented by a confidence-based classification committee\ncomposed of three custom deep learning models: Dem3D ResNet, Dem3D CNN, and\nDem3D EfficientNet. These models work synergistically to enhance\ndecision-making accuracy, leveraging their collective strengths. Tested on the\nOpen Access Series of Imaging Studies(OASIS) dataset, our method achieved an\nimpressive accuracy of 94.12%, surpassing existing methodologies. Furthermore,\nvalidation on the Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset\nconfirmed the robustness and generalizability of our approach. The use of\nexplainable AI (XAI) techniques and comprehensive ablation studies further\nsubstantiate the effectiveness of our techniques, providing insights into the\ndecision-making process and the importance of our methodology. This research\noffers a significant advancement in dementia diagnosis, providing a highly\naccurate and efficient tool for clinical applications.\n","authors":["Nikolaos Ntampakis","Konstantinos Diamantaras","Ioanna Chouvarda","Vasileios Argyriou","Panagiotis Sarigianndis"],"pdf_url":"https://arxiv.org/pdf/2407.17324v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17904v1","updated":"2024-07-25T09:49:04Z","published":"2024-07-25T09:49:04Z","title":"Exploring the Effect of Dataset Diversity in Self-Supervised Learning\n  for Surgical Computer Vision","summary":"  Over the past decade, computer vision applications in minimally invasive\nsurgery have rapidly increased. Despite this growth, the impact of surgical\ncomputer vision remains limited compared to other medical fields like pathology\nand radiology, primarily due to the scarcity of representative annotated data.\nWhereas transfer learning from large annotated datasets such as ImageNet has\nbeen conventionally the norm to achieve high-performing models, recent\nadvancements in self-supervised learning (SSL) have demonstrated superior\nperformance. In medical image analysis, in-domain SSL pretraining has already\nbeen shown to outperform ImageNet-based initialization. Although unlabeled data\nin the field of surgical computer vision is abundant, the diversity within this\ndata is limited. This study investigates the role of dataset diversity in SSL\nfor surgical computer vision, comparing procedure-specific datasets against a\nmore heterogeneous general surgical dataset across three different downstream\nsurgical applications. The obtained results show that using solely\nprocedure-specific data can lead to substantial improvements of 13.8%, 9.5%,\nand 36.8% compared to ImageNet pretraining. However, extending this data with\nmore heterogeneous surgical data further increases performance by an additional\n5.0%, 5.2%, and 2.5%, suggesting that increasing diversity within SSL data is\nbeneficial for model performance. The code and pretrained model weights are\nmade publicly available at https://github.com/TimJaspers0801/SurgeNet.\n","authors":["Tim J. M. Jaspers","Ronald L. P. D. de Jong","Yasmina Al Khalil","Tijn Zeelenberg","Carolus H. J. Kusters","Yiping Li","Romy C. van Jaarsveld","Franciscus H. A. Bakker","Jelle P. Ruurda","Willem M. Brinkman","Peter H. N. De With","Fons van der Sommen"],"pdf_url":"https://arxiv.org/pdf/2407.17904v1.pdf","comment":"accepted - Data Engineering in Medical Imaging (DEMI) Workshop @\n  MICCAI2024"},{"id":"http://arxiv.org/abs/2405.16813v2","updated":"2024-07-25T09:47:05Z","published":"2024-05-27T04:14:20Z","title":"SiNGR: Brain Tumor Segmentation via Signed Normalized Geodesic Transform\n  Regression","summary":"  One of the primary challenges in brain tumor segmentation arises from the\nuncertainty of voxels close to tumor boundaries. However, the conventional\nprocess of generating ground truth segmentation masks fails to treat such\nuncertainties properly. Those \"hard labels\" with 0s and 1s conceptually\ninfluenced the majority of prior studies on brain image segmentation. As a\nresult, tumor segmentation is often solved through voxel classification. In\nthis work, we instead view this problem as a voxel-level regression, where the\nground truth represents a certainty mapping from any pixel to the border of the\ntumor. We propose a novel ground truth label transformation, which is based on\na signed geodesic transform, to capture the uncertainty in brain tumors'\nvicinity. We combine this idea with a Focal-like regression L1-loss that\nenables effective regression learning in high-dimensional output space by\nappropriately weighting voxels according to their difficulty. We thoroughly\nconduct an experimental evaluation to validate the components of our proposed\nmethod, compare it to a diverse array of state-of-the-art segmentation models,\nand show that it is architecture-agnostic. The code of our method is made\npublicly available (\\url{https://github.com/Oulu-IMEDS/SiNGR/}).\n","authors":["Trung Dang","Huy Hoang Nguyen","Aleksei Tiulpin"],"pdf_url":"https://arxiv.org/pdf/2405.16813v2.pdf","comment":"Accepted as a conference paper at MICCAI 2024"},{"id":"http://arxiv.org/abs/2405.07987v5","updated":"2024-07-25T09:33:50Z","published":"2024-05-13T17:58:30Z","title":"The Platonic Representation Hypothesis","summary":"  We argue that representations in AI models, particularly deep networks, are\nconverging. First, we survey many examples of convergence in the literature:\nover time and across multiple domains, the ways by which different neural\nnetworks represent data are becoming more aligned. Next, we demonstrate\nconvergence across data modalities: as vision models and language models get\nlarger, they measure distance between datapoints in a more and more alike way.\nWe hypothesize that this convergence is driving toward a shared statistical\nmodel of reality, akin to Plato's concept of an ideal reality. We term such a\nrepresentation the platonic representation and discuss several possible\nselective pressures toward it. Finally, we discuss the implications of these\ntrends, their limitations, and counterexamples to our analysis.\n","authors":["Minyoung Huh","Brian Cheung","Tongzhou Wang","Phillip Isola"],"pdf_url":"https://arxiv.org/pdf/2405.07987v5.pdf","comment":"Equal contributions. Project: https://phillipi.github.io/prh/ Code:\n  https://github.com/minyoungg/platonic-rep"},{"id":"http://arxiv.org/abs/2405.16815v2","updated":"2024-07-25T09:29:36Z","published":"2024-05-27T04:17:10Z","title":"Image-level Regression for Uncertainty-aware Retinal Image Segmentation","summary":"  Accurate retinal vessel (RV) segmentation is a crucial step in the\nquantitative assessment of retinal vasculature, which is needed for the early\ndetection of retinal diseases and other conditions. Numerous studies have been\nconducted to tackle the problem of segmenting vessels automatically using a\npixel-wise classification approach. The common practice of creating ground\ntruth labels is to categorize pixels as foreground and background. This\napproach is, however, biased, and it ignores the uncertainty of a human\nannotator when it comes to annotating e.g. thin vessels. In this work, we\npropose a simple and effective method that casts the RV segmentation task as an\nimage-level regression. For this purpose, we first introduce a novel\nSegmentation Annotation Uncertainty-Aware (SAUNA) transform, which adds pixel\nuncertainty to the ground truth using the pixel's closeness to the annotation\nboundary and vessel thickness. To train our model with soft labels, we\ngeneralize the earlier proposed Jaccard metric loss to arbitrary hypercubes for\nsoft Jaccard index (Intersection-over-Union) optimization. Additionally, we\nemploy a stable version of the Focal-L1 loss for pixel-wise regression. We\nconduct thorough experiments and compare our method to a diverse set of\nbaselines across 5 retinal image datasets. Our empirical results indicate that\nthe integration of the SAUNA transform and these segmentation losses led to\nsignificant performance boosts for different segmentation models. Particularly,\nour methodology enables UNet-like architectures to substantially outperform\ncomputational-intensive baselines. Our implementation is available at\n\\url{https://github.com/Oulu-IMEDS/SAUNA}.\n","authors":["Trung Dang","Huy Hoang Nguyen","Aleksei Tiulpin"],"pdf_url":"https://arxiv.org/pdf/2405.16815v2.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2407.17229v2","updated":"2024-07-25T09:29:21Z","published":"2024-07-24T12:32:24Z","title":"LPGen: Enhancing High-Fidelity Landscape Painting Generation through\n  Diffusion Model","summary":"  Generating landscape paintings expands the possibilities of artistic\ncreativity and imagination. Traditional landscape painting methods involve\nusing ink or colored ink on rice paper, which requires substantial time and\neffort. These methods are susceptible to errors and inconsistencies and lack\nprecise control over lines and colors. This paper presents LPGen, a\nhigh-fidelity, controllable model for landscape painting generation,\nintroducing a novel multi-modal framework that integrates image prompts into\nthe diffusion model. We extract its edges and contours by computing canny edges\nfrom the target landscape image. These, along with natural language text\nprompts and drawing style references, are fed into the latent diffusion model\nas conditions. We implement a decoupled cross-attention strategy to ensure\ncompatibility between image and text prompts, facilitating multi-modal image\ngeneration. A decoder generates the final image. Quantitative and qualitative\nanalyses demonstrate that our method outperforms existing approaches in\nlandscape painting generation and exceeds the current state-of-the-art. The\nLPGen network effectively controls the composition and color of landscape\npaintings, generates more accurate images, and supports further research in\ndeep learning-based landscape painting generation.\n","authors":["Wanggong Yang","Xiaona Wang","Yingrui Qiu","Yifei Zhao"],"pdf_url":"https://arxiv.org/pdf/2407.17229v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.14596v3","updated":"2024-07-25T09:20:22Z","published":"2023-06-26T11:13:22Z","title":"Deep Learning for Cancer Prognosis Prediction Using Portrait Photos by\n  StyleGAN Embedding","summary":"  Survival prediction for cancer patients is critical for optimal treatment\nselection and patient management. Current patient survival prediction methods\ntypically extract survival information from patients' clinical record data or\nbiological and imaging data. In practice, experienced clinicians can have a\npreliminary assessment of patients' health status based on patients' observable\nphysical appearances, which are mainly facial features. However, such\nassessment is highly subjective. In this work, the efficacy of objectively\ncapturing and using prognostic information contained in conventional portrait\nphotographs using deep learning for survival predication purposes is\ninvestigated for the first time. A pre-trained StyleGAN2 model is fine-tuned on\na custom dataset of our cancer patients' photos to empower its generator with\ngenerative ability suitable for patients' photos. The StyleGAN2 is then used to\nembed the photographs to its highly expressive latent space. Utilizing the\nstate-of-the-art survival analysis models and based on StyleGAN's latent space\nphoto embeddings, this approach achieved a C-index of 0.677, which is notably\nhigher than chance and evidencing the prognostic value embedded in simple 2D\nfacial images. In addition, thanks to StyleGAN's interpretable latent space,\nour survival prediction model can be validated for relying on essential facial\nfeatures, eliminating any biases from extraneous information like clothing or\nbackground. Moreover, a health attribute is obtained from regression\ncoefficients, which has important potential value for patient care.\n","authors":["Amr Hagag","Ahmed Gomaa","Dominik Kornek","Andreas Maier","Rainer Fietkau","Christoph Bert","Florian Putz","Yixing Huang"],"pdf_url":"https://arxiv.org/pdf/2306.14596v3.pdf","comment":"MICCAI 2024 Early Accept"},{"id":"http://arxiv.org/abs/2312.07537v2","updated":"2024-07-25T09:10:52Z","published":"2023-12-12T18:59:16Z","title":"FreeInit: Bridging Initialization Gap in Video Diffusion Models","summary":"  Though diffusion-based video generation has witnessed rapid progress, the\ninference results of existing models still exhibit unsatisfactory temporal\nconsistency and unnatural dynamics. In this paper, we delve deep into the noise\ninitialization of video diffusion models, and discover an implicit\ntraining-inference gap that attributes to the unsatisfactory inference\nquality.Our key findings are: 1) the spatial-temporal frequency distribution of\nthe initial noise at inference is intrinsically different from that for\ntraining, and 2) the denoising process is significantly influenced by the\nlow-frequency components of the initial noise. Motivated by these observations,\nwe propose a concise yet effective inference sampling strategy, FreeInit, which\nsignificantly improves temporal consistency of videos generated by diffusion\nmodels. Through iteratively refining the spatial-temporal low-frequency\ncomponents of the initial latent during inference, FreeInit is able to\ncompensate the initialization gap between training and inference, thus\neffectively improving the subject appearance and temporal consistency of\ngeneration results. Extensive experiments demonstrate that FreeInit\nconsistently enhances the generation quality of various text-to-video diffusion\nmodels without additional training or fine-tuning.\n","authors":["Tianxing Wu","Chenyang Si","Yuming Jiang","Ziqi Huang","Ziwei Liu"],"pdf_url":"https://arxiv.org/pdf/2312.07537v2.pdf","comment":"Project page: https://tianxingwu.github.io/pages/FreeInit/ Code:\n  https://github.com/TianxingWu/FreeInit"},{"id":"http://arxiv.org/abs/2407.17877v1","updated":"2024-07-25T08:47:27Z","published":"2024-07-25T08:47:27Z","title":"Advancing 3D Point Cloud Understanding through Deep Transfer Learning: A\n  Comprehensive Survey","summary":"  The 3D point cloud (3DPC) has significantly evolved and benefited from the\nadvance of deep learning (DL). However, the latter faces various issues,\nincluding the lack of data or annotated data, the existence of a significant\ngap between training data and test data, and the requirement for high\ncomputational resources. To that end, deep transfer learning (DTL), which\ndecreases dependency and costs by utilizing knowledge gained from a source\ndata/task in training a target data/task, has been widely investigated.\nNumerous DTL frameworks have been suggested for aligning point clouds obtained\nfrom several scans of the same scene. Additionally, DA, which is a subset of\nDTL, has been modified to enhance the point cloud data's quality by dealing\nwith noise and missing points. Ultimately, fine-tuning and DA approaches have\ndemonstrated their effectiveness in addressing the distinct difficulties\ninherent in point cloud data. This paper presents the first review shedding\nlight on this aspect. it provides a comprehensive overview of the latest\ntechniques for understanding 3DPC using DTL and domain adaptation (DA).\nAccordingly, DTL's background is first presented along with the datasets and\nevaluation metrics. A well-defined taxonomy is introduced, and detailed\ncomparisons are presented, considering different aspects such as different\nknowledge transfer strategies, and performance. The paper covers various\napplications, such as 3DPC object detection, semantic labeling, segmentation,\nclassification, registration, downsampling/upsampling, and denoising.\nFurthermore, the article discusses the advantages and limitations of the\npresented frameworks, identifies open challenges, and suggests potential\nresearch directions.\n","authors":["Shahab Saquib Sohail","Yassine Himeur","Hamza Kheddar","Abbes Amira","Fodil Fadli","Shadi Atalla","Abigail Copiaco","Wathiq Mansoor"],"pdf_url":"https://arxiv.org/pdf/2407.17877v1.pdf","comment":"55 pages, 9 tables, and 15 figures"},{"id":"http://arxiv.org/abs/2309.12865v3","updated":"2024-07-25T08:32:15Z","published":"2023-09-22T13:39:24Z","title":"Bridging Sensor Gaps via Attention Gated Tuning for Hyperspectral Image\n  Classification","summary":"  Data-hungry HSI classification methods require high-quality labeled HSIs,\nwhich are often costly to obtain. This characteristic limits the performance\npotential of data-driven methods when dealing with limited annotated samples.\nBridging the domain gap between data acquired from different sensors allows us\nto utilize abundant labeled data across sensors to break this bottleneck. In\nthis paper, we propose a novel Attention-Gated Tuning (AGT) strategy and a\ntriplet-structured transformer model, Tri-Former, to address this issue. The\nAGT strategy serves as a bridge, allowing us to leverage existing labeled HSI\ndatasets, even RGB datasets to enhance the performance on new HSI datasets with\nlimited samples. Instead of inserting additional parameters inside the basic\nmodel, we train a lightweight auxiliary branch that takes intermediate features\nas input from the basic model and makes predictions. The proposed AGT resolves\nconflicts between heterogeneous and even cross-modal data by suppressing the\ndisturbing information and enhances the useful information through a soft gate.\nAdditionally, we introduce Tri-Former, a triplet-structured transformer with a\nspectral-spatial separation design that enhances parameter utilization and\ncomputational efficiency, enabling easier and flexible fine-tuning. Comparison\nexperiments conducted on three representative HSI datasets captured by\ndifferent sensors demonstrate the proposed Tri-Former achieves better\nperformance compared to several state-of-the-art methods. Homologous,\nheterologous and cross-modal tuning experiments verified the effectiveness of\nthe proposed AGT. Code has been released at:\n\\href{https://github.com/Cecilia-xue/AGT}{https://github.com/Cecilia-xue/AGT}.\n","authors":["Xizhe Xue","Haokui Zhang","Zongwen Bai","Ying Li"],"pdf_url":"https://arxiv.org/pdf/2309.12865v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17857v1","updated":"2024-07-25T08:22:30Z","published":"2024-07-25T08:22:30Z","title":"Mew: Multiplexed Immunofluorescence Image Analysis through an Efficient\n  Multiplex Network","summary":"  Recent advancements in graph-based approaches for multiplexed\nimmunofluorescence (mIF) images have significantly propelled the field forward,\noffering deeper insights into patient-level phenotyping. However, current\ngraph-based methodologies encounter two primary challenges: (1) Cellular\nHeterogeneity, where existing approaches fail to adequately address the\ninductive biases inherent in graphs, particularly the homophily characteristic\nobserved in cellular connectivity and; (2) Scalability, where handling cellular\ngraphs from high-dimensional images faces difficulties in managing a high\nnumber of cells. To overcome these limitations, we introduce Mew, a novel\nframework designed to efficiently process mIF images through the lens of\nmultiplex network. Mew innovatively constructs a multiplex network comprising\ntwo distinct layers: a Voronoi network for geometric information and a\nCell-type network for capturing cell-wise homogeneity. This framework equips a\nscalable and efficient Graph Neural Network (GNN), capable of processing the\nentire graph during training. Furthermore, Mew integrates an interpretable\nattention module that autonomously identifies relevant layers for image\nclassification. Extensive experiments on a real-world patient dataset from\nvarious institutions highlight Mew's remarkable efficacy and efficiency,\nmarking a significant advancement in mIF image analysis. The source code of Mew\ncan be found here: \\url{https://github.com/UNITES-Lab/Mew}\n","authors":["Sukwon Yun","Jie Peng","Alexandro E. Trevino","Chanyoung Park","Tianlong Chen"],"pdf_url":"https://arxiv.org/pdf/2407.17857v1.pdf","comment":"ECCV 2024"},{"id":"http://arxiv.org/abs/2403.06461v3","updated":"2024-07-25T08:21:31Z","published":"2024-03-11T06:56:08Z","title":"Reliable Spatial-Temporal Voxels For Multi-Modal Test-Time Adaptation","summary":"  Multi-modal test-time adaptation (MM-TTA) is proposed to adapt models to an\nunlabeled target domain by leveraging the complementary multi-modal inputs in\nan online manner. Previous MM-TTA methods for 3D segmentation rely on\npredictions of cross-modal information in each input frame, while they ignore\nthe fact that predictions of geometric neighborhoods within consecutive frames\nare highly correlated, leading to unstable predictions across time. To fulfill\nthis gap, we propose ReLiable Spatial-temporal Voxels (Latte), an MM-TTA method\nthat leverages reliable cross-modal spatial-temporal correspondences for\nmulti-modal 3D segmentation. Motivated by the fact that reliable predictions\nshould be consistent with their spatial-temporal correspondences, Latte\naggregates consecutive frames in a slide window manner and constructs\nSpatial-Temopral (ST) voxels to capture temporally local prediction consistency\nfor each modality. After filtering out ST voxels with high ST entropy, Latte\nconducts cross-modal learning for each point and pixel by attending to those\nwith reliable and consistent predictions among both spatial and temporal\nneighborhoods. Experimental results show that Latte achieves state-of-the-art\nperformance on three different MM-TTA benchmarks compared to previous MM-TTA or\nTTA methods. Visit our project site https://sites.google.com/view/eccv24-latte.\n","authors":["Haozhi Cao","Yuecong Xu","Jianfei Yang","Pengyu Yin","Xingyu Ji","Shenghai Yuan","Lihua Xie"],"pdf_url":"https://arxiv.org/pdf/2403.06461v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.06903v2","updated":"2024-07-25T08:19:53Z","published":"2024-04-10T10:46:59Z","title":"DreamScene360: Unconstrained Text-to-3D Scene Generation with Panoramic\n  Gaussian Splatting","summary":"  The increasing demand for virtual reality applications has highlighted the\nsignificance of crafting immersive 3D assets. We present a text-to-3D\n360$^{\\circ}$ scene generation pipeline that facilitates the creation of\ncomprehensive 360$^{\\circ}$ scenes for in-the-wild environments in a matter of\nminutes. Our approach utilizes the generative power of a 2D diffusion model and\nprompt self-refinement to create a high-quality and globally coherent panoramic\nimage. This image acts as a preliminary \"flat\" (2D) scene representation.\nSubsequently, it is lifted into 3D Gaussians, employing splatting techniques to\nenable real-time exploration. To produce consistent 3D geometry, our pipeline\nconstructs a spatially coherent structure by aligning the 2D monocular depth\ninto a globally optimized point cloud. This point cloud serves as the initial\nstate for the centroids of 3D Gaussians. In order to address invisible issues\ninherent in single-view inputs, we impose semantic and geometric constraints on\nboth synthesized and input camera views as regularizations. These guide the\noptimization of Gaussians, aiding in the reconstruction of unseen regions. In\nsummary, our method offers a globally consistent 3D scene within a\n360$^{\\circ}$ perspective, providing an enhanced immersive experience over\nexisting techniques. Project website at: http://dreamscene360.github.io/\n","authors":["Shijie Zhou","Zhiwen Fan","Dejia Xu","Haoran Chang","Pradyumna Chari","Tejas Bharadwaj","Suya You","Zhangyang Wang","Achuta Kadambi"],"pdf_url":"https://arxiv.org/pdf/2404.06903v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.12832v2","updated":"2024-07-25T08:09:12Z","published":"2024-04-19T12:09:49Z","title":"COIN: Counterfactual inpainting for weakly supervised semantic\n  segmentation for medical images","summary":"  Deep learning is dramatically transforming the field of medical imaging and\nradiology, enabling the identification of pathologies in medical images,\nincluding computed tomography (CT) and X-ray scans. However, the performance of\ndeep learning models, particularly in segmentation tasks, is often limited by\nthe need for extensive annotated datasets. To address this challenge, the\ncapabilities of weakly supervised semantic segmentation are explored through\nthe lens of Explainable AI and the generation of counterfactual explanations.\nThe scope of this research is development of a novel counterfactual inpainting\napproach (COIN) that flips the predicted classification label from abnormal to\nnormal by using a generative model. For instance, if the classifier deems an\ninput medical image X as abnormal, indicating the presence of a pathology, the\ngenerative model aims to inpaint the abnormal region, thus reversing the\nclassifier's original prediction label. The approach enables us to produce\nprecise segmentations for pathologies without depending on pre-existing\nsegmentation masks. Crucially, image-level labels are utilized, which are\nsubstantially easier to acquire than creating detailed segmentation masks. The\neffectiveness of the method is demonstrated by segmenting synthetic targets and\nactual kidney tumors from CT images acquired from Tartu University Hospital in\nEstonia. The findings indicate that COIN greatly surpasses established\nattribution methods, such as RISE, ScoreCAM, and LayerCAM, as well as an\nalternative counterfactual explanation method introduced by Singla et al. This\nevidence suggests that COIN is a promising approach for semantic segmentation\nof tumors in CT images, and presents a step forward in making deep learning\napplications more accessible and effective in healthcare, where annotated data\nis scarce.\n","authors":["Dmytro Shvetsov","Joonas Ariva","Marharyta Domnich","Raul Vicente","Dmytro Fishman"],"pdf_url":"https://arxiv.org/pdf/2404.12832v2.pdf","comment":"This work has been accepted to be presented to The 2nd World\n  Conference on eXplainable Artificial Intelligence (xAI 2024), July 17-19,\n  2024 - Valletta, Malta"},{"id":"http://arxiv.org/abs/2407.17850v1","updated":"2024-07-25T08:07:40Z","published":"2024-07-25T08:07:40Z","title":"FlexiEdit: Frequency-Aware Latent Refinement for Enhanced Non-Rigid\n  Editing","summary":"  Current image editing methods primarily utilize DDIM Inversion, employing a\ntwo-branch diffusion approach to preserve the attributes and layout of the\noriginal image. However, these methods encounter challenges with non-rigid\nedits, which involve altering the image's layout or structure. Our\ncomprehensive analysis reveals that the high-frequency components of DDIM\nlatent, crucial for retaining the original image's key features and layout,\nsignificantly contribute to these limitations. Addressing this, we introduce\nFlexiEdit, which enhances fidelity to input text prompts by refining DDIM\nlatent, by reducing high-frequency components in targeted editing areas.\nFlexiEdit comprises two key components: (1) Latent Refinement, which modifies\nDDIM latent to better accommodate layout adjustments, and (2) Edit Fidelity\nEnhancement via Re-inversion, aimed at ensuring the edits more accurately\nreflect the input text prompts. Our approach represents notable progress in\nimage editing, particularly in performing complex non-rigid edits, showcasing\nits enhanced capability through comparative experiments.\n","authors":["Gwanhyeong Koo","Sunjae Yoon","Ji Woo Hong","Chang D. Yoo"],"pdf_url":"https://arxiv.org/pdf/2407.17850v1.pdf","comment":"ECCV 2024"},{"id":"http://arxiv.org/abs/2407.17847v1","updated":"2024-07-25T08:00:49Z","published":"2024-07-25T08:00:49Z","title":"Move and Act: Enhanced Object Manipulation and Background Integrity for\n  Image Editing","summary":"  Current methods commonly utilize three-branch structures of inversion,\nreconstruction, and editing, to tackle consistent image editing task. However,\nthese methods lack control over the generation position of the edited object\nand have issues with background preservation. To overcome these limitations, we\npropose a tuning-free method with only two branches: inversion and editing.\nThis approach allows users to simultaneously edit the object's action and\ncontrol the generation position of the edited object. Additionally, it achieves\nimproved background preservation. Specifically, we transfer the edited object\ninformation to the target area and repair or preserve the background of other\nareas during the inversion process at a specific time step. In the editing\nstage, we use the image features in self-attention to query the key and value\nof the corresponding time step in the inversion to achieve consistent image\nediting. Impressive image editing results and quantitative evaluation\ndemonstrate the effectiveness of our method. The code is available at\nhttps://github.com/mobiushy/move-act.\n","authors":["Pengfei Jiang","Mingbao Lin","Fei Chao","Rongrong Ji"],"pdf_url":"https://arxiv.org/pdf/2407.17847v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17843v1","updated":"2024-07-25T07:57:55Z","published":"2024-07-25T07:57:55Z","title":"DragText: Rethinking Text Embedding in Point-based Image Editing","summary":"  Point-based image editing enables accurate and flexible control through\ncontent dragging. However, the role of text embedding in the editing process\nhas not been thoroughly investigated. A significant aspect that remains\nunexplored is the interaction between text and image embeddings. In this study,\nwe show that during the progressive editing of an input image in a diffusion\nmodel, the text embedding remains constant. As the image embedding increasingly\ndiverges from its initial state, the discrepancy between the image and text\nembeddings presents a significant challenge. Moreover, we found that the text\nprompt significantly influences the dragging process, particularly in\nmaintaining content integrity and achieving the desired manipulation. To\nutilize these insights, we propose DragText, which optimizes text embedding in\nconjunction with the dragging process to pair with the modified image\nembedding. Simultaneously, we regularize the text optimization process to\npreserve the integrity of the original text prompt. Our approach can be\nseamlessly integrated with existing diffusion-based drag methods with only a\nfew lines of code.\n","authors":["Gayoon Choi","Taejin Jeong","Sujung Hong","Jaehoon Joo","Seong Jae Hwang"],"pdf_url":"https://arxiv.org/pdf/2407.17843v1.pdf","comment":"22 pages, 18 figures"},{"id":"http://arxiv.org/abs/2407.17838v1","updated":"2024-07-25T07:52:11Z","published":"2024-07-25T07:52:11Z","title":"UMono: Physical Model Informed Hybrid CNN-Transformer Framework for\n  Underwater Monocular Depth Estimation","summary":"  Underwater monocular depth estimation serves as the foundation for tasks such\nas 3D reconstruction of underwater scenes. However, due to the influence of\nlight and medium, the underwater environment undergoes a distinctive imaging\nprocess, which presents challenges in accurately estimating depth from a single\nimage. The existing methods fail to consider the unique characteristics of\nunderwater environments, leading to inadequate estimation results and limited\ngeneralization performance. Furthermore, underwater depth estimation requires\nextracting and fusing both local and global features, which is not fully\nexplored in existing methods. In this paper, an end-to-end learning framework\nfor underwater monocular depth estimation called UMono is presented, which\nincorporates underwater image formation model characteristics into network\narchitecture, and effectively utilize both local and global features of\nunderwater image. Experimental results demonstrate that the proposed method is\neffective for underwater monocular depth estimation and outperforms the\nexisting methods in both quantitative and qualitative analyses.\n","authors":["Jian Wang","Jing Wang","Shenghui Rong","Bo He"],"pdf_url":"https://arxiv.org/pdf/2407.17838v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16087v3","updated":"2024-07-25T07:50:58Z","published":"2024-06-23T12:02:17Z","title":"Imperative Learning: A Self-supervised Neural-Symbolic Learning\n  Framework for Robot Autonomy","summary":"  Data-driven methods such as reinforcement and imitation learning have\nachieved remarkable success in robot autonomy. However, their data-centric\nnature still hinders them from generalizing well to ever-changing environments.\nMoreover, collecting large datasets for robotic tasks is often impractical and\nexpensive. To overcome these challenges, we introduce a new self-supervised\nneural-symbolic (NeSy) computational framework, imperative learning (IL), for\nrobot autonomy, leveraging the generalization abilities of symbolic reasoning.\nThe framework of IL consists of three primary components: a neural module, a\nreasoning engine, and a memory system. We formulate IL as a special bilevel\noptimization (BLO), which enables reciprocal learning over the three modules.\nThis overcomes the label-intensive obstacles associated with data-driven\napproaches and takes advantage of symbolic reasoning concerning logical\nreasoning, physical principles, geometric analysis, etc. We discuss several\noptimization techniques for IL and verify their effectiveness in five distinct\nrobot autonomy tasks including path planning, rule induction, optimal control,\nvisual odometry, and multi-robot routing. Through various experiments, we show\nthat IL can significantly enhance robot autonomy capabilities and we anticipate\nthat it will catalyze further research across diverse domains.\n","authors":["Chen Wang","Kaiyi Ji","Junyi Geng","Zhongqiang Ren","Taimeng Fu","Fan Yang","Yifan Guo","Haonan He","Xiangyu Chen","Zitong Zhan","Qiwei Du","Shaoshu Su","Bowen Li","Yuheng Qiu","Yi Du","Qihang Li","Yifan Yang","Xiao Lin","Zhipeng Zhao"],"pdf_url":"https://arxiv.org/pdf/2406.16087v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17310v2","updated":"2024-07-25T07:47:11Z","published":"2024-07-24T14:22:55Z","title":"LangOcc: Self-Supervised Open Vocabulary Occupancy Estimation via Volume\n  Rendering","summary":"  The 3D occupancy estimation task has become an important challenge in the\narea of vision-based autonomous driving recently. However, most existing\ncamera-based methods rely on costly 3D voxel labels or LiDAR scans for\ntraining, limiting their practicality and scalability. Moreover, most methods\nare tied to a predefined set of classes which they can detect. In this work we\npresent a novel approach for open vocabulary occupancy estimation called\nLangOcc, that is trained only via camera images, and can detect arbitrary\nsemantics via vision-language alignment. In particular, we distill the\nknowledge of the strong vision-language aligned encoder CLIP into a 3D\noccupancy model via differentiable volume rendering. Our model estimates\nvision-language aligned features in a 3D voxel grid using only images. It is\ntrained in a self-supervised manner by rendering our estimations back to 2D\nspace, where ground-truth features can be computed. This training mechanism\nautomatically supervises the scene geometry, allowing for a straight-forward\nand powerful training method without any explicit geometry supervision. LangOcc\noutperforms LiDAR-supervised competitors in open vocabulary occupancy by a\nlarge margin, solely relying on vision-based training. We also achieve\nstate-of-the-art results in self-supervised semantic occupancy estimation on\nthe Occ3D-nuScenes dataset, despite not being limited to a specific set of\ncategories, thus demonstrating the effectiveness of our proposed\nvision-language training.\n","authors":["Simon Boeder","Fabian Gigengack","Benjamin Risse"],"pdf_url":"https://arxiv.org/pdf/2407.17310v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17834v1","updated":"2024-07-25T07:45:28Z","published":"2024-07-25T07:45:28Z","title":"Towards the Spectral bias Alleviation by Normalizations in Coordinate\n  Networks","summary":"  Representing signals using coordinate networks dominates the area of inverse\nproblems recently, and is widely applied in various scientific computing tasks.\nStill, there exists an issue of spectral bias in coordinate networks, limiting\nthe capacity to learn high-frequency components. This problem is caused by the\npathological distribution of the neural tangent kernel's (NTK's) eigenvalues of\ncoordinate networks. We find that, this pathological distribution could be\nimproved using classical normalization techniques (batch normalization and\nlayer normalization), which are commonly used in convolutional neural networks\nbut rarely used in coordinate networks. We prove that normalization techniques\ngreatly reduces the maximum and variance of NTK's eigenvalues while slightly\nmodifies the mean value, considering the max eigenvalue is much larger than the\nmost, this variance change results in a shift of eigenvalues' distribution from\na lower one to a higher one, therefore the spectral bias could be alleviated.\nFurthermore, we propose two new normalization techniques by combining these two\ntechniques in different ways. The efficacy of these normalization techniques is\nsubstantiated by the significant improvements and new state-of-the-arts\nachieved by applying normalization-based coordinate networks to various tasks,\nincluding the image compression, computed tomography reconstruction, shape\nrepresentation, magnetic resonance imaging, novel view synthesis and multi-view\nstereo reconstruction.\n","authors":["Zhicheng Cai","Hao Zhu","Qiu Shen","Xinran Wang","Xun Cao"],"pdf_url":"https://arxiv.org/pdf/2407.17834v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.03059v7","updated":"2024-07-25T07:42:15Z","published":"2023-10-04T16:49:36Z","title":"Point-PEFT: Parameter-Efficient Fine-Tuning for 3D Pre-trained Models","summary":"  The popularity of pre-trained large models has revolutionized downstream\ntasks across diverse fields, such as language, vision, and multi-modality. To\nminimize the adaption cost for downstream tasks, many Parameter-Efficient\nFine-Tuning (PEFT) techniques are proposed for language and 2D image\npre-trained models. However, the specialized PEFT method for 3D pre-trained\nmodels is still under-explored. To this end, we introduce Point-PEFT, a novel\nframework for adapting point cloud pre-trained models with minimal learnable\nparameters. Specifically, for a pre-trained 3D model, we freeze most of its\nparameters, and only tune the newly added PEFT modules on downstream tasks,\nwhich consist of a Point-prior Prompt and a Geometry-aware Adapter. The\nPoint-prior Prompt adopts a set of learnable prompt tokens, for which we\npropose to construct a memory bank with domain-specific knowledge, and utilize\na parameter-free attention to enhance the prompt tokens. The Geometry-aware\nAdapter aims to aggregate point cloud features within spatial neighborhoods to\ncapture fine-grained geometric information through local interactions.\nExtensive experiments indicate that our Point-PEFT can achieve better\nperformance than the full fine-tuning on various downstream tasks, while using\nonly 5% of the trainable parameters, demonstrating the efficiency and\neffectiveness of our approach. Code is released at\nhttps://github.com/Ivan-Tang-3D/Point-PEFT.\n","authors":["Yiwen Tang","Ray Zhang","Zoey Guo","Dong Wang","Zhigang Wang","Bin Zhao","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2310.03059v7.pdf","comment":"The specialized PEFT framework for 3D pre-trained models, which\n  achieves competitive performance to full fine-tuning, and significantly\n  reduces the computational resources. Project page:\n  https://github.com/Ivan-Tang-3D/Point-PEFT"},{"id":"http://arxiv.org/abs/2407.17829v1","updated":"2024-07-25T07:38:27Z","published":"2024-07-25T07:38:27Z","title":"Image Segmentation via Divisive Normalization: dealing with\n  environmental diversity","summary":"  Autonomous driving is a challenging scenario for image segmentation due to\nthe presence of uncontrolled environmental conditions and the eventually\ncatastrophic consequences of failures. Previous work suggested that a\nbiologically motivated computation, the so-called Divisive Normalization, could\nbe useful to deal with image variability, but its effects have not been\nsystematically studied over different data sources and environmental factors.\nHere we put segmentation U-nets augmented with Divisive Normalization to work\nfar from training conditions to find where this adaptation is more critical. We\ncategorize the scenes according to their radiance level and dynamic range\n(day/night), and according to their achromatic/chromatic contrasts. We also\nconsider video game (synthetic) images to broaden the range of environments. We\ncheck the performance in the extreme percentiles of such categorization. Then,\nwe push the limits further by artificially modifying the images in\nperceptually/environmentally relevant dimensions: luminance, contrasts and\nspectral radiance. Results show that neural networks with Divisive\nNormalization get better results in all the scenarios and their performance\nremains more stable with regard to the considered environmental factors and\nnature of the source. Finally, we explain the improvements in segmentation\nperformance in two ways: (1) by quantifying the invariance of the responses\nthat incorporate Divisive Normalization, and (2) by illustrating the adaptive\nnonlinearity of the different layers that depends on the local activity.\n","authors":["Pablo Hernández-Cámara","Jorge Vila-Tomás","Paula Dauden-Oliver","Nuria Alabau-Bosque","Valero Laparra","Jesús Malo"],"pdf_url":"https://arxiv.org/pdf/2407.17829v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17827v1","updated":"2024-07-25T07:35:27Z","published":"2024-07-25T07:35:27Z","title":"Unified Lexical Representation for Interpretable Visual-Language\n  Alignment","summary":"  Visual-Language Alignment (VLA) has gained a lot of attention since CLIP's\ngroundbreaking work. Although CLIP performs well, the typical direct latent\nfeature alignment lacks clarity in its representation and similarity scores. On\nthe other hand, lexical representation, a vector whose element represents the\nsimilarity between the sample and a word from the vocabulary, is a natural\nsparse representation and interpretable, providing exact matches for individual\nwords. However, lexical representations is difficult to learn due to no\nground-truth supervision and false-discovery issues, and thus requires complex\ndesign to train effectively. In this paper, we introduce LexVLA, a more\ninterpretable VLA framework by learning a unified lexical representation for\nboth modalities without complex design. We use DINOv2 as our visual model for\nits local-inclined features and Llama 2, a generative language model, to\nleverage its in-context lexical prediction ability. To avoid the false\ndiscovery, we propose an overuse penalty to refrain the lexical representation\nfrom falsely frequently activating meaningless words. We demonstrate that these\ntwo pre-trained uni-modal models can be well-aligned by fine-tuning on modest\nmulti-modal dataset and avoid intricate training configurations. On cross-modal\nretrieval benchmarks, LexVLA, trained on the CC-12M multi-modal dataset,\noutperforms baselines fine-tuned on larger datasets (e.g., YFCC15M) and those\ntrained from scratch on even bigger datasets (e.g., 1.1B data, including\nCC-12M). We conduct extensive experiments to analyze LexVLA.\n","authors":["Yifan Li","Yikai Wang","Yanwei Fu","Dongyu Ru","Zheng Zhang","Tong He"],"pdf_url":"https://arxiv.org/pdf/2407.17827v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10172v2","updated":"2024-07-25T07:26:40Z","published":"2024-07-14T11:59:22Z","title":"Restoring Images in Adverse Weather Conditions via Histogram Transformer","summary":"  Transformer-based image restoration methods in adverse weather have achieved\nsignificant progress. Most of them use self-attention along the channel\ndimension or within spatially fixed-range blocks to reduce computational load.\nHowever, such a compromise results in limitations in capturing long-range\nspatial features. Inspired by the observation that the weather-induced\ndegradation factors mainly cause similar occlusion and brightness, in this\nwork, we propose an efficient Histogram Transformer (Histoformer) for restoring\nimages affected by adverse weather. It is powered by a mechanism dubbed\nhistogram self-attention, which sorts and segments spatial features into\nintensity-based bins. Self-attention is then applied across bins or within each\nbin to selectively focus on spatial features of dynamic range and process\nsimilar degraded pixels of the long range together. To boost histogram\nself-attention, we present a dynamic-range convolution enabling conventional\nconvolution to conduct operation over similar pixels rather than neighbor\npixels. We also observe that the common pixel-wise losses neglect linear\nassociation and correlation between output and ground-truth. Thus, we propose\nto leverage the Pearson correlation coefficient as a loss function to enforce\nthe recovered pixels following the identical order as ground-truth. Extensive\nexperiments demonstrate the efficacy and superiority of our proposed method. We\nhave released the codes in Github.\n","authors":["Shangquan Sun","Wenqi Ren","Xinwei Gao","Rui Wang","Xiaochun Cao"],"pdf_url":"https://arxiv.org/pdf/2407.10172v2.pdf","comment":"19 pages, 7 figures, 10MB"},{"id":"http://arxiv.org/abs/2310.05483v5","updated":"2024-07-25T07:08:24Z","published":"2023-10-09T07:42:33Z","title":"HarmonicNeRF: Geometry-Informed Synthetic View Augmentation for 3D Scene\n  Reconstruction in Driving Scenarios","summary":"  In the realm of autonomous driving, achieving precise 3D reconstruction of\nthe driving environment is critical for ensuring safety and effective\nnavigation. Neural Radiance Fields (NeRF) have shown promise in creating highly\ndetailed and accurate models of complex environments. However, the application\nof NeRF in autonomous driving scenarios encounters several challenges,\nprimarily due to the sparsity of viewpoints inherent in camera trajectories and\nthe constraints on data collection in unbounded outdoor scenes, which typically\noccur along predetermined paths. This limitation not only reduces the available\nscene information but also poses significant challenges for NeRF training, as\nthe sparse and path-distributed observational data leads to\nunder-representation of the scene's geometry. In this paper, we introduce\nHarmonicNeRF, a novel approach for outdoor self-supervised monocular scene\nreconstruction. HarmonicNeRF capitalizes on the strengths of NeRF and enhances\nsurface reconstruction accuracy by augmenting the input space with\ngeometry-informed synthetic views. This is achieved through the application of\nspherical harmonics to generate novel radiance values, taking into careful\nconsideration the color observations from the limited available real-world\nviews. Additionally, our method incorporates proxy geometry to effectively\nmanage occlusion, generating radiance pseudo-labels that circumvent the\nlimitations of traditional image-warping techniques, which often fail in sparse\ndata conditions typical of autonomous driving environments. Extensive\nexperiments conducted on the KITTI, Argoverse, and NuScenes datasets\ndemonstrate our approach establishes new benchmarks in synthesizing novel depth\nviews and reconstructing scenes, significantly outperforming existing methods.\nProject page: https://github.com/Jiawei-Yao0812/HarmonicNeRF\n","authors":["Xiaochao Pan","Jiawei Yao","Hongrui Kou","Tong Wu","Canran Xiao"],"pdf_url":"https://arxiv.org/pdf/2310.05483v5.pdf","comment":"Accepted by ACM MM 2024, project page:\n  https://github.com/Jiawei-Yao0812/HarmonicNeRF"},{"id":"http://arxiv.org/abs/2407.11652v2","updated":"2024-07-25T07:04:32Z","published":"2024-07-16T12:18:20Z","title":"CCVA-FL: Cross-Client Variations Adaptive Federated Learning for Medical\n  Imaging","summary":"  Federated Learning (FL) offers a privacy-preserving approach to train models\non decentralized data. Its potential in healthcare is significant, but\nchallenges arise due to cross-client variations in medical image data,\nexacerbated by limited annotations. This paper introduces Cross-Client\nVariations Adaptive Federated Learning (CCVA-FL) to address these issues.\nCCVA-FL aims to minimize cross-client variations by transforming images into a\ncommon feature space. It involves expert annotation of a subset of images from\neach client, followed by the selection of a client with the least data\ncomplexity as the target. Synthetic medical images are then generated using\nScalable Diffusion Models with Transformers (DiT) based on the target client's\nannotated images. These synthetic images, capturing diversity and representing\nthe original data, are shared with other clients. Each client then translates\nits local images into the target image space using image-to-image translation.\nThe translated images are subsequently used in a federated learning setting to\ndevelop a server model. Our results demonstrate that CCVA-FL outperforms\nVanilla Federated Averaging by effectively addressing data distribution\ndifferences across clients without compromising privacy.\n","authors":["Sunny Gupta","Amit Sethi"],"pdf_url":"https://arxiv.org/pdf/2407.11652v2.pdf","comment":"10 pages, 6 figures"},{"id":"http://arxiv.org/abs/2407.17813v1","updated":"2024-07-25T06:59:15Z","published":"2024-07-25T06:59:15Z","title":"Enhancing Model Performance: Another Approach to Vision-Language\n  Instruction Tuning","summary":"  The integration of large language models (LLMs) with vision-language (VL)\ntasks has been a transformative development in the realm of artificial\nintelligence, highlighting the potential of LLMs as a versatile general-purpose\nchatbot. However, the current trend in this evolution focuses on the\nintegration of vision and language to create models that can operate in more\ndiverse and real-world contexts. We present a novel approach, termed Bottleneck\nAdapter, specifically crafted for enhancing the multimodal functionalities of\nthese complex models, enabling joint optimization of the entire multimodal LLM\nframework through a process known as Multimodal Model Tuning (MMT). Our\napproach utilizes lightweight adapters to connect the image encoder and LLM\nwithout the need for large, complex neural networks. Unlike the conventional\nmodular training schemes, our approach adopts an end-to-end optimization\nregime, which, when combined with the adapters, facilitates the joint\noptimization using a significantly smaller parameter set. Our method exhibits\nrobust performance with 90.12\\% accuracy, outperforming both human-level\nperformance (88.4\\%) and LaVIN-7B (89.41\\%).\n","authors":[" Vedanshu","MM Tripathi","Bhavnesh Jaint"],"pdf_url":"https://arxiv.org/pdf/2407.17813v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.01926v3","updated":"2024-07-25T06:54:01Z","published":"2024-07-02T03:43:39Z","title":"Chemical Shift Encoding based Double Bonds Quantification in\n  Triglycerides using Deep Image Prior","summary":"  This study evaluated a deep learning-based method using Deep Image Prior\n(DIP) to quantify triglyceride double bonds from chemical-shift encoded\nmulti-echo gradient echo images without network training. We employed a cost\nfunction based on signal constraints to iteratively update the neural network\non a single dataset. The method was validated using phantom experiments and in\nvivo scans. Results showed close alignment between measured and reference\ndouble bond values, with phantom experiments yielding a Pearson correlation\ncoefficient of 0.96 (p = .0005). In vivo results demonstrated good agreement in\nsubcutaneous fat. We conclude that Deep Image Prior shows feasibility for\nquantifying double bonds and fatty acid content from chemical-shift encoded\nmulti-echo MRI.\n","authors":["Chaoxing Huang","Ziqiang Yu","Zijian Gao","Qiuyi Shen","Queenie Chan","Vincent Wai-Sun Wong","Winnie Chiu-Wing Chu","Weitian Chen"],"pdf_url":"https://arxiv.org/pdf/2407.01926v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17170v2","updated":"2024-07-25T06:40:39Z","published":"2024-07-24T11:22:02Z","title":"Domain Generalized Recaptured Screen Image Identification Using SWIN\n  Transformer","summary":"  An increasing number of classification approaches have been developed to\naddress the issue of image rebroadcast and recapturing, a standard attack\nstrategy in insurance frauds, face spoofing, and video piracy. However, most of\nthem neglected scale variations and domain generalization scenarios, performing\npoorly in instances involving domain shifts, typically made worse by\ninter-domain and cross-domain scale variances. To overcome these issues, we\npropose a cascaded data augmentation and SWIN transformer domain generalization\nframework (DAST-DG) in the current research work Initially, we examine the\ndisparity in dataset representation. A feature generator is trained to make\nauthentic images from various domains indistinguishable. This process is then\napplied to recaptured images, creating a dual adversarial learning setup.\nExtensive experiments demonstrate that our approach is practical and surpasses\nstate-of-the-art methods across different databases. Our model achieves an\naccuracy of approximately 82\\% with a precision of 95\\% on high-variance\ndatasets.\n","authors":["Preeti Mehta","Aman Sagar","Suchi Kumari"],"pdf_url":"https://arxiv.org/pdf/2407.17170v2.pdf","comment":"11 pages, 10 figures, 9 tables"},{"id":"http://arxiv.org/abs/2407.17797v1","updated":"2024-07-25T06:10:33Z","published":"2024-07-25T06:10:33Z","title":"A Unified Understanding of Adversarial Vulnerability Regarding Unimodal\n  Models and Vision-Language Pre-training Models","summary":"  With Vision-Language Pre-training (VLP) models demonstrating powerful\nmultimodal interaction capabilities, the application scenarios of neural\nnetworks are no longer confined to unimodal domains but have expanded to more\ncomplex multimodal V+L downstream tasks. The security vulnerabilities of\nunimodal models have been extensively examined, whereas those of VLP models\nremain challenging. We note that in CV models, the understanding of images\ncomes from annotated information, while VLP models are designed to learn image\nrepresentations directly from raw text. Motivated by this discrepancy, we\ndeveloped the Feature Guidance Attack (FGA), a novel method that uses text\nrepresentations to direct the perturbation of clean images, resulting in the\ngeneration of adversarial images. FGA is orthogonal to many advanced attack\nstrategies in the unimodal domain, facilitating the direct application of rich\nresearch findings from the unimodal to the multimodal scenario. By\nappropriately introducing text attack into FGA, we construct Feature Guidance\nwith Text Attack (FGA-T). Through the interaction of attacking two modalities,\nFGA-T achieves superior attack effects against VLP models. Moreover,\nincorporating data augmentation and momentum mechanisms significantly improves\nthe black-box transferability of FGA-T. Our method demonstrates stable and\neffective attack capabilities across various datasets, downstream tasks, and\nboth black-box and white-box settings, offering a unified baseline for\nexploring the robustness of VLP models.\n","authors":["Haonan Zheng","Xinyang Deng","Wen Jiang","Wenrui Li"],"pdf_url":"https://arxiv.org/pdf/2407.17797v1.pdf","comment":"14 pages, 9 figures, published in ACMMM2024(oral)"},{"id":"http://arxiv.org/abs/2407.17792v1","updated":"2024-07-25T06:03:02Z","published":"2024-07-25T06:03:02Z","title":"Harnessing Temporal Causality for Advanced Temporal Action Detection","summary":"  As a fundamental task in long-form video understanding, temporal action\ndetection (TAD) aims to capture inherent temporal relations in untrimmed videos\nand identify candidate actions with precise boundaries. Over the years, various\nnetworks, including convolutions, graphs, and transformers, have been explored\nfor effective temporal modeling for TAD. However, these modules typically treat\npast and future information equally, overlooking the crucial fact that changes\nin action boundaries are essentially causal events. Inspired by this insight,\nwe propose leveraging the temporal causality of actions to enhance TAD\nrepresentation by restricting the model's access to only past or future\ncontext. We introduce CausalTAD, which combines causal attention and causal\nMamba to achieve state-of-the-art performance on multiple benchmarks. Notably,\nwith CausalTAD, we ranked 1st in the Action Recognition, Action Detection, and\nAudio-Based Interaction Detection tracks at the EPIC-Kitchens Challenge 2024,\nas well as 1st in the Moment Queries track at the Ego4D Challenge 2024. Our\ncode is available at https://github.com/sming256/OpenTAD/causaltad.\n","authors":["Shuming Liu","Lin Sui","Chen-Lin Zhang","Fangzhou Mu","Chen Zhao","Bernard Ghanem"],"pdf_url":"https://arxiv.org/pdf/2407.17792v1.pdf","comment":"1st in Moment Queries track at the Ego4D Challenge 2024; 1st in\n  Action Recognition, Action Detection, and Audio-Based Interaction Detection\n  tracks at the EPIC-Kitchens Challenge 2024"},{"id":"http://arxiv.org/abs/2310.05989v3","updated":"2024-07-25T06:02:18Z","published":"2023-10-07T21:55:29Z","title":"QE-BEV: Query Evolution for Bird's Eye View Object Detection in Varied\n  Contexts","summary":"  3D object detection plays a pivotal role in autonomous driving and robotics,\ndemanding precise interpretation of Bird's Eye View (BEV) images. The dynamic\nnature of real-world environments necessitates the use of dynamic query\nmechanisms in 3D object detection to adaptively capture and process the complex\nspatio-temporal relationships present in these scenes. However, prior\nimplementations of dynamic queries have often faced difficulties in effectively\nleveraging these relationships, particularly when it comes to integrating\ntemporal information in a computationally efficient manner. Addressing this\nlimitation, we introduce a framework utilizing dynamic query evolution\nstrategy, harnesses K-means clustering and Top-K attention mechanisms for\nrefined spatio-temporal data processing. By dynamically segmenting the BEV\nspace and prioritizing key features through Top-K attention, our model achieves\na real-time, focused analysis of pertinent scene elements. Our extensive\nevaluation on the nuScenes and Waymo dataset showcases a marked improvement in\ndetection accuracy, setting a new benchmark in the domain of query-based BEV\nobject detection. Our dynamic query evolution strategy has the potential to\npush the boundaries of current BEV methods with enhanced adaptability and\ncomputational efficiency. Project page:\nhttps://github.com/Jiawei-Yao0812/QE-BEV\n","authors":["Jiawei Yao","Yingxin Lai","Hongrui Kou","Tong Wu","Ruixi Liu"],"pdf_url":"https://arxiv.org/pdf/2310.05989v3.pdf","comment":"Accepted by ACM MM 2024, project page:\n  https://github.com/Jiawei-Yao0812/QE-BEV"},{"id":"http://arxiv.org/abs/2407.17791v1","updated":"2024-07-25T05:58:58Z","published":"2024-07-25T05:58:58Z","title":"Investigating learning-independent abstract reasoning in artificial\n  neural networks","summary":"  Humans are capable of solving complex abstract reasoning tests. Whether this\nability reflects a learning-independent inference mechanism applicable to any\nnovel unlearned problem or whether it is a manifestation of extensive training\nthroughout life is an open question. Addressing this question in humans is\nchallenging because it is impossible to control their prior training. However,\nassuming a similarity between the cognitive processing of Artificial Neural\nNetworks (ANNs) and humans, the extent to which training is required for ANNs'\nabstract reasoning is informative about this question in humans. Previous\nstudies demonstrated that ANNs can solve abstract reasoning tests. However,\nthis success required extensive training. In this study, we examined the\nlearning-independent abstract reasoning of ANNs. Specifically, we evaluated\ntheir performance without any pretraining, with the ANNs' weights being\nrandomly-initialized, and only change in the process of problem solving. We\nfound that naive ANN models can solve non-trivial visual reasoning tests,\nsimilar to those used to evaluate human learning-independent reasoning. We\nfurther studied the mechanisms that support this ability. Our results suggest\nthe possibility of learning-independent abstract reasoning that does not\nrequire extensive training.\n","authors":["Tomer Barak","Yonatan Loewenstein"],"pdf_url":"https://arxiv.org/pdf/2407.17791v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17786v1","updated":"2024-07-25T05:30:09Z","published":"2024-07-25T05:30:09Z","title":"Topology-Preserving Downsampling of Binary Images","summary":"  We present a novel discrete optimization-based approach to generate\ndownsampled versions of binary images that are guaranteed to have the same\ntopology as the original, measured by the zeroth and first Betti numbers of the\nblack regions, while having good similarity to the original image as measured\nby IoU and Dice scores. To our best knowledge, all existing binary image\ndownsampling methods do not have such topology-preserving guarantees. We also\nimplemented a baseline morphological operation (dilation)-based approach that\nalways generates topologically correct results. However, we found the\nsimilarity scores to be much worse. We demonstrate several applications of our\napproach. First, generating smaller versions of medical image segmentation\nmasks for easier human inspection. Second, improving the efficiency of binary\nimage operations, including persistent homology computation and shortest path\ncomputation, by substituting the original images with smaller ones. In\nparticular, the latter is a novel application that is made feasible only by the\nfull topology-preservation guarantee of our method.\n","authors":["Chia-Chia Chen","Chi-Han Peng"],"pdf_url":"https://arxiv.org/pdf/2407.17786v1.pdf","comment":"Accepted to The 18th European Conference on Computer Vision (ECCV)\n  2024"},{"id":"http://arxiv.org/abs/2406.19407v4","updated":"2024-07-25T05:24:41Z","published":"2024-06-12T06:41:23Z","title":"YOLOv10 to Its Genesis: A Decadal and Comprehensive Review of The You\n  Only Look Once (YOLO) Series","summary":"  This review systematically examines the progression of the You Only Look Once\n(YOLO) object detection algorithms from YOLOv1 to the recently unveiled\nYOLOv10. Employing a reverse chronological analysis, this study examines the\nadvancements introduced by YOLO algorithms, beginning with YOLOv10 and\nprogressing through YOLOv9, YOLOv8, and subsequent versions to explore each\nversion's contributions to enhancing speed, accuracy, and computational\nefficiency in real-time object detection. The study highlights the\ntransformative impact of YOLO across five critical application areas:\nautomotive safety, healthcare, industrial manufacturing, surveillance, and\nagriculture. By detailing the incremental technological advancements in\nsubsequent YOLO versions, this review chronicles the evolution of YOLO, and\ndiscusses the challenges and limitations in each earlier versions. The\nevolution signifies a path towards integrating YOLO with multimodal,\ncontext-aware, and General Artificial Intelligence (AGI) systems for the next\nYOLO decade, promising significant implications for future developments in\nAI-driven applications.\n","authors":["Ranjan Sapkota","Rizwan Qureshi","Marco Flores Calero","Chetan Badjugar","Upesh Nepal","Alwin Poulose","Peter Zeno","Uday Bhanu Prakash Vaddevolu","Sheheryar Khan","Maged Shoman","Hong Yan","Manoj Karkee"],"pdf_url":"https://arxiv.org/pdf/2406.19407v4.pdf","comment":"11 Figures, 7 Tables"},{"id":"http://arxiv.org/abs/2407.17028v2","updated":"2024-07-25T05:23:24Z","published":"2024-07-24T06:15:28Z","title":"Enhancing Environmental Monitoring through Multispectral Imaging: The\n  WasteMS Dataset for Semantic Segmentation of Lakeside Waste","summary":"  Environmental monitoring of lakeside green areas is crucial for environmental\nprotection. Compared to manual inspections, computer vision technologies offer\na more efficient solution when deployed on-site. Multispectral imaging provides\ndiverse information about objects under different spectrums, aiding in the\ndifferentiation between waste and lakeside lawn environments. This study\nintroduces WasteMS, the first multispectral dataset established for the\nsemantic segmentation of lakeside waste. WasteMS includes a diverse range of\nwaste types in lawn environments, captured under various lighting conditions.\nWe implemented a rigorous annotation process to label waste in images.\nRepresentative semantic segmentation frameworks were used to evaluate\nsegmentation accuracy using WasteMS. Challenges encountered when using WasteMS\nfor segmenting waste on lakeside lawns were discussed. The WasteMS dataset is\navailable at https://github.com/zhuqinfeng1999/WasteMS.\n","authors":["Qinfeng Zhu","Ningxin Weng","Lei Fan","Yuanzhi Cai"],"pdf_url":"https://arxiv.org/pdf/2407.17028v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17783v1","updated":"2024-07-25T05:23:20Z","published":"2024-07-25T05:23:20Z","title":"How Lightweight Can A Vision Transformer Be","summary":"  In this paper, we explore a strategy that uses Mixture-of-Experts (MoE) to\nstreamline, rather than augment, vision transformers. Each expert in an MoE\nlayer is a SwiGLU feedforward network, where V and W2 are shared across the\nlayer. No complex attention or convolutional mechanisms are employed.\nDepth-wise scaling is applied to progressively reduce the size of the hidden\nlayer and the number of experts is increased in stages. Grouped query attention\nis used. We studied the proposed approach with and without pre-training on\nsmall datasets and investigated whether transfer learning works at this scale.\nWe found that the architecture is competitive even at a size of 0.67M\nparameters.\n","authors":["Jen Hong Tan"],"pdf_url":"https://arxiv.org/pdf/2407.17783v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17780v1","updated":"2024-07-25T05:21:48Z","published":"2024-07-25T05:21:48Z","title":"HF-Fed: Hierarchical based customized Federated Learning Framework for\n  X-Ray Imaging","summary":"  In clinical applications, X-ray technology is vital for noninvasive\nexaminations like mammography, providing essential anatomical information.\nHowever, the radiation risk associated with X-ray procedures raises concerns.\nX-ray reconstruction is crucial in medical imaging for detailed visual\nrepresentations of internal structures, aiding diagnosis and treatment without\ninvasive procedures. Recent advancements in deep learning (DL) have shown\npromise in X-ray reconstruction, but conventional DL methods often require\ncentralized aggregation of large datasets, leading to domain shifts and privacy\nissues. To address these challenges, we introduce the Hierarchical\nFramework-based Federated Learning method (HF-Fed) for customized X-ray\nimaging. HF-Fed tackles X-ray imaging optimization by decomposing the problem\ninto local data adaptation and holistic X-ray imaging. It employs a\nhospital-specific hierarchical framework and a shared common imaging network\ncalled Network of Networks (NoN) to acquire stable features from diverse data\ndistributions. The hierarchical hypernetwork extracts domain-specific\nhyperparameters, conditioning the NoN for customized X-ray reconstruction.\nExperimental results demonstrate HF-Fed's competitive performance, offering a\npromising solution for enhancing X-ray imaging without data sharing. This study\nsignificantly contributes to the literature on federated learning in\nhealthcare, providing valuable insights for policymakers and healthcare\nproviders. The source code and pre-trained HF-Fed model are available at\n\\url{https://tisharepo.github.io/Webpage/}.\n","authors":["Tajamul Ashraf","Tisha Madame"],"pdf_url":"https://arxiv.org/pdf/2407.17780v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17779v1","updated":"2024-07-25T05:18:18Z","published":"2024-07-25T05:18:18Z","title":"DAC: 2D-3D Retrieval with Noisy Labels via Divide-and-Conquer Alignment\n  and Correction","summary":"  With the recent burst of 2D and 3D data, cross-modal retrieval has attracted\nincreasing attention recently. However, manual labeling by non-experts will\ninevitably introduce corrupted annotations given ambiguous 2D/3D content.\nThough previous works have addressed this issue by designing a naive division\nstrategy with hand-crafted thresholds, their performance generally exhibits\ngreat sensitivity to the threshold value. Besides, they fail to fully utilize\nthe valuable supervisory signals within each divided subset. To tackle this\nproblem, we propose a Divide-and-conquer 2D-3D cross-modal Alignment and\nCorrection framework (DAC), which comprises Multimodal Dynamic Division (MDD)\nand Adaptive Alignment and Correction (AAC). Specifically, the former performs\naccurate sample division by adaptive credibility modeling for each sample based\non the compensation information within multimodal loss distribution. Then in\nAAC, samples in distinct subsets are exploited with different alignment\nstrategies to fully enhance the semantic compactness and meanwhile alleviate\nover-fitting to noisy labels, where a self-correction strategy is introduced to\nimprove the quality of representation. Moreover. To evaluate the effectiveness\nin real-world scenarios, we introduce a challenging noisy benchmark, namely\nObjaverse-N200, which comprises 200k-level samples annotated with 1156\nrealistic noisy labels. Extensive experiments on both traditional and the newly\nproposed benchmarks demonstrate the generality and superiority of our DAC,\nwhere DAC outperforms state-of-the-art models by a large margin. (i.e., with\n+5.9% gain on ModelNet40 and +5.8% on Objaverse-N200).\n","authors":["Chaofan Gan","Yuanpeng Tu","Yuxi Li","Weiyao Lin"],"pdf_url":"https://arxiv.org/pdf/2407.17779v1.pdf","comment":"accepted by ACM MM 2024"},{"id":"http://arxiv.org/abs/2407.17773v1","updated":"2024-07-25T05:02:39Z","published":"2024-07-25T05:02:39Z","title":"KiVA: Kid-inspired Visual Analogies for Testing Large Multimodal Models","summary":"  This paper investigates visual analogical reasoning in large multimodal\nmodels (LMMs) compared to human adults and children. A \"visual analogy\" is an\nabstract rule inferred from one image and applied to another. While benchmarks\nexist for testing visual reasoning in LMMs, they require advanced skills and\nomit basic visual analogies that even young children can make. Inspired by\ndevelopmental psychology, we propose a new benchmark of 1,400 visual\ntransformations of everyday objects to test LMMs on visual analogical reasoning\nand compare them to children and adults. We structure the evaluation into three\nstages: identifying what changed (e.g., color, number, etc.), how it changed\n(e.g., added one object), and applying the rule to new scenarios. Our findings\nshow that while models like GPT-4V, LLaVA-1.5, and MANTIS identify the \"what\"\neffectively, they struggle with quantifying the \"how\" and extrapolating this\nrule to new objects. In contrast, children and adults exhibit much stronger\nanalogical reasoning at all three stages. Additionally, the strongest tested\nmodel, GPT-4V, performs better in tasks involving simple visual attributes like\ncolor and size, correlating with quicker human adult response times.\nConversely, more complex tasks such as number, rotation, and reflection, which\nnecessitate extensive cognitive processing and understanding of the 3D physical\nworld, present more significant challenges. Altogether, these findings\nhighlight the limitations of training models on data that primarily consists of\n2D images and text.\n","authors":["Eunice Yiu","Maan Qraitem","Charlie Wong","Anisa Noor Majhi","Yutong Bai","Shiry Ginosar","Alison Gopnik","Kate Saenko"],"pdf_url":"https://arxiv.org/pdf/2407.17773v1.pdf","comment":"9 pages. For the KiVA benchmark, see https://github.com/ey242/KiVA"},{"id":"http://arxiv.org/abs/2407.17772v1","updated":"2024-07-25T05:02:27Z","published":"2024-07-25T05:02:27Z","title":"ERIT Lightweight Multimodal Dataset for Elderly Emotion Recognition and\n  Multimodal Fusion Evaluation","summary":"  ERIT is a novel multimodal dataset designed to facilitate research in a\nlightweight multimodal fusion. It contains text and image data collected from\nvideos of elderly individuals reacting to various situations, as well as seven\nemotion labels for each data sample. Because of the use of labeled images of\nelderly users reacting emotionally, it is also facilitating research on emotion\nrecognition in an underrepresented age group in machine learning visual emotion\nrecognition. The dataset is validated through comprehensive experiments\nindicating its importance in neural multimodal fusion research.\n","authors":["Rita Frieske","Bertrand E. Shi"],"pdf_url":"https://arxiv.org/pdf/2407.17772v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.02307v2","updated":"2024-07-25T04:40:04Z","published":"2024-03-04T18:44:30Z","title":"Harnessing Intra-group Variations Via a Population-Level Context for\n  Pathology Detection","summary":"  Realizing sufficient separability between the distributions of healthy and\npathological samples is a critical obstacle for pathology detection\nconvolutional models. Moreover, these models exhibit a bias for contrast-based\nimages, with diminished performance on texture-based medical images. This study\nintroduces the notion of a population-level context for pathology detection and\nemploys a graph theoretic approach to model and incorporate it into the latent\ncode of an autoencoder via a refinement module we term PopuSense. PopuSense\nseeks to capture additional intra-group variations inherent in biomedical data\nthat a local or global context of the convolutional model might miss or smooth\nout. Proof-of-concept experiments on contrast-based and texture-based images,\nwith minimal adaptation, encounter the existing preference for intensity-based\ninput. Nevertheless, PopuSense demonstrates improved separability in\ncontrast-based images, presenting an additional avenue for refining\nrepresentations learned by a model.\n","authors":["P. Bilha Githinji","Xi Yuan","Zhenglin Chen","Ijaz Gul","Dingqi Shang","Wen Liang","Jianming Deng","Dan Zeng","Dongmei yu","Chenggang Yan","Peiwu Qin"],"pdf_url":"https://arxiv.org/pdf/2403.02307v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17762v1","updated":"2024-07-25T04:33:19Z","published":"2024-07-25T04:33:19Z","title":"Mpox Detection Advanced: Rapid Epidemic Response Through Synthetic Data","summary":"  Rapid development of disease detection models using computer vision is\ncrucial in responding to medical emergencies, such as epidemics or bioterrorism\nevents. Traditional data collection methods are often too slow in these\nscenarios, requiring innovative approaches for quick, reliable model generation\nfrom minimal data. Our study introduces a novel approach by constructing a\ncomprehensive computer vision model to detect Mpox lesions using only synthetic\ndata. Initially, these models generated a diverse set of synthetic images\nrepresenting Mpox lesions on various body parts (face, back, chest, leg, neck,\narm) across different skin tones as defined by the Fitzpatrick scale (fair,\nbrown, dark skin). Subsequently, we trained and tested a vision model with this\nsynthetic dataset to evaluate the diffusion models' efficacy in producing\nhigh-quality training data and its impact on the vision model's medical image\nrecognition performance. The results were promising; the vision model achieved\na 97% accuracy rate, with 96% precision and recall for Mpox cases, and\nsimilarly high metrics for normal and other skin disorder cases, demonstrating\nits ability to correctly identify true positives and minimize false positives.\nThe model achieved an F1-Score of 96% for Mpox cases and 98% for normal and\nother skin disorders, reflecting a balanced precision-recall relationship, thus\nensuring reliability and robustness in its predictions. Our proposed\nSynthVision methodology indicates the potential to develop accurate computer\nvision models with minimal data input for future medical emergencies.\n","authors":["Yudara Kularathne","Prathapa Janitha","Sithira Ambepitiya","Prarththanan Sothyrajah","Thanveer Ahamed","Dinuka Wijesundara"],"pdf_url":"https://arxiv.org/pdf/2407.17762v1.pdf","comment":"8 pages, 4 figures, 1 table"},{"id":"http://arxiv.org/abs/2407.06581v4","updated":"2024-07-25T04:19:58Z","published":"2024-07-09T06:20:17Z","title":"Vision language models are blind","summary":"  While large language models with vision capabilities (VLMs), e.g., GPT-4o and\nGemini 1.5 Pro, are powering various image-text applications and scoring high\non many vision-understanding benchmarks, we find that they are surprisingly\nstill struggling with low-level vision tasks that are easy to humans.\nSpecifically, on BlindTest, our suite of 7 very simple tasks such as\nidentifying (a) whether two circles overlap; (b) whether two lines intersect;\n(c) which letter is being circled in a word; and (d) counting circles in an\nOlympic-like logo, four state-of-the-art VLMs are only 58.57% accurate on\naverage. Claude 3.5 Sonnet performs the best at 74.01% accuracy, but this is\nstill far from the human expected accuracy of 100%. Across different image\nresolutions and line widths, VLMs consistently struggle with tasks that require\nprecise spatial information and recognizing geometric primitives that overlap\nor are close together. Code and data are available at:\nhttps://vlmsareblind.github.io\n","authors":["Pooyan Rahmanzadehgervi","Logan Bolton","Mohammad Reza Taesiri","Anh Totti Nguyen"],"pdf_url":"https://arxiv.org/pdf/2407.06581v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17757v1","updated":"2024-07-25T04:12:49Z","published":"2024-07-25T04:12:49Z","title":"CRASH: Crash Recognition and Anticipation System Harnessing with\n  Context-Aware and Temporal Focus Attentions","summary":"  Accurately and promptly predicting accidents among surrounding traffic agents\nfrom camera footage is crucial for the safety of autonomous vehicles (AVs).\nThis task presents substantial challenges stemming from the unpredictable\nnature of traffic accidents, their long-tail distribution, the intricacies of\ntraffic scene dynamics, and the inherently constrained field of vision of\nonboard cameras. To address these challenges, this study introduces a novel\naccident anticipation framework for AVs, termed CRASH. It seamlessly integrates\nfive components: object detector, feature extractor, object-aware module,\ncontext-aware module, and multi-layer fusion. Specifically, we develop the\nobject-aware module to prioritize high-risk objects in complex and ambiguous\nenvironments by calculating the spatial-temporal relationships between traffic\nagents. In parallel, the context-aware is also devised to extend global visual\ninformation from the temporal to the frequency domain using the Fast Fourier\nTransform (FFT) and capture fine-grained visual features of potential objects\nand broader context cues within traffic scenes. To capture a wider range of\nvisual cues, we further propose a multi-layer fusion that dynamically computes\nthe temporal dependencies between different scenes and iteratively updates the\ncorrelations between different visual features for accurate and timely accident\nprediction. Evaluated on real-world datasets--Dashcam Accident Dataset (DAD),\nCar Crash Dataset (CCD), and AnAn Accident Detection (A3D) datasets--our model\nsurpasses existing top baselines in critical evaluation metrics like Average\nPrecision (AP) and mean Time-To-Accident (mTTA). Importantly, its robustness\nand adaptability are particularly evident in challenging driving scenarios with\nmissing or limited training data, demonstrating significant potential for\napplication in real-world autonomous driving systems.\n","authors":["Haicheng Liao","Haoyu Sun","Huanming Shen","Chengyue Wang","Kahou Tam","Chunlin Tian","Li Li","Chengzhong Xu","Zhenning Li"],"pdf_url":"https://arxiv.org/pdf/2407.17757v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17755v1","updated":"2024-07-25T04:09:17Z","published":"2024-07-25T04:09:17Z","title":"Enhancing Eye Disease Diagnosis with Deep Learning and Synthetic Data\n  Augmentation","summary":"  In recent years, the focus is on improving the diagnosis of diabetic\nretinopathy (DR) using machine learning and deep learning technologies.\nResearchers have explored various approaches, including the use of\nhigh-definition medical imaging, AI-driven algorithms such as convolutional\nneural networks (CNNs) and generative adversarial networks (GANs). Among all\nthe available tools, CNNs have emerged as a preferred tool due to their\nsuperior classification accuracy and efficiency. Although the accuracy of CNNs\nis comparatively better but it can be improved by introducing some hybrid\nmodels by combining various machine learning and deep learning models.\nTherefore, in this paper, an ensemble learning technique is proposed for early\ndetection and management of DR with higher accuracy. The proposed model is\ntested on the APTOS dataset and it is showing supremacy on the validation\naccuracy ($99\\%)$ in comparison to the previous models. Hence, the model can be\nhelpful for early detection and treatment of the DR, thereby enhancing the\noverall quality of care for affected individuals.\n","authors":["Saideep Kilaru","Kothamasu Jayachandra","Tanishka Yagneshwar","Suchi Kumari"],"pdf_url":"https://arxiv.org/pdf/2407.17755v1.pdf","comment":"18 pages, 7 figures, 2 Tables"},{"id":"http://arxiv.org/abs/2407.15706v3","updated":"2024-07-25T03:51:18Z","published":"2024-07-22T15:16:47Z","title":"Multi-Modality Co-Learning for Efficient Skeleton-based Action\n  Recognition","summary":"  Skeleton-based action recognition has garnered significant attention due to\nthe utilization of concise and resilient skeletons. Nevertheless, the absence\nof detailed body information in skeletons restricts performance, while other\nmultimodal methods require substantial inference resources and are inefficient\nwhen using multimodal data during both training and inference stages. To\naddress this and fully harness the complementary multimodal features, we\npropose a novel multi-modality co-learning (MMCL) framework by leveraging the\nmultimodal large language models (LLMs) as auxiliary networks for efficient\nskeleton-based action recognition, which engages in multi-modality co-learning\nduring the training stage and keeps efficiency by employing only concise\nskeletons in inference. Our MMCL framework primarily consists of two modules.\nFirst, the Feature Alignment Module (FAM) extracts rich RGB features from video\nframes and aligns them with global skeleton features via contrastive learning.\nSecond, the Feature Refinement Module (FRM) uses RGB images with temporal\ninformation and text instruction to generate instructive features based on the\npowerful generalization of multimodal LLMs. These instructive text features\nwill further refine the classification scores and the refined scores will\nenhance the model's robustness and generalization in a manner similar to soft\nlabels. Extensive experiments on NTU RGB+D, NTU RGB+D 120 and Northwestern-UCLA\nbenchmarks consistently verify the effectiveness of our MMCL, which outperforms\nthe existing skeleton-based action recognition methods. Meanwhile, experiments\non UTD-MHAD and SYSU-Action datasets demonstrate the commendable generalization\nof our MMCL in zero-shot and domain-adaptive action recognition. Our code is\npublicly available at: https://github.com/liujf69/MMCL-Action.\n","authors":["Jinfu Liu","Chen Chen","Mengyuan Liu"],"pdf_url":"https://arxiv.org/pdf/2407.15706v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17744v1","updated":"2024-07-25T03:35:24Z","published":"2024-07-25T03:35:24Z","title":"Balancing Complementarity and Consistency via Delayed Activation in\n  Incomplete Multi-view Clustering","summary":"  This paper study one challenging issue in incomplete multi-view clustering,\nwhere valuable complementary information from other views is always ignored. To\nbe specific, we propose a framework that effectively balances Complementarity\nand Consistency information in Incomplete Multi-view Clustering (CoCo-IMC).\nSpecifically, we design a dual network of delayed activation, which achieves a\nbalance of complementarity and consistency among different views. The delayed\nactivation could enriches the complementarity information that was ignored\nduring consistency learning. Then, we recover the incomplete information and\nenhance the consistency learning by minimizing the conditional entropy and\nmaximizing the mutual information across different views. This could be the\nfirst theoretical attempt to incorporate delayed activation into incomplete\ndata recovery and the balance of complementarity and consistency. We have\nproved the effectiveness of CoCo-IMC in extensive comparative experiments with\n12 state-of-the-art baselines on four publicly available datasets.\n","authors":["Bo Li"],"pdf_url":"https://arxiv.org/pdf/2407.17744v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17738v1","updated":"2024-07-25T03:26:41Z","published":"2024-07-25T03:26:41Z","title":"Enhancing Fine-grained Object Detection in Aerial Images via Orthogonal\n  Mapping","summary":"  Fine-Grained Object Detection (FGOD) is a critical task in high-resolution\naerial image analysis. This letter introduces Orthogonal Mapping (OM), a simple\nyet effective method aimed at addressing the challenge of semantic confusion\ninherent in FGOD. OM introduces orthogonal constraints in the feature space by\ndecoupling features from the last layer of the classification branch with a\nclass-wise orthogonal vector basis. This effectively mitigates semantic\nconfusion and enhances classification accuracy. Moreover, OM can be seamlessly\nintegrated into mainstream object detectors. Extensive experiments conducted on\nthree FGOD datasets (FAIR1M, ShipRSImageNet, and MAR20) demonstrate the\neffectiveness and superiority of the proposed approach. Notably, with just one\nline of code, OM achieves a 4.08% improvement in mean Average Precision (mAP)\nover FCOS on the ShipRSImageNet dataset. Codes are released at\nhttps://github.com/ZhuHaoranEIS/Orthogonal-FGOD.\n","authors":["Haoran Zhu","Yifan Zhou","Chang Xu","Ruixiang Zhang","Wen Yang"],"pdf_url":"https://arxiv.org/pdf/2407.17738v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17734v1","updated":"2024-07-25T03:12:57Z","published":"2024-07-25T03:12:57Z","title":"Cost-effective Instruction Learning for Pathology Vision and Language\n  Analysis","summary":"  The advent of vision-language models fosters the interactive conversations\nbetween AI-enabled models and humans. Yet applying these models into clinics\nmust deal with daunting challenges around large-scale training data, financial,\nand computational resources. Here we propose a cost-effective instruction\nlearning framework for conversational pathology named as CLOVER. CLOVER only\ntrains a lightweight module and uses instruction tuning while freezing the\nparameters of the large language model. Instead of using costly GPT-4, we\npropose well-designed prompts on GPT-3.5 for building generation-based\ninstructions, emphasizing the utility of pathological knowledge derived from\nthe Internet source. To augment the use of instructions, we construct a\nhigh-quality set of template-based instructions in the context of digital\npathology. From two benchmark datasets, our findings reveal the strength of\nhybrid-form instructions in the visual question-answer in pathology. Extensive\nresults show the cost-effectiveness of CLOVER in answering both open-ended and\nclosed-ended questions, where CLOVER outperforms strong baselines that possess\n37 times more training parameters and use instruction data generated from\nGPT-4. Through the instruction tuning, CLOVER exhibits robustness of few-shot\nlearning in the external clinical dataset. These findings demonstrate that\ncost-effective modeling of CLOVER could accelerate the adoption of rapid\nconversational applications in the landscape of digital pathology.\n","authors":["Kaitao Chen","Mianxin Liu","Fang Yan","Lei Ma","Xiaoming Shi","Lilong Wang","Xiaosong Wang","Lifeng Zhu","Zhe Wang","Mu Zhou","Shaoting Zhang"],"pdf_url":"https://arxiv.org/pdf/2407.17734v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17726v1","updated":"2024-07-25T02:55:39Z","published":"2024-07-25T02:55:39Z","title":"Multi-modal Data Binding for Survival Analysis Modeling with Incomplete\n  Data and Annotations","summary":"  Survival analysis stands as a pivotal process in cancer treatment research,\ncrucial for predicting patient survival rates accurately. Recent advancements\nin data collection techniques have paved the way for enhancing survival\npredictions by integrating information from multiple modalities. However,\nreal-world scenarios often present challenges with incomplete data,\nparticularly when dealing with censored survival labels. Prior works have\naddressed missing modalities but have overlooked incomplete labels, which can\nintroduce bias and limit model efficacy. To bridge this gap, we introduce a\nnovel framework that simultaneously handles incomplete data across modalities\nand censored survival labels. Our approach employs advanced foundation models\nto encode individual modalities and align them into a universal representation\nspace for seamless fusion. By generating pseudo labels and incorporating\nuncertainty, we significantly enhance predictive accuracy. The proposed method\ndemonstrates outstanding prediction accuracy in two survival analysis tasks on\nboth employed datasets. This innovative approach overcomes limitations\nassociated with disparate modalities and improves the feasibility of\ncomprehensive survival analysis using multiple large foundation models.\n","authors":["Linhao Qu","Dan Huang","Shaoting Zhang","Xiaosong Wang"],"pdf_url":"https://arxiv.org/pdf/2407.17726v1.pdf","comment":"Accepted by MICCAI 2024"},{"id":"http://arxiv.org/abs/2407.01599v2","updated":"2024-07-25T02:25:11Z","published":"2024-06-26T02:20:23Z","title":"JailbreakZoo: Survey, Landscapes, and Horizons in Jailbreaking Large\n  Language and Vision-Language Models","summary":"  The rapid evolution of artificial intelligence (AI) through developments in\nLarge Language Models (LLMs) and Vision-Language Models (VLMs) has brought\nsignificant advancements across various technological domains. While these\nmodels enhance capabilities in natural language processing and visual\ninteractive tasks, their growing adoption raises critical concerns regarding\nsecurity and ethical alignment. This survey provides an extensive review of the\nemerging field of jailbreaking--deliberately circumventing the ethical and\noperational boundaries of LLMs and VLMs--and the consequent development of\ndefense mechanisms. Our study categorizes jailbreaks into seven distinct types\nand elaborates on defense strategies that address these vulnerabilities.\nThrough this comprehensive examination, we identify research gaps and propose\ndirections for future studies to enhance the security frameworks of LLMs and\nVLMs. Our findings underscore the necessity for a unified perspective that\nintegrates both jailbreak strategies and defensive solutions to foster a\nrobust, secure, and reliable environment for the next generation of language\nmodels. More details can be found on our website:\n\\url{https://chonghan-chen.com/llm-jailbreak-zoo-survey/}.\n","authors":["Haibo Jin","Leyang Hu","Xinuo Li","Peiyan Zhang","Chonghan Chen","Jun Zhuang","Haohan Wang"],"pdf_url":"https://arxiv.org/pdf/2407.01599v2.pdf","comment":"45 pages"},{"id":"http://arxiv.org/abs/2403.17839v2","updated":"2024-07-25T02:08:30Z","published":"2024-03-26T16:27:37Z","title":"ReMamber: Referring Image Segmentation with Mamba Twister","summary":"  Referring Image Segmentation~(RIS) leveraging transformers has achieved great\nsuccess on the interpretation of complex visual-language tasks. However, the\nquadratic computation cost makes it resource-consuming in capturing long-range\nvisual-language dependencies. Fortunately, Mamba addresses this with efficient\nlinear complexity in processing. However, directly applying Mamba to\nmulti-modal interactions presents challenges, primarily due to inadequate\nchannel interactions for the effective fusion of multi-modal data. In this\npaper, we propose ReMamber, a novel RIS architecture that integrates the power\nof Mamba with a multi-modal Mamba Twister block. The Mamba Twister explicitly\nmodels image-text interaction, and fuses textual and visual features through\nits unique channel and spatial twisting mechanism. We achieve competitive\nresults on three challenging benchmarks with a simple and efficient\narchitecture. Moreover, we conduct thorough analyses of ReMamber and discuss\nother fusion designs using Mamba. These provide valuable perspectives for\nfuture research. The code has been released at:\nhttps://github.com/yyh-rain-song/ReMamber.\n","authors":["Yuhuan Yang","Chaofan Ma","Jiangchao Yao","Zhun Zhong","Ya Zhang","Yanfeng Wang"],"pdf_url":"https://arxiv.org/pdf/2403.17839v2.pdf","comment":"ECCV 2024"},{"id":"http://arxiv.org/abs/2407.05623v3","updated":"2024-07-25T01:59:16Z","published":"2024-07-08T05:31:51Z","title":"Momentum Auxiliary Network for Supervised Local Learning","summary":"  Deep neural networks conventionally employ end-to-end backpropagation for\ntheir training process, which lacks biological credibility and triggers a\nlocking dilemma during network parameter updates, leading to significant GPU\nmemory use. Supervised local learning, which segments the network into multiple\nlocal blocks updated by independent auxiliary networks. However, these methods\ncannot replace end-to-end training due to lower accuracy, as gradients only\npropagate within their local block, creating a lack of information exchange\nbetween blocks. To address this issue and establish information transfer across\nblocks, we propose a Momentum Auxiliary Network (MAN) that establishes a\ndynamic interaction mechanism. The MAN leverages an exponential moving average\n(EMA) of the parameters from adjacent local blocks to enhance information flow.\nThis auxiliary network, updated through EMA, helps bridge the informational gap\nbetween blocks. Nevertheless, we observe that directly applying EMA parameters\nhas certain limitations due to feature discrepancies among local blocks. To\novercome this, we introduce learnable biases, further boosting performance. We\nhave validated our method on four image classification datasets (CIFAR-10,\nSTL-10, SVHN, ImageNet), attaining superior performance and substantial memory\nsavings. Notably, our method can reduce GPU memory usage by more than 45\\% on\nthe ImageNet dataset compared to end-to-end training, while achieving higher\nperformance. The Momentum Auxiliary Network thus offers a new perspective for\nsupervised local learning. Our code is available at:\nhttps://github.com/JunhaoSu0/MAN.\n","authors":["Junhao Su","Changpeng Cai","Feiyu Zhu","Chenghao He","Xiaojie Xu","Dongzhi Guan","Chenyang Si"],"pdf_url":"https://arxiv.org/pdf/2407.05623v3.pdf","comment":"Accepted by ECCV2024"},{"id":"http://arxiv.org/abs/2407.17705v1","updated":"2024-07-25T01:58:10Z","published":"2024-07-25T01:58:10Z","title":"ALMRR: Anomaly Localization Mamba on Industrial Textured Surface with\n  Feature Reconstruction and Refinement","summary":"  Unsupervised anomaly localization on industrial textured images has achieved\nremarkable results through reconstruction-based methods, yet existing\napproaches based on image reconstruction and feature reconstruc-tion each have\ntheir own shortcomings. Firstly, image-based methods tend to reconstruct both\nnormal and anomalous regions well, which lead to over-generalization.\nFeature-based methods contain a large amount of distin-guishable semantic\ninformation, however, its feature structure is redundant and lacks anomalous\ninformation, which leads to significant reconstruction errors. In this paper,\nwe propose an Anomaly Localization method based on Mamba with Feature\nReconstruction and Refinement(ALMRR) which re-constructs semantic features\nbased on Mamba and then refines them through a feature refinement module. To\nequip the model with prior knowledge of anomalies, we enhance it by adding\nartificially simulated anomalies to the original images. Unlike image\nreconstruction or repair, the features of synthesized defects are repaired\nalong with those of normal areas. Finally, the aligned features containing rich\nsemantic information are fed in-to the refinement module to obtain the anomaly\nmap. Extensive experiments have been conducted on the MVTec-AD-Textured dataset\nand other real-world industrial dataset, which has demonstrated superior\nperformance com-pared to state-of-the-art (SOTA) methods.\n","authors":["Shichen Qu","Xian Tao","Zhen Qu","Xinyi Gong","Zhengtao Zhang","Mukesh Prasad"],"pdf_url":"https://arxiv.org/pdf/2407.17705v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19919v4","updated":"2024-07-25T01:50:46Z","published":"2024-03-29T02:10:38Z","title":"Diff-Reg v1: Diffusion Matching Model for Registration Problem","summary":"  Establishing reliable correspondences is essential for registration tasks\nsuch as 3D and 2D3D registration. Existing methods commonly leverage geometric\nor semantic point features to generate potential correspondences. However,\nthese features may face challenges such as large deformation, scale\ninconsistency, and ambiguous matching problems (e.g., symmetry). Additionally,\nmany previous methods, which rely on single-pass prediction, may struggle with\nlocal minima in complex scenarios. To mitigate these challenges, we introduce a\ndiffusion matching model for robust correspondence construction. Our approach\ntreats correspondence estimation as a denoising diffusion process within the\ndoubly stochastic matrix space, which gradually denoises (refines) a doubly\nstochastic matching matrix to the ground-truth one for high-quality\ncorrespondence estimation. It involves a forward diffusion process that\ngradually introduces Gaussian noise into the ground truth matching matrix and a\nreverse denoising process that iteratively refines the noisy matching matrix.\nIn particular, the feature extraction from the backbone occurs only once during\nthe inference phase. Our lightweight denoising module utilizes the same feature\nat each reverse sampling step. Evaluation of our method on both 3D and 2D3D\nregistration tasks confirms its effectiveness. The code is available at\nhttps://github.com/wuqianliang/Diff-Reg.\n","authors":["Qianliang Wu","Haobo Jiang","Lei Luo","Jun Li","Yaqing Ding","Jin Xie","Jian Yang"],"pdf_url":"https://arxiv.org/pdf/2403.19919v4.pdf","comment":"arXiv admin note: text overlap with arXiv:2401.00436"},{"id":"http://arxiv.org/abs/2402.17228v4","updated":"2024-07-25T01:20:23Z","published":"2024-02-27T05:42:38Z","title":"Feature Re-Embedding: Towards Foundation Model-Level Performance in\n  Computational Pathology","summary":"  Multiple instance learning (MIL) is the most widely used framework in\ncomputational pathology, encompassing sub-typing, diagnosis, prognosis, and\nmore. However, the existing MIL paradigm typically requires an offline instance\nfeature extractor, such as a pre-trained ResNet or a foundation model. This\napproach lacks the capability for feature fine-tuning within the specific\ndownstream tasks, limiting its adaptability and performance. To address this\nissue, we propose a Re-embedded Regional Transformer (R$^2$T) for re-embedding\nthe instance features online, which captures fine-grained local features and\nestablishes connections across different regions. Unlike existing works that\nfocus on pre-training powerful feature extractor or designing sophisticated\ninstance aggregator, R$^2$T is tailored to re-embed instance features online.\nIt serves as a portable module that can seamlessly integrate into mainstream\nMIL models. Extensive experimental results on common computational pathology\ntasks validate that: 1) feature re-embedding improves the performance of MIL\nmodels based on ResNet-50 features to the level of foundation model features,\nand further enhances the performance of foundation model features; 2) the\nR$^2$T can introduce more significant performance improvements to various MIL\nmodels; 3) R$^2$T-MIL, as an R$^2$T-enhanced AB-MIL, outperforms other latest\nmethods by a large margin.The code is available at:\nhttps://github.com/DearCaat/RRT-MIL.\n","authors":["Wenhao Tang","Fengtao Zhou","Sheng Huang","Xiang Zhu","Yi Zhang","Bo Liu"],"pdf_url":"https://arxiv.org/pdf/2402.17228v4.pdf","comment":"Accepted by CVPR2024"},{"id":"http://arxiv.org/abs/2407.17689v1","updated":"2024-07-25T01:12:48Z","published":"2024-07-25T01:12:48Z","title":"SAM-MIL: A Spatial Contextual Aware Multiple Instance Learning Approach\n  for Whole Slide Image Classification","summary":"  Multiple Instance Learning (MIL) represents the predominant framework in\nWhole Slide Image (WSI) classification, covering aspects such as sub-typing,\ndiagnosis, and beyond. Current MIL models predominantly rely on instance-level\nfeatures derived from pretrained models such as ResNet. These models segment\neach WSI into independent patches and extract features from these local\npatches, leading to a significant loss of global spatial context and\nrestricting the model's focus to merely local features. To address this issue,\nwe propose a novel MIL framework, named SAM-MIL, that emphasizes spatial\ncontextual awareness and explicitly incorporates spatial context by extracting\ncomprehensive, image-level information. The Segment Anything Model (SAM)\nrepresents a pioneering visual segmentation foundational model that can capture\nsegmentation features without the need for additional fine-tuning, rendering it\nan outstanding tool for extracting spatial context directly from raw WSIs. Our\napproach includes the design of group feature extraction based on spatial\ncontext and a SAM-Guided Group Masking strategy to mitigate class imbalance\nissues. We implement a dynamic mask ratio for different segmentation categories\nand supplement these with representative group features of categories.\nMoreover, SAM-MIL divides instances to generate additional pseudo-bags, thereby\naugmenting the training set, and introduces consistency of spatial context\nacross pseudo-bags to further enhance the model's performance. Experimental\nresults on the CAMELYON-16 and TCGA Lung Cancer datasets demonstrate that our\nproposed SAM-MIL model outperforms existing mainstream methods in WSIs\nclassification. Our open-source implementation code is is available at\nhttps://github.com/FangHeng/SAM-MIL.\n","authors":["Heng Fang","Sheng Huang","Wenhao Tang","Luwen Huangfu","Bo Liu"],"pdf_url":"https://arxiv.org/pdf/2407.17689v1.pdf","comment":"accepted by ACM Multimedia 2024"},{"id":"http://arxiv.org/abs/2302.00509v2","updated":"2024-07-25T01:09:57Z","published":"2023-02-01T15:28:55Z","title":"Exploring Semantic Perturbations on Grover","summary":"  With news and information being as easy to access as they currently are, it\nis more important than ever to ensure that people are not mislead by what they\nread. Recently, the rise of neural fake news (AI-generated fake news) and its\ndemonstrated effectiveness at fooling humans has prompted the development of\nmodels to detect it. One such model is the Grover model, which can both detect\nneural fake news to prevent it, and generate it to demonstrate how a model\ncould be misused to fool human readers. In this work we explore the Grover\nmodel's fake news detection capabilities by performing targeted attacks through\nperturbations on input news articles. Through this we test Grover's resilience\nto these adversarial attacks and expose some potential vulnerabilities which\nshould be addressed in further iterations to ensure it can detect all types of\nfake news accurately.\n","authors":["Ziqing Ji","Pranav Kulkarni","Marko Neskovic","Kevin Nolan","Yan Xu"],"pdf_url":"https://arxiv.org/pdf/2302.00509v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.12736v3","updated":"2024-07-25T00:00:18Z","published":"2024-07-17T16:56:06Z","title":"CHOSEN: Compilation to Hardware Optimization Stack for Efficient Vision\n  Transformer Inference","summary":"  Vision Transformers (ViTs) represent a groundbreaking shift in machine\nlearning approaches to computer vision. Unlike traditional approaches, ViTs\nemploy the self-attention mechanism, which has been widely used in natural\nlanguage processing, to analyze image patches. Despite their advantages in\nmodeling visual tasks, deploying ViTs on hardware platforms, notably\nField-Programmable Gate Arrays (FPGAs), introduces considerable challenges.\nThese challenges stem primarily from the non-linear calculations and high\ncomputational and memory demands of ViTs. This paper introduces CHOSEN, a\nsoftware-hardware co-design framework to address these challenges and offer an\nautomated framework for ViT deployment on the FPGAs in order to maximize\nperformance. Our framework is built upon three fundamental contributions:\nmulti-kernel design to maximize the bandwidth, mainly targeting benefits of\nmulti DDR memory banks, approximate non-linear functions that exhibit minimal\naccuracy degradation, and efficient use of available logic blocks on the FPGA,\nand efficient compiler to maximize the performance and memory-efficiency of the\ncomputing kernels by presenting a novel algorithm for design space exploration\nto find optimal hardware configuration that achieves optimal throughput and\nlatency. Compared to the state-of-the-art ViT accelerators, CHOSEN achieves a\n1.5x and 1.42x improvement in the throughput on the DeiT-S and DeiT-B models.\n","authors":["Mohammad Erfan Sadeghi","Arash Fayyazi","Suhas Somashekar","Massoud Pedram"],"pdf_url":"https://arxiv.org/pdf/2407.12736v3.pdf","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2407.18058v1","updated":"2024-07-25T14:15:05Z","published":"2024-07-25T14:15:05Z","title":"I can listen but cannot read: An evaluation of two-tower multimodal\n  systems for instrument recognition","summary":"  Music two-tower multimodal systems integrate audio and text modalities into a\njoint audio-text space, enabling direct comparison between songs and their\ncorresponding labels. These systems enable new approaches for classification\nand retrieval, leveraging both modalities. Despite the promising results they\nhave shown for zero-shot classification and retrieval tasks, closer inspection\nof the embeddings is needed. This paper evaluates the inherent zero-shot\nproperties of joint audio-text spaces for the case-study of instrument\nrecognition. We present an evaluation and analysis of two-tower systems for\nzero-shot instrument recognition and a detailed analysis of the properties of\nthe pre-joint and joint embeddings spaces. Our findings suggest that audio\nencoders alone demonstrate good quality, while challenges remain within the\ntext encoder or joint space projection. Specifically, two-tower systems exhibit\nsensitivity towards specific words, favoring generic prompts over musically\ninformed ones. Despite the large size of textual encoders, they do not yet\nleverage additional textual context or infer instruments accurately from their\ndescriptions. Lastly, a novel approach for quantifying the semantic\nmeaningfulness of the textual space leveraging an instrument ontology is\nproposed. This method reveals deficiencies in the systems' understanding of\ninstruments and provides evidence of the need for fine-tuning text encoders on\nmusical data.\n","authors":["Yannis Vasilakis","Rachel Bittner","Johan Pauwels"],"pdf_url":"https://arxiv.org/pdf/2407.18058v1.pdf","comment":"Accepted to ISMIR 2024"},{"id":"http://arxiv.org/abs/2212.06543v2","updated":"2024-07-25T13:27:08Z","published":"2022-12-13T12:56:55Z","title":"Improving Stance Detection by Leveraging Measurement Knowledge from\n  Social Sciences: A Case Study of Dutch Political Tweets and Traditional\n  Gender Role Division","summary":"  Stance detection (SD) concerns automatically determining the viewpoint (i.e.,\nin favour of, against, or neutral) of a text's author towards a target. SD has\nbeen applied to many research topics, among which the detection of stances\nbehind political tweets is an important one. In this paper, we apply SD to a\ndataset of tweets from official party accounts in the Netherlands between 2017\nand 2021, with a focus on stances towards traditional gender role division, a\ndividing issue between (some) Dutch political parties. To implement and improve\nSD of traditional gender role division, we propose to leverage an established\nsurvey instrument from social sciences, which has been validated for the\npurpose of measuring attitudes towards traditional gender role division. Based\non our experiments, we show that using such a validated survey instrument helps\nto improve SD performance.\n","authors":["Qixiang Fang","Anastasia Giachanou","Ayoub Bagheri"],"pdf_url":"https://arxiv.org/pdf/2212.06543v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.12111v2","updated":"2024-07-25T12:52:54Z","published":"2023-12-19T12:33:38Z","title":"General-Purpose User Modeling with Behavioral Logs: A Snapchat Case\n  Study","summary":"  Learning general-purpose user representations based on user behavioral logs\nis an increasingly popular user modeling approach. It benefits from easily\navailable, privacy-friendly yet expressive data, and does not require extensive\nre-tuning of the upstream user model for different downstream tasks. While this\napproach has shown promise in search engines and e-commerce applications, its\nfit for instant messaging platforms, a cornerstone of modern digital\ncommunication, remains largely uncharted. We explore this research gap using\nSnapchat data as a case study. Specifically, we implement a Transformer-based\nuser model with customized training objectives and show that the model can\nproduce high-quality user representations across a broad range of evaluation\ntasks, among which we introduce three new downstream tasks that concern pivotal\ntopics in user research: user safety, engagement and churn. We also tackle the\nchallenge of efficient extrapolation of long sequences at inference time, by\napplying a novel positional encoding method.\n","authors":["Qixiang Fang","Zhihan Zhou","Francesco Barbieri","Yozen Liu","Leonardo Neves","Dong Nguyen","Daniel L. Oberski","Maarten W. Bos","Ron Dotsch"],"pdf_url":"https://arxiv.org/pdf/2312.12111v2.pdf","comment":"SIGIR 2024"},{"id":"http://arxiv.org/abs/2310.06491v2","updated":"2024-07-25T07:17:59Z","published":"2023-10-10T09:59:08Z","title":"Bridging Items and Language: A Transition Paradigm for Large Language\n  Model-Based Recommendation","summary":"  Harnessing Large Language Models (LLMs) for recommendation is rapidly\nemerging, which relies on two fundamental steps to bridge the recommendation\nitem space and the language space: 1) item indexing utilizes identifiers to\nrepresent items in the language space, and 2) generation grounding associates\nLLMs' generated token sequences to in-corpus items. However, previous methods\nexhibit inherent limitations in the two steps. Existing ID-based identifiers\n(e.g., numeric IDs) and description-based identifiers (e.g., titles) either\nlose semantics or lack adequate distinctiveness. Moreover, prior generation\ngrounding methods might generate invalid identifiers, thus misaligning with\nin-corpus items. To address these issues, we propose a novel Transition\nparadigm for LLM-based Recommender (named TransRec) to bridge items and\nlanguage. Specifically, TransRec presents multi-facet identifiers, which\nsimultaneously incorporate ID, title, and attribute for item indexing to pursue\nboth distinctiveness and semantics. Additionally, we introduce a specialized\ndata structure for TransRec to ensure generating valid identifiers only and\nutilize substring indexing to encourage LLMs to generate from any position of\nidentifiers. Lastly, TransRec presents an aggregated grounding module to\nleverage generated multi-facet identifiers to rank in-corpus items efficiently.\nWe instantiate TransRec on two backbone models, BART-large and LLaMA-7B.\nExtensive results on three real-world datasets under diverse settings validate\nthe superiority of TransRec.\n","authors":["Xinyu Lin","Wenjie Wang","Yongqi Li","Fuli Feng","See-Kiong Ng","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2310.06491v2.pdf","comment":"Accepted by KDD 2024"},{"id":"http://arxiv.org/abs/2407.17802v1","updated":"2024-07-25T06:22:08Z","published":"2024-07-25T06:22:08Z","title":"Sample Enrichment via Temporary Operations on Subsequences for\n  Sequential Recommendation","summary":"  Sequential recommendation leverages interaction sequences to predict\nforthcoming user behaviors, crucial for crafting personalized recommendations.\nHowever, the true preferences of a user are inherently complex and\nhigh-dimensional, while the observed data is merely a simplified and\nlow-dimensional projection of the rich preferences, which often leads to\nprevalent issues like data sparsity and inaccurate model training. To learn\ntrue preferences from the sparse data, most existing works endeavor to\nintroduce some extra information or design some ingenious models. Although they\nhave shown to be effective, extra information usually increases the cost of\ndata collection, and complex models may result in difficulty in deployment.\nInnovatively, we avoid the use of extra information or alterations to the\nmodel; instead, we fill the transformation space between the observed data and\nthe underlying preferences with randomness. Specifically, we propose a novel\nmodel-agnostic and highly generic framework for sequential recommendation\ncalled sample enrichment via temporary operations on subsequences (SETO), which\ntemporarily and separately enriches the transformation space via sequence\nenhancement operations with rationality constraints in training. The\ntransformation space not only exists in the process from input samples to\npreferences but also in preferences to target samples. We highlight our SETO's\neffectiveness and versatility over multiple representative and state-of-the-art\nsequential recommendation models (including six single-domain sequential models\nand two cross-domain sequential models) across multiple real-world datasets\n(including three single-domain datasets, three cross-domain datasets and a\nlarge-scale industry dataset).\n","authors":["Shu Chen","Jinwei Luo","Weike Pan","Jiangxing Yu","Xin Huang","Zhong Ming"],"pdf_url":"https://arxiv.org/pdf/2407.17802v1.pdf","comment":"12 pages, 6 figures"},{"id":"http://arxiv.org/abs/2407.17722v1","updated":"2024-07-25T02:48:56Z","published":"2024-07-25T02:48:56Z","title":"Text-Driven Neural Collaborative Filtering Model for Paper Source\n  Tracing","summary":"  Identifying significant references within the complex interrelations of a\ncitation knowledge graph is challenging, which encompasses connections through\ncitations, authorship, keywords, and other relational attributes. The Paper\nSource Tracing (PST) task seeks to automate the identification of pivotal\nreferences for given scholarly articles utilizing advanced data mining\ntechniques. In the KDD CUP 2024, we design a recommendation-based framework\ntailored for the PST task. This framework employs the Neural Collaborative\nFiltering (NCF) model to generate final predictions. To process the textual\nattributes of the papers and extract input features for the model, we utilize\nSciBERT, a pre-trained language model. According to the experimental results,\nour method achieved a score of 0.37814 on the Mean Average Precision (MAP)\nmetric, outperforming baseline models and ranking 11th among all participating\nteams. The source code is publicly available at\nhttps://github.com/MyLove-XAB/KDDCupFinal.\n","authors":["Aobo Xu","Bingyu Chang","Qingpeng Liu","Ling Jian"],"pdf_url":"https://arxiv.org/pdf/2407.17722v1.pdf","comment":"KDD CUP 2024 OAG-Challenges, Paper Source Tracing, Technical Report\n  of Team AoboSama @ KDD CUP 2024. August 25--29, 2024. Barcelona, Spain"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2407.18251v1","updated":"2024-07-25T17:59:48Z","published":"2024-07-25T17:59:48Z","title":"Sparse vs Contiguous Adversarial Pixel Perturbations in Multimodal\n  Models: An Empirical Analysis","summary":"  Assessing the robustness of multimodal models against adversarial examples is\nan important aspect for the safety of its users. We craft L0-norm perturbation\nattacks on the preprocessed input images. We launch them in a black-box setup\nagainst four multimodal models and two unimodal DNNs, considering both targeted\nand untargeted misclassification. Our attacks target less than 0.04% of\nperturbed image area and integrate different spatial positioning of perturbed\npixels: sparse positioning and pixels arranged in different contiguous shapes\n(row, column, diagonal, and patch). To the best of our knowledge, we are the\nfirst to assess the robustness of three state-of-the-art multimodal models\n(ALIGN, AltCLIP, GroupViT) against different sparse and contiguous pixel\ndistribution perturbations. The obtained results indicate that unimodal DNNs\nare more robust than multimodal models. Furthermore, models using CNN-based\nImage Encoder are more vulnerable than models with ViT - for untargeted\nattacks, we obtain a 99% success rate by perturbing less than 0.02% of the\nimage area.\n","authors":["Cristian-Alexandru Botocan","Raphael Meier","Ljiljana Dolamic"],"pdf_url":"https://arxiv.org/pdf/2407.18251v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.18245v1","updated":"2024-07-25T17:58:17Z","published":"2024-07-25T17:58:17Z","title":"VGGHeads: A Large-Scale Synthetic Dataset for 3D Human Heads","summary":"  Human head detection, keypoint estimation, and 3D head model fitting are\nimportant tasks with many applications. However, traditional real-world\ndatasets often suffer from bias, privacy, and ethical concerns, and they have\nbeen recorded in laboratory environments, which makes it difficult for trained\nmodels to generalize. Here, we introduce VGGHeads -- a large scale synthetic\ndataset generated with diffusion models for human head detection and 3D mesh\nestimation. Our dataset comprises over 1 million high-resolution images, each\nannotated with detailed 3D head meshes, facial landmarks, and bounding boxes.\nUsing this dataset we introduce a new model architecture capable of\nsimultaneous heads detection and head meshes reconstruction from a single image\nin a single step. Through extensive experimental evaluations, we demonstrate\nthat models trained on our synthetic data achieve strong performance on real\nimages. Furthermore, the versatility of our dataset makes it applicable across\na broad spectrum of tasks, offering a general and comprehensive representation\nof human heads. Additionally, we provide detailed information about the\nsynthetic data generation pipeline, enabling it to be re-used for other tasks\nand domains.\n","authors":["Orest Kupyn","Eugene Khvedchenia","Christian Rupprecht"],"pdf_url":"https://arxiv.org/pdf/2407.18245v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.18242v1","updated":"2024-07-25T17:57:12Z","published":"2024-07-25T17:57:12Z","title":"LoRA-Pro: Are Low-Rank Adapters Properly Optimized?","summary":"  Low-Rank Adaptation, also known as LoRA, has emerged as a prominent method\nfor parameter-efficient fine-tuning foundation models by re-parameterizing the\noriginal matrix into the product of two low-rank matrices. Despite its\nefficiency, LoRA often yields inferior performance compared to full\nfine-tuning. In this paper, we propose LoRA-Pro to bridge this performance gap.\nFirstly, we delve into the optimization processes in LoRA and full fine-tuning.\nWe reveal that while LoRA employs low-rank approximation, it neglects to\napproximate the optimization process of full fine-tuning. To address this, we\nintroduce a novel concept called the \"equivalent gradient.\" This virtual\ngradient makes the optimization process on the re-parameterized matrix\nequivalent to LoRA, which can be used to quantify the differences between LoRA\nand full fine-tuning. The equivalent gradient is derived from the gradients of\nmatrices $A$ and $B$. To narrow the performance gap, our approach minimizes the\ndifferences between the equivalent gradient and the gradient obtained from full\nfine-tuning during the optimization process. By solving this objective, we\nderive optimal closed-form solutions for updating matrices $A$ and $B$. Our\nmethod constrains the optimization process, shrinking the performance gap\nbetween LoRA and full fine-tuning. Extensive experiments on natural language\nprocessing tasks validate the effectiveness of our method.\n","authors":["Zhengbo Wang","Jian Liang"],"pdf_url":"https://arxiv.org/pdf/2407.18242v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.18241v1","updated":"2024-07-25T17:55:33Z","published":"2024-07-25T17:55:33Z","title":"Numerical Literals in Link Prediction: A Critical Examination of Models\n  and Datasets","summary":"  Link Prediction(LP) is an essential task over Knowledge Graphs(KGs),\ntraditionally focussed on using and predicting the relations between entities.\nTextual entity descriptions have already been shown to be valuable, but models\nthat incorporate numerical literals have shown minor improvements on existing\nbenchmark datasets. It is unclear whether a model is actually better in using\nnumerical literals, or better capable of utilizing the graph structure. This\nraises doubts about the effectiveness of these methods and about the\nsuitability of the existing benchmark datasets.\n  We propose a methodology to evaluate LP models that incorporate numerical\nliterals. We propose i) a new synthetic dataset to better understand how well\nthese models use numerical literals and ii) dataset ablations strategies to\ninvestigate potential difficulties with the existing datasets. We identify a\nprevalent trend: many models underutilize literal information and potentially\nrely on additional parameters for performance gains. Our investigation\nhighlights the need for more extensive evaluations when releasing new models\nand datasets.\n","authors":["Moritz Blum","Basil Ell","Hannes Ill","Philipp Cimiano"],"pdf_url":"https://arxiv.org/pdf/2407.18241v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.03853v4","updated":"2024-07-25T17:54:12Z","published":"2023-12-06T19:07:38Z","title":"Dr. Jekyll and Mr. Hyde: Two Faces of LLMs","summary":"  Recently, we have witnessed a rise in the use of Large Language Models\n(LLMs), especially in applications like chatbot assistants. Safety mechanisms\nand specialized training procedures are implemented to prevent improper\nresponses from these assistants. In this work, we bypass these measures for\nChatGPT and Gemini (and, to some extent, Bing chat) by making them impersonate\ncomplex personas with personality characteristics that are not aligned with a\ntruthful assistant. We start by creating elaborate biographies of these\npersonas, which we then use in a new session with the same chatbots. Our\nconversations then follow a role-play style to elicit prohibited responses.\nUsing personas, we show that prohibited responses are actually provided, making\nit possible to obtain unauthorized, illegal, or harmful information. This work\nshows that by using adversarial personas, one can overcome safety mechanisms\nset out by ChatGPT and Gemini. We also introduce several ways of activating\nsuch adversarial personas, which show that both chatbots are vulnerable to this\nkind of attack. With the same principle, we introduce two defenses that push\nthe model to interpret trustworthy personalities and make it more robust\nagainst such attacks.\n","authors":["Matteo Gioele Collu","Tom Janssen-Groesbeek","Stefanos Koffas","Mauro Conti","Stjepan Picek"],"pdf_url":"https://arxiv.org/pdf/2312.03853v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16445v2","updated":"2024-07-25T17:53:38Z","published":"2024-07-23T12:54:06Z","title":"Can time series forecasting be automated? A benchmark and analysis","summary":"  In the field of machine learning and artificial intelligence, time series\nforecasting plays a pivotal role across various domains such as finance,\nhealthcare, and weather. However, the task of selecting the most suitable\nforecasting method for a given dataset is a complex task due to the diversity\nof data patterns and characteristics. This research aims to address this\nchallenge by proposing a comprehensive benchmark for evaluating and ranking\ntime series forecasting methods across a wide range of datasets. This study\ninvestigates the comparative performance of many methods from two prominent\ntime series forecasting frameworks, AutoGluon-Timeseries, and sktime to shed\nlight on their applicability in different real-world scenarios. This research\ncontributes to the field of time series forecasting by providing a robust\nbenchmarking methodology and facilitating informed decision-making when\nchoosing forecasting methods for achieving optimal prediction.\n","authors":["Anvitha Thirthapura Sreedhara","Joaquin Vanschoren"],"pdf_url":"https://arxiv.org/pdf/2407.16445v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10444v2","updated":"2024-07-25T17:51:50Z","published":"2024-03-15T16:28:22Z","title":"Block Verification Accelerates Speculative Decoding","summary":"  Speculative decoding is an effective method for lossless acceleration of\nlarge language models during inference. It uses a fast model to draft a block\nof tokens which are then verified in parallel by the target model, and provides\na guarantee that the output is distributed identically to a sample from the\ntarget model. In prior works, draft verification is performed independently\ntoken-by-token. Surprisingly, we show that this approach is not optimal. We\npropose Block Verification, a simple draft verification algorithm that verifies\nthe entire block jointly and provides additional wall-clock speedup. We prove\nthat the proposed mechanism is optimal in the expected number of tokens\nproduced each iteration and specifically is never worse than the standard\ntoken-level verification. Empirically, block verification provides modest but\nconsistent wall-clock speedups over the standard token verification algorithm\nof 5%-8% in a range of tasks and datasets. Given that block verification does\nnot increase code complexity, maintains the strong lossless guarantee of the\nstandard speculative decoding verification algorithm, cannot deteriorate\nperformance, and, in fact, consistently improves it, it can be used as a good\ndefault in speculative decoding implementations.\n","authors":["Ziteng Sun","Uri Mendlovic","Yaniv Leviathan","Asaf Aharoni","Ahmad Beirami","Jae Hun Ro","Ananda Theertha Suresh"],"pdf_url":"https://arxiv.org/pdf/2403.10444v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.18227v1","updated":"2024-07-25T17:46:38Z","published":"2024-07-25T17:46:38Z","title":"Automated Ensemble Multimodal Machine Learning for Healthcare","summary":"  The application of machine learning in medicine and healthcare has led to the\ncreation of numerous diagnostic and prognostic models. However, despite their\nsuccess, current approaches generally issue predictions using data from a\nsingle modality. This stands in stark contrast with clinician decision-making\nwhich employs diverse information from multiple sources. While several\nmultimodal machine learning approaches exist, significant challenges in\ndeveloping multimodal systems remain that are hindering clinical adoption. In\nthis paper, we introduce a multimodal framework, AutoPrognosis-M, that enables\nthe integration of structured clinical (tabular) data and medical imaging using\nautomated machine learning. AutoPrognosis-M incorporates 17 imaging models,\nincluding convolutional neural networks and vision transformers, and three\ndistinct multimodal fusion strategies. In an illustrative application using a\nmultimodal skin lesion dataset, we highlight the importance of multimodal\nmachine learning and the power of combining multiple fusion strategies using\nensemble learning. We have open-sourced our framework as a tool for the\ncommunity and hope it will accelerate the uptake of multimodal machine learning\nin healthcare and spur further innovation.\n","authors":["Fergus Imrie","Stefan Denner","Lucas S. Brunschwig","Klaus Maier-Hein","Mihaela van der Schaar"],"pdf_url":"https://arxiv.org/pdf/2407.18227v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.18219v1","updated":"2024-07-25T17:35:59Z","published":"2024-07-25T17:35:59Z","title":"Recursive Introspection: Teaching Language Model Agents How to\n  Self-Improve","summary":"  A central piece in enabling intelligent agentic behavior in foundation models\nis to make them capable of introspecting upon their behavior, reasoning, and\ncorrecting their mistakes as more computation or interaction is available. Even\nthe strongest proprietary large language models (LLMs) do not quite exhibit the\nability of continually improving their responses sequentially, even in\nscenarios where they are explicitly told that they are making a mistake. In\nthis paper, we develop RISE: Recursive IntroSpEction, an approach for\nfine-tuning LLMs to introduce this capability, despite prior work hypothesizing\nthat this capability may not be possible to attain. Our approach prescribes an\niterative fine-tuning procedure, which attempts to teach the model how to alter\nits response after having executed previously unsuccessful attempts to solve a\nhard test-time problem, with optionally additional environment feedback. RISE\nposes fine-tuning for a single-turn prompt as solving a multi-turn Markov\ndecision process (MDP), where the initial state is the prompt. Inspired by\nprinciples in online imitation learning and reinforcement learning, we propose\nstrategies for multi-turn data collection and training so as to imbue an LLM\nwith the capability to recursively detect and correct its previous mistakes in\nsubsequent iterations. Our experiments show that RISE enables Llama2, Llama3,\nand Mistral models to improve themselves with more turns on math reasoning\ntasks, outperforming several single-turn strategies given an equal amount of\ninference-time computation. We also find that RISE scales well, often attaining\nlarger benefits with more capable models. Our analysis shows that RISE makes\nmeaningful improvements to responses to arrive at the correct solution for\nchallenging prompts, without disrupting one-turn abilities as a result of\nexpressing more complex distributions.\n","authors":["Yuxiao Qu","Tianjun Zhang","Naman Garg","Aviral Kumar"],"pdf_url":"https://arxiv.org/pdf/2407.18219v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.18213v1","updated":"2024-07-25T17:26:41Z","published":"2024-07-25T17:26:41Z","title":"Exploring Scaling Trends in LLM Robustness","summary":"  Language model capabilities predictably improve from scaling a model's size\nand training data. Motivated by this, increasingly large language models have\nbeen trained, yielding an array of impressive capabilities. Yet these models\nare vulnerable to adversarial prompts, such as \"jailbreaks\" that hijack models\nto perform undesired behaviors, posing a significant risk of misuse. Prior work\nindicates that computer vision models become more robust with model and data\nscaling, raising the question: does language model robustness also improve with\nscale? We study this question empirically, finding that larger models respond\nsubstantially better to adversarial training, but there is little to no benefit\nfrom model scale in the absence of explicit defenses.\n","authors":["Nikolhaus Howe","Michał Zajac","Ian McKenzie","Oskar Hollinsworth","Tom Tseng","Pierre-Luc Bacon","Adam Gleave"],"pdf_url":"https://arxiv.org/pdf/2407.18213v1.pdf","comment":"31 pages"},{"id":"http://arxiv.org/abs/2406.05981v3","updated":"2024-07-25T17:20:48Z","published":"2024-06-10T02:47:55Z","title":"ShiftAddLLM: Accelerating Pretrained LLMs via Post-Training\n  Multiplication-Less Reparameterization","summary":"  Large language models (LLMs) have shown impressive performance on language\ntasks but face challenges when deployed on resource-constrained devices due to\ntheir extensive parameters and reliance on dense multiplications, resulting in\nhigh memory demands and latency bottlenecks. Shift-and-add reparameterization\noffers a promising solution by replacing costly multiplications with\nhardware-friendly primitives in both the attention and multi-layer perceptron\n(MLP) layers of an LLM. However, current reparameterization techniques require\ntraining from scratch or full parameter fine-tuning to restore accuracy, which\nis resource-intensive for LLMs. To address this, we propose accelerating\npretrained LLMs through post-training shift-and-add reparameterization,\ncreating efficient multiplication-free models, dubbed ShiftAddLLM.\nSpecifically, we quantize each weight matrix into binary matrices paired with\ngroup-wise scaling factors. The associated multiplications are reparameterized\ninto (1) shifts between activations and scaling factors and (2) queries and\nadds according to the binary matrices. To reduce accuracy loss, we present a\nmulti-objective optimization method to minimize both weight and output\nactivation reparameterization errors. Additionally, based on varying\nsensitivity across layers to reparameterization, we develop an automated bit\nallocation strategy to further reduce memory usage and latency. Experiments on\nfive LLM families and eight tasks consistently validate the effectiveness of\nShiftAddLLM, achieving average perplexity improvements of 5.6 and 22.7 points\nat comparable or lower latency compared to the most competitive quantized LLMs\nat 3 and 2 bits, respectively, and more than 80% memory and energy reductions\nover the original LLMs. Codes and models are available at\nhttps://github.com/GATECH-EIC/ShiftAddLLM.\n","authors":["Haoran You","Yipin Guo","Yichao Fu","Wei Zhou","Huihong Shi","Xiaofan Zhang","Souvik Kundu","Amir Yazdanbakhsh","Yingyan Celine Lin"],"pdf_url":"https://arxiv.org/pdf/2406.05981v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.06446v6","updated":"2024-07-25T17:19:31Z","published":"2023-06-10T13:53:41Z","title":"ShiftAddViT: Mixture of Multiplication Primitives Towards Efficient\n  Vision Transformer","summary":"  Vision Transformers (ViTs) have shown impressive performance and have become\na unified backbone for multiple vision tasks. However, both the attention\nmechanism and multi-layer perceptrons (MLPs) in ViTs are not sufficiently\nefficient due to dense multiplications, leading to costly training and\ninference. To this end, we propose to reparameterize pre-trained ViTs with a\nmixture of multiplication primitives, e.g., bitwise shifts and additions,\ntowards a new type of multiplication-reduced model, dubbed\n$\\textbf{ShiftAddViT}$, which aims to achieve end-to-end inference speedups on\nGPUs without requiring training from scratch. Specifically, all\n$\\texttt{MatMuls}$ among queries, keys, and values are reparameterized using\nadditive kernels, after mapping queries and keys to binary codes in Hamming\nspace. The remaining MLPs or linear layers are then reparameterized with shift\nkernels. We utilize TVM to implement and optimize those customized kernels for\npractical hardware deployment on GPUs. We find that such a reparameterization\non attention maintains model accuracy, while inevitably leading to accuracy\ndrops when being applied to MLPs. To marry the best of both worlds, we further\npropose a new mixture of experts (MoE) framework to reparameterize MLPs by\ntaking multiplication or its primitives as experts, e.g., multiplication and\nshift, and designing a new latency-aware load-balancing loss. Such a loss helps\nto train a generic router for assigning a dynamic amount of input tokens to\ndifferent experts according to their latency. Extensive experiments on various\n2D/3D Transformer-based vision tasks consistently validate the effectiveness of\nour proposed ShiftAddViT, achieving up to $\\textbf{5.18$\\times$}$ latency\nreductions on GPUs and $\\textbf{42.9}$% energy savings, while maintaining a\ncomparable accuracy as original or efficient ViTs.\n","authors":["Haoran You","Huihong Shi","Yipin Guo","Yingyan Celine Lin"],"pdf_url":"https://arxiv.org/pdf/2306.06446v6.pdf","comment":"Accepted by NeurIPS 2023"},{"id":"http://arxiv.org/abs/2406.07368v2","updated":"2024-07-25T17:18:01Z","published":"2024-06-11T15:34:43Z","title":"When Linear Attention Meets Autoregressive Decoding: Towards More\n  Effective and Efficient Linearized Large Language Models","summary":"  Autoregressive Large Language Models (LLMs) have achieved impressive\nperformance in language tasks but face two significant bottlenecks: (1)\nquadratic complexity in the attention module as the number of tokens increases,\nand (2) limited efficiency due to the sequential processing nature of\nautoregressive LLMs during generation. While linear attention and speculative\ndecoding offer potential solutions, their applicability and synergistic\npotential for enhancing autoregressive LLMs remain uncertain. We conduct the\nfirst comprehensive study on the efficacy of existing linear attention methods\nfor autoregressive LLMs, integrating them with speculative decoding. We\nintroduce an augmentation technique for linear attention that ensures\ncompatibility with speculative decoding, enabling more efficient training and\nserving of LLMs. Extensive experiments and ablation studies involving seven\nexisting linear attention models and five encoder/decoder-based LLMs\nconsistently validate the effectiveness of our augmented linearized LLMs.\nNotably, our approach achieves up to a 6.67 reduction in perplexity on the\nLLaMA model and up to a 2$\\times$ speedup during generation compared to prior\nlinear attention methods. Codes and models are available at\nhttps://github.com/GATECH-EIC/Linearized-LLM.\n","authors":["Haoran You","Yichao Fu","Zheng Wang","Amir Yazdanbakhsh","Yingyan Celine Lin"],"pdf_url":"https://arxiv.org/pdf/2406.07368v2.pdf","comment":"Accepted by ICML 2024; 17 pages; 10 figures; 16 tables"},{"id":"http://arxiv.org/abs/2407.18207v1","updated":"2024-07-25T17:17:10Z","published":"2024-07-25T17:17:10Z","title":"Geometry Fidelity for Spherical Images","summary":"  Spherical or omni-directional images offer an immersive visual format\nappealing to a wide range of computer vision applications. However, geometric\nproperties of spherical images pose a major challenge for models and metrics\ndesigned for ordinary 2D images. Here, we show that direct application of\nFr\\'echet Inception Distance (FID) is insufficient for quantifying geometric\nfidelity in spherical images. We introduce two quantitative metrics accounting\nfor geometric constraints, namely Omnidirectional FID (OmniFID) and\nDiscontinuity Score (DS). OmniFID is an extension of FID tailored to\nadditionally capture field-of-view requirements of the spherical format by\nleveraging cubemap projections. DS is a kernel-based seam alignment score of\ncontinuity across borders of 2D representations of spherical images. In\nexperiments, OmniFID and DS quantify geometry fidelity issues that are\nundetected by FID.\n","authors":["Anders Christensen","Nooshin Mojab","Khushman Patel","Karan Ahuja","Zeynep Akata","Ole Winther","Mar Gonzalez-Franco","Andrea Colaco"],"pdf_url":"https://arxiv.org/pdf/2407.18207v1.pdf","comment":"Accepted at ECCV 2024"},{"id":"http://arxiv.org/abs/2407.18202v1","updated":"2024-07-25T17:11:00Z","published":"2024-07-25T17:11:00Z","title":"Differentiable Quantum Architecture Search in Asynchronous Quantum\n  Reinforcement Learning","summary":"  The emergence of quantum reinforcement learning (QRL) is propelled by\nadvancements in quantum computing (QC) and machine learning (ML), particularly\nthrough quantum neural networks (QNN) built on variational quantum circuits\n(VQC). These advancements have proven successful in addressing sequential\ndecision-making tasks. However, constructing effective QRL models demands\nsignificant expertise due to challenges in designing quantum circuit\narchitectures, including data encoding and parameterized circuits, which\nprofoundly influence model performance. In this paper, we propose addressing\nthis challenge with differentiable quantum architecture search (DiffQAS),\nenabling trainable circuit parameters and structure weights using\ngradient-based optimization. Furthermore, we enhance training efficiency\nthrough asynchronous reinforcement learning (RL) methods facilitating parallel\ntraining. Through numerical simulations, we demonstrate that our proposed\nDiffQAS-QRL approach achieves performance comparable to manually-crafted\ncircuit architectures across considered environments, showcasing stability\nacross diverse scenarios. This methodology offers a pathway for designing QRL\nmodels without extensive quantum knowledge, ensuring robust performance and\nfostering broader application of QRL.\n","authors":["Samuel Yen-Chi Chen"],"pdf_url":"https://arxiv.org/pdf/2407.18202v1.pdf","comment":"Accepted by IEEE International Conference on Quantum Computing and\n  Engineering - QCE 2024"},{"id":"http://arxiv.org/abs/2407.18200v1","updated":"2024-07-25T17:09:22Z","published":"2024-07-25T17:09:22Z","title":"Sparse Incremental Aggregation in Multi-Hop Federated Learning","summary":"  This paper investigates federated learning (FL) in a multi-hop communication\nsetup, such as in constellations with inter-satellite links. In this setup,\npart of the FL clients are responsible for forwarding other client's results to\nthe parameter server. Instead of using conventional routing, the communication\nefficiency can be improved significantly by using in-network model aggregation\nat each intermediate hop, known as incremental aggregation (IA). Prior works\n[1] have indicated diminishing gains for IA under gradient sparsification. Here\nwe study this issue and propose several novel correlated sparsification methods\nfor IA. Numerical results show that, for some of these algorithms, the full\npotential of IA is still available under sparsification without impairing\nconvergence. We demonstrate a 15x improvement in communication efficiency over\nconventional routing and a 11x improvement over state-of-the-art (SoA) sparse\nIA.\n","authors":["Sourav Mukherjee","Nasrin Razmi","Armin Dekorsy","Petar Popovski","Bho Matthiesen"],"pdf_url":"https://arxiv.org/pdf/2407.18200v1.pdf","comment":"This paper is accepted for the 25th IEEE International Workshop on\n  Signal Processing Advances in Wireless Communications (SPAWC) conference"},{"id":"http://arxiv.org/abs/2310.09149v2","updated":"2024-07-25T17:05:37Z","published":"2023-10-13T14:43:11Z","title":"Wasserstein approximation schemes based on Voronoi partitions","summary":"  We consider structured approximation of measures in Wasserstein space\n$\\mathrm{W}_p(\\mathbb{R}^d)$ for $p\\in[1,\\infty)$ using general measure\napproximants compactly supported on Voronoi regions derived from a scaled\nVoronoi partition of $\\mathbb{R}^d$. We show that if a full rank lattice\n$\\Lambda$ is scaled by a factor of $h\\in(0,1]$, then approximation of a measure\nbased on the Voronoi partition of $h\\Lambda$ is $O(h)$ regardless of $d$ or\n$p$. We then use a covering argument to show that $N$-term approximations of\ncompactly supported measures is $O(N^{-\\frac1d})$ which matches known rates for\noptimal quantizers and empirical measure approximation in most instances.\nAdditionally, we generalize our construction to nonuniform Voronoi partitions,\nhighlighting the flexibility and robustness of our approach for various measure\napproximation scenarios. Finally, we extend these results to noncompactly\nsupported measures with sufficient decay. Our findings are pertinent to\napplications in computer vision and machine learning where measures are used to\nrepresent structured data such as images.\n","authors":["Keaton Hamm","Varun Khurana"],"pdf_url":"https://arxiv.org/pdf/2310.09149v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14236v4","updated":"2024-07-25T16:52:15Z","published":"2024-03-21T08:54:24Z","title":"A Unified Framework for Model Editing","summary":"  ROME and MEMIT are largely believed to be two different model editing\nalgorithms, with the major difference between them being the ability to perform\nbatched edits. In this paper, we unify these two algorithms under a single\nconceptual umbrella, optimizing for the same goal, which we call the\npreservation-memorization objective. ROME uses an equality constraint to\noptimize this objective to perform one edit at a time, whereas MEMIT employs a\nmore flexible least-square constraint that allows for batched edits. We\ngeneralize ROME and enable batched editing with equality constraint in the form\nof EMMET - an Equality-constrained Mass Model Editing algorithm for\nTransformers, a new batched memory-editing algorithm. EMMET can perform\nbatched-edits up to a batch-size of 10,000, with very similar performance to\nMEMIT across multiple dimensions. With the introduction of EMMET, we truly\nunify ROME and MEMIT and show that both algorithms are equivalent in terms of\ntheir optimization objective, their abilities (singular and batched editing),\ntheir model editing performance and their limitations.\n","authors":["Akshat Gupta","Dev Sajnani","Gopala Anumanchipalli"],"pdf_url":"https://arxiv.org/pdf/2403.14236v4.pdf","comment":"Under review. To appear as poster at KnowledgeableLM Workshop\n  co-located with ACL 2024"},{"id":"http://arxiv.org/abs/2407.18184v1","updated":"2024-07-25T16:43:56Z","published":"2024-07-25T16:43:56Z","title":"AsEP: Benchmarking Deep Learning Methods for Antibody-specific Epitope\n  Prediction","summary":"  Epitope identification is vital for antibody design yet challenging due to\nthe inherent variability in antibodies. While many deep learning methods have\nbeen developed for general protein binding site prediction tasks, whether they\nwork for epitope prediction remains an understudied research question. The\nchallenge is also heightened by the lack of a consistent evaluation pipeline\nwith sufficient dataset size and epitope diversity. We introduce a filtered\nantibody-antigen complex structure dataset, AsEP (Antibody-specific Epitope\nPrediction). AsEP is the largest of its kind and provides clustered epitope\ngroups, allowing the community to develop and test novel epitope prediction\nmethods. AsEP comes with an easy-to-use interface in Python and pre-built graph\nrepresentations of each antibody-antigen complex while also supporting\ncustomizable embedding methods. Based on this new dataset, we benchmarked\nvarious representative general protein-binding site prediction methods and find\nthat their performances are not satisfactory as expected for epitope\nprediction. We thus propose a new method, WALLE, that leverages both protein\nlanguage models and graph neural networks. WALLE demonstrate about 5X\nperformance gain over existing methods. Our empirical findings evidence that\nepitope prediction benefits from combining sequential embeddings provided by\nlanguage models and geometrical information from graph representations,\nproviding a guideline for future method design. In addition, we reformulate the\ntask as bipartite link prediction, allowing easy model performance attribution\nand interpretability. We open-source our data and code at\nhttps://github.com/biochunan/AsEP-dataset.\n","authors":["Chunan Liu","Lilian Denzler","Yihong Chen","Andrew Martin","Brooks Paige"],"pdf_url":"https://arxiv.org/pdf/2407.18184v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.18181v1","updated":"2024-07-25T16:42:08Z","published":"2024-07-25T16:42:08Z","title":"Gene Regulatory Network Inference from Pre-trained Single-Cell\n  Transcriptomics Transformer with Joint Graph Learning","summary":"  Inferring gene regulatory networks (GRNs) from single-cell RNA sequencing\n(scRNA-seq) data is a complex challenge that requires capturing the intricate\nrelationships between genes and their regulatory interactions. In this study,\nwe tackle this challenge by leveraging the single-cell BERT-based pre-trained\ntransformer model (scBERT), trained on extensive unlabeled scRNA-seq data, to\naugment structured biological knowledge from existing GRNs. We introduce a\nnovel joint graph learning approach that combines the rich contextual\nrepresentations learned by pre-trained single-cell language models with the\nstructured knowledge encoded in GRNs using graph neural networks (GNNs). By\nintegrating these two modalities, our approach effectively reasons over boththe\ngene expression level constraints provided by the scRNA-seq data and the\nstructured biological knowledge inherent in GRNs. We evaluate our method on\nhuman cell benchmark datasets from the BEELINE study with cell type-specific\nground truth networks. The results demonstrate superior performance over\ncurrent state-of-the-art baselines, offering a deeper understanding of cellular\nregulatory mechanisms.\n","authors":["Sindhura Kommu","Yizhi Wang","Yue Wang","Xuan Wang"],"pdf_url":"https://arxiv.org/pdf/2407.18181v1.pdf","comment":"Accepted into the ICML 2024 AI for Science workshop"},{"id":"http://arxiv.org/abs/2407.18175v1","updated":"2024-07-25T16:35:46Z","published":"2024-07-25T16:35:46Z","title":"Quasar-ViT: Hardware-Oriented Quantization-Aware Architecture Search for\n  Vision Transformers","summary":"  Vision transformers (ViTs) have demonstrated their superior accuracy for\ncomputer vision tasks compared to convolutional neural networks (CNNs).\nHowever, ViT models are often computation-intensive for efficient deployment on\nresource-limited edge devices. This work proposes Quasar-ViT, a\nhardware-oriented quantization-aware architecture search framework for ViTs, to\ndesign efficient ViT models for hardware implementation while preserving the\naccuracy. First, Quasar-ViT trains a supernet using our row-wise flexible\nmixed-precision quantization scheme, mixed-precision weight entanglement, and\nsupernet layer scaling techniques. Then, it applies an efficient\nhardware-oriented search algorithm, integrated with hardware latency and\nresource modeling, to determine a series of optimal subnets from supernet under\ndifferent inference latency targets. Finally, we propose a series of\nmodel-adaptive designs on the FPGA platform to support the architecture search\nand mitigate the gap between the theoretical computation reduction and the\npractical inference speedup. Our searched models achieve 101.5, 159.6, and\n251.6 frames-per-second (FPS) inference speed on the AMD/Xilinx ZCU102 FPGA\nwith 80.4%, 78.6%, and 74.9% top-1 accuracy, respectively, for the ImageNet\ndataset, consistently outperforming prior works.\n","authors":["Zhengang Li","Alec Lu","Yanyue Xie","Zhenglun Kong","Mengshu Sun","Hao Tang","Zhong Jia Xue","Peiyan Dong","Caiwen Ding","Yanzhi Wang","Xue Lin","Zhenman Fang"],"pdf_url":"https://arxiv.org/pdf/2407.18175v1.pdf","comment":"Accepted by ICS 2024"},{"id":"http://arxiv.org/abs/2407.18170v1","updated":"2024-07-25T16:33:35Z","published":"2024-07-25T16:33:35Z","title":"RIDA: A Robust Attack Framework on Incomplete Graphs","summary":"  Graph Neural Networks (GNNs) are vital in data science but are increasingly\nsusceptible to adversarial attacks. To help researchers develop more robust GNN\nmodels, it's essential to focus on designing strong attack models as\nfoundational benchmarks and guiding references. Among adversarial attacks,\ngray-box poisoning attacks are noteworthy due to their effectiveness and fewer\nconstraints. These attacks exploit GNNs' need for retraining on updated data,\nthereby impacting their performance by perturbing these datasets. However,\ncurrent research overlooks the real-world scenario of incomplete graphs.To\naddress this gap, we introduce the Robust Incomplete Deep Attack Framework\n(RIDA). It is the first algorithm for robust gray-box poisoning attacks on\nincomplete graphs. The approach innovatively aggregates distant vertex\ninformation and ensures powerful data utilization.Extensive tests against 9\nSOTA baselines on 3 real-world datasets demonstrate RIDA's superiority in\nhandling incompleteness and high attack performance on the incomplete graph.\n","authors":["Jianke Yu","Hanchen Wang","Chen Chen","Xiaoyang Wang","Wenjie Zhang","Ying Zhang"],"pdf_url":"https://arxiv.org/pdf/2407.18170v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12120v2","updated":"2024-07-25T16:27:49Z","published":"2024-03-18T18:00:00Z","title":"Light Curve Classification with DistClassiPy: a new distance-based\n  classifier","summary":"  The rise of synoptic sky surveys has ushered in an era of big data in\ntime-domain astronomy, making data science and machine learning essential tools\nfor studying celestial objects. While tree-based models (e.g. Random Forests)\nand deep learning models dominate the field, we explore the use of different\ndistance metrics to aid in the classification of astrophysical objects. We\ndeveloped DistClassiPy, a new distance metric based classifier. The direct use\nof distance metrics is unexplored in time-domain astronomy, but distance-based\nmethods can help make classification more interpretable and decrease\ncomputational costs. In particular, we applied DistClassiPy to classify light\ncurves of variable stars, comparing the distances between objects of different\nclasses. Using 18 distance metrics on a catalog of 6,000 variable stars across\n10 classes, we demonstrate classification and dimensionality reduction. Our\nclassifier meets state-of-the-art performance but has lower computational\nrequirements and improved interpretability. Additionally, DistClassiPy can be\ntailored to specific objects by identifying the most effective distance metric\nfor that classification. To facilitate broader applications within and beyond\nastronomy, we have made DistClassiPy open-source and available at\nhttps://pypi.org/project/distclassipy/.\n","authors":["Siddharth Chaini","Ashish Mahabal","Ajit Kembhavi","Federica B. Bianco"],"pdf_url":"https://arxiv.org/pdf/2403.12120v2.pdf","comment":"Accepted for publication in Astronomy and Computing (2024). 24 pages,\n  19 figures"},{"id":"http://arxiv.org/abs/2407.14207v2","updated":"2024-07-25T16:24:59Z","published":"2024-07-19T11:12:08Z","title":"Longhorn: State Space Models are Amortized Online Learners","summary":"  The most fundamental capability of modern AI methods such as Large Language\nModels (LLMs) is the ability to predict the next token in a long sequence of\ntokens, known as ``sequence modeling.\" Although the Transformers model is the\ncurrent dominant approach to sequence modeling, its quadratic computational\ncost with respect to sequence length is a significant drawback. State-space\nmodels (SSMs) offer a promising alternative due to their linear decoding\nefficiency and high parallelizability during training. However, existing SSMs\noften rely on seemingly ad hoc linear recurrence designs. In this work, we\nexplore SSM design through the lens of online learning, conceptualizing SSMs as\nmeta-modules for specific online learning problems. This approach links SSM\ndesign to formulating precise online learning objectives, with state transition\nrules derived from optimizing these objectives. Based on this insight, we\nintroduce a novel deep SSM architecture based on the implicit update for\noptimizing an online regression objective. Our experimental results show that\nour models outperform state-of-the-art SSMs, including the Mamba model, on\nstandard sequence modeling benchmarks and language modeling tasks.\n","authors":["Bo Liu","Rui Wang","Lemeng Wu","Yihao Feng","Peter Stone","Qiang Liu"],"pdf_url":"https://arxiv.org/pdf/2407.14207v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.19708v2","updated":"2024-07-25T16:16:46Z","published":"2024-04-30T17:00:32Z","title":"Harmonic LLMs are Trustworthy","summary":"  We introduce an intuitive method to test the robustness (stability and\nexplainability) of any black-box LLM in real-time via its local deviation from\nharmoniticity, denoted as $\\gamma$. To the best of our knowledge this is the\nfirst completely model-agnostic and unsupervised method of measuring the\nrobustness of any given response from an LLM, based upon the model itself\nconforming to a purely mathematical standard. To show general application and\nimmediacy of results, we measure $\\gamma$ in 10 popular LLMs (ChatGPT,\nClaude-2.1, Claude3.0, GPT-4, GPT-4o, Smaug-72B, Mixtral-8x7B, Llama2-7B,\nMistral-7B and MPT-7B) across thousands of queries in three objective domains:\nWebQA, ProgrammingQA, and TruthfulQA. Across all models and domains tested,\nhuman annotation confirms that $\\gamma \\to 0$ indicates trustworthiness, and\nconversely searching higher values of $\\gamma$ easily exposes examples of\nhallucination, a fact that enables efficient adversarial prompt generation\nthrough stochastic gradient ascent in $\\gamma$. The low-$\\gamma$ leaders among\nthe models in the respective domains are GPT-4o, GPT-4, and Smaug-72B,\nproviding evidence that mid-size open-source models can win out against large\ncommercial models.\n","authors":["Nicholas S. Kersting","Mohammad Rahman","Suchismitha Vedala","Yang Wang"],"pdf_url":"https://arxiv.org/pdf/2404.19708v2.pdf","comment":"15 pages, 2 figures, 16 tables; added Claude-3.0, GPT-4o, Mistral-7B,\n  Mixtral-8x7B, and more annotation for other models"},{"id":"http://arxiv.org/abs/2407.18158v1","updated":"2024-07-25T16:13:58Z","published":"2024-07-25T16:13:58Z","title":"Unlocking Tokens as Data Points for Generalization Bounds on Larger\n  Language Models","summary":"  Large language models (LLMs) with billions of parameters excel at predicting\nthe next token in a sequence. Recent work computes non-vacuous\ncompression-based generalization bounds for LLMs, but these bounds are vacuous\nfor large models at the billion-parameter scale. Moreover, these bounds are\nobtained through restrictive compression techniques, bounding compressed models\nthat generate low-quality text. Additionally, the tightness of these existing\nbounds depends on the number of IID documents in a training set rather than the\nmuch larger number of non-IID constituent tokens, leaving untapped potential\nfor tighter bounds. In this work, we instead use properties of martingales to\nderive generalization bounds that benefit from the vast number of tokens in LLM\ntraining sets. Since a dataset contains far more tokens than documents, our\ngeneralization bounds not only tolerate but actually benefit from far less\nrestrictive compression schemes. With Monarch matrices, Kronecker\nfactorizations, and post-training quantization, we achieve non-vacuous\ngeneralization bounds for LLMs as large as LLaMA2-70B. Unlike previous\napproaches, our work achieves the first non-vacuous bounds for models that are\ndeployed in practice and generate high-quality text.\n","authors":["Sanae Lotfi","Yilun Kuang","Brandon Amos","Micah Goldblum","Marc Finzi","Andrew Gordon Wilson"],"pdf_url":"https://arxiv.org/pdf/2407.18158v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.00662v2","updated":"2024-07-25T16:04:49Z","published":"2024-05-01T17:50:16Z","title":"No Representation, No Trust: Connecting Representation, Collapse, and\n  Trust Issues in PPO","summary":"  Reinforcement learning (RL) is inherently rife with non-stationarity since\nthe states and rewards the agent observes during training depend on its\nchanging policy. Therefore, networks in deep RL must be capable of adapting to\nnew observations and fitting new targets. However, previous works have observed\nthat networks in off-policy deep value-based methods exhibit a decrease in\nrepresentation rank, often correlated with an inability to continue learning or\na collapse in performance. Although this phenomenon has generally been\nattributed to neural network learning under non-stationarity, it has been\noverlooked in on-policy policy optimization methods which are often thought\ncapable of training indefinitely. In this work, we empirically study\nrepresentation dynamics in Proximal Policy Optimization (PPO) on the Atari and\nMuJoCo environments, revealing that PPO agents are also affected by feature\nrank deterioration and loss of plasticity. We show that this is aggravated with\nstronger non-stationarity, ultimately driving the actor's performance to\ncollapse, regardless of the performance of the critic. We ask why the trust\nregion, specific to methods like PPO, cannot alleviate or prevent the collapse.\nWe find that there is a connection between representation collapse and the\ndegradation of the trust region, one exacerbating the other, and present\nProximal Feature Optimization (PFO), a novel auxiliary loss that, along with\nother interventions, shows that regularizing the representation dynamics\nimproves the performance of PPO agents.\n","authors":["Skander Moalla","Andrea Miele","Razvan Pascanu","Caglar Gulcehre"],"pdf_url":"https://arxiv.org/pdf/2405.00662v2.pdf","comment":"ICML ARLET workshop version. Code and run histories are available at\n  https://github.com/CLAIRE-Labo/no-representation-no-trust"},{"id":"http://arxiv.org/abs/2406.12839v2","updated":"2024-07-25T16:01:04Z","published":"2024-06-18T17:56:10Z","title":"Evaluating the design space of diffusion-based generative models","summary":"  Most existing theoretical investigations of the accuracy of diffusion models,\nalbeit significant, assume the score function has been approximated to a\ncertain accuracy, and then use this a priori bound to control the error of\ngeneration. This article instead provides a first quantitative understanding of\nthe whole generation process, i.e., both training and sampling. More precisely,\nit conducts a non-asymptotic convergence analysis of denoising score matching\nunder gradient descent. In addition, a refined sampling error analysis for\nvariance exploding models is also provided. The combination of these two\nresults yields a full error analysis, which elucidates (again, but this time\ntheoretically) how to design the training and sampling processes for effective\ngeneration. For instance, our theory implies a preference toward noise\ndistribution and loss weighting in training that qualitatively agree with the\nones used in [Karras et al. 2022]. It also provides perspectives on the choices\nof time and variance schedules in sampling: when the score is well trained, the\ndesign in [Song et al. 2020] is more preferable, but when it is less trained,\nthe design in [Karras et al. 2022] becomes more preferable.\n","authors":["Yuqing Wang","Ye He","Molei Tao"],"pdf_url":"https://arxiv.org/pdf/2406.12839v2.pdf","comment":"Comments are welcome. Out of admiration we titled our paper after\n  EDM, and hoped theorists' humor is not too corny"},{"id":"http://arxiv.org/abs/2407.18148v1","updated":"2024-07-25T15:58:56Z","published":"2024-07-25T15:58:56Z","title":"StraightLine: An End-to-End Resource-Aware Scheduler for Machine\n  Learning Application Requests","summary":"  The life cycle of machine learning (ML) applications consists of two stages:\nmodel development and model deployment. However, traditional ML systems (e.g.,\ntraining-specific or inference-specific systems) focus on one particular stage\nor phase of the life cycle of ML applications. These systems often aim at\noptimizing model training or accelerating model inference, and they frequently\nassume homogeneous infrastructure, which may not always reflect real-world\nscenarios that include cloud data centers, local servers, containers, and\nserverless platforms. We present StraightLine, an end-to-end resource-aware\nscheduler that schedules the optimal resources (e.g., container, virtual\nmachine, or serverless) for different ML application requests in a hybrid\ninfrastructure. The key innovation is an empirical dynamic placing algorithm\nthat intelligently places requests based on their unique characteristics (e.g.,\nrequest frequency, input data size, and data distribution). In contrast to\nexisting ML systems, StraightLine offers end-to-end resource-aware placement,\nthereby it can significantly reduce response time and failure rate for model\ndeployment when facing different computing resources in the hybrid\ninfrastructure.\n","authors":["Cheng-Wei Ching","Boyuan Guan","Hailu Xu","Liting Hu"],"pdf_url":"https://arxiv.org/pdf/2407.18148v1.pdf","comment":"6 pages, 8 figures, to appear in AIoTC'24"},{"id":"http://arxiv.org/abs/2407.18143v1","updated":"2024-07-25T15:48:24Z","published":"2024-07-25T15:48:24Z","title":"Maximum Entropy On-Policy Actor-Critic via Entropy Advantage Estimation","summary":"  Entropy Regularisation is a widely adopted technique that enhances policy\noptimisation performance and stability. A notable form of entropy\nregularisation is augmenting the objective with an entropy term, thereby\nsimultaneously optimising the expected return and the entropy. This framework,\nknown as maximum entropy reinforcement learning (MaxEnt RL), has shown\ntheoretical and empirical successes. However, its practical application in\nstraightforward on-policy actor-critic settings remains surprisingly\nunderexplored. We hypothesise that this is due to the difficulty of managing\nthe entropy reward in practice. This paper proposes a simple method of\nseparating the entropy objective from the MaxEnt RL objective, which\nfacilitates the implementation of MaxEnt RL in on-policy settings. Our\nempirical evaluations demonstrate that extending Proximal Policy Optimisation\n(PPO) and Trust Region Policy Optimisation (TRPO) within the MaxEnt framework\nimproves policy optimisation performance in both MuJoCo and Procgen tasks.\nAdditionally, our results highlight MaxEnt RL's capacity to enhance\ngeneralisation.\n","authors":["Jean Seong Bjorn Choe","Jong-Kook Kim"],"pdf_url":"https://arxiv.org/pdf/2407.18143v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.18141v1","updated":"2024-07-25T15:45:17Z","published":"2024-07-25T15:45:17Z","title":"IRIS: Wireless Ring for Vision-based Smart Home Interaction","summary":"  Integrating cameras into wireless smart rings has been challenging due to\nsize and power constraints. We introduce IRIS, the first wireless\nvision-enabled smart ring system for smart home interactions. Equipped with a\ncamera, Bluetooth radio, inertial measurement unit (IMU), and an onboard\nbattery, IRIS meets the small size, weight, and power (SWaP) requirements for\nring devices. IRIS is context-aware, adapting its gesture set to the detected\ndevice, and can last for 16-24 hours on a single charge. IRIS leverages the\nscene semantics to achieve instance-level device recognition. In a study\ninvolving 23 participants, IRIS consistently outpaced voice commands, with a\nhigher proportion of participants expressing a preference for IRIS over voice\ncommands regarding toggling a device's state, granular control, and social\nacceptability. Our work pushes the boundary of what is possible with ring\nform-factor devices, addressing system challenges and opening up novel\ninteraction capabilities.\n","authors":["Maruchi Kim","Antonio Glenn","Bandhav Veluri","Yunseo Lee","Eyoel Gebre","Aditya Bagaria","Shwetak Patel","Shyamnath Gollakota"],"pdf_url":"https://arxiv.org/pdf/2407.18141v1.pdf","comment":"15 pages, 17 figures, 6 tables, to be published in UIST 2024"},{"id":"http://arxiv.org/abs/2407.18134v1","updated":"2024-07-25T15:38:16Z","published":"2024-07-25T15:38:16Z","title":"$\\mathbb{X}$-Sample Contrastive Loss: Improving Contrastive Learning\n  with Sample Similarity Graphs","summary":"  Learning good representations involves capturing the diverse ways in which\ndata samples relate. Contrastive loss - an objective matching related samples -\nunderlies methods from self-supervised to multimodal learning. Contrastive\nlosses, however, can be viewed more broadly as modifying a similarity graph to\nindicate how samples should relate in the embedding space. This view reveals a\nshortcoming in contrastive learning: the similarity graph is binary, as only\none sample is the related positive sample. Crucially, similarities\n\\textit{across} samples are ignored. Based on this observation, we revise the\nstandard contrastive loss to explicitly encode how a sample relates to others.\nWe experiment with this new objective, called $\\mathbb{X}$-Sample Contrastive,\nto train vision models based on similarities in class or text caption\ndescriptions. Our study spans three scales: ImageNet-1k with 1 million, CC3M\nwith 3 million, and CC12M with 12 million samples. The representations learned\nvia our objective outperform both contrastive self-supervised and\nvision-language models trained on the same data across a range of tasks. When\ntraining on CC12M, we outperform CLIP by $0.6\\%$ on both ImageNet and ImageNet\nReal. Our objective appears to work particularly well in lower-data regimes,\nwith gains over CLIP of $16.8\\%$ on ImageNet and $18.1\\%$ on ImageNet Real when\ntraining with CC3M. Finally, our objective seems to encourage the model to\nlearn representations that separate objects from their attributes and\nbackgrounds, with gains of $3.3$-$5.6$\\% over CLIP on ImageNet9. We hope the\nproposed solution takes a small step towards developing richer learning\nobjectives for understanding sample relations in foundation models.\n","authors":["Vlad Sobal","Mark Ibrahim","Randall Balestriero","Vivien Cabannes","Diane Bouchacourt","Pietro Astolfi","Kyunghyun Cho","Yann LeCun"],"pdf_url":"https://arxiv.org/pdf/2407.18134v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17449v2","updated":"2024-07-25T15:33:00Z","published":"2024-07-24T17:30:21Z","title":"Looking at Model Debiasing through the Lens of Anomaly Detection","summary":"  It is widely recognized that deep neural networks are sensitive to bias in\nthe data. This means that during training these models are likely to learn\nspurious correlations between data and labels, resulting in limited\ngeneralization abilities and low performance. In this context, model debiasing\napproaches can be devised aiming at reducing the model's dependency on such\nunwanted correlations, either leveraging the knowledge of bias information or\nnot. In this work, we focus on the latter and more realistic scenario, showing\nthe importance of accurately predicting the bias-conflicting and bias-aligned\nsamples to obtain compelling performance in bias mitigation. On this ground, we\npropose to conceive the problem of model bias from an out-of-distribution\nperspective, introducing a new bias identification method based on anomaly\ndetection. We claim that when data is mostly biased, bias-conflicting samples\ncan be regarded as outliers with respect to the bias-aligned distribution in\nthe feature space of a biased model, thus allowing for precisely detecting them\nwith an anomaly detection method. Coupling the proposed bias identification\napproach with bias-conflicting data upsampling and augmentation in a two-step\nstrategy, we reach state-of-the-art performance on synthetic and real benchmark\ndatasets. Ultimately, our proposed approach shows that the data bias issue does\nnot necessarily require complex debiasing methods, given that an accurate bias\nidentification procedure is defined.\n","authors":["Vito Paolo Pastore","Massimiliano Ciranni","Davide Marinelli","Francesca Odone","Vittorio Murino"],"pdf_url":"https://arxiv.org/pdf/2407.17449v2.pdf","comment":"15 pages, 7 figures"},{"id":"http://arxiv.org/abs/2310.20498v2","updated":"2024-07-25T15:25:27Z","published":"2023-10-31T14:37:37Z","title":"Generative Learning of Continuous Data by Tensor Networks","summary":"  Beyond their origin in modeling many-body quantum systems, tensor networks\nhave emerged as a promising class of models for solving machine learning\nproblems, notably in unsupervised generative learning. While possessing many\ndesirable features arising from their quantum-inspired nature, tensor network\ngenerative models have previously been largely restricted to binary or\ncategorical data, limiting their utility in real-world modeling problems. We\novercome this by introducing a new family of tensor network generative models\nfor continuous data, which are capable of learning from distributions\ncontaining continuous random variables. We develop our method in the setting of\nmatrix product states, first deriving a universal expressivity theorem proving\nthe ability of this model family to approximate any reasonably smooth\nprobability density function with arbitrary precision. We then benchmark the\nperformance of this model on several synthetic and real-world datasets, finding\nthat the model learns and generalizes well on distributions of continuous and\ndiscrete variables. We develop methods for modeling different data domains, and\nintroduce a trainable compression layer which is found to increase model\nperformance given limited memory or computational resources. Overall, our\nmethods give important theoretical and empirical evidence of the efficacy of\nquantum-inspired methods for the rapidly growing field of generative learning.\n","authors":["Alex Meiburg","Jing Chen","Jacob Miller","Raphaëlle Tihon","Guillaume Rabusseau","Alejandro Perdomo-Ortiz"],"pdf_url":"https://arxiv.org/pdf/2310.20498v2.pdf","comment":"21 pages, 15 figures"},{"id":"http://arxiv.org/abs/2407.18114v1","updated":"2024-07-25T15:21:54Z","published":"2024-07-25T15:21:54Z","title":"Unsupervised Training of Neural Cellular Automata on Edge Devices","summary":"  The disparity in access to machine learning tools for medical imaging across\ndifferent regions significantly limits the potential for universal healthcare\ninnovation, particularly in remote areas. Our research addresses this issue by\nimplementing Neural Cellular Automata (NCA) training directly on smartphones\nfor accessible X-ray lung segmentation. We confirm the practicality and\nfeasibility of deploying and training these advanced models on five Android\ndevices, improving medical diagnostics accessibility and bridging the tech\ndivide to extend machine learning benefits in medical imaging to low- and\nmiddle-income countries (LMICs). We further enhance this approach with an\nunsupervised adaptation method using the novel Variance-Weighted Segmentation\nLoss (VWSL), which efficiently learns from unlabeled data by minimizing the\nvariance from multiple NCA predictions. This strategy notably improves model\nadaptability and performance across diverse medical imaging contexts without\nthe need for extensive computational resources or labeled datasets, effectively\nlowering the participation threshold. Our methodology, tested on three\nmultisite X-ray datasets -- Padchest, ChestX-ray8, and MIMIC-III --\ndemonstrates improvements in segmentation Dice accuracy by 0.7 to 2.8%,\ncompared to the classic Med-NCA. Additionally, in extreme cases where no\ndigital copy is available and images must be captured by a phone from an X-ray\nlightbox or monitor, VWSL enhances Dice accuracy by 5-20%, demonstrating the\nmethod's robustness even with suboptimal image sources.\n","authors":["John Kalkhof","Amin Ranem","Anirban Mukhopadhyay"],"pdf_url":"https://arxiv.org/pdf/2407.18114v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.18108v1","updated":"2024-07-25T15:12:46Z","published":"2024-07-25T15:12:46Z","title":"Graph Neural Ordinary Differential Equations for Coarse-Grained\n  Socioeconomic Dynamics","summary":"  We present a data-driven machine-learning approach for modeling space-time\nsocioeconomic dynamics. Through coarse-graining fine-scale observations, our\nmodeling framework simplifies these complex systems to a set of tractable\nmechanistic relationships -- in the form of ordinary differential equations --\nwhile preserving critical system behaviors. This approach allows for expedited\n'what if' studies and sensitivity analyses, essential for informed\npolicy-making. Our findings, from a case study of Baltimore, MD, indicate that\nthis machine learning-augmented coarse-grained model serves as a powerful\ninstrument for deciphering the complex interactions between social factors,\ngeography, and exogenous stressors, offering a valuable asset for system\nforecasting and resilience planning.\n","authors":["James Koch","Pranab Roy Chowdhury","Heng Wan","Parin Bhaduri","Jim Yoon","Vivek Srikrishnan","W. Brent Daniel"],"pdf_url":"https://arxiv.org/pdf/2407.18108v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.18103v1","updated":"2024-07-25T15:07:35Z","published":"2024-07-25T15:07:35Z","title":"Fine-Tuning Large Language Models for Stock Return Prediction Using\n  Newsflow","summary":"  Large language models (LLMs) and their fine-tuning techniques have\ndemonstrated superior performance in various language understanding and\ngeneration tasks. This paper explores fine-tuning LLMs for stock return\nforecasting with financial newsflow. In quantitative investing, return\nforecasting is fundamental for subsequent tasks like stock picking, portfolio\noptimization, etc. We formulate the model to include text representation and\nforecasting modules. We propose to compare the encoder-only and decoder-only\nLLMs, considering they generate text representations in distinct ways. The\nimpact of these different representations on forecasting performance remains an\nopen question. Meanwhile, we compare two simple methods of integrating LLMs'\ntoken-level representations into the forecasting module. The experiments on\nreal news and investment universes reveal that: (1) aggregated representations\nfrom LLMs' token-level embeddings generally produce return predictions that\nenhance the performance of long-only and long-short portfolios; (2) in the\nrelatively large investment universe, the decoder LLMs-based prediction model\nleads to stronger portfolios, whereas in the small universes, there are no\nconsistent winners. Among the three LLMs studied (DeBERTa, Mistral, Llama),\nMistral performs more robustly across different universes; (3) return\npredictions derived from LLMs' text representations are a strong signal for\nportfolio construction, outperforming conventional sentiment scores.\n","authors":["Tian Guo","Emmanuel Hauptmann"],"pdf_url":"https://arxiv.org/pdf/2407.18103v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.16571v2","updated":"2024-07-25T15:04:31Z","published":"2023-09-28T16:27:07Z","title":"Review of Machine Learning Methods for Additive Manufacturing of\n  Functionally Graded Materials","summary":"  Additive Manufacturing (AM) is a transformative manufacturing technology\nenabling direct fabrication of complex parts layer-be-layer from 3D modeling\ndata. Among AM applications, the fabrication of Functionally Graded Materials\n(FGMs) has significant importance due to the potential to enhance component\nperformance across several industries. FGMs are manufactured with a gradient\ncomposition transition between dissimilar materials, enabling the design of new\nmaterials with location-dependent mechanical and physical properties. This\nstudy presents a comprehensive review of published literature pertaining to the\nimplementation of Machine Learning (ML) techniques in AM, with an emphasis on\nML-based methods for optimizing FGMs fabrication processes. Through an\nextensive survey of the literature, this review article explores the role of ML\nin addressing the inherent challenges in FGMs fabrication and encompasses\nparameter optimization, defect detection, and real-time monitoring. The article\nalso provides a discussion of future research directions and challenges in\nemploying ML-based methods in AM fabrication of FGMs.\n","authors":["Mohammad Karimzadeh","Deekshith Basvoju","Aleksandar Vakanski","Indrajit Charit","Fei Xu","Xinchang Zhang"],"pdf_url":"https://arxiv.org/pdf/2309.16571v2.pdf","comment":"23 pages"},{"id":"http://arxiv.org/abs/2402.00300v2","updated":"2024-07-25T14:48:34Z","published":"2024-02-01T03:27:26Z","title":"Self-supervised learning of video representations from a child's\n  perspective","summary":"  Children learn powerful internal models of the world around them from a few\nyears of egocentric visual experience. Can such internal models be learned from\na child's visual experience with highly generic learning algorithms or do they\nrequire strong inductive biases? Recent advances in collecting large-scale,\nlongitudinal, developmentally realistic video datasets and generic\nself-supervised learning (SSL) algorithms are allowing us to begin to tackle\nthis nature vs. nurture question. However, existing work typically focuses on\nimage-based SSL algorithms and visual capabilities that can be learned from\nstatic images (e.g. object recognition), thus ignoring temporal aspects of the\nworld. To close this gap, here we train self-supervised video models on\nlongitudinal, egocentric headcam recordings collected from a child over a two\nyear period in their early development (6-31 months). The resulting models are\nhighly effective at facilitating the learning of action concepts from a small\nnumber of labeled examples; they have favorable data size scaling properties;\nand they display emergent video interpolation capabilities. Video models also\nlearn more robust object representations than image-based models trained with\nthe exact same data. These results suggest that important temporal aspects of a\nchild's internal model of the world may be learnable from their visual\nexperience using highly generic learning algorithms and without strong\ninductive biases.\n","authors":["A. Emin Orhan","Wentao Wang","Alex N. Wang","Mengye Ren","Brenden M. Lake"],"pdf_url":"https://arxiv.org/pdf/2402.00300v2.pdf","comment":"Published as a conference paper at CogSci 2024; code & models\n  available from https://github.com/eminorhan/video-models"},{"id":"http://arxiv.org/abs/2406.14549v2","updated":"2024-07-25T14:33:33Z","published":"2024-06-20T17:56:17Z","title":"Uncovering Latent Memories: Assessing Data Leakage and Memorization\n  Patterns in Frontier AI Models","summary":"  Frontier AI systems are making transformative impacts across society, but\nsuch benefits are not without costs: models trained on web-scale datasets\ncontaining personal and private data raise profound concerns about data privacy\nand security. Language models are trained on extensive corpora including\npotentially sensitive or proprietary information, and the risk of data leakage\n- where the model response reveals pieces of such information - remains\ninadequately understood. Prior work has investigated what factors drive\nmemorization and have identified that sequence complexity and the number of\nrepetitions drive memorization. Here, we focus on the evolution of memorization\nover training. We begin by reproducing findings that the probability of\nmemorizing a sequence scales logarithmically with the number of times it is\npresent in the data. We next show that sequences which are apparently not\nmemorized after the first encounter can be \"uncovered\" throughout the course of\ntraining even without subsequent encounters, a phenomenon we term \"latent\nmemorization\". The presence of latent memorization presents a challenge for\ndata privacy as memorized sequences may be hidden at the final checkpoint of\nthe model but remain easily recoverable. To this end, we develop a diagnostic\ntest relying on the cross entropy loss to uncover latent memorized sequences\nwith high accuracy.\n","authors":["Sunny Duan","Mikail Khona","Abhiram Iyer","Rylan Schaeffer","Ila R Fiete"],"pdf_url":"https://arxiv.org/pdf/2406.14549v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.05679v3","updated":"2024-07-25T14:32:51Z","published":"2023-03-10T03:18:03Z","title":"Clustering with minimum spanning trees: How good can it be?","summary":"  Minimum spanning trees (MSTs) provide a convenient representation of datasets\nin numerous pattern recognition activities. Moreover, they are relatively fast\nto compute. In this paper, we quantify the extent to which they are meaningful\nin low-dimensional partitional data clustering tasks. By identifying the upper\nbounds for the agreement between the best (oracle) algorithm and the expert\nlabels from a large battery of benchmark data, we discover that MST methods can\nbe very competitive. Next, we review, study, extend, and generalise a few\nexisting, state-of-the-art MST-based partitioning schemes. This leads to some\nnew noteworthy approaches. Overall, the Genie and the information-theoretic\nmethods often outperform the non-MST algorithms such as K-means, Gaussian\nmixtures, spectral clustering, Birch, density-based, and classical hierarchical\nagglomerative procedures. Nevertheless, we identify that there is still some\nroom for improvement, and thus the development of novel algorithms is\nencouraged.\n","authors":["Marek Gagolewski","Anna Cena","Maciej Bartoszuk","Łukasz Brzozowski"],"pdf_url":"https://arxiv.org/pdf/2303.05679v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2209.02935v4","updated":"2024-07-25T14:31:03Z","published":"2022-09-07T05:08:34Z","title":"Normalised clustering accuracy: An asymmetric external cluster validity\n  measure","summary":"  There is no, nor will there ever be, single best clustering algorithm.\nNevertheless, we would still like to be able to distinguish between methods\nthat work well on certain task types and those that systematically\nunderperform. Clustering algorithms are traditionally evaluated using either\ninternal or external validity measures. Internal measures quantify different\naspects of the obtained partitions, e.g., the average degree of cluster\ncompactness or point separability. However, their validity is questionable\nbecause the clusterings they endorse can sometimes be meaningless. External\nmeasures, on the other hand, compare the algorithms' outputs to fixed ground\ntruth groupings provided by experts. In this paper, we argue that the commonly\nused classical partition similarity scores, such as the normalised mutual\ninformation, Fowlkes-Mallows, or adjusted Rand index, miss some desirable\nproperties. In particular, they do not identify worst-case scenarios correctly,\nnor are they easily interpretable. As a consequence, the evaluation of\nclustering algorithms on diverse benchmark datasets can be difficult. To remedy\nthese issues, we propose and analyse a new measure: a version of the optimal\nset-matching accuracy, which is normalised, monotonic with respect to some\nsimilarity relation, scale-invariant, and corrected for the imbalancedness of\ncluster sizes (but neither symmetric nor adjusted for chance).\n","authors":["Marek Gagolewski"],"pdf_url":"https://arxiv.org/pdf/2209.02935v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.10885v3","updated":"2024-07-25T14:30:22Z","published":"2024-02-16T18:43:02Z","title":"3D Diffuser Actor: Policy Diffusion with 3D Scene Representations","summary":"  Diffusion policies are conditional diffusion models that learn robot action\ndistributions conditioned on the robot and environment state. They have\nrecently shown to outperform both deterministic and alternative action\ndistribution learning formulations. 3D robot policies use 3D scene feature\nrepresentations aggregated from a single or multiple camera views using sensed\ndepth. They have shown to generalize better than their 2D counterparts across\ncamera viewpoints. We unify these two lines of work and present 3D Diffuser\nActor, a neural policy equipped with a novel 3D denoising transformer that\nfuses information from the 3D visual scene, a language instruction and\nproprioception to predict the noise in noised 3D robot pose trajectories. 3D\nDiffuser Actor sets a new state-of-the-art on RLBench with an absolute\nperformance gain of 18.1% over the current SOTA on a multi-view setup and an\nabsolute gain of 13.1% on a single-view setup. On the CALVIN benchmark, it\nimproves over the current SOTA by a 9% relative increase. It also learns to\ncontrol a robot manipulator in the real world from a handful of demonstrations.\nThrough thorough comparisons with the current SOTA policies and ablations of\nour model, we show 3D Diffuser Actor's design choices dramatically outperform\n2D representations, regression and classification objectives, absolute\nattentions, and holistic non-tokenized 3D scene embeddings.\n","authors":["Tsung-Wei Ke","Nikolaos Gkanatsios","Katerina Fragkiadaki"],"pdf_url":"https://arxiv.org/pdf/2402.10885v3.pdf","comment":"First two authors contributed equally"},{"id":"http://arxiv.org/abs/2407.18074v1","updated":"2024-07-25T14:28:58Z","published":"2024-07-25T14:28:58Z","title":"Principal-Agent Reinforcement Learning","summary":"  Contracts are the economic framework which allows a principal to delegate a\ntask to an agent -- despite misaligned interests, and even without directly\nobserving the agent's actions. In many modern reinforcement learning settings,\nself-interested agents learn to perform a multi-stage task delegated to them by\na principal. We explore the significant potential of utilizing contracts to\nincentivize the agents. We model the delegated task as an MDP, and study a\nstochastic game between the principal and agent where the principal learns what\ncontracts to use, and the agent learns an MDP policy in response. We present a\nlearning-based algorithm for optimizing the principal's contracts, which\nprovably converges to the subgame-perfect equilibrium of the principal-agent\ngame. A deep RL implementation allows us to apply our method to very large MDPs\nwith unknown transition dynamics. We extend our approach to multiple agents,\nand demonstrate its relevance to resolving a canonical sequential social\ndilemma with minimal intervention to agent rewards.\n","authors":["Dima Ivanov","Paul Dütting","Inbal Talgam-Cohen","Tonghan Wang","David C. Parkes"],"pdf_url":"https://arxiv.org/pdf/2407.18074v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.18067v1","updated":"2024-07-25T14:21:50Z","published":"2024-07-25T14:21:50Z","title":"HVM-1: Large-scale video models pretrained with nearly 5000 hours of\n  human-like video data","summary":"  We introduce Human-like Video Models (HVM-1), large-scale video models\npretrained with nearly 5000 hours of curated human-like video data (mostly\negocentric, temporally extended, continuous video recordings), using the\nspatiotemporal masked autoencoder (ST-MAE) algorithm. We release two 633M\nparameter models trained at spatial resolutions of 224x224 and 448x448 pixels.\nWe evaluate the performance of these models in downstream few-shot video and\nimage recognition tasks and compare them against a model pretrained with 1330\nhours of short action-oriented video clips from YouTube (Kinetics-700). HVM-1\nmodels perform competitively against the Kinetics-700 pretrained model in\ndownstream evaluations despite substantial qualitative differences between the\nspatiotemporal characteristics of the corresponding pretraining datasets. HVM-1\nmodels also learn more accurate and more robust object representations compared\nto models pretrained with the image-based MAE algorithm on the same data,\ndemonstrating the potential benefits of learning to predict temporal\nregularities in natural videos for learning better object representations.\n","authors":["A. Emin Orhan"],"pdf_url":"https://arxiv.org/pdf/2407.18067v1.pdf","comment":"10 pages, 5 figures, 1 table; code & models available from\n  https://github.com/eminorhan/hvm-1"},{"id":"http://arxiv.org/abs/2407.18066v1","updated":"2024-07-25T14:19:59Z","published":"2024-07-25T14:19:59Z","title":"Multi-Agent Deep Reinforcement Learning for Resilience Optimization in\n  5G RAN","summary":"  Resilience is defined as the ability of a network to resist, adapt, and\nquickly recover from disruptions, and to continue to maintain an acceptable\nlevel of services from users' perspective. With the advent of future radio\nnetworks, including advanced 5G and upcoming 6G, critical services become\nintegral to future networks, requiring uninterrupted service delivery for end\nusers. Unfortunately, with the growing network complexity, user mobility and\ndiversity, it becomes challenging to scale current resilience management\ntechniques that rely on local optimizations to large dense network deployments.\nThis paper aims to address this problem by globally optimizing the resilience\nof a dense multi-cell network based on multi-agent deep reinforcement learning.\nSpecifically, our proposed solution can dynamically tilt cell antennas and\nreconfigure transmit power to mitigate outages and increase both coverage and\nservice availability. A multi-objective optimization problem is formulated to\nsimultaneously satisfy resiliency constraints while maximizing the service\nquality in the network area in order to minimize the impact of outages on\nneighbouring cells. Extensive simulations then demonstrate that with our\nproposed solution, the average service availability in terms of user throughput\ncan be increased by up to 50-60% on average, while reaching a coverage\navailability of 99% in best cases.\n","authors":["Soumeya Kaada","Dinh-Hieu Tran","Nguyen Van Huynh","Marie-Line Alberi Morel","Sofiene Jelassi","Gerardo Rubino"],"pdf_url":"https://arxiv.org/pdf/2407.18066v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07709v2","updated":"2024-07-25T14:17:40Z","published":"2024-06-11T20:44:04Z","title":"Diagnosing and fixing common problems in Bayesian optimization for\n  molecule design","summary":"  Bayesian optimization (BO) is a principled approach to molecular design\ntasks. In this paper we explain three pitfalls of BO which can cause poor\nempirical performance: an incorrect prior width, over-smoothing, and inadequate\nacquisition function maximization. We show that with these issues addressed,\neven a basic BO setup is able to achieve the highest overall performance on the\nPMO benchmark for molecule design (Gao et al 2022). These results suggest that\nBO may benefit from more attention in the machine learning for molecules\ncommunity.\n","authors":["Austin Tripp","José Miguel Hernández-Lobato"],"pdf_url":"https://arxiv.org/pdf/2406.07709v2.pdf","comment":"8 pages, 4 figures. ICML 2024 AI for science workshop\n  (https://openreview.net/forum?id=V4aG4wsoIt). Code at:\n  https://github.com/AustinT/basic-mol-bo-workshop2024"},{"id":"http://arxiv.org/abs/2407.18060v1","updated":"2024-07-25T14:16:02Z","published":"2024-07-25T14:16:02Z","title":"Cross-Vendor Reproducibility of Radiomics-based Machine Learning Models\n  for Computer-aided Diagnosis","summary":"  Background: The reproducibility of machine-learning models in prostate cancer\ndetection across different MRI vendors remains a significant challenge.\nMethods: This study investigates Support Vector Machines (SVM) and Random\nForest (RF) models trained on radiomic features extracted from T2-weighted MRI\nimages using Pyradiomics and MRCradiomics libraries. Feature selection was\nperformed using the maximum relevance minimum redundancy (MRMR) technique. We\naimed to enhance clinical decision support through multimodal learning and\nfeature fusion. Results: Our SVM model, utilizing combined features from\nPyradiomics and MRCradiomics, achieved an AUC of 0.74 on the Multi-Improd\ndataset (Siemens scanner) but decreased to 0.60 on the Philips test set. The RF\nmodel showed similar trends, with notable robustness for models using\nPyradiomics features alone (AUC of 0.78 on Philips). Conclusions: These\nfindings demonstrate the potential of multimodal feature integration to improve\nthe robustness and generalizability of machine-learning models for clinical\ndecision support in prostate cancer detection. This study marks a significant\nstep towards developing reliable AI-driven diagnostic tools that maintain\nefficacy across various imaging platforms.\n","authors":["Jatin Chaudhary","Ivan Jambor","Hannu Aronen","Otto Ettala","Jani Saunavaara","Peter Boström","Jukka Heikkonen","Rajeev Kanth","Harri Merisaari"],"pdf_url":"https://arxiv.org/pdf/2407.18060v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.18058v1","updated":"2024-07-25T14:15:05Z","published":"2024-07-25T14:15:05Z","title":"I can listen but cannot read: An evaluation of two-tower multimodal\n  systems for instrument recognition","summary":"  Music two-tower multimodal systems integrate audio and text modalities into a\njoint audio-text space, enabling direct comparison between songs and their\ncorresponding labels. These systems enable new approaches for classification\nand retrieval, leveraging both modalities. Despite the promising results they\nhave shown for zero-shot classification and retrieval tasks, closer inspection\nof the embeddings is needed. This paper evaluates the inherent zero-shot\nproperties of joint audio-text spaces for the case-study of instrument\nrecognition. We present an evaluation and analysis of two-tower systems for\nzero-shot instrument recognition and a detailed analysis of the properties of\nthe pre-joint and joint embeddings spaces. Our findings suggest that audio\nencoders alone demonstrate good quality, while challenges remain within the\ntext encoder or joint space projection. Specifically, two-tower systems exhibit\nsensitivity towards specific words, favoring generic prompts over musically\ninformed ones. Despite the large size of textual encoders, they do not yet\nleverage additional textual context or infer instruments accurately from their\ndescriptions. Lastly, a novel approach for quantifying the semantic\nmeaningfulness of the textual space leveraging an instrument ontology is\nproposed. This method reveals deficiencies in the systems' understanding of\ninstruments and provides evidence of the need for fine-tuning text encoders on\nmusical data.\n","authors":["Yannis Vasilakis","Rachel Bittner","Johan Pauwels"],"pdf_url":"https://arxiv.org/pdf/2407.18058v1.pdf","comment":"Accepted to ISMIR 2024"},{"id":"http://arxiv.org/abs/2407.18057v1","updated":"2024-07-25T14:10:42Z","published":"2024-07-25T14:10:42Z","title":"Physics-informed nonlinear vector autoregressive models for the\n  prediction of dynamical systems","summary":"  Machine learning techniques have recently been of great interest for solving\ndifferential equations. Training these models is classically a data-fitting\ntask, but knowledge of the expression of the differential equation can be used\nto supplement the training objective, leading to the development of\nphysics-informed scientific machine learning. In this article, we focus on one\nclass of models called nonlinear vector autoregression (NVAR) to solve ordinary\ndifferential equations (ODEs). Motivated by connections to numerical\nintegration and physics-informed neural networks, we explicitly derive the\nphysics-informed NVAR (piNVAR) which enforces the right-hand side of the\nunderlying differential equation regardless of NVAR construction. Because NVAR\nand piNVAR completely share their learned parameters, we propose an augmented\nprocedure to jointly train the two models. Then, using both data-driven and\nODE-driven metrics, we evaluate the ability of the piNVAR model to predict\nsolutions to various ODE systems, such as the undamped spring, a Lotka-Volterra\npredator-prey nonlinear model, and the chaotic Lorenz system.\n","authors":["James H. Adler","Samuel Hocking","Xiaozhe Hu","Shafiqul Islam"],"pdf_url":"https://arxiv.org/pdf/2407.18057v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.18044v1","updated":"2024-07-25T13:47:01Z","published":"2024-07-25T13:47:01Z","title":"The Geometry of Queries: Query-Based Innovations in Retrieval-Augmented\n  Generation","summary":"  Digital health chatbots powered by Large Language Models (LLMs) have the\npotential to significantly improve personal health management for chronic\nconditions by providing accessible and on-demand health coaching and\nquestion-answering. However, these chatbots risk providing unverified and\ninaccurate information because LLMs generate responses based on patterns\nlearned from diverse internet data. Retrieval Augmented Generation (RAG) can\nhelp mitigate hallucinations and inaccuracies in LLM responses by grounding it\non reliable content. However, efficiently and accurately retrieving most\nrelevant set of content for real-time user questions remains a challenge. In\nthis work, we introduce Query-Based Retrieval Augmented Generation (QB-RAG), a\nnovel approach that pre-computes a database of potential queries from a content\nbase using LLMs. For an incoming patient question, QB-RAG efficiently matches\nit against this pre-generated query database using vector search, improving\nalignment between user questions and the content. We establish a theoretical\nfoundation for QB-RAG and provide a comparative analysis of existing retrieval\nenhancement techniques for RAG systems. Finally, our empirical evaluation\ndemonstrates that QB-RAG significantly improves the accuracy of healthcare\nquestion answering, paving the way for robust and trustworthy LLM applications\nin digital health.\n","authors":["Eric Yang","Jonathan Amar","Jong Ha Lee","Bhawesh Kumar","Yugang Jia"],"pdf_url":"https://arxiv.org/pdf/2407.18044v1.pdf","comment":"22 pages"},{"id":"http://arxiv.org/abs/2407.18042v1","updated":"2024-07-25T13:44:42Z","published":"2024-07-25T13:44:42Z","title":"Lifelong Graph Summarization with Neural Networks: 2012, 2022, and a\n  Time Warp","summary":"  Summarizing web graphs is challenging due to the heterogeneity of the modeled\ninformation and its changes over time. We investigate the use of neural\nnetworks for lifelong graph summarization. Assuming we observe the web graph at\na certain time, we train the networks to summarize graph vertices. We apply\nthis trained network to summarize the vertices of the changed graph at the next\npoint in time. Subsequently, we continue training and evaluating the network to\nperform lifelong graph summarization. We use the GNNs Graph-MLP and GraphSAINT,\nas well as an MLP baseline, to summarize the temporal graphs. We compare\n$1$-hop and $2$-hop summaries. We investigate the impact of reusing parameters\nfrom a previous snapshot by measuring the backward and forward transfer and the\nforgetting rate of the neural networks. Our extensive experiments on ten weekly\nsnapshots of a web graph with over $100$M edges, sampled in 2012 and 2022, show\nthat all networks predominantly use $1$-hop information to determine the\nsummary, even when performing $2$-hop summarization. Due to the heterogeneity\nof web graphs, in some snapshots, the $2$-hop summary produces over ten times\nmore vertex summaries than the $1$-hop summary. When using the network trained\non the last snapshot from 2012 and applying it to the first snapshot of 2022,\nwe observe a strong drop in accuracy. We attribute this drop over the ten-year\ntime warp to the strongly increased heterogeneity of the web graph in 2022.\n","authors":["Jonatan Frank","Marcel Hoffmann","Nicolas Lell","David Richerby","Ansgar Scherp"],"pdf_url":"https://arxiv.org/pdf/2407.18042v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.18041v1","updated":"2024-07-25T13:39:11Z","published":"2024-07-25T13:39:11Z","title":"How to Train the Teacher Model for Effective Knowledge Distillation","summary":"  Recently, it was shown that the role of the teacher in knowledge distillation\n(KD) is to provide the student with an estimate of the true Bayes conditional\nprobability density (BCPD). Notably, the new findings propose that the\nstudent's error rate can be upper-bounded by the mean squared error (MSE)\nbetween the teacher's output and BCPD. Consequently, to enhance KD efficacy,\nthe teacher should be trained such that its output is close to BCPD in MSE\nsense. This paper elucidates that training the teacher model with MSE loss\nequates to minimizing the MSE between its output and BCPD, aligning with its\ncore responsibility of providing the student with a BCPD estimate closely\nresembling it in MSE terms. In this respect, through a comprehensive set of\nexperiments, we demonstrate that substituting the conventional teacher trained\nwith cross-entropy loss with one trained using MSE loss in state-of-the-art KD\nmethods consistently boosts the student's accuracy, resulting in improvements\nof up to 2.6\\%.\n","authors":["Shayan Mohajer Hamidi","Xizhen Deng","Renhao Tan","Linfeng Ye","Ahmed Hussein Salamah"],"pdf_url":"https://arxiv.org/pdf/2407.18041v1.pdf","comment":"The paper was accepted at ECCV2024"},{"id":"http://arxiv.org/abs/2402.05981v2","updated":"2024-07-25T13:37:16Z","published":"2024-02-08T08:02:57Z","title":"Anatomizing Deep Learning Inference in Web Browsers","summary":"  Web applications have increasingly adopted Deep Learning (DL) through\nin-browser inference, wherein DL inference performs directly within Web\nbrowsers. The actual performance of in-browser inference and its impacts on the\nquality of experience (QoE) remain unexplored, and urgently require new QoE\nmeasurements beyond traditional ones, e.g., mainly focusing on page load time.\nTo bridge this gap, we make the first comprehensive performance measurement of\nin-browser inference to date. Our approach proposes new metrics to measure\nin-browser inference: responsiveness, smoothness, and inference accuracy. Our\nextensive analysis involves 9 representative DL models across Web browsers of\n50 popular PC devices and 20 mobile devices. The results reveal that in-browser\ninference exhibits a substantial latency gap, averaging 16.9 times slower on\nCPU and 4.9 times slower on GPU compared to native inference on PC devices. The\ngap on mobile CPU and mobile GPU is 15.8 times and 7.8 times, respectively.\nFurthermore, we identify contributing factors to such latency gap, including\nunderutilized hardware instruction sets, inherent overhead in the runtime\nenvironment, resource contention within the browser, and inefficiencies in\nsoftware libraries and GPU abstractions. Additionally, in-browser inference\nimposes significant memory demands, at times exceeding 334.6 times the size of\nthe DL models themselves, partly attributable to suboptimal memory management.\nWe also observe that in-browser inference leads to a significant 67.2% increase\nin the time it takes for GUI components to render within Web browsers,\nsignificantly affecting the overall user QoE of Web applications reliant on\nthis technology\n","authors":["Qipeng Wang","Shiqi Jiang","Zhenpeng Chen","Xu Cao","Yuanchun Li","Aoyu Li","Yun Ma","Ting Cao","Xuanzhe Liu"],"pdf_url":"https://arxiv.org/pdf/2402.05981v2.pdf","comment":"Accepted by ACM Transactions on Software Engineering and Methodology\n  (TOSEM)"},{"id":"http://arxiv.org/abs/2407.18039v1","updated":"2024-07-25T13:36:42Z","published":"2024-07-25T13:36:42Z","title":"Peak-Controlled Logits Poisoning Attack in Federated Distillation","summary":"  Federated Distillation (FD) offers an innovative approach to distributed\nmachine learning, leveraging knowledge distillation for efficient and flexible\ncross-device knowledge transfer without necessitating the upload of extensive\nmodel parameters to a central server. While FD has gained popularity, its\nvulnerability to poisoning attacks remains underexplored. To address this gap,\nwe previously introduced FDLA (Federated Distillation Logits Attack), a method\nthat manipulates logits communication to mislead and degrade the performance of\nclient models. However, the impact of FDLA on participants with different\nidentities and the effects of malicious modifications at various stages of\nknowledge transfer remain unexplored. To this end, we present PCFDLA\n(Peak-Controlled Federated Distillation Logits Attack), an advanced and more\nstealthy logits poisoning attack method for FD. PCFDLA enhances the\neffectiveness of FDLA by carefully controlling the peak values of logits to\ncreate highly misleading yet inconspicuous modifications. Furthermore, we\nintroduce a novel metric for better evaluating attack efficacy, demonstrating\nthat PCFDLA maintains stealth while being significantly more disruptive to\nvictim models compared to its predecessors. Experimental results across various\ndatasets confirm the superior impact of PCFDLA on model accuracy, solidifying\nits potential threat in federated distillation systems.\n","authors":["Yuhan Tang","Aoxu Zhang","Zhiyuan Wu","Bo Gao","Tian Wen","Yuwei Wang","Sheng Sun"],"pdf_url":"https://arxiv.org/pdf/2407.18039v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2401.03685"},{"id":"http://arxiv.org/abs/2407.18033v1","updated":"2024-07-25T13:27:10Z","published":"2024-07-25T13:27:10Z","title":"ECG Arrhythmia Detection Using Disease-specific Attention-based Deep\n  Learning Model","summary":"  The electrocardiogram (ECG) is one of the most commonly-used tools to\ndiagnose cardiovascular disease in clinical practice. Although deep learning\nmodels have achieved very impressive success in the field of automatic ECG\nanalysis, they often lack model interpretability that is significantly\nimportant in the healthcare applications. To this end, many schemes such as\ngeneral-purpose attention mechanism, Grad-CAM technique and ECG knowledge graph\nwere proposed to be integrated with deep learning models. However, they either\nresult in decreased classification performance or do not consist with the one\nin cardiologists' mind when interpreting ECG. In this study, we propose a novel\ndisease-specific attention-based deep learning model (DANet) for arrhythmia\ndetection from short ECG recordings. The novel idea is to introduce a\nsoft-coding or hard-coding waveform enhanced module into existing deep neural\nnetworks, which amends original ECG signals with the guidance of the rule for\ndiagnosis of a given disease type before being fed into the classification\nmodule. For the soft-coding DANet, we also develop a learning framework\ncombining self-supervised pre-training with two-stage supervised training. To\nverify the effectiveness of our proposed DANet, we applied it to the problem of\natrial premature contraction detection and the experimental results shows that\nit demonstrates superior performance compared to the benchmark model. Moreover,\nit also provides the waveform regions that deserve special attention in the\nmodel's decision-making process, allowing it to be a medical diagnostic\nassistant for physicians.\n","authors":["Linpeng Jin"],"pdf_url":"https://arxiv.org/pdf/2407.18033v1.pdf","comment":"11 pages, 5 figures"},{"id":"http://arxiv.org/abs/2407.18022v1","updated":"2024-07-25T13:15:25Z","published":"2024-07-25T13:15:25Z","title":"Learning mental states estimation through self-observation: a\n  developmental synergy between intentions and beliefs representations in a\n  deep-learning model of Theory of Mind","summary":"  Theory of Mind (ToM), the ability to attribute beliefs, intentions, or mental\nstates to others, is a crucial feature of human social interaction. In complex\nenvironments, where the human sensory system reaches its limits, behaviour is\nstrongly driven by our beliefs about the state of the world around us.\nAccessing others' mental states, e.g., beliefs and intentions, allows for more\neffective social interactions in natural contexts. Yet, these variables are not\ndirectly observable, making understanding ToM a challenging quest of interest\nfor different fields, including psychology, machine learning and robotics. In\nthis paper, we contribute to this topic by showing a developmental synergy\nbetween learning to predict low-level mental states (e.g., intentions, goals)\nand attributing high-level ones (i.e., beliefs). Specifically, we assume that\nlearning beliefs attribution can occur by observing one's own decision\nprocesses involving beliefs, e.g., in a partially observable environment. Using\na simple feed-forward deep learning model, we show that, when learning to\npredict others' intentions and actions, more accurate predictions can be\nacquired earlier if beliefs attribution is learnt simultaneously. Furthermore,\nwe show that the learning performance improves even when observed actors have a\ndifferent embodiment than the observer and the gain is higher when observing\nbeliefs-driven chunks of behaviour. We propose that our computational approach\ncan inform the understanding of human social cognitive development and be\nrelevant for the design of future adaptive social robots able to autonomously\nunderstand, assist, and learn from human interaction partners in novel natural\nenvironments and tasks.\n","authors":["Francesca Bianco","Silvia Rigato","Maria Laura Filippetti","Dimitri Ognibene"],"pdf_url":"https://arxiv.org/pdf/2407.18022v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.18021v1","updated":"2024-07-25T13:15:16Z","published":"2024-07-25T13:15:16Z","title":"Quadratic Advantage with Quantum Randomized Smoothing Applied to\n  Time-Series Analysis","summary":"  As quantum machine learning continues to develop at a rapid pace, the\nimportance of ensuring the robustness and efficiency of quantum algorithms\ncannot be overstated. Our research presents an analysis of quantum randomized\nsmoothing, how data encoding and perturbation modeling approaches can be\nmatched to achieve meaningful robustness certificates. By utilizing an\ninnovative approach integrating Grover's algorithm, a quadratic sampling\nadvantage over classical randomized smoothing is achieved. This strategy\nnecessitates a basis state encoding, thus restricting the space of meaningful\nperturbations. We show how constrained $k$-distant Hamming weight perturbations\nare a suitable noise distribution here, and elucidate how they can be\nconstructed on a quantum computer. The efficacy of the proposed framework is\ndemonstrated on a time series classification task employing a Bag-of-Words\npre-processing solution. The advantage of quadratic sample reduction is\nrecovered especially in the regime with large number of samples. This may allow\nquantum computers to efficiently scale randomized smoothing to more complex\ntasks beyond the reach of classical methods.\n","authors":["Nicola Franco","Marie Kempkes","Jakob Spiegelberg","Jeanette Miriam Lorenz"],"pdf_url":"https://arxiv.org/pdf/2407.18021v1.pdf","comment":"Accepted at the IEEE International Conference on Quantum Computing\n  and Engineering (QCE)"},{"id":"http://arxiv.org/abs/2406.19146v2","updated":"2024-07-25T13:09:18Z","published":"2024-06-27T13:02:43Z","title":"Resolving Discrepancies in Compute-Optimal Scaling of Language Models","summary":"  Kaplan et al. and Hoffmann et al. developed influential scaling laws for the\noptimal model size as a function of the compute budget, but these laws yield\nsubstantially different predictions. We explain the discrepancy by reproducing\nthe Kaplan scaling law on two datasets (OpenWebText2 and RefinedWeb) and\nidentifying three factors causing the difference: last layer computational\ncost, warmup duration, and scale-dependent optimizer tuning. With these factors\ncorrected, we obtain excellent agreement with the Hoffmann et al. (i.e.,\n\"Chinchilla\") scaling law. Counter to a hypothesis of Hoffmann et al., we find\nthat careful learning rate decay is not essential for the validity of their\nscaling law. As a secondary result, we derive scaling laws for the optimal\nlearning rate and batch size, finding that tuning the AdamW $\\beta_2$ parameter\nis essential at lower batch sizes.\n","authors":["Tomer Porian","Mitchell Wortsman","Jenia Jitsev","Ludwig Schmidt","Yair Carmon"],"pdf_url":"https://arxiv.org/pdf/2406.19146v2.pdf","comment":"Fixing bug in small models with tuned LR"},{"id":"http://arxiv.org/abs/2407.18013v1","updated":"2024-07-25T13:06:30Z","published":"2024-07-25T13:06:30Z","title":"Self-Supervision Improves Diffusion Models for Tabular Data Imputation","summary":"  The ubiquity of missing data has sparked considerable attention and focus on\ntabular data imputation methods. Diffusion models, recognized as the\ncutting-edge technique for data generation, demonstrate significant potential\nin tabular data imputation tasks. However, in pursuit of diversity, vanilla\ndiffusion models often exhibit sensitivity to initialized noises, which hinders\nthe models from generating stable and accurate imputation results.\nAdditionally, the sparsity inherent in tabular data poses challenges for\ndiffusion models in accurately modeling the data manifold, impacting the\nrobustness of these models for data imputation. To tackle these challenges,\nthis paper introduces an advanced diffusion model named Self-supervised\nimputation Diffusion Model (SimpDM for brevity), specifically tailored for\ntabular data imputation tasks. To mitigate sensitivity to noise, we introduce a\nself-supervised alignment mechanism that aims to regularize the model, ensuring\nconsistent and stable imputation predictions. Furthermore, we introduce a\ncarefully devised state-dependent data augmentation strategy within SimpDM,\nenhancing the robustness of the diffusion model when dealing with limited data.\nExtensive experiments demonstrate that SimpDM matches or outperforms\nstate-of-the-art imputation methods across various scenarios.\n","authors":["Yixin Liu","Thalaiyasingam Ajanthan","Hisham Husain","Vu Nguyen"],"pdf_url":"https://arxiv.org/pdf/2407.18013v1.pdf","comment":"10 pages, 5 figures. Accepted by CIKM 2024"},{"id":"http://arxiv.org/abs/2201.07794v5","updated":"2024-07-25T13:05:25Z","published":"2022-01-18T23:31:06Z","title":"A Non-Expert's Introduction to Data Ethics for Mathematicians","summary":"  I give a short introduction to data ethics. I begin with some background\ninformation and societal context for data ethics. I then discuss data ethics in\nmathematical-science education and indicate some available course material. I\nbriefly highlight a few efforts -- at my home institution and elsewhere -- on\ndata ethics, society, and social good. I then discuss open data in research,\nresearch replicability and some other ethical issues in research, and the\ntension between privacy and open data and code, and a few controversial studies\nand reactions to studies. I then discuss ethical principles, institutional\nreview boards, and a few other considerations in the scientific use of human\ndata. I then briefly survey a variety of research and lay articles that are\nrelevant to data ethics and data privacy. I conclude with a brief summary and\nsome closing remarks.\n  My focal audience is mathematicians, but I hope that this chapter will also\nbe useful to others. I am not an expert about data ethics, and this chapter\nprovides only a starting point on this wide-ranging topic. I encourage you to\nexamine the resources that I discuss and to reflect carefully on data ethics,\nits role in mathematics education, and the societal implications of data and\ndata analysis. As data and technology continue to evolve, I hope that such\ncareful reflection will continue throughout your life.\n","authors":["Mason A. Porter"],"pdf_url":"https://arxiv.org/pdf/2201.07794v5.pdf","comment":"A few more small tweaks. This is a book chapter. It is associated\n  with my data-ethics lecture at the 2021 AMS Short Course on Mathematical and\n  Computational Methods for Complex Social Systems"},{"id":"http://arxiv.org/abs/2407.18011v1","updated":"2024-07-25T13:05:00Z","published":"2024-07-25T13:05:00Z","title":"HANNA: Hard-constraint Neural Network for Consistent Activity\n  Coefficient Prediction","summary":"  We present the first hard-constraint neural network for predicting activity\ncoefficients (HANNA), a thermodynamic mixture property that is the basis for\nmany applications in science and engineering. Unlike traditional neural\nnetworks, which ignore physical laws and result in inconsistent predictions,\nour model is designed to strictly adhere to all thermodynamic consistency\ncriteria. By leveraging deep-set neural networks, HANNA maintains symmetry\nunder the permutation of the components. Furthermore, by hard-coding physical\nconstraints in the network architecture, we ensure consistency with the\nGibbs-Duhem equation and in modeling the pure components. The model was trained\nand evaluated on 317,421 data points for activity coefficients in binary\nmixtures from the Dortmund Data Bank, achieving significantly higher prediction\naccuracies than the current state-of-the-art model UNIFAC. Moreover, HANNA only\nrequires the SMILES of the components as input, making it applicable to any\nbinary mixture of interest. HANNA is fully open-source and available for free\nuse.\n","authors":["Thomas Specht","Mayank Nagda","Sophie Fellenz","Stephan Mandt","Hans Hasse","Fabian Jirasek"],"pdf_url":"https://arxiv.org/pdf/2407.18011v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12856v2","updated":"2024-07-25T12:56:35Z","published":"2024-03-19T16:01:25Z","title":"Equivariant Ensembles and Regularization for Reinforcement Learning in\n  Map-based Path Planning","summary":"  In reinforcement learning (RL), exploiting environmental symmetries can\nsignificantly enhance efficiency, robustness, and performance. However,\nensuring that the deep RL policy and value networks are respectively\nequivariant and invariant to exploit these symmetries is a substantial\nchallenge. Related works try to design networks that are equivariant and\ninvariant by construction, limiting them to a very restricted library of\ncomponents, which in turn hampers the expressiveness of the networks. This\npaper proposes a method to construct equivariant policies and invariant value\nfunctions without specialized neural network components, which we term\nequivariant ensembles. We further add a regularization term for adding\ninductive bias during training. In a map-based path planning case study, we\nshow how equivariant ensembles and regularization benefit sample efficiency and\nperformance.\n","authors":["Mirco Theile","Hongpeng Cao","Marco Caccamo","Alberto L. Sangiovanni-Vincentelli"],"pdf_url":"https://arxiv.org/pdf/2403.12856v2.pdf","comment":"Accepted at IROS 2024. A video can be found here:\n  https://youtu.be/L6NOdvU7n7s. The code is available at\n  https://github.com/theilem/uavSim"},{"id":"http://arxiv.org/abs/2407.18002v1","updated":"2024-07-25T12:53:21Z","published":"2024-07-25T12:53:21Z","title":"Network Inversion of Convolutional Neural Nets","summary":"  Neural networks have emerged as powerful tools across various applications,\nyet their decision-making process often remains opaque, leading to them being\nperceived as \"black boxes.\" This opacity raises concerns about their\ninterpretability and reliability, especially in safety-critical scenarios.\nNetwork inversion techniques offer a solution by allowing us to peek inside\nthese black boxes, revealing the features and patterns learned by the networks\nbehind their decision-making processes and thereby provide valuable insights\ninto how neural networks arrive at their conclusions, making them more\ninterpretable and trustworthy. This paper presents a simple yet effective\napproach to network inversion using a carefully conditioned generator that\nlearns the data distribution in the input space of the trained neural network,\nenabling the reconstruction of inputs that would most likely lead to the\ndesired outputs. To capture the diversity in the input space for a given\noutput, instead of simply revealing the conditioning labels to the generator,\nwe hideously encode the conditioning label information into vectors, further\nexemplified by heavy dropout in the generation process and minimisation of\ncosine similarity between the features corresponding to the generated images.\nThe paper concludes with immediate applications of Network Inversion including\nin interpretability, explainability and generation of adversarial samples.\n","authors":["Pirzada Suhail","Amit Sethi"],"pdf_url":"https://arxiv.org/pdf/2407.18002v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17999v1","updated":"2024-07-25T12:48:56Z","published":"2024-07-25T12:48:56Z","title":"Lightweight Industrial Cohorted Federated Learning for Heterogeneous\n  Assets","summary":"  Federated Learning (FL) is the most widely adopted collaborative learning\napproach for training decentralized Machine Learning (ML) models by exchanging\nlearning between clients without sharing the data and compromising privacy.\nHowever, since great data similarity or homogeneity is taken for granted in all\nFL tasks, FL is still not specifically designed for the industrial setting.\nRarely this is the case in industrial data because there are differences in\nmachine type, firmware version, operational conditions, environmental factors,\nand hence, data distribution. Albeit its popularity, it has been observed that\nFL performance degrades if the clients have heterogeneous data distributions.\nTherefore, we propose a Lightweight Industrial Cohorted FL (LICFL) algorithm\nthat uses model parameters for cohorting without any additional on-edge\n(clientlevel) computations and communications than standard FL and mitigates\nthe shortcomings from data heterogeneity in industrial applications. Our\napproach enhances client-level model performance by allowing them to\ncollaborate with similar clients and train more specialized or personalized\nmodels. Also, we propose an adaptive aggregation algorithm that extends the\nLICFL to Adaptive LICFL (ALICFL) for further improving the global model\nperformance and speeding up the convergence. Through numerical experiments on\nreal-time data, we demonstrate the efficacy of the proposed algorithms and\ncompare the performance with existing approaches.\n","authors":["Madapu Amarlingam","Abhishek Wani","Adarsh NL"],"pdf_url":"https://arxiv.org/pdf/2407.17999v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17998v1","updated":"2024-07-25T12:48:41Z","published":"2024-07-25T12:48:41Z","title":"iNNspector: Visual, Interactive Deep Model Debugging","summary":"  Deep learning model design, development, and debugging is a process driven by\nbest practices, guidelines, trial-and-error, and the personal experiences of\nmodel developers. At multiple stages of this process, performance and internal\nmodel data can be logged and made available. However, due to the sheer\ncomplexity and scale of this data and process, model developers often resort to\nevaluating their model performance based on abstract metrics like accuracy and\nloss. We argue that a structured analysis of data along the model's\narchitecture and at multiple abstraction levels can considerably streamline the\ndebugging process. Such a systematic analysis can further connect the\ndeveloper's design choices to their impacts on the model behavior, facilitating\nthe understanding, diagnosis, and refinement of deep learning models. Hence, in\nthis paper, we (1) contribute a conceptual framework structuring the data space\nof deep learning experiments. Our framework, grounded in literature analysis\nand requirements interviews, captures design dimensions and proposes mechanisms\nto make this data explorable and tractable. To operationalize our framework in\na ready-to-use application, we (2) present the iNNspector system. iNNspector\nenables tracking of deep learning experiments and provides interactive\nvisualizations of the data on all levels of abstraction from multiple models to\nindividual neurons. Finally, we (3) evaluate our approach with three real-world\nuse-cases and a user study with deep learning developers and data analysts,\nproving its effectiveness and usability.\n","authors":["Thilo Spinner","Daniel Fürst","Mennatallah El-Assady"],"pdf_url":"https://arxiv.org/pdf/2407.17998v1.pdf","comment":"41 pages paper, 4 pages references, 3 pages appendix, 19 figures, 2\n  tables"},{"id":"http://arxiv.org/abs/2407.17997v1","updated":"2024-07-25T12:44:45Z","published":"2024-07-25T12:44:45Z","title":"On the Effect of Purely Synthetic Training Data for Different Automatic\n  Speech Recognition Architectures","summary":"  In this work we evaluate the utility of synthetic data for training automatic\nspeech recognition (ASR). We use the ASR training data to train a\ntext-to-speech (TTS) system similar to FastSpeech-2. With this TTS we reproduce\nthe original training data, training ASR systems solely on synthetic data. For\nASR, we use three different architectures, attention-based encoder-decoder,\nhybrid deep neural network hidden Markov model and a Gaussian mixture hidden\nMarkov model, showing the different sensitivity of the models to synthetic data\ngeneration. In order to extend previous work, we present a number of ablation\nstudies on the effectiveness of synthetic vs. real training data for ASR. In\nparticular we focus on how the gap between training on synthetic and real data\nchanges by varying the speaker embedding or by scaling the model size. For the\nlatter we show that the TTS models generalize well, even when training scores\nindicate overfitting.\n","authors":["Nick Rossenbach","Benedikt Hilmes","Ralf Schlüter"],"pdf_url":"https://arxiv.org/pdf/2407.17997v1.pdf","comment":"Accepted at the SynData4GenAI 2024 workshop"},{"id":"http://arxiv.org/abs/2407.17992v1","updated":"2024-07-25T12:38:08Z","published":"2024-07-25T12:38:08Z","title":"Amortized Active Learning for Nonparametric Functions","summary":"  Active learning (AL) is a sequential learning scheme aiming to select the\nmost informative data. AL reduces data consumption and avoids the cost of\nlabeling large amounts of data. However, AL trains the model and solves an\nacquisition optimization for each selection. It becomes expensive when the\nmodel training or acquisition optimization is challenging. In this paper, we\nfocus on active nonparametric function learning, where the gold standard\nGaussian process (GP) approaches suffer from cubic time complexity. We propose\nan amortized AL method, where new data are suggested by a neural network which\nis trained up-front without any real data (Figure 1). Our method avoids\nrepeated model training and requires no acquisition optimization during the AL\ndeployment. We (i) utilize GPs as function priors to construct an AL simulator,\n(ii) train an AL policy that can zero-shot generalize from simulation to real\nlearning problems of nonparametric functions and (iii) achieve real-time data\nselection and comparable learning performances to time-consuming baseline\nmethods.\n","authors":["Cen-You Li","Marc Toussaint","Barbara Rakitsch","Christoph Zimmer"],"pdf_url":"https://arxiv.org/pdf/2407.17992v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08210v2","updated":"2024-07-25T12:23:26Z","published":"2024-06-12T13:41:07Z","title":"Expressivity and Generalization: Fragment-Biases for Molecular GNNs","summary":"  Although recent advances in higher-order Graph Neural Networks (GNNs) improve\nthe theoretical expressiveness and molecular property predictive performance,\nthey often fall short of the empirical performance of models that explicitly\nuse fragment information as inductive bias. However, for these approaches,\nthere exists no theoretic expressivity study. In this work, we propose the\nFragment-WL test, an extension to the well-known Weisfeiler & Leman (WL) test,\nwhich enables the theoretic analysis of these fragment-biased GNNs. Building on\nthe insights gained from the Fragment-WL test, we develop a new GNN\narchitecture and a fragmentation with infinite vocabulary that significantly\nboosts expressiveness. We show the effectiveness of our model on synthetic and\nreal-world data where we outperform all GNNs on Peptides and have 12% lower\nerror than all GNNs on ZINC and 34% lower error than other fragment-biased\nmodels. Furthermore, we show that our model exhibits superior generalization\ncapabilities compared to the latest transformer-based architectures,\npositioning it as a robust solution for a range of molecular modeling tasks.\n","authors":["Tom Wollschläger","Niklas Kemper","Leon Hetzel","Johanna Sommer","Stephan Günnemann"],"pdf_url":"https://arxiv.org/pdf/2406.08210v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17436v3","updated":"2024-07-25T11:51:04Z","published":"2024-03-26T07:05:06Z","title":"Particle identification with machine learning from incomplete data in\n  the ALICE experiment","summary":"  The ALICE experiment at the LHC measures properties of the strongly\ninteracting matter formed in ultrarelativistic heavy-ion collisions. Such\nstudies require accurate particle identification (PID). ALICE provides PID\ninformation via several detectors for particles with momentum from about 100\nMeV/c up to 20 GeV/c. Traditionally, particles are selected with rectangular\ncuts. A much better performance can be achieved with machine learning (ML)\nmethods. Our solution uses multiple neural networks (NN) serving as binary\nclassifiers. Moreover, we extended our particle classifier with Feature Set\nEmbedding and attention in order to train on data with incomplete samples. We\nalso present the integration of the ML project with the ALICE analysis\nsoftware, and we discuss domain adaptation, the ML technique needed to transfer\nthe knowledge between simulated and real experimental data.\n","authors":["Maja Karwowska","Łukasz Graczykowski","Kamil Deja","Miłosz Kasak","Małgorzata Janik"],"pdf_url":"https://arxiv.org/pdf/2403.17436v3.pdf","comment":"Proceedings of 3rd Artificial Intelligence for the Electron-Ion\n  Collider workshop -- AI4EIC2023, 28.11-1.12.2023"},{"id":"http://arxiv.org/abs/2308.12112v4","updated":"2024-07-25T11:49:54Z","published":"2023-08-23T13:02:52Z","title":"Category Adaptation Meets Projected Distillation in Generalized\n  Continual Category Discovery","summary":"  Generalized Continual Category Discovery (GCCD) tackles learning from\nsequentially arriving, partially labeled datasets while uncovering new\ncategories. Traditional methods depend on feature distillation to prevent\nforgetting the old knowledge. However, this strategy restricts the model's\nability to adapt and effectively distinguish new categories. To address this,\nwe introduce a novel technique integrating a learnable projector with feature\ndistillation, thus enhancing model adaptability without sacrificing past\nknowledge. The resulting distribution shift of the previously learned\ncategories is mitigated with the auxiliary category adaptation network. We\ndemonstrate that while each component offers modest benefits individually,\ntheir combination - dubbed CAMP (Category Adaptation Meets Projected\ndistillation) - significantly improves the balance between learning new\ninformation and retaining old. CAMP exhibits superior performance across\nseveral GCCD and Class Incremental Learning scenarios. The code is available at\nhttps://github.com/grypesc/CAMP.\n","authors":["Grzegorz Rypeść","Daniel Marczak","Sebastian Cygert","Tomasz Trzciński","Bartłomiej Twardowski"],"pdf_url":"https://arxiv.org/pdf/2308.12112v4.pdf","comment":"Accepted for ECCV 2024"},{"id":"http://arxiv.org/abs/2404.00725v2","updated":"2024-07-25T11:37:54Z","published":"2024-03-31T15:55:49Z","title":"The Larger the Better? Improved LLM Code-Generation via Budget\n  Reallocation","summary":"  It is a common belief that large language models (LLMs) are better than\nsmaller-sized ones. However, larger models also require significantly more time\nand compute during inference. This begs the question: what happens when both\nmodels operate under the same budget? (e.g., compute, run-time). To address\nthis question, we analyze code generation LLMs of various sizes and make\ncomparisons such as running a 70B model once vs. generating five outputs from a\n13B model. We consider a standard unit-test setup, which can be used to select\nthe correct output from the smaller model. Our findings reveal that the\nrepeated use of smaller models can yield consistent improvements, with gains of\nup to 15% across five tasks. On the other hand, in scenarios where unit-tests\nare unavailable, a ranking-based selection of candidates from the smaller model\nfalls short of the performance of a single output from larger ones. Our results\nhighlight the potential of using smaller models instead of larger ones, and the\nimportance of studying approaches for ranking LLM outputs.\n","authors":["Michael Hassid","Tal Remez","Jonas Gehring","Roy Schwartz","Yossi Adi"],"pdf_url":"https://arxiv.org/pdf/2404.00725v2.pdf","comment":"COLM 2024"},{"id":"http://arxiv.org/abs/2407.17963v1","updated":"2024-07-25T11:35:22Z","published":"2024-07-25T11:35:22Z","title":"Relating the Seemingly Unrelated: Principled Understanding of\n  Generalization for Generative Models in Arithmetic Reasoning Tasks","summary":"  Large language models (LLMs) have demonstrated impressive versatility across\nnumerous tasks, yet their generalization capabilities remain poorly understood.\nTo investigate these behaviors, arithmetic tasks serve as important venues. In\nprevious studies, seemingly unrelated mysteries still exist -- (1) models with\nappropriate positional embeddings can correctly perform longer unseen\narithmetic operations such as addition, but their effectiveness varies in more\ncomplex tasks like multiplication; (2) models perform well for longer unseen\ncases in modular addition under specific moduli (e.g., modulo 100) but struggle\nunder very close moduli (e.g., modulo 101), regardless of the positional\nencoding used. We believe previous studies have been treating the symptoms\nrather than addressing the root cause -- they have paid excessive attention to\nimproving model components, while overlooking the differences in task\nproperties that may be the real drivers. This is confirmed by our unified\ntheoretical framework for different arithmetic scenarios. For example, unlike\nmultiplication, the digital addition task has the property of translation\ninvariance which naturally aligns with the relative positional encoding, and\nthis combination leads to successful generalization of addition to unseen\nlonger domains. The discrepancy in operations modulo 100 and 101 arises from\nthe base. Modulo 100, unlike 101, is compatible with the decimal system (base\n10), such that unseen information in digits beyond the units digit and the tens\ndigit is actually not needed for the task. Extensive experiments with GPT-like\nmodels validate our theoretical predictions. These findings deepen our\nunderstanding of the generalization mechanisms, and facilitate more\ndata-efficient model training and objective-oriented AI alignment.\n","authors":["Xingcheng Xu","Zibo Zhao","Haipeng Zhang","Yanqing Yang"],"pdf_url":"https://arxiv.org/pdf/2407.17963v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17957v1","updated":"2024-07-25T11:24:44Z","published":"2024-07-25T11:24:44Z","title":"Neural Networks for Generating Better Local Optima in Topology\n  Optimization","summary":"  Neural networks have recently been employed as material discretizations\nwithin adjoint optimization frameworks for inverse problems and topology\noptimization. While advantageous regularization effects and better optima have\nbeen found for some inverse problems, the benefit for topology optimization has\nbeen limited -- where the focus of investigations has been the compliance\nproblem. We demonstrate how neural network material discretizations can, under\ncertain conditions, find better local optima in more challenging optimization\nproblems, where we here specifically consider acoustic topology optimization.\nThe chances of identifying a better optimum can significantly be improved by\nrunning multiple partial optimizations with different neural network\ninitializations. Furthermore, we show that the neural network material\ndiscretization's advantage comes from the interplay with the Adam optimizer and\nemphasize its current limitations when competing with constrained and\nhigher-order optimization techniques. At the moment, this discretization has\nonly been shown to be beneficial for unconstrained first-order optimization.\n","authors":["Leon Herrmann","Ole Sigmund","Viola Muning Li","Christian Vogl","Stefan Kollmannsberger"],"pdf_url":"https://arxiv.org/pdf/2407.17957v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.16639v2","updated":"2024-07-25T11:21:50Z","published":"2024-05-26T17:30:44Z","title":"A unified law of robustness for Bregman divergence losses","summary":"  In contemporary deep learning practice, models are often trained to near zero\nloss i.e. to nearly interpolate the training data. However, the number of\nparameters in the model is usually far more than the number of data points $n$,\nthe theoretical minimum needed for interpolation: a phenomenon referred to as\noverparameterization. In an interesting piece of work that contributes to the\nconsiderable research that has been devoted to understand overparameterization,\nBubeck and Sellke showed that for a broad class of covariate distributions\n(specifically those satisfying a natural notion of concentration of measure),\noverparameterization is necessary for robust interpolation i.e. if the\ninterpolating function is required to be Lipschitz. However, their robustness\nresults were proved only in the setting of regression with square loss. In\npractice, however many other kinds of losses are used, e.g. cross entropy loss\nfor classification. In this work, we generalize Bubeck and Selke's result to\nBregman divergence losses, which form a common generalization of square loss\nand cross-entropy loss. Our generalization relies on identifying a\nbias-variance type decomposition that lies at the heart of the proof and Bubeck\nand Sellke.\n","authors":["Santanu Das","Jatin Batra","Piyush Srivastava"],"pdf_url":"https://arxiv.org/pdf/2405.16639v2.pdf","comment":"18 pages"},{"id":"http://arxiv.org/abs/2407.17954v1","updated":"2024-07-25T11:19:55Z","published":"2024-07-25T11:19:55Z","title":"Scaling Training Data with Lossy Image Compression","summary":"  Empirically-determined scaling laws have been broadly successful in\npredicting the evolution of large machine learning models with training data\nand number of parameters. As a consequence, they have been useful for\noptimizing the allocation of limited resources, most notably compute time.\n  In certain applications, storage space is an important constraint, and data\nformat needs to be chosen carefully as a consequence. Computer vision is a\nprominent example: images are inherently analog, but are always stored in a\ndigital format using a finite number of bits. Given a dataset of digital\nimages, the number of bits $L$ to store each of them can be further reduced\nusing lossy data compression. This, however, can degrade the quality of the\nmodel trained on such images, since each example has lower resolution.\n  In order to capture this trade-off and optimize storage of training data, we\npropose a `storage scaling law' that describes the joint evolution of test\nerror with sample size and number of bits per image. We prove that this law\nholds within a stylized model for image compression, and verify it empirically\non two computer vision tasks, extracting the relevant parameters. We then show\nthat this law can be used to optimize the lossy compression level. At given\nstorage, models trained on optimally compressed images present a significantly\nsmaller test error with respect to models trained on the original data.\nFinally, we investigate the potential benefits of randomizing the compression\nlevel.\n","authors":["Katherine L. Mentzer","Andrea Montanari"],"pdf_url":"https://arxiv.org/pdf/2407.17954v1.pdf","comment":"21 pages, 27 figures"},{"id":"http://arxiv.org/abs/2407.17950v1","updated":"2024-07-25T11:11:05Z","published":"2024-07-25T11:11:05Z","title":"Real Time American Sign Language Detection Using Yolo-v9","summary":"  This paper focuses on real-time American Sign Language Detection. YOLO is a\nconvolutional neural network (CNN) based model, which was first released in\n2015. In recent years, it gained popularity for its real-time detection\ncapabilities. Our study specifically targets YOLO-v9 model, released in 2024.\nAs the model is newly introduced, not much work has been done on it, especially\nnot in Sign Language Detection. Our paper provides deep insight on how YOLO- v9\nworks and better than previous model.\n","authors":["Amna Imran","Meghana Shashishekhara Hulikal","Hamza A. A. Gardi"],"pdf_url":"https://arxiv.org/pdf/2407.17950v1.pdf","comment":"11 pages, 13 figures, 1 table"},{"id":"http://arxiv.org/abs/2407.17949v1","updated":"2024-07-25T11:08:53Z","published":"2024-07-25T11:08:53Z","title":"Fast convergence of the Expectation Maximization algorithm under a\n  logarithmic Sobolev inequality","summary":"  By utilizing recently developed tools for constructing gradient flows on\nWasserstein spaces, we extend an analysis technique commonly employed to\nunderstand alternating minimization algorithms on Euclidean space to the\nExpectation Maximization (EM) algorithm via its representation as\ncoordinate-wise minimization on the product of a Euclidean space and a space of\nprobability distributions due to Neal and Hinton (1998). In so doing we obtain\nfinite sample error bounds and exponential convergence of the EM algorithm\nunder a natural generalisation of a log-Sobolev inequality. We further\ndemonstrate that the analysis technique is sufficiently flexible to allow also\nthe analysis of several variants of the EM algorithm.\n","authors":["Rocco Caprio","Adam M Johansen"],"pdf_url":"https://arxiv.org/pdf/2407.17949v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08061v2","updated":"2024-07-25T10:52:26Z","published":"2024-04-11T18:03:59Z","title":"Physics-Enhanced Graph Neural Networks For Soft Sensing in Industrial\n  Internet of Things","summary":"  The Industrial Internet of Things (IIoT) is reshaping manufacturing,\nindustrial processes, and infrastructure management. By fostering new levels of\nautomation, efficiency, and predictive maintenance, IIoT is transforming\ntraditional industries into intelligent, seamlessly interconnected ecosystems.\nHowever, achieving highly reliable IIoT can be hindered by factors such as the\ncost of installing large numbers of sensors, limitations in retrofitting\nexisting systems with sensors, or harsh environmental conditions that may make\nsensor installation impractical. Soft (virtual) sensing leverages mathematical\nmodels to estimate variables from physical sensor data, offering a solution to\nthese challenges. Data-driven and physics-based modeling are the two main\nmethodologies widely used for soft sensing. The choice between these strategies\ndepends on the complexity of the underlying system, with the data-driven\napproach often being preferred when the physics-based inference models are\nintricate and present challenges for state estimation. However, conventional\ndeep learning models are typically hindered by their inability to explicitly\nrepresent the complex interactions among various sensors. To address this\nlimitation, we adopt Graph Neural Networks (GNNs), renowned for their ability\nto effectively capture the complex relationships between sensor measurements.\nIn this research, we propose physics-enhanced GNNs, which integrate principles\nof physics into graph-based methodologies. This is achieved by augmenting\nadditional nodes in the input graph derived from the underlying characteristics\nof the physical processes. Our evaluation of the proposed methodology on the\ncase study of district heating networks reveals significant improvements over\npurely data-driven GNNs, even in the presence of noise and parameter\ninaccuracies.\n","authors":["Keivan Faghih Niresi","Hugo Bissig","Henri Baumann","Olga Fink"],"pdf_url":"https://arxiv.org/pdf/2404.08061v2.pdf","comment":"14 pages, 10 figures. Accepted to IEEE Internet of Things Journal"},{"id":"http://arxiv.org/abs/2407.17930v1","updated":"2024-07-25T10:39:50Z","published":"2024-07-25T10:39:50Z","title":"Comparison of different Artificial Neural Networks for Bitcoin price\n  forecasting","summary":"  This study investigates the impact of varying sequence lengths on the\naccuracy of predicting cryptocurrency returns using Artificial Neural Networks\n(ANNs). Utilizing the Mean Absolute Error (MAE) as a threshold criterion, we\naim to enhance prediction accuracy by excluding returns that are smaller than\nthis threshold, thus mitigating errors associated with minor returns. The\nsubsequent evaluation focuses on the accuracy of predicted returns that exceed\nthis threshold. We compare four sequence lengths 168 hours (7 days), 72 hours\n(3 days), 24 hours, and 12 hours each with a return prediction interval of 2\nhours. Our findings reveal the influence of sequence length on prediction\naccuracy and underscore the potential for optimized sequence configurations in\nfinancial forecasting models.\n","authors":["Silas Baumann","Karl A. Busch","Hamza A. A. Gardi"],"pdf_url":"https://arxiv.org/pdf/2407.17930v1.pdf","comment":"9 pages, 8 figures, 2 tables"},{"id":"http://arxiv.org/abs/2407.15900v2","updated":"2024-07-25T10:39:15Z","published":"2024-07-22T11:07:52Z","title":"Improving probabilistic forecasts of extreme wind speeds by training\n  statistical post-processing models with weighted scoring rules","summary":"  Accurate forecasts of extreme wind speeds are of high importance for many\napplications. Such forecasts are usually generated by ensembles of numerical\nweather prediction (NWP) models, which however can be biased and have errors in\ndispersion, thus necessitating the application of statistical post-processing\ntechniques. In this work we aim to improve statistical post-processing models\nfor probabilistic predictions of extreme wind speeds. We do this by adjusting\nthe training procedure used to fit ensemble model output statistics (EMOS)\nmodels - a commonly applied post-processing technique - and propose estimating\nparameters using the so-called threshold-weighted continuous ranked probability\nscore (twCRPS), a proper scoring rule that places special emphasis on\npredictions over a threshold. We show that training using the twCRPS leads to\nimproved extreme event performance of post-processing models for a variety of\nthresholds. We find a distribution body-tail trade-off where improved\nperformance for probabilistic predictions of extreme events comes with worse\nperformance for predictions of the distribution body. However, we introduce\nstrategies to mitigate this trade-off based on weighted training and linear\npooling. Finally, we consider some synthetic experiments to explain the\ntraining impact of the twCRPS and derive closed-form expressions of the twCRPS\nfor a number of distributions, giving the first such collection in the\nliterature. The results will enable researchers and practitioners alike to\nimprove the performance of probabilistic forecasting models for extremes and\nother events of interest.\n","authors":["Jakob Benjamin Wessel","Christopher A. T. Ferro","Gavin R. Evans","Frank Kwasniok"],"pdf_url":"https://arxiv.org/pdf/2407.15900v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17929v1","updated":"2024-07-25T10:38:32Z","published":"2024-07-25T10:38:32Z","title":"Guided Latent Slot Diffusion for Object-Centric Learning","summary":"  Slot attention aims to decompose an input image into a set of meaningful\nobject files (slots). These latent object representations enable various\ndownstream tasks. Yet, these slots often bind to object parts, not objects\nthemselves, especially for real-world datasets. To address this, we introduce\nGuided Latent Slot Diffusion - GLASS, an object-centric model that uses\ngenerated captions as a guiding signal to better align slots with objects. Our\nkey insight is to learn the slot-attention module in the space of generated\nimages. This allows us to repurpose the pre-trained diffusion decoder model,\nwhich reconstructs the images from the slots, as a semantic mask generator\nbased on the generated captions. GLASS learns an object-level representation\nsuitable for multiple tasks simultaneously, e.g., segmentation, image\ngeneration, and property prediction, outperforming previous methods. For object\ndiscovery, GLASS achieves approx. a +35% and +10% relative improvement for mIoU\nover the previous state-of-the-art (SOTA) method on the VOC and COCO datasets,\nrespectively, and establishes a new SOTA FID score for conditional image\ngeneration amongst slot-attention-based methods. For the segmentation task,\nGLASS surpasses SOTA weakly-supervised and language-based segmentation models,\nwhich were specifically designed for the task.\n","authors":["Krishnakant Singh","Simone Schaub-Meyer","Stefan Roth"],"pdf_url":"https://arxiv.org/pdf/2407.17929v1.pdf","comment":"Project Page: https://guided-sa.github.io"},{"id":"http://arxiv.org/abs/2401.13429v3","updated":"2024-07-25T10:15:51Z","published":"2024-01-24T12:58:08Z","title":"Detection of Correlated Random Vectors","summary":"  In this paper, we investigate the problem of deciding whether two standard\nnormal random vectors $\\mathsf{X}\\in\\mathbb{R}^{n}$ and\n$\\mathsf{Y}\\in\\mathbb{R}^{n}$ are correlated or not. This is formulated as a\nhypothesis testing problem, where under the null hypothesis, these vectors are\nstatistically independent, while under the alternative, $\\mathsf{X}$ and a\nrandomly and uniformly permuted version of $\\mathsf{Y}$, are correlated with\ncorrelation $\\rho$. We analyze the thresholds at which optimal testing is\ninformation-theoretically impossible and possible, as a function of $n$ and\n$\\rho$. To derive our information-theoretic lower bounds, we develop a novel\ntechnique for evaluating the second moment of the likelihood ratio using an\northogonal polynomials expansion, which among other things, reveals a\nsurprising connection to integer partition functions. We also study a\nmulti-dimensional generalization of the above setting, where rather than two\nvectors we observe two databases/matrices, and furthermore allow for partial\ncorrelations between these two.\n","authors":["Dor Elimelech","Wasim Huleihel"],"pdf_url":"https://arxiv.org/pdf/2401.13429v3.pdf","comment":"42 pages"},{"id":"http://arxiv.org/abs/2212.03117v2","updated":"2024-07-25T10:11:29Z","published":"2022-12-06T16:29:47Z","title":"Q-Pensieve: Boosting Sample Efficiency of Multi-Objective RL Through\n  Memory Sharing of Q-Snapshots","summary":"  Many real-world continuous control problems are in the dilemma of weighing\nthe pros and cons, multi-objective reinforcement learning (MORL) serves as a\ngeneric framework of learning control policies for different preferences over\nobjectives. However, the existing MORL methods either rely on multiple passes\nof explicit search for finding the Pareto front and therefore are not\nsample-efficient, or utilizes a shared policy network for coarse knowledge\nsharing among policies. To boost the sample efficiency of MORL, we propose\nQ-Pensieve, a policy improvement scheme that stores a collection of Q-snapshots\nto jointly determine the policy update direction and thereby enables data\nsharing at the policy level. We show that Q-Pensieve can be naturally\nintegrated with soft policy iteration with convergence guarantee. To\nsubstantiate this concept, we propose the technique of Q replay buffer, which\nstores the learned Q-networks from the past iterations, and arrive at a\npractical actor-critic implementation. Through extensive experiments and an\nablation study, we demonstrate that with much fewer samples, the proposed\nalgorithm can outperform the benchmark MORL methods on a variety of MORL\nbenchmark tasks.\n","authors":["Wei Hung","Bo-Kai Huang","Ping-Chun Hsieh","Xi Liu"],"pdf_url":"https://arxiv.org/pdf/2212.03117v2.pdf","comment":"20 pages, 15 figures"},{"id":"http://arxiv.org/abs/2407.17910v1","updated":"2024-07-25T10:02:11Z","published":"2024-07-25T10:02:11Z","title":"Causal Deepsets for Off-policy Evaluation under Spatial or\n  Spatio-temporal Interferences","summary":"  Off-policy evaluation (OPE) is widely applied in sectors such as\npharmaceuticals and e-commerce to evaluate the efficacy of novel products or\npolicies from offline datasets. This paper introduces a causal deepset\nframework that relaxes several key structural assumptions, primarily the\nmean-field assumption, prevalent in existing OPE methodologies that handle\nspatio-temporal interference. These traditional assumptions frequently prove\ninadequate in real-world settings, thereby restricting the capability of\ncurrent OPE methods to effectively address complex interference effects. In\nresponse, we advocate for the implementation of the permutation invariance (PI)\nassumption. This innovative approach enables the data-driven, adaptive learning\nof the mean-field function, offering a more flexible estimation method beyond\nconventional averaging. Furthermore, we present novel algorithms that\nincorporate the PI assumption into OPE and thoroughly examine their theoretical\nfoundations. Our numerical analyses demonstrate that this novel approach yields\nsignificantly more precise estimations than existing baseline algorithms,\nthereby substantially improving the practical applicability and effectiveness\nof OPE methodologies. A Python implementation of our proposed method is\navailable at https://github.com/BIG-S2/Causal-Deepsets.\n","authors":["Runpeng Dai","Jianing Wang","Fan Zhou","Shikai Luo","Zhiwei Qin","Chengchun Shi","Hongtu Zhu"],"pdf_url":"https://arxiv.org/pdf/2407.17910v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17909v1","updated":"2024-07-25T10:00:21Z","published":"2024-07-25T10:00:21Z","title":"Separating Novel Features for Logical Anomaly Detection: A\n  Straightforward yet Effective Approach","summary":"  Vision-based inspection algorithms have significantly contributed to quality\ncontrol in industrial settings, particularly in addressing structural defects\nlike dent and contamination which are prevalent in mass production. Extensive\nresearch efforts have led to the development of related benchmarks such as\nMVTec AD (Bergmann et al., 2019). However, in industrial settings, there can be\ninstances of logical defects, where acceptable items are found in unsuitable\nlocations or product pairs do not match as expected. Recent methods tackling\nlogical defects effectively employ knowledge distillation to generate\ndifference maps. Knowledge distillation (KD) is used to learn normal data\ndistribution in unsupervised manner. Despite their effectiveness, these methods\noften overlook the potential false negatives. Excessive similarity between the\nteacher network and student network can hinder the generation of a suitable\ndifference map for logical anomaly detection. This technical report provides\ninsights on handling potential false negatives by utilizing a simple constraint\nin KD-based logical anomaly detection methods. We select EfficientAD as a\nstate-of-the-art baseline and apply a margin-based constraint to its\nunsupervised learning scheme. Applying this constraint, we can improve the\nAUROC for MVTec LOCO AD by 1.3 %.\n","authors":["Kangil Lee","Geonuk Kim"],"pdf_url":"https://arxiv.org/pdf/2407.17909v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17907v1","updated":"2024-07-25T09:53:12Z","published":"2024-07-25T09:53:12Z","title":"Amortized Posterior Sampling with Diffusion Prior Distillation","summary":"  We propose a variational inference approach to sample from the posterior\ndistribution for solving inverse problems. From a pre-trained diffusion model,\nour approach trains a conditional flow model to minimize the divergence between\nthe proposal variational distribution and the posterior distribution implicitly\ndefined through the diffusion model. Once trained, the flow model is capable of\nsampling from the posterior distribution with a single NFE, amortized with\nrespect to the measurement. The proposed method paves a new path for distilling\na diffusion prior for efficient posterior sampling. We show that our method is\napplicable to standard signals in Euclidean space, as well as signals on\nmanifold.\n","authors":["Abbas Mammadov","Hyungjin Chung","Jong Chul Ye"],"pdf_url":"https://arxiv.org/pdf/2407.17907v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17900v1","updated":"2024-07-25T09:42:24Z","published":"2024-07-25T09:42:24Z","title":"The Power of Combining Data and Knowledge: GPT-4o is an Effective\n  Interpreter of Machine Learning Models in Predicting Lymph Node Metastasis of\n  Lung Cancer","summary":"  Lymph node metastasis (LNM) is a crucial factor in determining the initial\ntreatment for patients with lung cancer, yet accurate preoperative diagnosis of\nLNM remains challenging. Recently, large language models (LLMs) have garnered\nsignificant attention due to their remarkable text generation capabilities.\nLeveraging the extensive medical knowledge learned from vast corpora, LLMs can\nestimate probabilities for clinical problems, though their performance has\nhistorically been inferior to data-driven machine learning models. In this\npaper, we propose a novel ensemble method that combines the medical knowledge\nacquired by LLMs with the latent patterns identified by machine learning models\nto enhance LNM prediction performance. Initially, we developed machine learning\nmodels using patient data. We then designed a prompt template to integrate the\npatient data with the predicted probability from the machine learning model.\nSubsequently, we instructed GPT-4o, the most advanced LLM developed by OpenAI,\nto estimate the likelihood of LNM based on patient data and then adjust the\nestimate using the machine learning output. Finally, we collected three outputs\nfrom the GPT-4o using the same prompt and ensembled these results as the final\nprediction. Using the proposed method, our models achieved an AUC value of\n0.765 and an AP value of 0.415 for LNM prediction, significantly improving\npredictive performance compared to baseline machine learning models. The\nexperimental results indicate that GPT-4o can effectively leverage its medical\nknowledge and the probabilities predicted by machine learning models to achieve\nmore accurate LNM predictions. These findings demonstrate that LLMs can perform\nwell in clinical risk prediction tasks, offering a new paradigm for integrating\nmedical knowledge and patient data in clinical predictions.\n","authors":["Danqing Hu","Bing Liu","Xiaofeng Zhu","Nan Wu"],"pdf_url":"https://arxiv.org/pdf/2407.17900v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.07987v5","updated":"2024-07-25T09:33:50Z","published":"2024-05-13T17:58:30Z","title":"The Platonic Representation Hypothesis","summary":"  We argue that representations in AI models, particularly deep networks, are\nconverging. First, we survey many examples of convergence in the literature:\nover time and across multiple domains, the ways by which different neural\nnetworks represent data are becoming more aligned. Next, we demonstrate\nconvergence across data modalities: as vision models and language models get\nlarger, they measure distance between datapoints in a more and more alike way.\nWe hypothesize that this convergence is driving toward a shared statistical\nmodel of reality, akin to Plato's concept of an ideal reality. We term such a\nrepresentation the platonic representation and discuss several possible\nselective pressures toward it. Finally, we discuss the implications of these\ntrends, their limitations, and counterexamples to our analysis.\n","authors":["Minyoung Huh","Brian Cheung","Tongzhou Wang","Phillip Isola"],"pdf_url":"https://arxiv.org/pdf/2405.07987v5.pdf","comment":"Equal contributions. Project: https://phillipi.github.io/prh/ Code:\n  https://github.com/minyoungg/platonic-rep"},{"id":"http://arxiv.org/abs/2407.17892v1","updated":"2024-07-25T09:26:07Z","published":"2024-07-25T09:26:07Z","title":"An Iterative Approach to Topic Modelling","summary":"  Topic modelling has become increasingly popular for summarizing text data,\nsuch as social media posts and articles. However, topic modelling is usually\ncompleted in one shot. Assessing the quality of resulting topics is\nchallenging. No effective methods or measures have been developed for assessing\nthe results or for making further enhancements to the topics. In this research,\nwe propose we propose to use an iterative process to perform topic modelling\nthat gives rise to a sense of completeness of the resulting topics when the\nprocess is complete. Using the BERTopic package, a popular method in topic\nmodelling, we demonstrate how the modelling process can be applied iteratively\nto arrive at a set of topics that could not be further improved upon using one\nof the three selected measures for clustering comparison as the decision\ncriteria. This demonstration is conducted using a subset of the COVIDSenti-A\ndataset. The early success leads us to believe that further research using in\nusing this approach in conjunction with other topic modelling algorithms could\nbe viable.\n","authors":["Albert Wong","Florence Wing Yau Cheng","Ashley Keung","Yamileth Hercules","Mary Alexandra Garcia","Yew-Wei Lim","Lien Pham"],"pdf_url":"https://arxiv.org/pdf/2407.17892v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.02737v2","updated":"2024-07-25T09:18:24Z","published":"2024-03-05T07:45:29Z","title":"Neural Fractional Differential Equations","summary":"  Fractional Differential Equations (FDEs) are essential tools for modelling\ncomplex systems in science and engineering. They extend the traditional\nconcepts of differentiation and integration to non-integer orders, enabling a\nmore precise representation of processes characterised by non-local and\nmemory-dependent behaviours.\n  This property is useful in systems where variables do not respond to changes\ninstantaneously, but instead exhibit a strong memory of past interactions.\n  Having this in mind, and drawing inspiration from Neural Ordinary\nDifferential Equations (Neural ODEs), we propose the Neural FDE, a novel deep\nneural network architecture that adjusts a FDE to the dynamics of data.\n  This work provides a comprehensive overview of the numerical method employed\nin Neural FDEs and the Neural FDE architecture. The numerical outcomes suggest\nthat, despite being more computationally demanding, the Neural FDE may\noutperform the Neural ODE in modelling systems with memory or dependencies on\npast states, and it can effectively be applied to learn more intricate\ndynamical systems.\n","authors":["C. Coelho","M. Fernanda P. Costa","L. L. Ferrás"],"pdf_url":"https://arxiv.org/pdf/2403.02737v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.02750v2","updated":"2024-07-25T09:16:05Z","published":"2024-02-05T06:06:47Z","title":"KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache","summary":"  Efficiently serving large language models (LLMs) requires batching of many\nrequests to reduce the cost per request. Yet, with larger batch sizes and\nlonger context lengths, the key-value (KV) cache, which stores attention keys\nand values to avoid re-computations, significantly increases memory demands and\nbecomes the new bottleneck in speed and memory usage. Additionally, the loading\nof the KV cache causes the computational core to be idle, which limits the\ninference speed. A straightforward and effective solution to reduce KV cache\nsize is quantization, which decreases the total bytes taken by KV cache.\nHowever, there is a lack of in-depth studies that explore the element\ndistribution of KV cache to understand the hardness and limitation of KV cache\nquantization. To fill the gap, we conducted a comprehensive study on the\nelement distribution in KV cache of popular LLMs. Our findings indicate that\nthe key cache should be quantized per-channel, i.e., group elements along the\nchannel dimension and quantize them together. In contrast, the value cache\nshould be quantized per-token. From this analysis, we developed a tuning-free\n2bit KV cache quantization algorithm named KIVI. With hardware-friendly\nimplementation, KIVI can enable Llama, Falcon, and Mistral models to maintain\nalmost the same quality while using $\\mathbf{2.6\\times}$ less peak memory\n(including model weight). This reduction in memory usage enables up to\n$\\mathbf{4\\times}$ larger batch size, bringing $\\mathbf{2.35\\times \\sim\n3.47\\times}$ throughput on real LLM inference workload. The source code is\navailable at https://github.com/jy-yuan/KIVI.\n","authors":["Zirui Liu","Jiayi Yuan","Hongye Jin","Shaochen Zhong","Zhaozhuo Xu","Vladimir Braverman","Beidi Chen","Xia Hu"],"pdf_url":"https://arxiv.org/pdf/2402.02750v2.pdf","comment":"ICML2024"},{"id":"http://arxiv.org/abs/2405.09597v2","updated":"2024-07-25T08:59:36Z","published":"2024-05-15T13:50:23Z","title":"When AI Eats Itself: On the Caveats of Data Pollution in the Era of\n  Generative AI","summary":"  Generative artificial intelligence (AI) technologies and large models are\nproducing realistic outputs across various domains, such as images, text,\nspeech, and music. Creating these advanced generative models requires\nsignificant resources, particularly large and high-quality datasets. To\nminimize training expenses, many algorithm developers use data created by the\nmodels themselves as a cost-effective training solution. However, not all\nsynthetic data effectively improve model performance, necessitating a strategic\nbalance in the use of real versus synthetic data to optimize outcomes.\n  Currently, the previously well-controlled integration of real and synthetic\ndata is becoming uncontrollable. The widespread and unregulated dissemination\nof synthetic data online leads to the contamination of datasets traditionally\ncompiled through web scraping, now mixed with unlabeled synthetic data. This\ntrend portends a future where generative AI systems may increasingly rely\nblindly on consuming self-generated data, raising concerns about model\nperformance and ethical issues. What will happen if generative AI continuously\nconsumes itself without discernment? What measures can we take to mitigate the\npotential adverse effects?\n  There is a significant gap in the scientific literature regarding the impact\nof synthetic data use in generative AI, particularly in terms of the fusion of\nmultimodal information. To address this research gap, this review investigates\nthe consequences of integrating synthetic data blindly on training generative\nAI on both image and text modalities and explores strategies to mitigate these\neffects. The goal is to offer a comprehensive view of synthetic data's role,\nadvocating for a balanced approach to its use and exploring practices that\npromote the sustainable development of generative AI technologies in the era of\nlarge models.\n","authors":["Xiaodan Xing","Fadong Shi","Jiahao Huang","Yinzhe Wu","Yang Nan","Sheng Zhang","Yingying Fang","Mike Roberts","Carola-Bibiane Schönlieb","Javier Del Ser","Guang Yang"],"pdf_url":"https://arxiv.org/pdf/2405.09597v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17880v1","updated":"2024-07-25T08:48:07Z","published":"2024-07-25T08:48:07Z","title":"DAM: Towards A Foundation Model for Time Series Forecasting","summary":"  It is challenging to scale time series forecasting models such that they\nforecast accurately for multiple distinct domains and datasets, all with\npotentially different underlying collection procedures (e.g., sample\nresolution), patterns (e.g., periodicity), and prediction requirements (e.g.,\nreconstruction vs. forecasting). We call this general task universal\nforecasting. Existing methods usually assume that input data is regularly\nsampled, and they forecast to pre-determined horizons, resulting in failure to\ngeneralise outside of the scope of their training. We propose the DAM - a\nneural model that takes randomly sampled histories and outputs an adjustable\nbasis composition as a continuous function of time for forecasting to non-fixed\nhorizons. It involves three key components: (1) a flexible approach for using\nrandomly sampled histories from a long-tail distribution, that enables an\nefficient global perspective of the underlying temporal dynamics while\nretaining focus on the recent history; (2) a transformer backbone that is\ntrained on these actively sampled histories to produce, as representational\noutput, (3) the basis coefficients of a continuous function of time. We show\nthat a single univariate DAM, trained on 25 time series datasets, either\noutperformed or closely matched existing SoTA models at multivariate long-term\nforecasting across 18 datasets, including 8 held-out for zero-shot transfer,\neven though these models were trained to specialise for each dataset-horizon\ncombination. This single DAM excels at zero-shot transfer and very-long-term\nforecasting, performs well at imputation, is interpretable via basis function\ncomposition and attention, can be tuned for different inference-cost\nrequirements, is robust to missing and irregularly sampled data {by design}.\n","authors":["Luke Darlow","Qiwen Deng","Ahmed Hassan","Martin Asenov","Rajkarn Singh","Artjom Joosen","Adam Barker","Amos Storkey"],"pdf_url":"https://arxiv.org/pdf/2407.17880v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17876v1","updated":"2024-07-25T08:46:49Z","published":"2024-07-25T08:46:49Z","title":"A Large-Scale Sensitivity Analysis on Latent Embeddings and\n  Dimensionality Reductions for Text Spatializations","summary":"  The semantic similarity between documents of a text corpus can be visualized\nusing map-like metaphors based on two-dimensional scatterplot layouts. These\nlayouts result from a dimensionality reduction on the document-term matrix or a\nrepresentation within a latent embedding, including topic models. Thereby, the\nresulting layout depends on the input data and hyperparameters of the\ndimensionality reduction and is therefore affected by changes in them.\nFurthermore, the resulting layout is affected by changes in the input data and\nhyperparameters of the dimensionality reduction. However, such changes to the\nlayout require additional cognitive efforts from the user. In this work, we\npresent a sensitivity study that analyzes the stability of these layouts\nconcerning (1) changes in the text corpora, (2) changes in the hyperparameter,\nand (3) randomness in the initialization. Our approach has two stages: data\nmeasurement and data analysis. First, we derived layouts for the combination of\nthree text corpora and six text embeddings and a grid-search-inspired\nhyperparameter selection of the dimensionality reductions. Afterward, we\nquantified the similarity of the layouts through ten metrics, concerning local\nand global structures and class separation. Second, we analyzed the resulting\n42817 tabular data points in a descriptive statistical analysis. From this, we\nderived guidelines for informed decisions on the layout algorithm and highlight\nspecific hyperparameter settings. We provide our implementation as a Git\nrepository at\nhttps://github.com/hpicgs/Topic-Models-and-Dimensionality-Reduction-Sensitivity-Study\nand results as Zenodo archive at https://doi.org/10.5281/zenodo.12772898.\n","authors":["Daniel Atzberger","Tim Cech","Willy Scheibel","Jürgen Döllner","Michael Behrisch","Tobias Schreck"],"pdf_url":"https://arxiv.org/pdf/2407.17876v1.pdf","comment":"To be published at IEEE VIS 2024 conference"},{"id":"http://arxiv.org/abs/2407.17869v1","updated":"2024-07-25T08:42:23Z","published":"2024-07-25T08:42:23Z","title":"EllipBench: A Large-scale Benchmark for Machine-learning based\n  Ellipsometry Modeling","summary":"  Ellipsometry is used to indirectly measure the optical properties and\nthickness of thin films. However, solving the inverse problem of ellipsometry\nis time-consuming since it involves human expertise to apply the data fitting\ntechniques. Many studies use traditional machine learning-based methods to\nmodel the complex mathematical fitting process. In our work, we approach this\nproblem from a deep learning perspective. First, we introduce a large-scale\nbenchmark dataset to facilitate deep learning methods. The proposed dataset\nencompasses 98 types of thin film materials and 4 types of substrate materials,\nincluding metals, alloys, compounds, and polymers, among others. Additionally,\nwe propose a deep learning framework that leverages residual connections and\nself-attention mechanisms to learn the massive data points. We also introduce a\nreconstruction loss to address the common challenge of multiple solutions in\nthin film thickness prediction. Compared to traditional machine learning\nmethods, our framework achieves state-of-the-art (SOTA) performance on our\nproposed dataset. The dataset and code will be available upon acceptance.\n","authors":["Yiming Ma","Xinjie Li","Xin Sun","Zhiyong Wang","Lionel Z. Wang"],"pdf_url":"https://arxiv.org/pdf/2407.17869v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.14729v2","updated":"2024-07-25T08:34:58Z","published":"2022-12-30T14:15:54Z","title":"Batchless Normalization: How to Normalize Activations Across Instances\n  with Minimal Memory Requirements","summary":"  In training neural networks, batch normalization has many benefits, not all\nof them entirely understood. But it also has some drawbacks. Foremost is\narguably memory consumption, as computing the batch statistics requires all\ninstances within the batch to be processed simultaneously, whereas without\nbatch normalization it would be possible to process them one by one while\naccumulating the weight gradients. Another drawback is that that distribution\nparameters (mean and standard deviation) are unlike all other model parameters\nin that they are not trained using gradient descent but require special\ntreatment, complicating implementation. In this paper, I show a simple and\nstraightforward way to address these issues. The idea, in short, is to add\nterms to the loss that, for each activation, cause the minimization of the\nnegative log likelihood of a Gaussian distribution that is used to normalize\nthe activation. Among other benefits, this will hopefully contribute to the\ndemocratization of AI research by means of lowering the hardware requirements\nfor training larger models.\n","authors":["Benjamin Berger","Victor Uc Cetina"],"pdf_url":"https://arxiv.org/pdf/2212.14729v2.pdf","comment":"17 pages (12 without appendices), 12 figures, 5 tables"},{"id":"http://arxiv.org/abs/2407.11055v3","updated":"2024-07-25T08:26:35Z","published":"2024-07-09T22:04:23Z","title":"Knowledge boosting during low-latency inference","summary":"  Models for low-latency, streaming applications could benefit from the\nknowledge capacity of larger models, but edge devices cannot run these models\ndue to resource constraints. A possible solution is to transfer hints during\ninference from a large model running remotely to a small model running\non-device. However, this incurs a communication delay that breaks real-time\nrequirements and does not guarantee that both models will operate on the same\ndata at the same time. We propose knowledge boosting, a novel technique that\nallows a large model to operate on time-delayed input during inference, while\nstill boosting small model performance. Using a streaming neural network that\nprocesses 8 ms chunks, we evaluate different speech separation and enhancement\ntasks with communication delays of up to six chunks or 48 ms. Our results show\nlarger gains where the performance gap between the small and large models is\nwide, demonstrating a promising method for large-small model collaboration for\nlow-latency applications. Code, dataset, and audio samples available at\nhttps://knowledgeboosting.cs.washington.edu/.\n","authors":["Vidya Srinivas","Malek Itani","Tuochao Chen","Sefik Emre Eskimez","Takuya Yoshioka","Shyamnath Gollakota"],"pdf_url":"https://arxiv.org/pdf/2407.11055v3.pdf","comment":"Accepted by Interspeech 2024"},{"id":"http://arxiv.org/abs/2407.17856v1","updated":"2024-07-25T08:21:46Z","published":"2024-07-25T08:21:46Z","title":"MDS-ED: Multimodal Decision Support in the Emergency Department -- a\n  Benchmark Dataset for Diagnoses and Deterioration Prediction in Emergency\n  Medicine","summary":"  Background: Benchmarking medical decision support algorithms often struggles\ndue to limited access to datasets, narrow prediction tasks, and restricted\ninput modalities. These limitations affect their clinical relevance and\nperformance in high-stakes areas like emergency care, complicating replication,\nvalidation, and improvement of benchmarks.\n  Methods: We introduce a dataset based on MIMIC-IV, benchmarking protocol, and\ninitial results for evaluating multimodal decision support in the emergency\ndepartment (ED). We use diverse data modalities from the first 1.5 hours of\npatient arrival, including demographics, biometrics, vital signs, lab values,\nand electrocardiogram waveforms. We analyze 1443 clinical labels across two\ncontexts: predicting diagnoses with ICD-10 codes and forecasting patient\ndeterioration.\n  Results: Our multimodal diagnostic model achieves an AUROC score over 0.8 in\na statistically significant manner for 357 out of 1428 conditions, including\ncardiac issues like myocardial infarction and non-cardiac conditions such as\nrenal disease and diabetes. The deterioration model scores above 0.8 in a\nstatistically significant manner for 13 out of 15 targets, including critical\nevents like cardiac arrest and mechanical ventilation, ICU admission as well as\nshort- and long-term mortality. Incorporating raw waveform data significantly\nimproves model performance, which represents one of the first robust\ndemonstrations of this effect.\n  Conclusions: This study highlights the uniqueness of our dataset, which\nencompasses a wide range of clinical tasks and utilizes a comprehensive set of\nfeatures collected early during the emergency after arriving at the ED. The\nstrong performance, as evidenced by high AUROC scores across diagnostic and\ndeterioration targets, underscores the potential of our approach to\nrevolutionize decision-making in acute and emergency medicine.\n","authors":["Juan Miguel Lopez Alcaraz","Nils Strodthoff"],"pdf_url":"https://arxiv.org/pdf/2407.17856v1.pdf","comment":"14 pages, 1 figure, code available under\n  https://github.com/AI4HealthUOL/MDS-ED"},{"id":"http://arxiv.org/abs/2401.00773v3","updated":"2024-07-25T08:13:27Z","published":"2024-01-01T14:34:11Z","title":"Unsupervised Outlier Detection using Random Subspace and Subsampling\n  Ensembles of Dirichlet Process Mixtures","summary":"  Probabilistic mixture models are recognized as effective tools for\nunsupervised outlier detection owing to their interpretability and global\ncharacteristics. Among these, Dirichlet process mixture models stand out as a\nstrong alternative to conventional finite mixture models for both clustering\nand outlier detection tasks. Unlike finite mixture models, Dirichlet process\nmixtures are infinite mixture models that automatically determine the number of\nmixture components based on the data. Despite their advantages, the adoption of\nDirichlet process mixture models for unsupervised outlier detection has been\nlimited by challenges related to computational inefficiency and sensitivity to\noutliers in the construction of outlier detectors. Additionally, Dirichlet\nprocess Gaussian mixtures struggle to effectively model non-Gaussian data with\ndiscrete or binary features. To address these challenges, we propose a novel\noutlier detection method that utilizes ensembles of Dirichlet process Gaussian\nmixtures. This unsupervised algorithm employs random subspace and subsampling\nensembles to ensure efficient computation and improve the robustness of the\noutlier detector. The ensemble approach further improves the suitability of the\nproposed method for detecting outliers in non-Gaussian data. Furthermore, our\nmethod uses variational inference for Dirichlet process mixtures, which ensures\nboth efficient and rapid computation. Empirical analyses using benchmark\ndatasets demonstrate that our method outperforms existing approaches in\nunsupervised outlier detection.\n","authors":["Dongwook Kim","Juyeon Park","Hee Cheol Chung","Seonghyun Jeong"],"pdf_url":"https://arxiv.org/pdf/2401.00773v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.12832v2","updated":"2024-07-25T08:09:12Z","published":"2024-04-19T12:09:49Z","title":"COIN: Counterfactual inpainting for weakly supervised semantic\n  segmentation for medical images","summary":"  Deep learning is dramatically transforming the field of medical imaging and\nradiology, enabling the identification of pathologies in medical images,\nincluding computed tomography (CT) and X-ray scans. However, the performance of\ndeep learning models, particularly in segmentation tasks, is often limited by\nthe need for extensive annotated datasets. To address this challenge, the\ncapabilities of weakly supervised semantic segmentation are explored through\nthe lens of Explainable AI and the generation of counterfactual explanations.\nThe scope of this research is development of a novel counterfactual inpainting\napproach (COIN) that flips the predicted classification label from abnormal to\nnormal by using a generative model. For instance, if the classifier deems an\ninput medical image X as abnormal, indicating the presence of a pathology, the\ngenerative model aims to inpaint the abnormal region, thus reversing the\nclassifier's original prediction label. The approach enables us to produce\nprecise segmentations for pathologies without depending on pre-existing\nsegmentation masks. Crucially, image-level labels are utilized, which are\nsubstantially easier to acquire than creating detailed segmentation masks. The\neffectiveness of the method is demonstrated by segmenting synthetic targets and\nactual kidney tumors from CT images acquired from Tartu University Hospital in\nEstonia. The findings indicate that COIN greatly surpasses established\nattribution methods, such as RISE, ScoreCAM, and LayerCAM, as well as an\nalternative counterfactual explanation method introduced by Singla et al. This\nevidence suggests that COIN is a promising approach for semantic segmentation\nof tumors in CT images, and presents a step forward in making deep learning\napplications more accessible and effective in healthcare, where annotated data\nis scarce.\n","authors":["Dmytro Shvetsov","Joonas Ariva","Marharyta Domnich","Raul Vicente","Dmytro Fishman"],"pdf_url":"https://arxiv.org/pdf/2404.12832v2.pdf","comment":"This work has been accepted to be presented to The 2nd World\n  Conference on eXplainable Artificial Intelligence (xAI 2024), July 17-19,\n  2024 - Valletta, Malta"},{"id":"http://arxiv.org/abs/2401.14351v2","updated":"2024-07-25T08:08:11Z","published":"2024-01-25T17:55:07Z","title":"ServerlessLLM: Low-Latency Serverless Inference for Large Language\n  Models","summary":"  This paper presents ServerlessLLM, a distributed system designed to support\nlow-latency serverless inference for Large Language Models (LLMs). By\nharnessing the substantial near-GPU storage and memory capacities of inference\nservers, ServerlessLLM achieves effective local checkpoint storage, minimizing\nthe need for remote checkpoint downloads and ensuring efficient checkpoint\nloading. The design of ServerlessLLM features three core contributions: (i)\n\\emph{fast multi-tier checkpoint loading}, featuring a new loading-optimized\ncheckpoint format and a multi-tier loading system, fully utilizing the\nbandwidth of complex storage hierarchies on GPU servers; (ii) \\emph{efficient\nlive migration of LLM inference}, which enables newly initiated inferences to\ncapitalize on local checkpoint storage while ensuring minimal user\ninterruption; and (iii) \\emph{startup-time-optimized model scheduling}, which\nassesses the locality statuses of checkpoints on each server and schedules the\nmodel onto servers that minimize the time to start the inference. Comprehensive\nevaluations, including microbenchmarks and real-world scenarios, demonstrate\nthat ServerlessLLM dramatically outperforms state-of-the-art serverless\nsystems, reducing latency by 10 - 200X across various LLM inference workloads.\n","authors":["Yao Fu","Leyang Xue","Yeqi Huang","Andrei-Octavian Brabete","Dmitrii Ustiugov","Yuvraj Patel","Luo Mai"],"pdf_url":"https://arxiv.org/pdf/2401.14351v2.pdf","comment":"18th USENIX Symposium on Operating Systems Design and Implementation"},{"id":"http://arxiv.org/abs/2406.08401v2","updated":"2024-07-25T08:01:32Z","published":"2024-06-12T16:50:12Z","title":"Nyström Kernel Stein Discrepancy","summary":"  Kernel methods underpin many of the most successful approaches in data\nscience and statistics, and they allow representing probability measures as\nelements of a reproducing kernel Hilbert space without loss of information.\nRecently, the kernel Stein discrepancy (KSD), which combines Stein's method\nwith kernel techniques, gained considerable attention. Through the Stein\noperator, KSD allows the construction of powerful goodness-of-fit tests where\nit is sufficient to know the target distribution up to a multiplicative\nconstant. However, the typical U- and V-statistic-based KSD estimators suffer\nfrom a quadratic runtime complexity, which hinders their application in\nlarge-scale settings. In this work, we propose a Nystr\\\"om-based KSD\nacceleration -- with runtime $\\mathcal O\\!\\left(mn+m^3\\right)$ for $n$ samples\nand $m\\ll n$ Nystr\\\"om points -- , show its $\\sqrt{n}$-consistency under the\nnull with a classical sub-Gaussian assumption, and demonstrate its\napplicability for goodness-of-fit testing on a suite of benchmarks.\n","authors":["Florian Kalinke","Zoltan Szabo","Bharath K. Sriperumbudur"],"pdf_url":"https://arxiv.org/pdf/2406.08401v2.pdf","comment":"Update proof of Lemma B.3, milder Assumption 1, more experiments"},{"id":"http://arxiv.org/abs/2404.12810v2","updated":"2024-07-25T08:00:44Z","published":"2024-04-19T11:47:17Z","title":"Enhancing Counterfactual Explanation Search with Diffusion Distance and\n  Directional Coherence","summary":"  A pressing issue in the adoption of AI models is the increasing demand for\nmore human-centric explanations of their predictions. To advance towards more\nhuman-centric explanations, understanding how humans produce and select\nexplanations has been beneficial. In this work, inspired by insights of human\ncognition we propose and test the incorporation of two novel biases to enhance\nthe search for effective counterfactual explanations. Central to our\nmethodology is the application of diffusion distance, which emphasizes data\nconnectivity and actionability in the search for feasible counterfactual\nexplanations. In particular, diffusion distance effectively weights more those\npoints that are more interconnected by numerous short-length paths. This\napproach brings closely connected points nearer to each other, identifying a\nfeasible path between them. We also introduce a directional coherence term that\nallows the expression of a preference for the alignment between the joint and\nmarginal directional changes in feature space to reach a counterfactual. This\nterm enables the generation of counterfactual explanations that align with a\nset of marginal predictions based on expectations of how the outcome of the\nmodel varies by changing one feature at a time. We evaluate our method, named\nCoherent Directional Counterfactual Explainer (CoDiCE), and the impact of the\ntwo novel biases against existing methods such as DiCE, FACE, Prototypes, and\nGrowing Spheres. Through a series of ablation experiments on both synthetic and\nreal datasets with continuous and mixed-type features, we demonstrate the\neffectiveness of our method.\n","authors":["Marharyta Domnich","Raul Vicente"],"pdf_url":"https://arxiv.org/pdf/2404.12810v2.pdf","comment":"This work has been accepted to be presented to The 2nd World\n  Conference on eXplainable Artificial Intelligence (xAI 2024), July 17-19,\n  2024 - Valletta, Malta"},{"id":"http://arxiv.org/abs/2407.17844v1","updated":"2024-07-25T07:58:19Z","published":"2024-07-25T07:58:19Z","title":"Innovative Speech-Based Deep Learning Approaches for Parkinson's Disease\n  Classification: A Systematic Review","summary":"  Parkinson's disease (PD), the second most prevalent neurodegenerative\ndisorder worldwide, frequently presents with early-stage speech impairments.\nRecent advancements in Artificial Intelligence (AI), particularly deep learning\n(DL), have significantly enhanced PD diagnosis through the analysis of speech\ndata. Nevertheless, the progress of research is restricted by the limited\navailability of publicly accessible speech-based PD datasets, primarily due to\nprivacy and ethical concerns. This review covers the latest DL-based AI\napproaches for speech-based PD classification, focusing on performance,\navailable resources and associated challenges of 33 scientific works published\nbetween 2020 and March 2024. These DL approaches are categorized into\nend-to-end (E2E) learning, transfer learning (TL) and deep acoustic features\n(DAF) extraction. Among E2E approaches, Convolutional Neural Networks (CNNs)\nare prevalent, though Transformers are increasingly popular. E2E approaches\nface challenges such as limited data and computational resources, especially\nwith Transformers. TL addresses these issues by providing more robust PD\ndiagnosis and better generalizability across languages. DAF extraction aims to\nimprove the explainability and interpretability of results by examining the\nspecific effects of deep features on both other DL approaches and more\ntraditional machine learning (ML) methods. However, it often underperforms\ncompared to E2E and TL approaches. This review also discusses unresolved issues\nrelated to bias, explainability and privacy, highlighting the need for future\nresearch.\n","authors":["Lisanne van Gelderen","Cristian Tejedor-García"],"pdf_url":"https://arxiv.org/pdf/2407.17844v1.pdf","comment":"Submitted in Applied Sciences - peer reviewed Open Access journal.\n  This research was funded by the NWO research programme AiNed Fellowship\n  Grants under the project Responsible AI for Voice Diagnostics (RAIVD) - grant\n  number NGF.1607.22.013"},{"id":"http://arxiv.org/abs/2407.17842v1","updated":"2024-07-25T07:57:34Z","published":"2024-07-25T07:57:34Z","title":"On the Opportunities of (Re)-Exploring Atmospheric Science by Foundation\n  Models: A Case Study","summary":"  Most state-of-the-art AI applications in atmospheric science are based on\nclassic deep learning approaches. However, such approaches cannot automatically\nintegrate multiple complicated procedures to construct an intelligent agent,\nsince each functionality is enabled by a separate model learned from\nindependent climate datasets. The emergence of foundation models, especially\nmultimodal foundation models, with their ability to process heterogeneous input\ndata and execute complex tasks, offers a substantial opportunity to overcome\nthis challenge. In this report, we want to explore a central question - how the\nstate-of-the-art foundation model, i.e., GPT-4o, performs various atmospheric\nscientific tasks. Toward this end, we conduct a case study by categorizing the\ntasks into four main classes, including climate data processing, physical\ndiagnosis, forecast and prediction, and adaptation and mitigation. For each\ntask, we comprehensively evaluate the GPT-4o's performance along with a\nconcrete discussion. We hope that this report may shed new light on future AI\napplications and research in atmospheric science.\n","authors":["Lujia Zhang","Hanzhe Cui","Yurong Song","Chenyue Li","Binhang Yuan","Mengqian Lu"],"pdf_url":"https://arxiv.org/pdf/2407.17842v1.pdf","comment":"28 pages, 12 figures"},{"id":"http://arxiv.org/abs/2407.17839v1","updated":"2024-07-25T07:54:07Z","published":"2024-07-25T07:54:07Z","title":"Long-term Fairness in Ride-Hailing Platform","summary":"  Matching in two-sided markets such as ride-hailing has recently received\nsignificant attention. However, existing studies on ride-hailing mainly focus\non optimising efficiency, and fairness issues in ride-hailing have been\nneglected. Fairness issues in ride-hailing, including significant earning\ndifferences between drivers and variance of passenger waiting times among\ndifferent locations, have potential impacts on economic and ethical aspects.\nThe recent studies that focus on fairness in ride-hailing exploit traditional\noptimisation methods and the Markov Decision Process to balance efficiency and\nfairness. However, there are several issues in these existing studies, such as\nmyopic short-term decision-making from traditional optimisation and instability\nof fairness in a comparably longer horizon from both traditional optimisation\nand Markov Decision Process-based methods. To address these issues, we propose\na dynamic Markov Decision Process model to alleviate fairness issues currently\nfaced by ride-hailing, and seek a balance between efficiency and fairness, with\ntwo distinct characteristics: (i) a prediction module to predict the number of\nrequests that will be raised in the future from different locations to allow\nthe proposed method to consider long-term fairness based on the whole timeline\ninstead of consider fairness only based on historical and current data\npatterns; (ii) a customised scalarisation function for multi-objective\nmulti-agent Q Learning that aims to balance efficiency and fairness. Extensive\nexperiments on a publicly available real-world dataset demonstrate that our\nproposed method outperforms existing state-of-the-art methods.\n","authors":["Yufan Kang","Jeffrey Chan","Wei Shao","Flora D. Salim","Christopher Leckie"],"pdf_url":"https://arxiv.org/pdf/2407.17839v1.pdf","comment":"Accepted by ECML PKDD 2024"},{"id":"http://arxiv.org/abs/2406.16087v3","updated":"2024-07-25T07:50:58Z","published":"2024-06-23T12:02:17Z","title":"Imperative Learning: A Self-supervised Neural-Symbolic Learning\n  Framework for Robot Autonomy","summary":"  Data-driven methods such as reinforcement and imitation learning have\nachieved remarkable success in robot autonomy. However, their data-centric\nnature still hinders them from generalizing well to ever-changing environments.\nMoreover, collecting large datasets for robotic tasks is often impractical and\nexpensive. To overcome these challenges, we introduce a new self-supervised\nneural-symbolic (NeSy) computational framework, imperative learning (IL), for\nrobot autonomy, leveraging the generalization abilities of symbolic reasoning.\nThe framework of IL consists of three primary components: a neural module, a\nreasoning engine, and a memory system. We formulate IL as a special bilevel\noptimization (BLO), which enables reciprocal learning over the three modules.\nThis overcomes the label-intensive obstacles associated with data-driven\napproaches and takes advantage of symbolic reasoning concerning logical\nreasoning, physical principles, geometric analysis, etc. We discuss several\noptimization techniques for IL and verify their effectiveness in five distinct\nrobot autonomy tasks including path planning, rule induction, optimal control,\nvisual odometry, and multi-robot routing. Through various experiments, we show\nthat IL can significantly enhance robot autonomy capabilities and we anticipate\nthat it will catalyze further research across diverse domains.\n","authors":["Chen Wang","Kaiyi Ji","Junyi Geng","Zhongqiang Ren","Taimeng Fu","Fan Yang","Yifan Guo","Haonan He","Xiangyu Chen","Zitong Zhan","Qiwei Du","Shaoshu Su","Bowen Li","Yuheng Qiu","Yi Du","Qihang Li","Yifan Yang","Xiao Lin","Zhipeng Zhao"],"pdf_url":"https://arxiv.org/pdf/2406.16087v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17835v1","updated":"2024-07-25T07:46:30Z","published":"2024-07-25T07:46:30Z","title":"IsUMap: Manifold Learning and Data Visualization leveraging\n  Vietoris-Rips filtrations","summary":"  This work introduces IsUMap, a novel manifold learning technique that\nenhances data representation by integrating aspects of UMAP and Isomap with\nVietoris-Rips filtrations. We present a systematic and detailed construction of\na metric representation for locally distorted metric spaces that captures\ncomplex data structures more accurately than the previous schemes. Our approach\naddresses limitations in existing methods by accommodating non-uniform data\ndistributions and intricate local geometries. We validate its performance\nthrough extensive experiments on examples of various geometric objects and\nbenchmark real-world datasets, demonstrating significant improvements in\nrepresentation quality.\n","authors":["Lukas Silvester Barth"," Fatemeh"," Fahimi","Parvaneh Joharinad","Jürgen Jost","Janis Keck"],"pdf_url":"https://arxiv.org/pdf/2407.17835v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.03059v7","updated":"2024-07-25T07:42:15Z","published":"2023-10-04T16:49:36Z","title":"Point-PEFT: Parameter-Efficient Fine-Tuning for 3D Pre-trained Models","summary":"  The popularity of pre-trained large models has revolutionized downstream\ntasks across diverse fields, such as language, vision, and multi-modality. To\nminimize the adaption cost for downstream tasks, many Parameter-Efficient\nFine-Tuning (PEFT) techniques are proposed for language and 2D image\npre-trained models. However, the specialized PEFT method for 3D pre-trained\nmodels is still under-explored. To this end, we introduce Point-PEFT, a novel\nframework for adapting point cloud pre-trained models with minimal learnable\nparameters. Specifically, for a pre-trained 3D model, we freeze most of its\nparameters, and only tune the newly added PEFT modules on downstream tasks,\nwhich consist of a Point-prior Prompt and a Geometry-aware Adapter. The\nPoint-prior Prompt adopts a set of learnable prompt tokens, for which we\npropose to construct a memory bank with domain-specific knowledge, and utilize\na parameter-free attention to enhance the prompt tokens. The Geometry-aware\nAdapter aims to aggregate point cloud features within spatial neighborhoods to\ncapture fine-grained geometric information through local interactions.\nExtensive experiments indicate that our Point-PEFT can achieve better\nperformance than the full fine-tuning on various downstream tasks, while using\nonly 5% of the trainable parameters, demonstrating the efficiency and\neffectiveness of our approach. Code is released at\nhttps://github.com/Ivan-Tang-3D/Point-PEFT.\n","authors":["Yiwen Tang","Ray Zhang","Zoey Guo","Dong Wang","Zhigang Wang","Bin Zhao","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2310.03059v7.pdf","comment":"The specialized PEFT framework for 3D pre-trained models, which\n  achieves competitive performance to full fine-tuning, and significantly\n  reduces the computational resources. Project page:\n  https://github.com/Ivan-Tang-3D/Point-PEFT"},{"id":"http://arxiv.org/abs/2407.17125v2","updated":"2024-07-25T07:39:44Z","published":"2024-07-24T09:48:48Z","title":"Behavioral Testing: Can Large Language Models Implicitly Resolve\n  Ambiguous Entities?","summary":"  One of the major aspects contributing to the striking performance of large\nlanguage models (LLMs) is the vast amount of factual knowledge accumulated\nduring pre-training. Yet, many LLMs suffer from self-inconsistency, which\nraises doubts about their trustworthiness and reliability. In this paper, we\nfocus on entity type ambiguity and analyze current state-of-the-art LLMs for\ntheir proficiency and consistency in applying their factual knowledge when\nprompted for entities under ambiguity. To do so, we propose an evaluation\nprotocol that disentangles knowing from applying knowledge, and test\nstate-of-the-art LLMs on 49 entities. Our experiments reveal that LLMs perform\npoorly with ambiguous prompts, achieving only 80% accuracy. Our results further\ndemonstrate systematic discrepancies in LLM behavior and their failure to\nconsistently apply information, indicating that the models can exhibit\nknowledge without being able to utilize it, significant biases for preferred\nreadings, as well as self inconsistencies. Our study highlights the importance\nof handling entity ambiguity in future for more trustworthy LLMs\n","authors":["Anastasiia Sedova","Robert Litschko","Diego Frassinelli","Benjamin Roth","Barbara Plank"],"pdf_url":"https://arxiv.org/pdf/2407.17125v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17829v1","updated":"2024-07-25T07:38:27Z","published":"2024-07-25T07:38:27Z","title":"Image Segmentation via Divisive Normalization: dealing with\n  environmental diversity","summary":"  Autonomous driving is a challenging scenario for image segmentation due to\nthe presence of uncontrolled environmental conditions and the eventually\ncatastrophic consequences of failures. Previous work suggested that a\nbiologically motivated computation, the so-called Divisive Normalization, could\nbe useful to deal with image variability, but its effects have not been\nsystematically studied over different data sources and environmental factors.\nHere we put segmentation U-nets augmented with Divisive Normalization to work\nfar from training conditions to find where this adaptation is more critical. We\ncategorize the scenes according to their radiance level and dynamic range\n(day/night), and according to their achromatic/chromatic contrasts. We also\nconsider video game (synthetic) images to broaden the range of environments. We\ncheck the performance in the extreme percentiles of such categorization. Then,\nwe push the limits further by artificially modifying the images in\nperceptually/environmentally relevant dimensions: luminance, contrasts and\nspectral radiance. Results show that neural networks with Divisive\nNormalization get better results in all the scenarios and their performance\nremains more stable with regard to the considered environmental factors and\nnature of the source. Finally, we explain the improvements in segmentation\nperformance in two ways: (1) by quantifying the invariance of the responses\nthat incorporate Divisive Normalization, and (2) by illustrating the adaptive\nnonlinearity of the different layers that depends on the local activity.\n","authors":["Pablo Hernández-Cámara","Jorge Vila-Tomás","Paula Dauden-Oliver","Nuria Alabau-Bosque","Valero Laparra","Jesús Malo"],"pdf_url":"https://arxiv.org/pdf/2407.17829v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17827v1","updated":"2024-07-25T07:35:27Z","published":"2024-07-25T07:35:27Z","title":"Unified Lexical Representation for Interpretable Visual-Language\n  Alignment","summary":"  Visual-Language Alignment (VLA) has gained a lot of attention since CLIP's\ngroundbreaking work. Although CLIP performs well, the typical direct latent\nfeature alignment lacks clarity in its representation and similarity scores. On\nthe other hand, lexical representation, a vector whose element represents the\nsimilarity between the sample and a word from the vocabulary, is a natural\nsparse representation and interpretable, providing exact matches for individual\nwords. However, lexical representations is difficult to learn due to no\nground-truth supervision and false-discovery issues, and thus requires complex\ndesign to train effectively. In this paper, we introduce LexVLA, a more\ninterpretable VLA framework by learning a unified lexical representation for\nboth modalities without complex design. We use DINOv2 as our visual model for\nits local-inclined features and Llama 2, a generative language model, to\nleverage its in-context lexical prediction ability. To avoid the false\ndiscovery, we propose an overuse penalty to refrain the lexical representation\nfrom falsely frequently activating meaningless words. We demonstrate that these\ntwo pre-trained uni-modal models can be well-aligned by fine-tuning on modest\nmulti-modal dataset and avoid intricate training configurations. On cross-modal\nretrieval benchmarks, LexVLA, trained on the CC-12M multi-modal dataset,\noutperforms baselines fine-tuned on larger datasets (e.g., YFCC15M) and those\ntrained from scratch on even bigger datasets (e.g., 1.1B data, including\nCC-12M). We conduct extensive experiments to analyze LexVLA.\n","authors":["Yifan Li","Yikai Wang","Yanwei Fu","Dongyu Ru","Zheng Zhang","Tong He"],"pdf_url":"https://arxiv.org/pdf/2407.17827v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11869v3","updated":"2024-07-25T07:29:02Z","published":"2024-04-18T03:03:37Z","title":"Node-like as a Whole: Structure-aware Searching and Coarsening for Graph\n  Classification","summary":"  Graph Transformers (GTs) have made remarkable achievements in graph-level\ntasks. However, most existing works regard graph structures as a form of\nguidance or bias for enhancing node representations, which focuses on\nnode-central perspectives and lacks explicit representations of edges and\nstructures. One natural question is, can we treat graph structures node-like as\na whole to learn high-level features? Through experimental analysis, we explore\nthe feasibility of this assumption. Based on our findings, we propose a novel\nmulti-view graph representation learning model via structure-aware searching\nand coarsening (GRLsc) on GT architecture for graph classification.\nSpecifically, we build three unique views, original, coarsening, and\nconversion, to learn a thorough structural representation. We compress loops\nand cliques via hierarchical heuristic graph coarsening and restrict them with\nwell-designed constraints, which builds the coarsening view to learn high-level\ninteractions between structures. We also introduce line graphs for edge\nembeddings and switch to edge-central perspective to construct the conversion\nview. Experiments on eight real-world datasets demonstrate the improvements of\nGRLsc over 28 baselines from various architectures.\n","authors":["Xiaorui Qi","Qijie Bai","Yanlong Wen","Haiwei Zhang","Xiaojie Yuan"],"pdf_url":"https://arxiv.org/pdf/2404.11869v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17823v1","updated":"2024-07-25T07:25:06Z","published":"2024-07-25T07:25:06Z","title":"Optimal Hessian/Jacobian-Free Nonconvex-PL Bilevel Optimization","summary":"  Bilevel optimization is widely applied in many machine learning tasks such as\nhyper-parameter learning, meta learning and reinforcement learning. Although\nmany algorithms recently have been developed to solve the bilevel optimization\nproblems, they generally rely on the (strongly) convex lower-level problems.\nMore recently, some methods have been proposed to solve the nonconvex-PL\nbilevel optimization problems, where their upper-level problems are possibly\nnonconvex, and their lower-level problems are also possibly nonconvex while\nsatisfying Polyak-{\\L}ojasiewicz (PL) condition. However, these methods still\nhave a high convergence complexity or a high computation complexity such as\nrequiring compute expensive Hessian/Jacobian matrices and its inverses. In the\npaper, thus, we propose an efficient Hessian/Jacobian-free method (i.e.,\nHJFBiO) with the optimal convergence complexity to solve the nonconvex-PL\nbilevel problems. Theoretically, under some mild conditions, we prove that our\nHJFBiO method obtains an optimal convergence rate of $O(\\frac{1}{T})$, where\n$T$ denotes the number of iterations, and has an optimal gradient complexity of\n$O(\\epsilon^{-1})$ in finding an $\\epsilon$-stationary solution. We conduct\nsome numerical experiments on the bilevel PL game and hyper-representation\nlearning task to demonstrate efficiency of our proposed method.\n","authors":["Feihu Huang"],"pdf_url":"https://arxiv.org/pdf/2407.17823v1.pdf","comment":"ICML 2024 (Oral). arXiv admin note: text overlap with\n  arXiv:2311.04520"},{"id":"http://arxiv.org/abs/2407.17822v1","updated":"2024-07-25T07:24:41Z","published":"2024-07-25T07:24:41Z","title":"Advanced deep-reinforcement-learning methods for flow control:\n  group-invariant and positional-encoding networks improve learning speed and\n  quality","summary":"  Flow control is key to maximize energy efficiency in a wide range of\napplications. However, traditional flow-control methods face significant\nchallenges in addressing non-linear systems and high-dimensional data, limiting\ntheir application in realistic energy systems. This study advances\ndeep-reinforcement-learning (DRL) methods for flow control, particularly\nfocusing on integrating group-invariant networks and positional encoding into\nDRL architectures. Our methods leverage multi-agent reinforcement learning\n(MARL) to exploit policy invariance in space, in combination with\ngroup-invariant networks to ensure local symmetry invariance. Additionally, a\npositional encoding inspired by the transformer architecture is incorporated to\nprovide location information to the agents, mitigating action constraints from\nstrict invariance. The proposed methods are verified using a case study of\nRayleigh-B\\'enard convection, where the goal is to minimize the Nusselt number\nNu. The group-invariant neural networks (GI-NNs) show faster convergence\ncompared to the base MARL, achieving better average policy performance. The\nGI-NNs not only cut DRL training time in half but also notably enhance learning\nreproducibility. Positional encoding further enhances these results,\neffectively reducing the minimum Nu and stabilizing convergence. Interestingly,\ngroup invariant networks specialize in improving learning speed and positional\nencoding specializes in improving learning quality. These results demonstrate\nthat choosing a suitable feature-representation method according to the purpose\nas well as the characteristics of each control problem is essential. We believe\nthat the results of this study will not only inspire novel DRL methods with\ninvariant and unique representations, but also provide useful insights for\nindustrial applications.\n","authors":["Joogoo Jeon","Jean Rabault","Joel Vasanth","Francisco Alcántara-Ávila","Shilaj Baral","Ricardo Vinuesa"],"pdf_url":"https://arxiv.org/pdf/2407.17822v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15899v3","updated":"2024-07-25T07:18:05Z","published":"2024-07-22T10:20:34Z","title":"Spatial-Temporal Cross-View Contrastive Pre-training for Check-in\n  Sequence Representation Learning","summary":"  The rapid growth of location-based services (LBS) has yielded massive amounts\nof data on human mobility. Effectively extracting meaningful representations\nfor user-generated check-in sequences is pivotal for facilitating various\ndownstream services. However, the user-generated check-in data are\nsimultaneously influenced by the surrounding objective circumstances and the\nuser's subjective intention. Specifically, the temporal uncertainty and spatial\ndiversity exhibited in check-in data make it difficult to capture the\nmacroscopic spatial-temporal patterns of users and to understand the semantics\nof user mobility activities. Furthermore, the distinct characteristics of the\ntemporal and spatial information in check-in sequences call for an effective\nfusion method to incorporate these two types of information. In this paper, we\npropose a novel Spatial-Temporal Cross-view Contrastive Representation (STCCR)\nframework for check-in sequence representation learning. Specifically, STCCR\naddresses the above challenges by employing self-supervision from \"spatial\ntopic\" and \"temporal intention\" views, facilitating effective fusion of spatial\nand temporal information at the semantic level. Besides, STCCR leverages\ncontrastive clustering to uncover users' shared spatial topics from diverse\nmobility activities, while employing angular momentum contrast to mitigate the\nimpact of temporal uncertainty and noise. We extensively evaluate STCCR on\nthree real-world datasets and demonstrate its superior performance across three\ndownstream tasks.\n","authors":["Letian Gong","Huaiyu Wan","Shengnan Guo","Xiucheng Li","Yan Lin","Erwen Zheng","Tianyi Wang","Zeyu Zhou","Youfang Lin"],"pdf_url":"https://arxiv.org/pdf/2407.15899v3.pdf","comment":"This paper has been accepted as a regular paper at IEEE TKDE"},{"id":"http://arxiv.org/abs/2407.17817v1","updated":"2024-07-25T07:10:31Z","published":"2024-07-25T07:10:31Z","title":"Demystifying Verbatim Memorization in Large Language Models","summary":"  Large Language Models (LLMs) frequently memorize long sequences verbatim,\noften with serious legal and privacy implications. Much prior work has studied\nsuch verbatim memorization using observational data. To complement such work,\nwe develop a framework to study verbatim memorization in a controlled setting\nby continuing pre-training from Pythia checkpoints with injected sequences. We\nfind that (1) non-trivial amounts of repetition are necessary for verbatim\nmemorization to happen; (2) later (and presumably better) checkpoints are more\nlikely to verbatim memorize sequences, even for out-of-distribution sequences;\n(3) the generation of memorized sequences is triggered by distributed model\nstates that encode high-level features and makes important use of general\nlanguage modeling capabilities. Guided by these insights, we develop stress\ntests to evaluate unlearning methods and find they often fail to remove the\nverbatim memorized information, while also degrading the LM. Overall, these\nfindings challenge the hypothesis that verbatim memorization stems from\nspecific model weights or mechanisms. Rather, verbatim memorization is\nintertwined with the LM's general capabilities and thus will be very difficult\nto isolate and suppress without degrading model quality.\n","authors":["Jing Huang","Diyi Yang","Christopher Potts"],"pdf_url":"https://arxiv.org/pdf/2407.17817v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17816v1","updated":"2024-07-25T07:10:08Z","published":"2024-07-25T07:10:08Z","title":"NC-NCD: Novel Class Discovery for Node Classification","summary":"  Novel Class Discovery (NCD) involves identifying new categories within\nunlabeled data by utilizing knowledge acquired from previously established\ncategories. However, existing NCD methods often struggle to maintain a balance\nbetween the performance of old and new categories. Discovering unlabeled new\ncategories in a class-incremental way is more practical but also more\nchallenging, as it is frequently hindered by either catastrophic forgetting of\nold categories or an inability to learn new ones. Furthermore, the\nimplementation of NCD on continuously scalable graph-structured data remains an\nunder-explored area. In response to these challenges, we introduce for the\nfirst time a more practical NCD scenario for node classification (i.e.,\nNC-NCD), and propose a novel self-training framework with prototype replay and\ndistillation called SWORD, adopted to our NC-NCD setting. Our approach enables\nthe model to cluster unlabeled new category nodes after learning labeled nodes\nwhile preserving performance on old categories without reliance on old category\nnodes. SWORD achieves this by employing a self-training strategy to learn new\ncategories and preventing the forgetting of old categories through the joint\nuse of feature prototypes and knowledge distillation. Extensive experiments on\nfour common benchmarks demonstrate the superiority of SWORD over other\nstate-of-the-art methods.\n","authors":["Yue Hou","Xueyuan Chen","He Zhu","Romei Liu","Bowen Shi","Jiaheng Liu","Junran Wu","Ke Xu"],"pdf_url":"https://arxiv.org/pdf/2407.17816v1.pdf","comment":"Accepted by CIKM'24"},{"id":"http://arxiv.org/abs/2407.17815v1","updated":"2024-07-25T07:09:53Z","published":"2024-07-25T07:09:53Z","title":"Nested replicator dynamics, nested logit choice, and similarity-based\n  learning","summary":"  We consider a model of learning and evolution in games whose action sets are\nendowed with a partition-based similarity structure intended to capture\nexogenous similarities between strategies. In this model, revising agents have\na higher probability of comparing their current strategy with other strategies\nthat they deem similar, and they switch to the observed strategy with\nprobability proportional to its payoff excess. Because of this implicit bias\ntoward similar strategies, the resulting dynamics - which we call the nested\nreplicator dynamics - do not satisfy any of the standard monotonicity\npostulates for imitative game dynamics; nonetheless, we show that they retain\nthe main long-run rationality properties of the replicator dynamics, albeit at\nquantitatively different rates. We also show that the induced dynamics can be\nviewed as a stimulus-response model in the spirit of Erev & Roth (1998), with\nchoice probabilities given by the nested logit choice rule of Ben-Akiva (1973)\nand McFadden (1978). This result generalizes an existing relation between the\nreplicator dynamics and the exponential weights algorithm in online learning,\nand provides an additional layer of interpretation to our analysis and results.\n","authors":["Panayotis Mertikopoulos","William H. Sandholm"],"pdf_url":"https://arxiv.org/pdf/2407.17815v1.pdf","comment":"37 pages, 9 figures"},{"id":"http://arxiv.org/abs/2407.11652v2","updated":"2024-07-25T07:04:32Z","published":"2024-07-16T12:18:20Z","title":"CCVA-FL: Cross-Client Variations Adaptive Federated Learning for Medical\n  Imaging","summary":"  Federated Learning (FL) offers a privacy-preserving approach to train models\non decentralized data. Its potential in healthcare is significant, but\nchallenges arise due to cross-client variations in medical image data,\nexacerbated by limited annotations. This paper introduces Cross-Client\nVariations Adaptive Federated Learning (CCVA-FL) to address these issues.\nCCVA-FL aims to minimize cross-client variations by transforming images into a\ncommon feature space. It involves expert annotation of a subset of images from\neach client, followed by the selection of a client with the least data\ncomplexity as the target. Synthetic medical images are then generated using\nScalable Diffusion Models with Transformers (DiT) based on the target client's\nannotated images. These synthetic images, capturing diversity and representing\nthe original data, are shared with other clients. Each client then translates\nits local images into the target image space using image-to-image translation.\nThe translated images are subsequently used in a federated learning setting to\ndevelop a server model. Our results demonstrate that CCVA-FL outperforms\nVanilla Federated Averaging by effectively addressing data distribution\ndifferences across clients without compromising privacy.\n","authors":["Sunny Gupta","Amit Sethi"],"pdf_url":"https://arxiv.org/pdf/2407.11652v2.pdf","comment":"10 pages, 6 figures"},{"id":"http://arxiv.org/abs/2206.02617v7","updated":"2024-07-25T06:33:58Z","published":"2022-06-06T13:49:37Z","title":"Individual Privacy Accounting for Differentially Private Stochastic\n  Gradient Descent","summary":"  Differentially private stochastic gradient descent (DP-SGD) is the workhorse\nalgorithm for recent advances in private deep learning. It provides a single\nprivacy guarantee to all datapoints in the dataset. We propose output-specific\n$(\\varepsilon,\\delta)$-DP to characterize privacy guarantees for individual\nexamples when releasing models trained by DP-SGD. We also design an efficient\nalgorithm to investigate individual privacy across a number of datasets. We\nfind that most examples enjoy stronger privacy guarantees than the worst-case\nbound. We further discover that the training loss and the privacy parameter of\nan example are well-correlated. This implies groups that are underserved in\nterms of model utility simultaneously experience weaker privacy guarantees. For\nexample, on CIFAR-10, the average $\\varepsilon$ of the class with the lowest\ntest accuracy is 44.2\\% higher than that of the class with the highest\naccuracy.\n","authors":["Da Yu","Gautam Kamath","Janardhan Kulkarni","Tie-Yan Liu","Jian Yin","Huishuai Zhang"],"pdf_url":"https://arxiv.org/pdf/2206.02617v7.pdf","comment":"Add clarification about the applicability of Definition 4"},{"id":"http://arxiv.org/abs/2402.12656v4","updated":"2024-07-25T06:28:01Z","published":"2024-02-20T02:09:55Z","title":"HyperMoE: Towards Better Mixture of Experts via Transferring Among\n  Experts","summary":"  The Mixture of Experts (MoE) for language models has been proven effective in\naugmenting the capacity of models by dynamically routing each input token to a\nspecific subset of experts for processing. Despite the success, most existing\nmethods face a challenge for balance between sparsity and the availability of\nexpert knowledge: enhancing performance through increased use of expert\nknowledge often results in diminishing sparsity during expert selection. To\nmitigate this contradiction, we propose HyperMoE, a novel MoE framework built\nupon Hypernetworks. This framework integrates the computational processes of\nMoE with the concept of knowledge transferring in multi-task learning. Specific\nmodules generated based on the information of unselected experts serve as\nsupplementary information, which allows the knowledge of experts not selected\nto be used while maintaining selection sparsity. Our comprehensive empirical\nevaluations across multiple datasets and backbones establish that HyperMoE\nsignificantly outperforms existing MoE methods under identical conditions\nconcerning the number of experts.\n","authors":["Hao Zhao","Zihan Qiu","Huijia Wu","Zili Wang","Zhaofeng He","Jie Fu"],"pdf_url":"https://arxiv.org/pdf/2402.12656v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17803v1","updated":"2024-07-25T06:22:25Z","published":"2024-07-25T06:22:25Z","title":"Automatic Data Labeling for Software Vulnerability Prediction Models:\n  How Far Are We?","summary":"  Background: Software Vulnerability (SV) prediction needs large-sized and\nhigh-quality data to perform well. Current SV datasets mostly require expensive\nlabeling efforts by experts (human-labeled) and thus are limited in size.\nMeanwhile, there are growing efforts in automatic SV labeling at scale.\nHowever, the fitness of auto-labeled data for SV prediction is still largely\nunknown. Aims: We quantitatively and qualitatively study the quality and use of\nthe state-of-the-art auto-labeled SV data, D2A, for SV prediction. Method:\nUsing multiple sources and manual validation, we curate clean SV data from\nhuman-labeled SV-fixing commits in two well-known projects for investigating\nthe auto-labeled counterparts. Results: We discover that 50+% of the\nauto-labeled SVs are noisy (incorrectly labeled), and they hardly overlap with\nthe publicly reported ones. Yet, SV prediction models utilizing the noisy\nauto-labeled SVs can perform up to 22% and 90% better in Matthews Correlation\nCoefficient and Recall, respectively, than the original models. We also reveal\nthe promises and difficulties of applying noise-reduction methods for\nautomatically addressing the noise in auto-labeled SV data to maximize the data\nutilization for SV prediction. Conclusions: Our study informs the benefits and\nchallenges of using auto-labeled SVs, paving the way for large-scale SV\nprediction.\n","authors":["Triet H. M. Le","M. Ali Babar"],"pdf_url":"https://arxiv.org/pdf/2407.17803v1.pdf","comment":"Accepted as a full paper in the technical track at The International\n  Symposium on Empirical Software Engineering and Measurement (ESEM) 2024"},{"id":"http://arxiv.org/abs/2407.17236v2","updated":"2024-07-25T06:21:01Z","published":"2024-07-24T12:45:02Z","title":"Statistical Batch-Based Bearing Fault Detection","summary":"  In the domain of rotating machinery, bearings are vulnerable to different\nmechanical faults, including ball, inner, and outer race faults. Various\ntechniques can be used in condition-based monitoring, from classical signal\nanalysis to deep learning methods. Based on the complex working conditions of\nrotary machines, multivariate statistical process control charts such as\nHotelling's $T^2$ and Squared Prediction Error are useful for providing early\nwarnings. However, these methods are rarely applied to condition monitoring of\nrotating machinery due to the univariate nature of the datasets. In the present\npaper, we propose a multivariate statistical process control-based fault\ndetection method that utilizes multivariate data composed of Fourier transform\nfeatures extracted for fixed-time batches. Our approach makes use of the\nmultidimensional nature of Fourier transform characteristics, which record more\ndetailed information about the machine's status, in an effort to enhance early\ndefect detection and diagnosis. Experiments with varying vibration measurement\nlocations (Fan End, Drive End), fault types (ball, inner, and outer race\nfaults), and motor loads (0-3 horsepower) are used to validate the suggested\napproach. The outcomes illustrate our method's effectiveness in fault detection\nand point to possible broader uses in industrial maintenance.\n","authors":["Victoria Jorry","Zina-Sabrina Duma","Tuomas Sihvonen","Satu-Pia Reinikainen","Lassi Roininen"],"pdf_url":"https://arxiv.org/pdf/2407.17236v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17801v1","updated":"2024-07-25T06:20:03Z","published":"2024-07-25T06:20:03Z","title":"EEG-SSM: Leveraging State-Space Model for Dementia Detection","summary":"  State-space models (SSMs) have garnered attention for effectively processing\nlong data sequences, reducing the need to segment time series into shorter\nintervals for model training and inference. Traditionally, SSMs capture only\nthe temporal dynamics of time series data, omitting the equally critical\nspectral features. This study introduces EEG-SSM, a novel state-space\nmodel-based approach for dementia classification using EEG data. Our model\nfeatures two primary innovations: EEG-SSM temporal and EEG-SSM spectral\ncomponents. The temporal component is designed to efficiently process EEG\nsequences of varying lengths, while the spectral component enhances the model\nby integrating frequency-domain information from EEG signals. The synergy of\nthese components allows EEG-SSM to adeptly manage the complexities of\nmultivariate EEG data, significantly improving accuracy and stability across\ndifferent temporal resolutions. Demonstrating a remarkable 91.0 percent\naccuracy in classifying Healthy Control (HC), Frontotemporal Dementia (FTD),\nand Alzheimer's Disease (AD) groups, EEG-SSM outperforms existing models on the\nsame dataset. The development of EEG-SSM represents an improvement in the use\nof state-space models for screening dementia, offering more precise and\ncost-effective tools for clinical neuroscience.\n","authors":["Xuan-The Tran","Linh Le","Quoc Toan Nguyen","Thomas Do","Chin-Teng Lin"],"pdf_url":"https://arxiv.org/pdf/2407.17801v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17795v1","updated":"2024-07-25T06:09:44Z","published":"2024-07-25T06:09:44Z","title":"Enhancing Diversity in Multi-objective Feature Selection","summary":"  Feature selection plays a pivotal role in the data preprocessing and\nmodel-building pipeline, significantly enhancing model performance,\ninterpretability, and resource efficiency across diverse domains. In\npopulation-based optimization methods, the generation of diverse individuals\nholds utmost importance for adequately exploring the problem landscape,\nparticularly in highly multi-modal multi-objective optimization problems. Our\nstudy reveals that, in line with findings from several prior research papers,\ncommonly employed crossover and mutation operations lack the capability to\ngenerate high-quality diverse individuals and tend to become confined to\nlimited areas around various local optima. This paper introduces an\naugmentation to the diversity of the population in the well-established\nmulti-objective scheme of the genetic algorithm, NSGA-II. This enhancement is\nachieved through two key components: the genuine initialization method and the\nsubstitution of the worst individuals with new randomly generated individuals\nas a re-initialization approach in each generation. The proposed\nmulti-objective feature selection method undergoes testing on twelve real-world\nclassification problems, with the number of features ranging from 2,400 to\nnearly 50,000. The results demonstrate that replacing the last front of the\npopulation with an equivalent number of new random individuals generated using\nthe genuine initialization method and featuring a limited number of features\nsubstantially improves the population's quality and, consequently, enhances the\nperformance of the multi-objective algorithm.\n","authors":["Sevil Zanjani Miyandoab","Shahryar Rahnamayan","Azam Asilian Bidgoli","Sevda Ebrahimi","Masoud Makrehchi"],"pdf_url":"https://arxiv.org/pdf/2407.17795v1.pdf","comment":"8 pages, 3 figures, accepted to be published in IEEE WCCI 2024\n  conference"},{"id":"http://arxiv.org/abs/2404.09302v2","updated":"2024-07-25T06:05:54Z","published":"2024-04-14T16:57:41Z","title":"High Significant Fault Detection in Azure Core Workload Insights","summary":"  Azure Core workload insights have time-series data with different metric\nunits. Faults or Anomalies are observed in these time-series data owing to\nfaults observed with respect to metric name, resources region, dimensions, and\nits dimension value associated with the data. For Azure Core, an important task\nis to highlight faults or anomalies to the user on a dashboard that they can\nperceive easily. The number of anomalies reported should be highly significant\nand in a limited number, e.g., 5-20 anomalies reported per hour. The reported\nanomalies will have significant user perception and high reconstruction error\nin any time-series forecasting model. Hence, our task is to automatically\nidentify 'high significant anomalies' and their associated information for user\nperception.\n","authors":["Pranay Lohia","Laurent Boue","Sharath Rangappa","Vijay Agneeswaran"],"pdf_url":"https://arxiv.org/pdf/2404.09302v2.pdf","comment":"Published in IAAI 2024, which is the Industrial track of AAAI 2024"},{"id":"http://arxiv.org/abs/2305.14275v3","updated":"2024-07-25T05:53:46Z","published":"2023-05-23T17:24:04Z","title":"Variational Inference with Coverage Guarantees in Simulation-Based\n  Inference","summary":"  Amortized variational inference is an often employed framework in\nsimulation-based inference that produces a posterior approximation that can be\nrapidly computed given any new observation. Unfortunately, there are few\nguarantees about the quality of these approximate posteriors. We propose\nConformalized Amortized Neural Variational Inference (CANVI), a procedure that\nis scalable, easily implemented, and provides guaranteed marginal coverage.\nGiven a collection of candidate amortized posterior approximators, CANVI\nconstructs conformalized predictors based on each candidate, compares the\npredictors using a metric known as predictive efficiency, and returns the most\nefficient predictor. CANVI ensures that the resulting predictor constructs\nregions that contain the truth with a user-specified level of probability.\nCANVI is agnostic to design decisions in formulating the candidate\napproximators and only requires access to samples from the forward model,\npermitting its use in likelihood-free settings. We prove lower bounds on the\npredictive efficiency of the regions produced by CANVI and explore how the\nquality of a posterior approximation relates to the predictive efficiency of\nprediction regions based on that approximation. Finally, we demonstrate the\naccurate calibration and high predictive efficiency of CANVI on a suite of\nsimulation-based inference benchmark tasks and an important scientific task:\nanalyzing galaxy emission spectra.\n","authors":["Yash Patel","Declan McNamara","Jackson Loper","Jeffrey Regier","Ambuj Tewari"],"pdf_url":"https://arxiv.org/pdf/2305.14275v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17790v1","updated":"2024-07-25T05:52:48Z","published":"2024-07-25T05:52:48Z","title":"Exploring the Limitations of Kolmogorov-Arnold Networks in\n  Classification: Insights to Software Training and Hardware Implementation","summary":"  Kolmogorov-Arnold Networks (KANs), a novel type of neural network, have\nrecently gained popularity and attention due to the ability to substitute\nmulti-layer perceptions (MLPs) in artificial intelligence (AI) with higher\naccuracy and interoperability. However, KAN assessment is still limited and\ncannot provide an in-depth analysis of a specific domain. Furthermore, no study\nhas been conducted on the implementation of KANs in hardware design, which\nwould directly demonstrate whether KANs are truly superior to MLPs in practical\napplications. As a result, in this paper, we focus on verifying KANs for\nclassification issues, which are a common but significant topic in AI using\nfour different types of datasets. Furthermore, the corresponding hardware\nimplementation is considered using the Vitis high-level synthesis (HLS) tool.\nTo the best of our knowledge, this is the first article to implement hardware\nfor KAN. The results indicate that KANs cannot achieve more accuracy than MLPs\nin high complex datasets while utilizing substantially higher hardware\nresources. Therefore, MLP remains an effective approach for achieving accuracy\nand efficiency in software and hardware implementation.\n","authors":["an Duy Tran","Tran Xuan Hieu Le","Thi Diem Tran","Hoai Luan Pham","Vu Trung Duong Le","Tuan Hai Vu","Van Tinh Nguyen","Yasuhiko Nakashima"],"pdf_url":"https://arxiv.org/pdf/2407.17790v1.pdf","comment":"6 pages, 3 figures, 2 tables"},{"id":"http://arxiv.org/abs/2407.17781v1","updated":"2024-07-25T05:22:08Z","published":"2024-07-25T05:22:08Z","title":"Integrating Ensemble Kalman Filter with AI-based Weather Prediction\n  Model ClimaX","summary":"  Artificial intelligence (AI)-based weather prediction research is growing\nrapidly and has shown to be competitive with the advanced dynamic numerical\nweather prediction models. However, research combining AI-based weather\nprediction models with data assimilation remains limited partially because\nlong-term sequential data assimilation cycles are required to evaluate data\nassimilation systems. This study explores integrating the local ensemble\ntransform Kalman filter (LETKF) with an AI-based weather prediction model\nClimaX. Our experiments demonstrated that the ensemble data assimilation cycled\nstably for the AI-based weather prediction model using covariance inflation and\nlocalization techniques inside the LETKF. While ClimaX showed some limitations\nin capturing flow-dependent error covariance compared to dynamical models, the\nAI-based ensemble forecasts provided reasonable and beneficial error covariance\nin sparsely observed regions. These findings highlight the potential of AI\nmodels in weather forecasting and the importance of physical consistency and\naccurate error growth representation in improving ensemble data assimilation.\n","authors":["Shunji Kotsuki","Kenta Shiraishi","Atsushi Okazaki"],"pdf_url":"https://arxiv.org/pdf/2407.17781v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17773v1","updated":"2024-07-25T05:02:39Z","published":"2024-07-25T05:02:39Z","title":"KiVA: Kid-inspired Visual Analogies for Testing Large Multimodal Models","summary":"  This paper investigates visual analogical reasoning in large multimodal\nmodels (LMMs) compared to human adults and children. A \"visual analogy\" is an\nabstract rule inferred from one image and applied to another. While benchmarks\nexist for testing visual reasoning in LMMs, they require advanced skills and\nomit basic visual analogies that even young children can make. Inspired by\ndevelopmental psychology, we propose a new benchmark of 1,400 visual\ntransformations of everyday objects to test LMMs on visual analogical reasoning\nand compare them to children and adults. We structure the evaluation into three\nstages: identifying what changed (e.g., color, number, etc.), how it changed\n(e.g., added one object), and applying the rule to new scenarios. Our findings\nshow that while models like GPT-4V, LLaVA-1.5, and MANTIS identify the \"what\"\neffectively, they struggle with quantifying the \"how\" and extrapolating this\nrule to new objects. In contrast, children and adults exhibit much stronger\nanalogical reasoning at all three stages. Additionally, the strongest tested\nmodel, GPT-4V, performs better in tasks involving simple visual attributes like\ncolor and size, correlating with quicker human adult response times.\nConversely, more complex tasks such as number, rotation, and reflection, which\nnecessitate extensive cognitive processing and understanding of the 3D physical\nworld, present more significant challenges. Altogether, these findings\nhighlight the limitations of training models on data that primarily consists of\n2D images and text.\n","authors":["Eunice Yiu","Maan Qraitem","Charlie Wong","Anisa Noor Majhi","Yutong Bai","Shiry Ginosar","Alison Gopnik","Kate Saenko"],"pdf_url":"https://arxiv.org/pdf/2407.17773v1.pdf","comment":"9 pages. For the KiVA benchmark, see https://github.com/ey242/KiVA"},{"id":"http://arxiv.org/abs/2407.17767v1","updated":"2024-07-25T04:48:56Z","published":"2024-07-25T04:48:56Z","title":"Online Learning for Autonomous Management of Intent-based 6G Networks","summary":"  The growing complexity of networks and the variety of future scenarios with\ndiverse and often stringent performance requirements call for a higher level of\nautomation. Intent-based management emerges as a solution to attain high level\nof automation, enabling human operators to solely communicate with the network\nthrough high-level intents. The intents consist of the targets in the form of\nexpectations (i.e., latency expectation) from a service and based on the\nexpectations the required network configurations should be done accordingly. It\nis almost inevitable that when a network action is taken to fulfill one intent,\nit can cause negative impacts on the performance of another intent, which\nresults in a conflict. In this paper, we aim to address the conflict issue and\nautonomous management of intent-based networking, and propose an online\nlearning method based on the hierarchical multi-armed bandits approach for an\neffective management. Thanks to this hierarchical structure, it performs an\nefficient exploration and exploitation of network configurations with respect\nto the dynamic network conditions. We show that our algorithm is an effective\napproach regarding resource allocation and satisfaction of intent expectations.\n","authors":["Erciyes Karakaya","Ozgur Ercetin","Huseyin Ozkan","Mehmet Karaca","Elham Dehghan Biyar","Alexandros Palaios"],"pdf_url":"https://arxiv.org/pdf/2407.17767v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.02290v2","updated":"2024-07-25T04:43:32Z","published":"2024-02-03T23:04:32Z","title":"Goodness-of-Fit and Clustering of Spherical Data: the QuadratiK package\n  in R and Python","summary":"  We introduce the QuadratiK package that incorporates innovative data analysis\nmethodologies. The presented software, implemented in both R and Python, offers\na comprehensive set of goodness-of-fit tests and clustering techniques using\nkernel-based quadratic distances, thereby bridging the gap between the\nstatistical and machine learning literatures. Our software implements one, two\nand k-sample tests for goodness of fit, providing an efficient and\nmathematically sound way to assess the fit of probability distributions.\nExpanded capabilities of our software include supporting tests for uniformity\non the d-dimensional Sphere based on Poisson kernel densities. Particularly\nnoteworthy is the incorporation of a unique clustering algorithm specifically\ntailored for spherical data that leverages a mixture of Poisson kernel-based\ndensities on the sphere. Alongside this, our software includes additional\ngraphical functions, aiding the users in validating, as well as visualizing and\nrepresenting clustering results. This enhances interpretability and usability\nof the analysis. In summary, our R and Python packages serve as a powerful\nsuite of tools, offering researchers and practitioners the means to delve\ndeeper into their data, draw robust inference, and conduct potentially\nimpactful analyses and inference across a wide array of disciplines.\n","authors":["Giovanni Saraceno","Marianthi Markatou","Raktim Mukhopadhyay","Mojgan Golzy"],"pdf_url":"https://arxiv.org/pdf/2402.02290v2.pdf","comment":"36 pages, 9 figures"},{"id":"http://arxiv.org/abs/2308.16884v2","updated":"2024-07-25T04:30:15Z","published":"2023-08-31T17:43:08Z","title":"The Belebele Benchmark: a Parallel Reading Comprehension Dataset in 122\n  Language Variants","summary":"  We present Belebele, a multiple-choice machine reading comprehension (MRC)\ndataset spanning 122 language variants. Significantly expanding the language\ncoverage of natural language understanding (NLU) benchmarks, this dataset\nenables the evaluation of text models in high-, medium-, and low-resource\nlanguages. Each question is based on a short passage from the Flores-200\ndataset and has four multiple-choice answers. The questions were carefully\ncurated to discriminate between models with different levels of general\nlanguage comprehension. The English dataset on its own proves difficult enough\nto challenge state-of-the-art language models. Being fully parallel, this\ndataset enables direct comparison of model performance across all languages. We\nuse this dataset to evaluate the capabilities of multilingual masked language\nmodels (MLMs) and large language models (LLMs). We present extensive results\nand find that despite significant cross-lingual transfer in English-centric\nLLMs, much smaller MLMs pretrained on balanced multilingual data still\nunderstand far more languages. We also observe that larger vocabulary size and\nconscious vocabulary construction correlate with better performance on\nlow-resource languages. Overall, Belebele opens up new avenues for evaluating\nand analyzing the multilingual capabilities of NLP systems.\n","authors":["Lucas Bandarkar","Davis Liang","Benjamin Muller","Mikel Artetxe","Satya Narayan Shukla","Donald Husa","Naman Goyal","Abhinandan Krishnan","Luke Zettlemoyer","Madian Khabsa"],"pdf_url":"https://arxiv.org/pdf/2308.16884v2.pdf","comment":"ACL 2024"},{"id":"http://arxiv.org/abs/2407.11358v2","updated":"2024-07-25T04:20:12Z","published":"2024-07-16T03:46:57Z","title":"SES: Bridging the Gap Between Explainability and Prediction of Graph\n  Neural Networks","summary":"  Despite the Graph Neural Networks' (GNNs) proficiency in analyzing graph\ndata, achieving high-accuracy and interpretable predictions remains\nchallenging. Existing GNN interpreters typically provide post-hoc explanations\ndisjointed from GNNs' predictions, resulting in misrepresentations.\nSelf-explainable GNNs offer built-in explanations during the training process.\nHowever, they cannot exploit the explanatory outcomes to augment prediction\nperformance, and they fail to provide high-quality explanations of node\nfeatures and require additional processes to generate explainable subgraphs,\nwhich is costly. To address the aforementioned limitations, we propose a\nself-explained and self-supervised graph neural network (SES) to bridge the gap\nbetween explainability and prediction. SES comprises two processes: explainable\ntraining and enhanced predictive learning. During explainable training, SES\nemploys a global mask generator co-trained with a graph encoder and directly\nproduces crucial structure and feature masks, reducing time consumption and\nproviding node feature and subgraph explanations. In the enhanced predictive\nlearning phase, mask-based positive-negative pairs are constructed utilizing\nthe explanations to compute a triplet loss and enhance the node representations\nby contrastive learning.\n","authors":["Zhenhua Huang","Kunhao Li","Shaojie Wang","Zhaohong Jia","Wentao Zhu","Sharad Mehrotra"],"pdf_url":"https://arxiv.org/pdf/2407.11358v2.pdf","comment":"Accepted as a conference paper at ICDE 2024"},{"id":"http://arxiv.org/abs/2403.08757v3","updated":"2024-07-25T04:12:17Z","published":"2024-03-13T17:55:34Z","title":"Efficient Combinatorial Optimization via Heat Diffusion","summary":"  Combinatorial optimization problems are widespread but inherently challenging\ndue to their discrete nature. The primary limitation of existing methods is\nthat they can only access a small fraction of the solution space at each\niteration, resulting in limited efficiency for searching the global optimal.To\novercome this challenge, diverging from conventional efforts of expanding the\nsolver's search scope, we focus on enabling information to actively propagate\nto the solver through heat diffusion. By transforming the target function while\npreserving its optima, heat diffusion facilitates information flow from distant\nregions to the solver, providing more efficient navigation. Utilizing heat\ndiffusion, we propose a framework for solving general combinatorial\noptimization problems.The proposed methodology demonstrates superior\nperformance across a range of the most challenging and widely encountered\ncombinatorial optimizations. Echoing recent advancements in harnessing\nthermodynamics for generative artificial intelligence, our study further\nreveals its significant potential in advancing combinatorial optimization.\n","authors":["Hengyuan Ma","Wenlian Lu","Jianfeng Feng"],"pdf_url":"https://arxiv.org/pdf/2403.08757v3.pdf","comment":"Code is available in https://github.com/AwakerMhy/HeO"},{"id":"http://arxiv.org/abs/2407.17754v1","updated":"2024-07-25T04:09:12Z","published":"2024-07-25T04:09:12Z","title":"DualFed: Enjoying both Generalization and Personalization in Federated\n  Learning via Hierachical Representations","summary":"  In personalized federated learning (PFL), it is widely recognized that\nachieving both high model generalization and effective personalization poses a\nsignificant challenge due to their conflicting nature. As a result, existing\nPFL methods can only manage a trade-off between these two objectives. This\nraises an interesting question: Is it feasible to develop a model capable of\nachieving both objectives simultaneously? Our paper presents an affirmative\nanswer, and the key lies in the observation that deep models inherently exhibit\nhierarchical architectures, which produce representations with various levels\nof generalization and personalization at different stages. A straightforward\napproach stemming from this observation is to select multiple representations\nfrom these layers and combine them to concurrently achieve generalization and\npersonalization. However, the number of candidate representations is commonly\nhuge, which makes this method infeasible due to high computational costs.To\naddress this problem, we propose DualFed, a new method that can directly yield\ndual representations correspond to generalization and personalization\nrespectively, thereby simplifying the optimization task. Specifically, DualFed\ninserts a personalized projection network between the encoder and classifier.\nThe pre-projection representations are able to capture generalized information\nshareable across clients, and the post-projection representations are effective\nto capture task-specific information on local clients. This design minimizes\nthe mutual interference between generalization and personalization, thereby\nachieving a win-win situation. Extensive experiments show that DualFed can\noutperform other FL methods. Code is available at\nhttps://github.com/GuogangZhu/DualFed.\n","authors":["Guogang Zhu","Xuefeng Liu","Jianwei Niu","Shaojie Tang","Xinghao Wu","Jiayuan Zhang"],"pdf_url":"https://arxiv.org/pdf/2407.17754v1.pdf","comment":"Accepted by ACM MutltiMedia 2024"},{"id":"http://arxiv.org/abs/2404.01039v3","updated":"2024-07-25T03:35:48Z","published":"2024-04-01T10:50:34Z","title":"A Survey on Hypergraph Neural Networks: An In-Depth and Step-By-Step\n  Guide","summary":"  Higher-order interactions (HOIs) are ubiquitous in real-world complex systems\nand applications. Investigation of deep learning for HOIs, thus, has become a\nvaluable agenda for the data mining and machine learning communities. As\nnetworks of HOIs are expressed mathematically as hypergraphs, hypergraph neural\nnetworks (HNNs) have emerged as a powerful tool for representation learning on\nhypergraphs. Given the emerging trend, we present the first survey dedicated to\nHNNs, with an in-depth and step-by-step guide. Broadly, the present survey\noverviews HNN architectures, training strategies, and applications. First, we\nbreak existing HNNs down into four design components: (i) input features, (ii)\ninput structures, (iii) message-passing schemes, and (iv) training strategies.\nSecond, we examine how HNNs address and learn HOIs with each of their\ncomponents. Third, we overview the recent applications of HNNs in\nrecommendation, bioinformatics and medical science, time series analysis, and\ncomputer vision. Lastly, we conclude with a discussion on limitations and\nfuture directions.\n","authors":["Sunwoo Kim","Soo Yong Lee","Yue Gao","Alessia Antelmi","Mirko Polato","Kijung Shin"],"pdf_url":"https://arxiv.org/pdf/2404.01039v3.pdf","comment":"To appear in KDD 2024 (survey paper). The typo in Equation (5) has\n  been fixed"},{"id":"http://arxiv.org/abs/2311.07052v3","updated":"2024-07-25T03:20:15Z","published":"2023-11-13T03:36:18Z","title":"Towards the Law of Capacity Gap in Distilling Language Models","summary":"  Language model (LM) distillation is a trending area that aims to distil the\nknowledge residing in a large teacher LM to a small student one. While various\nmethods have been proposed to maximize the effectiveness of the distillation,\nsignificant challenges persist, particularly when there is a substantial\ncapacity gap between the teacher and student LMs. This issue, often referred to\nas the \\textit{curse} of capacity gap, suggests that a larger teacher does not\nnecessarily result in a superior student compared to one distilled from a\nsmaller teacher. In other words, there is likely an optimal teacher yielding\nthe best student along the scaling course of the teacher. However, the curse of\ncapacity gap can not be tackled without notable compute overhead, as indicated\nin previous studies. In the context of large LMs (LLMs), previously viable\napproaches become much less meaningful, as it is an impossible triangle to\ndistill an expected student from an optimal teacher student with small compute\noverhead. Fortunately, the impossible triangle can fortunately be possible\nprovided an inducted \\textit{law} of capacity gap. In this paper, we take the\nspirits of scaling law and reveal that the optimal teacher scale almost\nconsistently follows a linear scaling with the student scale across different\nmodel architectures and data scales. The law later guides us to distil a 3B\nstudent LM (termed \\textsc{MiniMA}) from LLaMA2-7B. \\textsc{MiniMA} is\ndemonstrated to outperform a wide range of 3B competitors and could even\ncompete with several 7B models.\n","authors":["Chen Zhang","Dawei Song","Zheyu Ye","Yan Gao"],"pdf_url":"https://arxiv.org/pdf/2311.07052v3.pdf","comment":"32 pages, 10 figures, 15 tables, work in progress. Code and\n  checkpoints are available at https://github.com/GeneZC/MiniMA"},{"id":"http://arxiv.org/abs/2402.18729v2","updated":"2024-07-25T03:06:54Z","published":"2024-02-28T22:19:55Z","title":"A Priori Uncertainty Quantification of Reacting Turbulence Closure\n  Models using Bayesian Neural Networks","summary":"  While many physics-based closure model forms have been posited for the\nsub-filter scale (SFS) in large eddy simulation (LES), vast amounts of data\navailable from direct numerical simulation (DNS) create opportunities to\nleverage data-driven modeling techniques. Albeit flexible, data-driven models\nstill depend on the dataset and the functional form of the model chosen.\nIncreased adoption of such models requires reliable uncertainty estimates both\nin the data-informed and out-of-distribution regimes. In this work, we employ\nBayesian neural networks (BNNs) to capture both epistemic and aleatoric\nuncertainties in a reacting flow model. In particular, we model the filtered\nprogress variable scalar dissipation rate which plays a key role in the\ndynamics of turbulent premixed flames. We demonstrate that BNN models can\nprovide unique insights about the structure of uncertainty of the data-driven\nclosure models. We also propose a method for the incorporation of\nout-of-distribution information in a BNN. The efficacy of the model is\ndemonstrated by a priori evaluation on a dataset consisting of a variety of\nflame conditions and fuels.\n","authors":["Graham Pash","Malik Hassanaly","Shashank Yellapantula"],"pdf_url":"https://arxiv.org/pdf/2402.18729v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17731v1","updated":"2024-07-25T03:03:20Z","published":"2024-07-25T03:03:20Z","title":"Optimal Trade and Industrial Policies in the Global Economy: A Deep\n  Learning Framework","summary":"  We propose a deep learning framework, DL-opt, designed to efficiently solve\nfor optimal policies in quantifiable general equilibrium trade models. DL-opt\nintegrates (i) a nested fixed point (NFXP) formulation of the optimization\nproblem, (ii) automatic implicit differentiation to enhance gradient descent\nfor solving unilateral optimal policies, and (iii) a best-response dynamics\napproach for finding Nash equilibria. Utilizing DL-opt, we solve for\nnon-cooperative tariffs and industrial subsidies across 7 economies and 44\nsectors, incorporating sectoral external economies of scale. Our quantitative\nanalysis reveals significant sectoral heterogeneity in Nash policies: Nash\nindustrial subsidies increase with scale elasticities, whereas Nash tariffs\ndecrease with trade elasticities. Moreover, we show that global dual\ncompetition, involving both tariffs and industrial subsidies, results in lower\ntariffs and higher welfare outcomes compared to a global tariff war. These\nfindings highlight the importance of considering sectoral heterogeneity and\npolicy combinations in understanding global economic competition.\n","authors":["Zi Wang","Xingcheng Xu","Yanqing Yang","Xiaodong Zhu"],"pdf_url":"https://arxiv.org/pdf/2407.17731v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17726v1","updated":"2024-07-25T02:55:39Z","published":"2024-07-25T02:55:39Z","title":"Multi-modal Data Binding for Survival Analysis Modeling with Incomplete\n  Data and Annotations","summary":"  Survival analysis stands as a pivotal process in cancer treatment research,\ncrucial for predicting patient survival rates accurately. Recent advancements\nin data collection techniques have paved the way for enhancing survival\npredictions by integrating information from multiple modalities. However,\nreal-world scenarios often present challenges with incomplete data,\nparticularly when dealing with censored survival labels. Prior works have\naddressed missing modalities but have overlooked incomplete labels, which can\nintroduce bias and limit model efficacy. To bridge this gap, we introduce a\nnovel framework that simultaneously handles incomplete data across modalities\nand censored survival labels. Our approach employs advanced foundation models\nto encode individual modalities and align them into a universal representation\nspace for seamless fusion. By generating pseudo labels and incorporating\nuncertainty, we significantly enhance predictive accuracy. The proposed method\ndemonstrates outstanding prediction accuracy in two survival analysis tasks on\nboth employed datasets. This innovative approach overcomes limitations\nassociated with disparate modalities and improves the feasibility of\ncomprehensive survival analysis using multiple large foundation models.\n","authors":["Linhao Qu","Dan Huang","Shaoting Zhang","Xiaosong Wang"],"pdf_url":"https://arxiv.org/pdf/2407.17726v1.pdf","comment":"Accepted by MICCAI 2024"},{"id":"http://arxiv.org/abs/2407.17723v1","updated":"2024-07-25T02:53:11Z","published":"2024-07-25T02:53:11Z","title":"Your Graph Recommender is Provably a Single-view Graph Contrastive\n  Learning","summary":"  Graph recommender (GR) is a type of graph neural network (GNNs) encoder that\nis customized for extracting information from the user-item interaction graph.\nDue to its strong performance on the recommendation task, GR has gained\nsignificant attention recently. Graph contrastive learning (GCL) is also a\npopular research direction that aims to learn, often unsupervised, GNNs with\ncertain contrastive objectives. As a general graph representation learning\nmethod, GCLs have been widely adopted with the supervised recommendation loss\nfor joint training of GRs. Despite the intersection of GR and GCL research,\ntheoretical understanding of the relationship between the two fields is\nsurprisingly sparse. This vacancy inevitably leads to inefficient scientific\nresearch.\n  In this paper, we aim to bridge the gap between the field of GR and GCL from\nthe perspective of encoders and loss functions. With mild assumptions, we\ntheoretically show an astonishing fact that graph recommender is equivalent to\na commonly-used single-view graph contrastive model. Specifically, we find that\n(1) the classic encoder in GR is essentially a linear graph convolutional\nnetwork with one-hot inputs, and (2) the loss function in GR is well bounded by\na single-view GCL loss with certain hyperparameters. The first observation\nenables us to explain crucial designs of GR models, e.g., the removal of\nself-loop and nonlinearity. And the second finding can easily prompt many\ncross-field research directions. We empirically show a remarkable result that\nthe recommendation loss and the GCL loss can be used interchangeably. The fact\nthat we can train GR models solely with the GCL loss is particularly\ninsightful, since before this work, GCLs were typically viewed as unsupervised\nmethods that need fine-tuning. We also discuss some potential future works\ninspired by our theory.\n","authors":["Wenjie Yang","Shengzhong Zhang","Jiaxing Guo","Zengfeng Huang"],"pdf_url":"https://arxiv.org/pdf/2407.17723v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17722v1","updated":"2024-07-25T02:48:56Z","published":"2024-07-25T02:48:56Z","title":"Text-Driven Neural Collaborative Filtering Model for Paper Source\n  Tracing","summary":"  Identifying significant references within the complex interrelations of a\ncitation knowledge graph is challenging, which encompasses connections through\ncitations, authorship, keywords, and other relational attributes. The Paper\nSource Tracing (PST) task seeks to automate the identification of pivotal\nreferences for given scholarly articles utilizing advanced data mining\ntechniques. In the KDD CUP 2024, we design a recommendation-based framework\ntailored for the PST task. This framework employs the Neural Collaborative\nFiltering (NCF) model to generate final predictions. To process the textual\nattributes of the papers and extract input features for the model, we utilize\nSciBERT, a pre-trained language model. According to the experimental results,\nour method achieved a score of 0.37814 on the Mean Average Precision (MAP)\nmetric, outperforming baseline models and ranking 11th among all participating\nteams. The source code is publicly available at\nhttps://github.com/MyLove-XAB/KDDCupFinal.\n","authors":["Aobo Xu","Bingyu Chang","Qingpeng Liu","Ling Jian"],"pdf_url":"https://arxiv.org/pdf/2407.17722v1.pdf","comment":"KDD CUP 2024 OAG-Challenges, Paper Source Tracing, Technical Report\n  of Team AoboSama @ KDD CUP 2024. August 25--29, 2024. Barcelona, Spain"},{"id":"http://arxiv.org/abs/2407.17721v1","updated":"2024-07-25T02:48:22Z","published":"2024-07-25T02:48:22Z","title":"A Two-Stage Imaging Framework Combining CNN and Physics-Informed Neural\n  Networks for Full-Inverse Tomography: A Case Study in Electrical Impedance\n  Tomography (EIT)","summary":"  Physics-Informed Neural Networks (PINNs) are a machine learning technique for\nsolving partial differential equations (PDEs) by incorporating PDEs as loss\nterms in neural networks and minimizing the loss function during training.\nTomographic imaging, a method to reconstruct internal properties from external\nmeasurement data, is highly complex and ill-posed, making it an inverse\nproblem. Recently, PINNs have shown significant potential in computational\nfluid dynamics (CFD) and have advantages in solving inverse problems. However,\nexisting research has primarily focused on semi-inverse Electrical Impedance\nTomography (EIT), where internal electric potentials are accessible. The\npractical full inverse EIT problem, where only boundary voltage measurements\nare available, remains challenging. To address this, we propose a two-stage\nhybrid learning framework combining Convolutional Neural Networks (CNNs) and\nPINNs to solve the full inverse EIT problem. This framework integrates\ndata-driven and model-driven approaches, combines supervised and unsupervised\nlearning, and decouples the forward and inverse problems within the PINN\nframework in EIT. Stage I: a U-Net constructs an end-to-end mapping from\nboundary voltage measurements to the internal potential distribution using\nsupervised learning. Stage II: a Multilayer Perceptron (MLP)-based PINN takes\nthe predicted internal potentials as input to solve for the conductivity\ndistribution through unsupervised learning.\n","authors":["Xuanxuan Yang","Yangming Zhang","Haofeng Chen","Gang Ma","Xiaojie Wang"],"pdf_url":"https://arxiv.org/pdf/2407.17721v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17716v1","updated":"2024-07-25T02:30:40Z","published":"2024-07-25T02:30:40Z","title":"Describe Where You Are: Improving Noise-Robustness for Speech Emotion\n  Recognition with Text Description of the Environment","summary":"  Speech emotion recognition (SER) systems often struggle in real-world\nenvironments, where ambient noise severely degrades their performance. This\npaper explores a novel approach that exploits prior knowledge of testing\nenvironments to maximize SER performance under noisy conditions. To address\nthis task, we propose a text-guided, environment-aware training where an SER\nmodel is trained with contaminated speech samples and their paired noise\ndescription. We use a pre-trained text encoder to extract the text-based\nenvironment embedding and then fuse it to a transformer-based SER model during\ntraining and inference. We demonstrate the effectiveness of our approach\nthrough our experiment with the MSP-Podcast corpus and real-world additive\nnoise samples collected from the Freesound repository. Our experiment indicates\nthat the text-based environment descriptions processed by a large language\nmodel (LLM) produce representations that improve the noise-robustness of the\nSER system. In addition, our proposed approach with an LLM yields better\nperformance than our environment-agnostic baselines, especially in low\nsignal-to-noise ratio (SNR) conditions. When testing at -5dB SNR level, our\nproposed method shows better performance than our best baseline model by 31.8 %\n(arousal), 23.5% (dominance), and 9.5% (valence).\n","authors":["Seong-Gyun Leem","Daniel Fulford","Jukka-Pekka Onnela","David Gard","Carlos Busso"],"pdf_url":"https://arxiv.org/pdf/2407.17716v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.06679v3","updated":"2024-07-25T02:30:32Z","published":"2023-09-13T02:34:21Z","title":"Robust experimental data assimilation for the Spalart-Allmaras\n  turbulence model","summary":"  This study presents a methodology focusing on the use of computational model\nand experimental data fusion to improve the Spalart-Allmaras (SA) closure model\nfor Reynolds-averaged Navier-Stokes solutions. In particular, our goal is to\ndevelop a technique that not only assimilates sparse experimental data to\nimprove turbulence model performance, but also preserves generalization for\nunseen cases by recovering classical SA behavior. We achieve our goals using\ndata assimilation, namely the Ensemble Kalman filtering approach (EnKF), to\ncalibrate the coefficients of the SA model for separated flows. A holistic\ncalibration strategy is implemented via the parameterization of the production,\ndiffusion, and destruction terms. This calibration relies on the assimilation\nof experimental data collected in the form of velocity profiles, skin friction,\nand pressure coefficients. Despite using observational data from a single flow\ncondition around a backward-facing step (BFS), the recalibrated SA model\ndemonstrates generalization to other separated flows, including cases such as\nthe 2D NASA wall mounted hump (2D-WMH) and modified BFS. Significant\nimprovement is observed in the quantities of interest, i.e., skin friction\ncoefficient ($C_f$) and pressure coefficient ($C_p$) for each flow tested.\nFinally, it is also demonstrated that the newly proposed model recovers SA\nproficiency for flows, such as a NACA-0012 airfoil and axisymmetric jet (ASJ),\nand that the individually calibrated terms in the SA model target specific\nflow-physics wherein the calibrated production term improves the re-circulation\nzone while destruction improves the recovery zone.\n","authors":["Deepinder Jot Singh Aulakh","Xiang Yang","Romit Maulik"],"pdf_url":"https://arxiv.org/pdf/2309.06679v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.01599v2","updated":"2024-07-25T02:25:11Z","published":"2024-06-26T02:20:23Z","title":"JailbreakZoo: Survey, Landscapes, and Horizons in Jailbreaking Large\n  Language and Vision-Language Models","summary":"  The rapid evolution of artificial intelligence (AI) through developments in\nLarge Language Models (LLMs) and Vision-Language Models (VLMs) has brought\nsignificant advancements across various technological domains. While these\nmodels enhance capabilities in natural language processing and visual\ninteractive tasks, their growing adoption raises critical concerns regarding\nsecurity and ethical alignment. This survey provides an extensive review of the\nemerging field of jailbreaking--deliberately circumventing the ethical and\noperational boundaries of LLMs and VLMs--and the consequent development of\ndefense mechanisms. Our study categorizes jailbreaks into seven distinct types\nand elaborates on defense strategies that address these vulnerabilities.\nThrough this comprehensive examination, we identify research gaps and propose\ndirections for future studies to enhance the security frameworks of LLMs and\nVLMs. Our findings underscore the necessity for a unified perspective that\nintegrates both jailbreak strategies and defensive solutions to foster a\nrobust, secure, and reliable environment for the next generation of language\nmodels. More details can be found on our website:\n\\url{https://chonghan-chen.com/llm-jailbreak-zoo-survey/}.\n","authors":["Haibo Jin","Leyang Hu","Xinuo Li","Peiyan Zhang","Chonghan Chen","Jun Zhuang","Haohan Wang"],"pdf_url":"https://arxiv.org/pdf/2407.01599v2.pdf","comment":"45 pages"},{"id":"http://arxiv.org/abs/2407.17712v1","updated":"2024-07-25T02:17:53Z","published":"2024-07-25T02:17:53Z","title":"Improving Online Algorithms via ML Predictions","summary":"  In this work we study the problem of using machine-learned predictions to\nimprove the performance of online algorithms. We consider two classical\nproblems, ski rental and non-clairvoyant job scheduling, and obtain new online\nalgorithms that use predictions to make their decisions. These algorithms are\noblivious to the performance of the predictor, improve with better predictions,\nbut do not degrade much if the predictions are poor.\n","authors":["Ravi Kumar","Manish Purohit","Zoya Svitkina"],"pdf_url":"https://arxiv.org/pdf/2407.17712v1.pdf","comment":"Conference version appeared in Neurips 2018"}],"Multimedia":[{"id":"http://arxiv.org/abs/2407.14242v2","updated":"2024-07-25T13:30:33Z","published":"2024-07-19T12:22:32Z","title":"Continual Panoptic Perception: Towards Multi-modal Incremental\n  Interpretation of Remote Sensing Images","summary":"  Continual learning (CL) breaks off the one-way training manner and enables a\nmodel to adapt to new data, semantics and tasks continuously. However, current\nCL methods mainly focus on single tasks. Besides, CL models are plagued by\ncatastrophic forgetting and semantic drift since the lack of old data, which\noften occurs in remote-sensing interpretation due to the intricate fine-grained\nsemantics. In this paper, we propose Continual Panoptic Perception (CPP), a\nunified continual learning model that leverages multi-task joint learning\ncovering pixel-level classification, instance-level segmentation and\nimage-level perception for universal interpretation in remote sensing images.\nConcretely, we propose a collaborative cross-modal encoder (CCE) to extract the\ninput image features, which supports pixel classification and caption\ngeneration synchronously. To inherit the knowledge from the old model without\nexemplar memory, we propose a task-interactive knowledge distillation (TKD)\nmethod, which leverages cross-modal optimization and task-asymmetric\npseudo-labeling (TPL) to alleviate catastrophic forgetting. Furthermore, we\nalso propose a joint optimization mechanism to achieve end-to-end multi-modal\npanoptic perception. Experimental results on the fine-grained panoptic\nperception dataset validate the effectiveness of the proposed model, and also\nprove that joint optimization can boost sub-task CL efficiency with over 13\\%\nrelative improvement on panoptic quality.\n","authors":["Bo Yuan","Danpei Zhao","Zhuoran Liu","Wentao Li","Tian Li"],"pdf_url":"https://arxiv.org/pdf/2407.14242v2.pdf","comment":"Accepted in ACMMM 2024"},{"id":"http://arxiv.org/abs/2407.17911v1","updated":"2024-07-25T10:06:26Z","published":"2024-07-25T10:06:26Z","title":"ReCorD: Reasoning and Correcting Diffusion for HOI Generation","summary":"  Diffusion models revolutionize image generation by leveraging natural\nlanguage to guide the creation of multimedia content. Despite significant\nadvancements in such generative models, challenges persist in depicting\ndetailed human-object interactions, especially regarding pose and object\nplacement accuracy. We introduce a training-free method named Reasoning and\nCorrecting Diffusion (ReCorD) to address these challenges. Our model couples\nLatent Diffusion Models with Visual Language Models to refine the generation\nprocess, ensuring precise depictions of HOIs. We propose an interaction-aware\nreasoning module to improve the interpretation of the interaction, along with\nan interaction correcting module to refine the output image for more precise\nHOI generation delicately. Through a meticulous process of pose selection and\nobject positioning, ReCorD achieves superior fidelity in generated images while\nefficiently reducing computational requirements. We conduct comprehensive\nexperiments on three benchmarks to demonstrate the significant progress in\nsolving text-to-image generation tasks, showcasing ReCorD's ability to render\ncomplex interactions accurately by outperforming existing methods in HOI\nclassification score, as well as FID and Verb CLIP-Score. Project website is\navailable at https://alberthkyhky.github.io/ReCorD/ .\n","authors":["Jian-Yu Jiang-Lin","Kang-Yang Huang","Ling Lo","Yi-Ning Huang","Terence Lin","Jhih-Ciang Wu","Hong-Han Shuai","Wen-Huang Cheng"],"pdf_url":"https://arxiv.org/pdf/2407.17911v1.pdf","comment":"Accepted by ACM MM 2024. Project website:\n  https://alberthkyhky.github.io/ReCorD/"},{"id":"http://arxiv.org/abs/2407.17854v1","updated":"2024-07-25T08:15:43Z","published":"2024-07-25T08:15:43Z","title":"Shapley Value-based Contrastive Alignment for Multimodal Information\n  Extraction","summary":"  The rise of social media and the exponential growth of multimodal\ncommunication necessitates advanced techniques for Multimodal Information\nExtraction (MIE). However, existing methodologies primarily rely on direct\nImage-Text interactions, a paradigm that often faces significant challenges due\nto semantic and modality gaps between images and text. In this paper, we\nintroduce a new paradigm of Image-Context-Text interaction, where large\nmultimodal models (LMMs) are utilized to generate descriptive textual context\nto bridge these gaps. In line with this paradigm, we propose a novel Shapley\nValue-based Contrastive Alignment (Shap-CA) method, which aligns both\ncontext-text and context-image pairs. Shap-CA initially applies the Shapley\nvalue concept from cooperative game theory to assess the individual\ncontribution of each element in the set of contexts, texts and images towards\ntotal semantic and modality overlaps. Following this quantitative evaluation, a\ncontrastive learning strategy is employed to enhance the interactive\ncontribution within context-text/image pairs, while minimizing the influence\nacross these pairs. Furthermore, we design an adaptive fusion module for\nselective cross-modal fusion. Extensive experiments across four MIE datasets\ndemonstrate that our method significantly outperforms existing state-of-the-art\nmethods.\n","authors":["Wen Luo","Yu Xia","Shen Tianshu","Sujian Li"],"pdf_url":"https://arxiv.org/pdf/2407.17854v1.pdf","comment":"Accepted at ACM Multimedia 2024"},{"id":"http://arxiv.org/abs/2407.17028v2","updated":"2024-07-25T05:23:24Z","published":"2024-07-24T06:15:28Z","title":"Enhancing Environmental Monitoring through Multispectral Imaging: The\n  WasteMS Dataset for Semantic Segmentation of Lakeside Waste","summary":"  Environmental monitoring of lakeside green areas is crucial for environmental\nprotection. Compared to manual inspections, computer vision technologies offer\na more efficient solution when deployed on-site. Multispectral imaging provides\ndiverse information about objects under different spectrums, aiding in the\ndifferentiation between waste and lakeside lawn environments. This study\nintroduces WasteMS, the first multispectral dataset established for the\nsemantic segmentation of lakeside waste. WasteMS includes a diverse range of\nwaste types in lawn environments, captured under various lighting conditions.\nWe implemented a rigorous annotation process to label waste in images.\nRepresentative semantic segmentation frameworks were used to evaluate\nsegmentation accuracy using WasteMS. Challenges encountered when using WasteMS\nfor segmenting waste on lakeside lawns were discussed. The WasteMS dataset is\navailable at https://github.com/zhuqinfeng1999/WasteMS.\n","authors":["Qinfeng Zhu","Ningxin Weng","Lei Fan","Yuanzhi Cai"],"pdf_url":"https://arxiv.org/pdf/2407.17028v2.pdf","comment":null}]}}